{"cells": [{"cell_type": "markdown", "id": "08a5c4f6", "metadata": {}, "source": ["# How automate and log your experiments"]}, {"cell_type": "markdown", "id": "dffa41f6", "metadata": {}, "source": ["## Automated experiment tracking with MLflow\n\nDuring the competition I did many experiments and quit often I submited results with some best score and after thet can't reproduce it. So, I used my personal help tool set:\n- Git repository. After submission I commit code that generate that submission file\n- Local log file to record all usefull information in parallel with printf\n- And MLflow. It help me to compare many different experiments and did aditional analyse on how feature engeeniring impact model perforamnce\n\nOfficial site: https://mlflow.org/\n\nInstalation tutorial: https://www.mlflow.org/docs/latest/tutorials-and-examples/tutorial.html\n\n- But you can quickly install: **pip install mlflow**\n- After installation go to folder with this notebook and run cmd **mlflow ui** in terminal\n- It will run server at http://localhost:5000\n- More examples here: https://github.com/mlflow/mlflow/tree/master/examples\n\nTIP: Some times mlflow could fail, so just run **mlflow.end_run()**"]}, {"cell_type": "code", "execution_count": 1, "id": "8dd0c7f0", "metadata": {}, "outputs": [], "source": ["import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport optuna\n\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import (\n    ElasticNet, \n    Lasso,\n    LinearRegression,\n    Ridge\n)\nfrom scipy.stats import norm, skew, boxcox_normmax #for some statistics\nfrom sklearn import utils\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.feature_selection import RFE, f_regression\nfrom scipy.special import boxcox1p\n\nimport datetime\nimport time"]}, {"cell_type": "markdown", "id": "1572b94f", "metadata": {}, "source": ["## Import mlflow\n- First of all you should create your new experiment. \n- Call get_tracking_uri return you a path to local folder with meta files/results of your experiments\n- Call get_artifact_uri return path to your artifacts"]}, {"cell_type": "code", "execution_count": 1, "id": "82caa913", "metadata": {}, "outputs": [], "source": ["# Note:  We can't set this here due to https://github.com/mlflow/mlflow/issues/608\n#tracking_uri='file:///mnt/pipelineai/users/experiments'\nfrom mlflow import log_metric, log_param, log_artifact\nimport mlflow.sklearn\nimport mlflow.xgboost\n\nexperiment_name = 'house_price'\nmlflow.set_experiment(experiment_name)\n# Forcing an end_run() to prevent \n#    https://github.com/mlflow/mlflow/issues/1335 \n#    https://github.com/mlflow/mlflow/issues/608\nmlflow.end_run()\n\nartifact_path = mlflow.get_artifact_uri()\nuri = mlflow.tracking.get_tracking_uri()\nprint(artifact_path)\nprint(uri)"]}, {"cell_type": "markdown", "id": "a5317b23", "metadata": {}, "source": ["- We will TAGs for more easy filtering between different models\n- Parameters help us to track training configuration / conditions\n- Metrics helps us filter models by score\n- Log_model will save model to the artifacts"]}, {"cell_type": "code", "execution_count": 1, "id": "7aff098a", "metadata": {}, "outputs": [], "source": ["def log_mlflow(model):\n    # Track params and metrics \n    with mlflow.start_run() as run:\n        mlflow.set_tag(\"model_name\", name)\n        mlflow.log_param(\"CV_n_folds\", CV_n_folds)\n        mlflow.log_param(\"TEST_PART\", TEST_PART)\n        mlflow.log_param(\"Train size\", X_train.shape)\n        mlflow.log_param(\"Colums\", str(X_train.columns.values.tolist()))\n        mlflow.log_metrics({'rmse_cv': score_cv.mean(), 'rmse': score})\n        mlflow.log_metric(\"rmse_train\", score_train)\n        # Save model to artifacts\n        mlflow.sklearn.log_model(model, name)\n    mlflow.end_run()"]}, {"cell_type": "markdown", "id": "288145e5", "metadata": {}, "source": ["If your code crashed and you didn't finish mlflow you shoud run the following code:"]}, {"cell_type": "code", "execution_count": 1, "id": "91e96d72", "metadata": {}, "outputs": [], "source": [" mlflow.end_run()"]}, {"cell_type": "markdown", "id": "29c7baed", "metadata": {}, "source": ["## Load data "]}, {"cell_type": "code", "execution_count": 1, "id": "852987e3", "metadata": {}, "outputs": [], "source": ["# Import data\npath =  '/kaggle/input/house-prices-advanced-regression-techniques/'\ntrain = pd.read_csv(path + 'train.csv')\ntest = pd.read_csv(path + 'test.csv')\nprint (\"Size ntrain= {} / ntest = {}\".format(train.shape, test.shape))\n# train.head()"]}, {"cell_type": "markdown", "id": "5cf6240d", "metadata": {}, "source": ["# Features engineering"]}, {"cell_type": "markdown", "id": "9f841fa5", "metadata": {}, "source": ["## Sales Price "]}, {"cell_type": "markdown", "id": "5131b05a", "metadata": {}, "source": ["The target variable is right skewed. As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed."]}, {"cell_type": "code", "execution_count": 1, "id": "8972fcd7", "metadata": {}, "outputs": [], "source": ["# We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])"]}, {"cell_type": "markdown", "id": "3f28be85", "metadata": {}, "source": ["## Remove outliers"]}, {"cell_type": "code", "execution_count": 1, "id": "f27eeea0", "metadata": {}, "outputs": [], "source": ["# Deleting outliers\ntrain = train.drop(train[(train['GrLivArea']>4600)].index)\ntrain = train.drop(train[(train['TotalBsmtSF']>5900)].index)\ntrain = train.drop(train[(train['1stFlrSF']>4000)].index)\ntrain = train.drop(train[(train['MasVnrArea']>1500)].index)\ntrain = train.drop(train[(train['GarageArea']>1230)].index)\ntrain = train.drop(train[(train['TotRmsAbvGrd']>13)].index)"]}, {"cell_type": "markdown", "id": "a55bb617", "metadata": {}, "source": ["### Group train and test datasets"]}, {"cell_type": "code", "execution_count": 1, "id": "babf1eab", "metadata": {}, "outputs": [], "source": ["ntrain = train.shape[0]\nntest = test.shape[0]\nprint (\"Size ntrain= {} / ntest = {}\".format(ntrain, ntest))\n# all_data = pd.concat((train, test)).reset_index(drop=True)\nall_data = train.append(test, sort=False).reset_index(drop=True)\n#To save original ID for final submission\norig_test = test.copy() \nlog_y_train = train['SalePrice']\n# To avoid normalization of SalesPrice - drop it from All data \nall_data.drop(['SalePrice'], axis=1, inplace=True)\n# Id no need for traning\nall_data.drop(['Id'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))"]}, {"cell_type": "markdown", "id": "f2f300f2", "metadata": {}, "source": ["### Show statistic how much data is missing"]}, {"cell_type": "code", "execution_count": 1, "id": "5742b5b9", "metadata": {}, "outputs": [], "source": ["all_data_na = (all_data.isnull().sum() / len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)"]}, {"cell_type": "markdown", "id": "ba6130d8", "metadata": {}, "source": ["### Data Correlation"]}, {"cell_type": "code", "execution_count": 1, "id": "bd3f09d2", "metadata": {}, "outputs": [], "source": ["threshold = 0.90\n# Absolute value correlation matrix\ncorr_matrix = train.corr().abs()\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n# upper.head(50)\n# Select columns with correlations above threshold\ncollinear_features = [column for column in upper.columns if any(upper[column] > threshold)]\nprint('There are %d features to remove.' % (len(collinear_features)))\nprint(collinear_features)"]}, {"cell_type": "markdown", "id": "9486695a", "metadata": {}, "source": ["### Imputing missing values"]}, {"cell_type": "code", "execution_count": 1, "id": "89262a33", "metadata": {}, "outputs": [], "source": ["# Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')\n    \nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)\n    \nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')\n\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\n    \n# Raplace null with None value.\nall_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")    \nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n\nall_data = all_data.drop(['Utilities'], axis=1)"]}, {"cell_type": "markdown", "id": "e5b63590", "metadata": {}, "source": ["### Generate new features"]}, {"cell_type": "code", "execution_count": 1, "id": "03e99cf6", "metadata": {}, "outputs": [], "source": ["# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']"]}, {"cell_type": "markdown", "id": "18ac57ea", "metadata": {}, "source": ["## Categorial and quantative features investigation"]}, {"cell_type": "markdown", "id": "ee38f65d", "metadata": {}, "source": ["### Convert str in Quality features to int"]}, {"cell_type": "code", "execution_count": 1, "id": "67069357", "metadata": {}, "outputs": [], "source": ["def convert_str_to_int(data, features, score):\n    all_data[features] = all_data[features].applymap(lambda s: score.get(s) if s in score else s)\n\nfeaturesQualCond = [\"ExterQual\", \"ExterCond\", \"BsmtQual\", \"BsmtCond\", \"KitchenQual\",\n                    \"FireplaceQu\", \"HeatingQC\", \"GarageQual\", \"GarageCond\", \"PoolQC\"]\nqual_score_QualCond = {\"None\":0, \"NA\":1, \"Po\":2, \"Fa\":3, \"TA\":4, \"Gd\":5, \"Ex\":6}\nconvert_str_to_int(all_data, featuresQualCond, qual_score_QualCond)\n\nfeaturesExposure = [\"BsmtExposure\"]\nqual_score = {\"None\":0, \"NA\":1, \"No\":2, \"Mn\":3, \"Av\":4, \"Gd\":5}\nconvert_str_to_int(all_data, featuresExposure, qual_score)\n\nfeaturesFinType = [\"BsmtFinType1\", \"BsmtFinType2\"]\nqual_score = {\"None\":0, \"NA\":1, \"Unf\":2, \"LwQ\":3, \"Rec\":4, \"BLQ\":5, \"ALQ\":6, \"GLQ\":7}\nconvert_str_to_int(all_data, featuresFinType, qual_score)\n\nfeaturesGarageFin = [\"GarageFinish\"]\nqual_score = {\"None\":0, \"NA\":1, \"Unf\":2, \"RFn\":3, \"Fin\":4}\nconvert_str_to_int(all_data, featuresGarageFin, qual_score)"]}, {"cell_type": "markdown", "id": "2593e66a", "metadata": {}, "source": ["### Some features should be categorial"]}, {"cell_type": "code", "execution_count": 1, "id": "c00590d5", "metadata": {}, "outputs": [], "source": ["#MSSubClass=The building class represented as int but it's category\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)"]}, {"cell_type": "markdown", "id": "3ba5cd08", "metadata": {}, "source": ["## Label Encoding \n\n### some categorical variables that may contain information in their ordering set"]}, {"cell_type": "code", "execution_count": 1, "id": "4183a19d", "metadata": {}, "outputs": [], "source": ["cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))"]}, {"cell_type": "markdown", "id": "a784f8d7", "metadata": {}, "source": ["## Skewed features"]}, {"cell_type": "markdown", "id": "b5a4c902", "metadata": {}, "source": ["**Box Cox Transformation of (highly) skewed features**"]}, {"cell_type": "code", "execution_count": 1, "id": "5dbd6259", "metadata": {}, "outputs": [], "source": ["numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskew_features = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"Skew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skew_features})\n\nskewness = skewness[abs(skew_features) > 0.05]\nprint(\"There are {} high skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nskewed_features = skewness.index\nfor i in skewed_features:\n    all_data[i] = boxcox1p(all_data[i], 0.15)\nprint('Shape all_data: {}'.format(all_data.shape))"]}, {"cell_type": "code", "execution_count": 1, "id": "6b89c5c2", "metadata": {}, "outputs": [], "source": ["# Getting Dummies from Condition1 and Condition2\nconditions = set([x for x in all_data['Condition1']] + [x for x in all_data['Condition2']])\ndummies = pd.DataFrame(data=np.zeros((len(all_data.index), len(conditions))),\n                       index=all_data.index, columns=conditions)\nfor i, cond in enumerate(zip(all_data['Condition1'], all_data['Condition2'])):\n#     dummies.ix[i, cond] = 1\n    dummies.ix[i, cond] = 1\nall_data = pd.concat([all_data, dummies.add_prefix('Condition_')], axis=1)\nall_data.drop(['Condition1', 'Condition2'], axis=1, inplace=True)"]}, {"cell_type": "markdown", "id": "44384761", "metadata": {}, "source": ["## convert categorical variable into dummy"]}, {"cell_type": "code", "execution_count": 1, "id": "baedc7e7", "metadata": {}, "outputs": [], "source": ["all_data = pd.get_dummies(all_data)\nprint(all_data.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "61f3aa60", "metadata": {}, "outputs": [], "source": ["cols = ('Condition_RRNn', 'Condition_RRAe', \n        'Condition_Artery', 'Condition_Feedr', 'Condition_Feedr', 'Condition_RRNe', \n        'Condition_PosA', 'Condition_Norm', 'Condition_RRAn', \n        'Condition_PosN')\n# process columns, apply LabelEncoder to categorical features\nlbl = LabelEncoder()\nfor c in cols:\n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(all_data[c].values)       \nprint('Shape all_data: {}'.format(all_data.shape))"]}, {"cell_type": "markdown", "id": "96de1ae2", "metadata": {}, "source": ["## Drop low importance features"]}, {"cell_type": "code", "execution_count": 1, "id": "e61655a2", "metadata": {}, "outputs": [], "source": ["todrop = ['GarageType_BuiltIn', 'MiscFeature_None', 'PoolQC', 'MiscVal', 'Condition_RRAn', 'Neighborhood_BrDale', 'GarageType_2Types', 'Exterior1st_Stucco', 'Neighborhood_Blmngtn', 'LotConfig_FR3', 'Neighborhood_Timber', 'SaleType_ConLI', 'Condition_PosA', 'LandContour_Bnk', 'Alley_None', 'Street_Pave', 'Street_Grvl', 'Condition_Norm', 'Condition_RRNn', '3SsnPorch', 'BldgType_TwnhsE', 'RoofMatl_Membran', 'RoofMatl_WdShake', 'RoofMatl_Roll', 'RoofMatl_Metal', 'Exterior2nd_Stone', 'Exterior2nd_MetalSd', 'MasVnrType_None', 'Exterior2nd_ImStucc', 'LowQualFinSF', 'RoofMatl_Tar&Grv', 'Exterior2nd_AsphShn', 'Heating_GasA', 'HouseStyle_2.5Unf', 'Exterior2nd_AsbShng', 'Exterior1st_WdShing', 'BldgType_Duplex', 'Exterior2nd_CBlock', 'SaleType_Oth', 'Condition_PosN', 'Neighborhood_Veenker', 'BldgType_2fmCon', 'MiscFeature_TenC', 'Neighborhood_Blueste', 'RoofStyle_Mansard', 'Foundation_Slab', 'HouseStyle_SFoyer', 'Heating_Floor', 'HouseStyle_2.5Fin', 'Exterior1st_Stone', 'Exterior1st_CBlock']\nprint (\"Before Drop = \", all_data.shape)\nall_data.drop(todrop, axis=1, inplace=True)\nprint (\"After Drop = \", all_data.shape)\n"]}, {"cell_type": "markdown", "id": "08cd42dc", "metadata": {}, "source": ["## Normalize data"]}, {"cell_type": "code", "execution_count": 1, "id": "725185c9", "metadata": {}, "outputs": [], "source": ["scaler = RobustScaler()\ndf_all = pd.DataFrame(scaler.fit_transform(all_data))"]}, {"cell_type": "markdown", "id": "77f60e81", "metadata": {}, "source": ["## Separate back train and test set"]}, {"cell_type": "code", "execution_count": 1, "id": "ba22cfcb", "metadata": {}, "outputs": [], "source": ["# all_data = all_data.iloc[:, 1:10]\n# data.iloc[:, 0:2] # first two columns of data frame with all rows\ntrain = all_data[:ntrain]\ntest = all_data[ntrain:]\ncolnames = train.columns\n\n# Check remaining missing values if any \ntrain_na = (train.isnull().sum() / len(all_data)) * 100\ntrain_na = train_na.drop(train_na[train_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :train_na})\nmissing_data.head()"]}, {"cell_type": "markdown", "id": "ef9a4951", "metadata": {}, "source": ["Before start training log into file all features that we will use for training. Some feature droped due to feature importance to improve model performance. So it nice to have. \nWe will use simple local file for this purpose. "]}, {"cell_type": "code", "execution_count": 1, "id": "4c526439", "metadata": {}, "outputs": [], "source": ["# Log into a file train shape and columns names\nf = open(\"models_training_log.txt\", \"a+\")\nprint(\"\\n-------------------\" + str(datetime.datetime.now().isoformat()) + \"-------------------\", file=f)\nprint(\"Train shape:\" + str(train.shape) , file=f)\nprint(\"feature names:\" + str(list(colnames)) , file=f)\nf.close()"]}, {"cell_type": "markdown", "id": "1e66039c", "metadata": {}, "source": ["# ====== Train model ======"]}, {"cell_type": "code", "execution_count": 1, "id": "9562c75a", "metadata": {}, "outputs": [], "source": ["def rmsle_cv(model, X_train, y_train, cv_n_folds):\n    kf = KFold(cv_n_folds, shuffle=True, random_state=42).get_n_splits(X_train.values)\n    rmse= np.sqrt(-cross_val_score(model, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n\ndef rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef print_rmse_score(y, y_pred):\n    score = rmse(y, y_pred)\n    print(\"RMSE score: {:.8f}\".format(score))\n    return score\n\ndef print_rmse_cv_score(model, X_train, y_train, cv_n_folds, prefix=\"\"):\n    score = rmsle_cv(model, X_train, y_train, cv_n_folds)\n    print(prefix + \"CV RMSE score: {:.8f} ({:.4f})\".format(score.mean(), score.std()))\n    return score\n\ndef prepare_datasets(X_matrix, Y_vector, test_part):\n    X_train = X_matrix\n    y_train = Y_vector\n    X_test = np.array([])\n    y_test = np.array([])\n    if test_part > 0:\n        X_train, X_test, y_train, y_test = train_test_split(X_matrix, Y_vector, random_state = 0, test_size = test_part)\n            \n    print (\"\\nTEST_PART = \",  test_part)\n    print (\"Train X| \" + str(X_train.shape) + \" Y| \" + str(y_train.shape))\n    print (\"Test X| \" + str(X_test.shape) + \" Y| \" + str(y_test.shape))\n    return [X_train, y_train, X_test, y_test]\n"]}, {"cell_type": "code", "execution_count": 1, "id": "a041c47e", "metadata": {}, "outputs": [], "source": ["X = train.copy()\nY = log_y_train.copy()\nX_submit = test.copy()\n\nTEST_PART = 0.1\nCV_n_folds = 3\n[X_train, y_train, X_test, y_test] = prepare_datasets(X, Y, TEST_PART)"]}, {"cell_type": "markdown", "id": "1677f3f9", "metadata": {}, "source": ["# -------  Models  ---------\n"]}, {"cell_type": "code", "execution_count": 1, "id": "4ec2feb4", "metadata": {}, "outputs": [], "source": ["classifiers=set([])\nmodels={}\nscores_cv={}\nscores={}\nscores_train={}\nsubmits={}"]}, {"cell_type": "code", "execution_count": 1, "id": "2d3960fa", "metadata": {}, "outputs": [], "source": ["# Define dictionary to store our rankings\nranks = {}\n# Create our function which stores the feature rankings to the ranks dictionary\ndef ranking(ranks, names, order=1):\n    minmax = MinMaxScaler()\n    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n    ranks = map(lambda x: round(x,2), ranks)\n    return dict(zip(names, ranks))"]}, {"cell_type": "markdown", "id": "33bcb28e", "metadata": {}, "source": ["## Ridge"]}, {"cell_type": "code", "execution_count": 1, "id": "a72e72f7", "metadata": {}, "outputs": [], "source": ["%%time\nmodel_ridge = Ridge(alpha=5.75)\nmodel = model_ridge.fit(X_train, y_train)\nscore_cv = score_ridge_cv = print_rmse_cv_score(model, X_train, y_train, CV_n_folds)\nscore = score_ridge = print_rmse_score(y_test, model.predict(X_test))\nridge_submit = np.expm1(model.predict(X_submit))\n\nname = \"Ridge\"\nclassifiers.add(name)\nmodels[name] = model \nscores_cv[name] = score_cv\nscores[name] = score\nscores_train[name] = score_train = print_rmse_score(y_train, model.predict(X_train))\nsubmits[name] = ridge_submit\nranks[name] = ranking(np.abs(model.coef_), colnames)\n\nlog_mlflow(model)"]}, {"cell_type": "markdown", "id": "9614a5a9", "metadata": {}, "source": ["## GradientBoostingRegressor"]}, {"cell_type": "code", "execution_count": 1, "id": "31e642ae", "metadata": {}, "outputs": [], "source": ["%%time\nparams = {\n    'learning_rate': 0.01,\n    \"n_estimators\":500,\n    'max_depth': 3,\n    'max_features': \"sqrt\",\n    \"loss\":\"huber\",\n    'min_samples_leaf': 12,\n    'min_samples_split': 11,\n    \"random_state\":5\n}\nmodel_GBoost = GradientBoostingRegressor(**params)\nmodel = model_GBoost.fit(X_train, y_train)\nscore_cv = score_GBoost_cv = print_rmse_cv_score(model, X_train, y_train, CV_n_folds)\nscore = score_GBoost = print_rmse_score(y_test, model.predict(X_test))\ngboost_submit = np.expm1(model.predict(X_submit))\n\nname = \"GBoost\"\nclassifiers.add(name)\nmodels[name] = model \nscores_cv[name] = score_cv\nscores[name] = score\nscores_train[name] = score_train = print_rmse_score(y_train, model.predict(X_train))\nsubmits[name] = gboost_submit\nranks[name] = ranking(np.abs(model.feature_importances_), colnames)\n\nlog_mlflow(model)"]}, {"cell_type": "markdown", "id": "22cbcd2f", "metadata": {}, "source": ["## KernelRidge"]}, {"cell_type": "code", "execution_count": 1, "id": "65677d9b", "metadata": {}, "outputs": [], "source": ["%%time\nmodel_KRR = KernelRidge(alpha=0.03525, \n                        kernel='polynomial', \n                        degree=1, coef0=1e-6)\nmodel = model_KRR.fit(X_train, y_train)\nscore_cv = score_KRR_cv = print_rmse_cv_score(model, X_train, y_train, CV_n_folds)\nscore = score_KRR = print_rmse_score(y_test, model.predict(X_test))\nkrr_submit = np.expm1(model.predict(X_submit))\n\nname = \"KRR\"\nclassifiers.add(name)\nmodels[name] = model \nscores_cv[name] = score_cv\nscores[name] = score\nscores_train[name] = score_train = print_rmse_score(y_train, model.predict(X_train))\nsubmits[name] = krr_submit\n\nlog_mlflow(model)"]}, {"cell_type": "markdown", "id": "f687c63d", "metadata": {}, "source": ["## LightGBM"]}, {"cell_type": "code", "execution_count": 1, "id": "836910ab", "metadata": {}, "outputs": [], "source": ["%%time\nparams = {'learning_rate': 0.01, 'num_leaves': 3, 'max_bin': 84,\n          'bagging_freq': 1, 'bagging_seed': 2, 'feature_fraction_seed': 97, \n          'bagging_fraction': 0.745, \"verbose\":-1,\n          'objective': 'regression',\"n_estimators\":1000\n         }\nmodel_lgbm = lgb.LGBMRegressor(**params)\nmodel = model_lgbm.fit(X_train, y_train)\nscore_cv = score_lgbm_cv = print_rmse_cv_score(model, X_train, y_train, CV_n_folds)\nscore = score_lgbm = print_rmse_score(y_test, model.predict(X_test))\nlgbm_submit = np.expm1(model.predict(X_submit))\n\nname = \"lgbm\"\nclassifiers.add(name)\nmodels[name] = model \nscores_cv[name] = score_cv\nscores[name] = score\nscores_train[name] = score_train = print_rmse_score(y_train, model.predict(X_train))\nsubmits[name] = lgbm_submit\nranks[name] = ranking(np.abs(model.feature_importances_), colnames)\nlog_mlflow(model)"]}, {"cell_type": "markdown", "id": "bade189d", "metadata": {}, "source": ["## XGBoost"]}, {"cell_type": "code", "execution_count": 1, "id": "b9c3ea58", "metadata": {}, "outputs": [], "source": ["%%time\nmodel_xgb = xgb.XGBRegressor(tree_method=\"hist\",\n                             colsample_bytree=0.4603, gamma=0.01468, \n                             learning_rate=0.05187, max_depth=3, \n                             min_child_weight=0.0817, n_estimators=200,\n                             reg_alpha=0.4640, reg_lambda=0.6571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nmodel = model_xgb.fit(X_train, y_train)\nscore_cv = score_xgb_cv = print_rmse_cv_score(model, X_train, y_train, CV_n_folds)\nscore = score_xgb = print_rmse_score(y_test, model.predict(X_test))\nxgb_submit = np.expm1(model.predict(X_submit))\n\nname = \"xgb\"\nclassifiers.add(name)\nmodels[name] = model \nscores_cv[name] = score_cv\nscores[name] = score\nscores_train[name] = score_train = print_rmse_score(y_train, model.predict(X_train))\nsubmits[name] = xgb_submit\nranks[name] = ranking(np.abs(model.feature_importances_), colnames)\n\nlog_mlflow(model)\n"]}, {"cell_type": "markdown", "id": "1b1614d8", "metadata": {}, "source": ["## Lasso"]}, {"cell_type": "code", "execution_count": 1, "id": "d6abf8b8", "metadata": {}, "outputs": [], "source": ["%%time\nmodel_lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.00055419, random_state=1, max_iter=50000))\nmodel = model_lasso.fit(X_train, y_train)\nscore_cv = score_lasso_cv = print_rmse_cv_score(model, X_train, y_train, CV_n_folds)\nscore = score_lasso = print_rmse_score(y_test, model.predict(X_test))\nlasso_submit = np.expm1(model.predict(X_submit))\n\nname = \"lasso\"\nclassifiers.add(name)\nmodels[name] = model \nscores_cv[name] = score_cv\nscores[name] = score\nscores_train[name] = score_train = print_rmse_score(y_train, model.predict(X_train))\nsubmits[name] = lasso_submit\nranks[name] = ranking(np.abs(model[1].coef_), colnames)\n\nlog_mlflow(model)"]}, {"cell_type": "markdown", "id": "77a3f3e3", "metadata": {}, "source": ["## ElasticNet"]}, {"cell_type": "code", "execution_count": 1, "id": "a2f874f7", "metadata": {}, "outputs": [], "source": ["%%time\nparams = {\n    \"alpha\":0.00185,\n    \"max_iter\":10000,\n    \"l1_ratio\":0.224,\n    \"random_state\":1\n}\nmodel_ENet = make_pipeline(RobustScaler(), ElasticNet(**params))\nmodel = model_ENet.fit(X_train, y_train)\nscore_cv = score_ENet_cv = print_rmse_cv_score(model, X_train, y_train, CV_n_folds)\nscore = score_ENet = print_rmse_score(y_test, model.predict(X_test))\nENet_submit = np.expm1(model.predict(X_submit))\n\nname = \"ENet\"\nclassifiers.add(name)\nmodels[name] = model \nscores_cv[name] = score_cv\nscores[name] = score\nscores_train[name] = score_train = print_rmse_score(y_train, model.predict(X_train))\nsubmits[name] = ENet_submit\nranks[name] = ranking(np.abs(model[1].coef_), colnames)\n\nlog_mlflow(model)"]}, {"cell_type": "markdown", "id": "0efd8145", "metadata": {}, "source": ["## Model Stacking"]}, {"cell_type": "code", "execution_count": 1, "id": "16b24c0e", "metadata": {}, "outputs": [], "source": ["class CustomEnsembleRegressor(BaseEstimator, RegressorMixin):\n    def __init__(self, regressors=None):\n        self.regressors = regressors\n\n    def fit(self, X, y):\n        for regressor in self.regressors:\n            regressor.fit(X, y)\n        return self\n\n    def predict(self, X):\n        self.predictions_ = list()\n        for regressor in self.regressors:\n            self.predictions_.append(np.exp(regressor.predict(X).ravel()))\n\n        return np.log1p(np.mean(self.predictions_, axis=0))"]}, {"cell_type": "code", "execution_count": 1, "id": "b5efc301", "metadata": {}, "outputs": [], "source": ["# Averaged base models class\nclass AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   "]}, {"cell_type": "code", "execution_count": 1, "id": "ab3408d0", "metadata": {}, "outputs": [], "source": ["%%time\nmodel_averaged = AveragingModels(models = (model_ENet, \n                                            model_lasso, \n                                            model_GBoost,\n                                            model_KRR))\n\nmodel = model_averaged.fit(X_train, y_train)\nscore_cv = score_averaged_cv = print_rmse_cv_score(model, X_train, y_train, CV_n_folds)\nscore = score_averaged = print_rmse_score(y_test, model.predict(X_test))\naveraged_models_submit = np.expm1(model.predict(X_submit))\n\nname = \"Averaged\"\nclassifiers.add(name)\nmodels[name] = model \nscores_cv[name] = score_cv\nscores[name] = score\nscores_train[name] = score_train = print_rmse_score(y_train, model.predict(X_train))\nsubmits[name] = averaged_models_submit\n\nlog_mlflow(model)"]}, {"cell_type": "code", "execution_count": 1, "id": "5227b437", "metadata": {}, "outputs": [], "source": ["# https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard\nclass StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=False, random_state=111)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                \n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)"]}, {"cell_type": "code", "execution_count": 1, "id": "b747fe35", "metadata": {}, "outputs": [], "source": ["%%time\nmodel_stacked_averaged = StackingAveragedModels(base_models = (model_ENet, model_GBoost, model_KRR),\n                                                 meta_model = model_lasso)\n\nmodel = model_stacked_averaged.fit(X_train.values, y_train.values)\nscore_cv = score_stacked_cv = print_rmse_cv_score(model, X_train, y_train.values, CV_n_folds)\nscore = score_stacked = print_rmse_score(y_test, model.predict(X_test.values))\nstacked_averaged_models_submit = np.expm1(model.predict(X_submit.values))\n\nname = \"Stacked\"\nclassifiers.add(name)\nmodels[name] = model \nscores_cv[name] = score_cv\nscores[name] = score\nscores_train[name] = score_train = print_rmse_score(y_train, model.predict(X_train))\nsubmits[name] = stacked_averaged_models_submit\n\nlog_mlflow(model)"]}, {"cell_type": "markdown", "id": "c819e6d5", "metadata": {}, "source": ["# All model comparison owerview\nIn this section compare model perforamnce of singe notebook run."]}, {"cell_type": "code", "execution_count": 1, "id": "ccd90fd4", "metadata": {}, "outputs": [], "source": ["CV_score=[]\ntrain_score=[]\nPred_score=[]\nscore_list=[]\nlog_y_test = np.expm1(y_test)\ny_pred_df = pd.DataFrame({\"test\":log_y_test}).reset_index(drop=True)\nstd=[]\n\nfor i, name in enumerate(classifiers):\n    model = models[name]\n    if name != \"Stacked\":\n        y_pred = np.expm1(model.predict(X_test))\n    else:\n        y_pred = np.expm1(model.predict(X_test.values))\n    CV_score.append(scores_cv[name].mean())\n    std.append(scores_cv[name].std())\n    Pred_score.append(scores[name])\n    train_score.append(scores_train[name])\n    score_list.append(scores_cv[name])\n    \n    new_y_pred = pd.DataFrame({name:y_pred})\n    y_pred_df = pd.concat([y_pred_df, new_y_pred], axis=1).reset_index(drop=True)\n    y_pred_df[\"del_\"+name] = y_pred_df[\"test\"] - y_pred_df[name]\n   \n    \nnew_models_dataframe2=pd.DataFrame({\"RMSE\":Pred_score, \"Train - RMSE\":train_score,'CV Mean':CV_score,'Std':std}, index=classifiers) \nprint(\"\\nTEST_PART=\", TEST_PART)\nprint(\"CV_n_folds=\", CV_n_folds)\nprint(new_models_dataframe2)\nprint(\"\\nMin RMSE score: {:.5f}\".format(new_models_dataframe2[\"RMSE\"].min()))\nprint(\"Min CV mean score: {:.5f}\".format(new_models_dataframe2[\"CV Mean\"].min()))\n\n# Log into a file iteration score\nf = open(\"models_training_log.txt\", \"a\")\nprint(\"TEST_PART=\", TEST_PART, file=f)\nprint(\"CV_n_folds=\", CV_n_folds, file=f)\nprint(new_models_dataframe2, file=f)\nprint(\"\\nMin RMSE score: {:.5f}\".format(new_models_dataframe2[\"RMSE\"].min()), file=f)\nprint(\"Min CV mean score: {:.5f}\".format(new_models_dataframe2[\"CV Mean\"].min()), file=f)\nf.close()\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "c85d7919", "metadata": {}, "outputs": [], "source": ["if CV_n_folds > 2:\n    plt.subplots(figsize=(12,6))\n    sns.boxplot(list(classifiers), score_list)"]}, {"cell_type": "markdown", "id": "60c7e84c", "metadata": {}, "source": ["## Feature importance"]}, {"cell_type": "code", "execution_count": 1, "id": "4ad29943", "metadata": {}, "outputs": [], "source": ["# Create empty dictionary to store the mean value calculated from all the scores\nr = {}\nfor name in colnames:\n    r[name] = round(np.mean([ranks[method][name] for method in ranks.keys()]), 2)\nmethods = sorted(ranks.keys())\nranks[\"Mean\"] = r\nmethods.append(\"Mean\")"]}, {"cell_type": "code", "execution_count": 1, "id": "f7a45e7f", "metadata": {}, "outputs": [], "source": ["meanplot = pd.DataFrame(list(r.items()), columns= ['Feature','Mean Ranking'])\n# Sort the dataframe\nmeanplot = meanplot.sort_values('Mean Ranking', ascending=False)\n# Let's plot the ranking of the features\nsns.factorplot(x=\"Mean Ranking\", y=\"Feature\", data = meanplot, kind=\"bar\", \n               size=14, aspect=1.9, palette='coolwarm')"]}, {"cell_type": "code", "execution_count": 1, "id": "c61940b0", "metadata": {}, "outputs": [], "source": ["todrop = list(meanplot[meanplot[\"Mean Ranking\"]<0.02][\"Feature\"])\nprint (\"Features to drop: {}\".format(len(todrop)))\nprint (todrop)"]}, {"cell_type": "markdown", "id": "29a24d5c", "metadata": {}, "source": ["### Show how real prise from Test set corelated with Predicted price by models"]}, {"cell_type": "code", "execution_count": 1, "id": "2faf467a", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(16, 8))\n# multiple line plot\nplt.plot(y_pred_df.index,'test', data=y_pred_df, marker='o', markerfacecolor='blue', markersize=6, color='skyblue', linewidth=4)\n\ncolors = ['red', 'yellow', 'green', 'black', 'cyan', 'magenta', 'black', 'green', 'black', 'cyan', 'magenta', 'black']\nfor i, name in enumerate(classifiers):\n    plt.plot(y_pred_df.index, name, data=y_pred_df, marker='o', markerfacecolor=colors[i], markersize=3)\n\nplt.legend()"]}, {"cell_type": "markdown", "id": "6ed14151", "metadata": {}, "source": ["Show just selected models"]}, {"cell_type": "code", "execution_count": 1, "id": "7aa58c6b", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(16, 8))\n# multiple line plot\nplt.plot(y_pred_df.index,'test', data=y_pred_df, marker='o', markerfacecolor='blue', markersize=6, color='skyblue', linewidth=4)\n# plt.plot(y_pred_df.index, \"lasso\", data=y_pred_df, marker='o', markerfacecolor=\"red\", markersize=3)\nplt.plot(y_pred_df.index, \"Stacked\", data=y_pred_df, marker='o', markerfacecolor=\"green\", markersize=3)\n# plt.plot(y_pred_df.index, \"xgb\", data=y_pred_df, marker='X', markerfacecolor=\"yellow\", markersize=6)\nplt.plot(y_pred_df.index, \"lgbm\", data=y_pred_df, marker='X', markerfacecolor=\"yellow\", markersize=6)\n\nplt.legend()"]}, {"cell_type": "markdown", "id": "817e977c", "metadata": {}, "source": ["### Show delta between real price and predicted"]}, {"cell_type": "code", "execution_count": 1, "id": "00edec11", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(16, 8))\n# multiple line plot\n# plt.plot(y_pred_df.index,'test', data=y_pred_df, marker='o', markerfacecolor='blue', markersize=6, color='skyblue', linewidth=4)\nplt.plot(y_pred_df.index, \"del_Averaged\", data=y_pred_df, marker='o', markerfacecolor=\"red\", markersize=3)\nplt.plot(y_pred_df.index, \"del_Stacked\", data=y_pred_df, marker='o', markerfacecolor=\"green\", markersize=3)\n# plt.plot(y_pred_df.index, \"del_xgb\", data=y_pred_df, marker='X', markerfacecolor=\"yellow\", markersize=6)\nplt.plot(y_pred_df.index, \"del_lgbm\", data=y_pred_df, marker='X', markerfacecolor=\"yellow\", markersize=6)\n# plt.plot(y_pred_df.index, \"del_lasso\", data=y_pred_df, marker='X', markerfacecolor=\"red\", markersize=6)\n\nplt.legend()"]}, {"cell_type": "markdown", "id": "f294e6b3", "metadata": {}, "source": ["### Let's try to see the biggest missprediction"]}, {"cell_type": "code", "execution_count": 1, "id": "2bd19f74", "metadata": {}, "outputs": [], "source": ["fail_idx = y_pred_df[abs(y_pred_df[\"del_xgb\"]) > 60000].index\ny_pred_df[abs(y_pred_df[\"del_xgb\"]) >60000]"]}, {"cell_type": "code", "execution_count": 1, "id": "35874d3c", "metadata": {}, "outputs": [], "source": ["df = X.iloc[fail_idx, :]\ndf[['OverallQual', 'MasVnrArea', 'TotalSF', '1stFlrSF', '2ndFlrSF',  \n    'LotArea', 'LotFrontage', 'GrLivArea', \"TotRmsAbvGrd\"]]"]}, {"cell_type": "code", "execution_count": 1, "id": "f507efd9", "metadata": {}, "outputs": [], "source": ["print(y_pred_df[\"del_Stacked\"].mean())\nprint(y_pred_df[\"del_xgb\"].mean())\nprint(y_pred_df[\"del_lgbm\"].mean())"]}, {"cell_type": "markdown", "id": "a2601518", "metadata": {}, "source": ["# Generate Submit file"]}, {"cell_type": "code", "execution_count": 1, "id": "ec8a5daf", "metadata": {}, "outputs": [], "source": ["def blend_models_predict(names, coeff, X):\n    pred = 0;\n    for i, name in enumerate(names):\n        if type(models[name]) == StackingAveragedModels:\n            pred += coeff[i] * models[name].predict(X.values)\n        else:\n            pred += coeff[i] * models[name].predict(X)\n              \n    return pred"]}, {"cell_type": "code", "execution_count": 1, "id": "c67cfe66", "metadata": {}, "outputs": [], "source": ["def log_to_file_stackconfig(models, coeff, rmse, rmse_train):\n    f = open(\"models_training_log.txt\", \"a\")\n    print(\"Stacking config:\" +str(models)+\" \"+str(coeff), file=f)\n    print(\"TEST={:0.5f}\".format(rmse) + \" TRAIN={:0.5f}\".format(rmse_train), file=f)\n    f.close()"]}, {"cell_type": "code", "execution_count": 1, "id": "1061d9d2", "metadata": {}, "outputs": [], "source": ["compare_models = [\"Stacked\",\n                  \"Averaged\", \n#                   \"xgb\", \n                  \"lgbm\"\n                 ]\ncoeff = [0.35, 0.35, 0.3]\n\ny_train_pred = blend_models_predict(compare_models, coeff, X_train)\ny_test_pred = blend_models_predict(compare_models, coeff, X_test)\n# Final result convert from log \nY_pred = np.expm1(blend_models_predict(compare_models, coeff, X_submit))\n\nprint(\"[TRAIN]:\")\ntrain_rmse_ = print_rmse_score(y_train, y_train_pred)\nprint(\"[TEST]:\")\ntest_score = print_rmse_score(y_test, y_test_pred)\nlog_to_file_stackconfig(compare_models, coeff, test_score, train_rmse_)\n\ndate = datetime.datetime.now().isoformat()\nsubmit_name = str(int(round(time.time() * 1000)))\nfor i, name in enumerate(compare_models):\n    submit_name += \"_\"+str(coeff[i])+\"_\" + name \nsubmit_name += \"_TEST_\" + str(TEST_PART) \nsubmit_name += \"_rmse_{:0.5f}\".format(test_score) + \".csv\"\nprint(\"=\"*80)\nsubmission = pd.DataFrame({\n        \"Id\": orig_test[\"Id\"],\n        \"SalePrice\": Y_pred\n    })\nsubmission.to_csv(submit_name, index=False)\nprint(submit_name)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}