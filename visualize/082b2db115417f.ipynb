{"cells": [{"cell_type": "code", "execution_count": 1, "id": "725c692c", "metadata": {}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]}, {"cell_type": "code", "execution_count": 1, "id": "ff78c5df", "metadata": {}, "outputs": [], "source": ["import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n"]}, {"cell_type": "code", "execution_count": 1, "id": "f7a5e392", "metadata": {}, "outputs": [], "source": ["df1=pd.read_csv('../input/forest-cover-type-prediction/train.csv')\ndf_test1=pd.read_csv('../input/forest-cover-type-prediction/test.csv')\ndf_test2=pd.read_csv('../input/forest-cover-type-prediction/test3.csv')\ndf=df1.copy()\ndf_test=df_test1.copy()"]}, {"cell_type": "code", "execution_count": 1, "id": "f80b22bc", "metadata": {}, "outputs": [], "source": ["df"]}, {"cell_type": "code", "execution_count": 1, "id": "a4e06965", "metadata": {}, "outputs": [], "source": ["pd.set_option('display.max_columns',None)\ndf.drop(columns=['Id','Cover_Type'],inplace=True)\ndf_test.drop(columns=['Id'],inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "958cfc8f", "metadata": {}, "outputs": [], "source": ["df_test"]}, {"cell_type": "code", "execution_count": 1, "id": "1348f680", "metadata": {}, "outputs": [], "source": ["X_train=df\nY_train=df1.iloc[:,-1]"]}, {"cell_type": "code", "execution_count": 1, "id": "a9744ef4", "metadata": {}, "outputs": [], "source": ["X_train"]}, {"cell_type": "code", "execution_count": 1, "id": "9af54fe9", "metadata": {}, "outputs": [], "source": ["df_test"]}, {"cell_type": "markdown", "id": "3edc7386", "metadata": {}, "source": ["## Modelling"]}, {"cell_type": "code", "execution_count": 1, "id": "afa2b226", "metadata": {}, "outputs": [], "source": ["from collections import Counter\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nfrom lightgbm import LGBMClassifier\n\nsns.set(style='white', context='notebook', palette='deep')\nkfold = StratifiedKFold(n_splits=10)\n"]}, {"cell_type": "markdown", "id": "15650c6f", "metadata": {}, "source": ["## Comparing all models"]}, {"cell_type": "code", "execution_count": 1, "id": "bdb2515a", "metadata": {}, "outputs": [], "source": ["\nrandom_state = 2\nclassifiers = []\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(LinearDiscriminantAnalysis())\nclassifiers.append(XGBClassifier(random_state = random_state))\nclassifiers.append(LGBMClassifier(random_state = random_state))\n\ncv_results = []\nfor classifier in classifiers :\n    score=cross_val_score(classifier, X_train, y = Y_train, scoring = \"accuracy\", cv = kfold, n_jobs=-1)\n    cv_results.append(score)\n    print('{} crossvalidation score:{}\\n'.format(classifier,score.mean()))\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"KNeighboors\",\"LogisticRegression\",\"LinearDiscriminantAnalysis\",'XGboost','LGboost']})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")\n"]}, {"cell_type": "markdown", "id": "ae8cfe19", "metadata": {}, "source": ["### Best models are random forst,extra trees, xgboost and lgboost"]}, {"cell_type": "markdown", "id": "b44f1c6b", "metadata": {}, "source": ["## Random Forest Classifier"]}, {"cell_type": "code", "execution_count": 1, "id": "40c201f3", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\nxtrain,xtest,ytrain,ytest=train_test_split(X_train.values,Y_train.values,test_size=0.2)"]}, {"cell_type": "code", "execution_count": 1, "id": "4c1c46d7", "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import accuracy_score\nRFC = RandomForestClassifier(random_state=random_state)\nRFC.fit(xtrain,ytrain)\nypred=RFC.predict(xtest)\nscore=cross_val_score(RFC,X_train,Y_train,scoring='accuracy',cv=kfold,n_jobs=-1)\n\n# Best score\nprint('Crossval score for random forest: {}'.format(score.mean()))\nprint('Accuracy score for random forest: {}'.format(accuracy_score(ytest,ypred)))"]}, {"cell_type": "markdown", "id": "4e3b0b43", "metadata": {}, "source": ["## Tuned Random Forest"]}, {"cell_type": "code", "execution_count": 1, "id": "fa1eb40c", "metadata": {}, "outputs": [], "source": ["RFC.get_params()"]}, {"cell_type": "code", "execution_count": 1, "id": "afe66471", "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import accuracy_score\nRFC2 = RandomForestClassifier(random_state=random_state,\n                             n_estimators=500,\n                             max_depth=32,\n                             min_samples_leaf=1,\n                             criterion='entropy')\nRFC2.fit(xtrain,ytrain)\nypred=RFC2.predict(xtest)\nscore=cross_val_score(RFC2,X_train,Y_train,scoring='accuracy',cv=kfold,n_jobs=-1)\n\n# Best score\nprint('Crossval score for random forest: {}'.format(score.mean()))\nprint('Accuracy score for random forest: {}'.format(accuracy_score(ytest,ypred)))"]}, {"cell_type": "markdown", "id": "87be5b44", "metadata": {}, "source": ["## Extra Trees Classifier"]}, {"cell_type": "code", "execution_count": 1, "id": "24fdf7ff", "metadata": {}, "outputs": [], "source": ["et=ExtraTreesClassifier(random_state=random_state)\net.fit(xtrain,ytrain)\nypred=et.predict(xtest)\nscore=cross_val_score(et,X_train,Y_train,scoring='accuracy',cv=kfold,n_jobs=-1)\n\n# Best score\nprint('Crossval score for extra trees classifier: {}'.format(score.mean()))\nprint('Accuracy score for extra trees classifier: {}'.format(accuracy_score(ytest,ypred)))\n"]}, {"cell_type": "markdown", "id": "0de9339b", "metadata": {}, "source": ["## Tuned extra trees model"]}, {"cell_type": "code", "execution_count": 1, "id": "4fa01870", "metadata": {}, "outputs": [], "source": ["et2=ExtraTreesClassifier()\net2.get_params()"]}, {"cell_type": "code", "execution_count": 1, "id": "5252778d", "metadata": {}, "outputs": [], "source": ["et2=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n                     criterion='entropy', max_depth=38, max_features='auto',\n                     max_leaf_nodes=None, max_samples=None,\n                     min_impurity_decrease=0.0, min_impurity_split=None,\n                     min_samples_leaf=1, min_samples_split=2,\n                     min_weight_fraction_leaf=0.0, n_estimators=500,\n                     n_jobs=None, oob_score=False, random_state=0, verbose=0,\n                     warm_start=False)\net2.fit(xtrain,ytrain)\nypred=et2.predict(xtest)\nscore=cross_val_score(et2,X_train,Y_train,scoring='accuracy',cv=kfold,n_jobs=-1)\n\n# Best score\nprint('Crossval score for extra trees classifier: {}'.format(score.mean()))\nprint('Accuracy score for extra trees classifier: {}'.format(accuracy_score(ytest,ypred)))"]}, {"cell_type": "markdown", "id": "f1554aab", "metadata": {}, "source": ["## LightGb Classifier"]}, {"cell_type": "code", "execution_count": 1, "id": "310f2446", "metadata": {}, "outputs": [], "source": ["lgb2=LGBMClassifier(random_state=random_state)\nlgb2.fit(xtrain,ytrain)\nypred=lgb2.predict(xtest)\nscore=cross_val_score(lgb2,X_train,Y_train,scoring='accuracy',cv=kfold,n_jobs=-1)\n\n# Best score\nprint('Crossval score for Lightgb classifier: {}'.format(score.mean()))\nprint('Accuracy score for Lightgb classifier: {}'.format(accuracy_score(ytest,ypred)))"]}, {"cell_type": "markdown", "id": "27e391d6", "metadata": {}, "source": ["## Tuned LightGB"]}, {"cell_type": "code", "execution_count": 1, "id": "879e1699", "metadata": {}, "outputs": [], "source": ["lgb=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n        importance_type='split', learning_rate=0.2, max_depth=-1,\n        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n        n_estimators=200, n_jobs=4, num_leaves=63, objective=None,\n        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n        subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\nlgb.fit(xtrain,ytrain)\nypred=lgb.predict(xtest)\nscore=cross_val_score(lgb,X_train,Y_train,scoring='accuracy',cv=kfold,n_jobs=-1)\n\n# Best score\nprint('Crossval score for Lightgb classifier: {}'.format(score.mean()))\nprint('Accuracy score for Lightgb classifier: {}'.format(accuracy_score(ytest,ypred)))"]}, {"cell_type": "markdown", "id": "e7b56c95", "metadata": {}, "source": ["# ENSEMBLE VOTING CLASSIFIER"]}, {"cell_type": "code", "execution_count": 1, "id": "6fea32ba", "metadata": {}, "outputs": [], "source": ["vc= VotingClassifier(estimators=[('rfc', RFC2), ('extc', et2),\n('lgb',lgb)], voting='soft', n_jobs=-1)\nvc.fit(xtrain,ytrain)\nypred=vc.predict(xtest)\nscore=cross_val_score(vc,X_train,Y_train,scoring='accuracy',cv=kfold,n_jobs=-1)\n\n# Best score\nprint('Crossval score for Lightgb classifier: {}'.format(score.mean()))\nprint('Accuracy score for Lightgb classifier: {}'.format(accuracy_score(ytest,ypred)))\n"]}, {"cell_type": "markdown", "id": "6b484071", "metadata": {}, "source": ["## ENSEMBLE STACKING CLASSIFIER"]}, {"cell_type": "code", "execution_count": 1, "id": "d50acb33", "metadata": {}, "outputs": [], "source": ["\"\"\"\nfrom sklearn.ensemble import StackingClassifier\nestimators = [ ('rf', RFC2),\n     ('et', et2)]\n\nsc= StackingClassifier(estimators=estimators, final_estimator=lgb)\nsc.fit(xtrain,ytrain)\nypred=sc.predict(xtest)\nscore=cross_val_score(sc,X_train,Y_train,scoring='accuracy',cv=kfold,n_jobs=-1)\n\n# Best score\nprint('Crossval score for Lightgb classifier: {}'.format(score.mean()))\nprint('Accuracy score for Lightgb classifier: {}'.format(accuracy_score(ytest,ypred)))\"\"\""]}, {"cell_type": "code", "execution_count": 1, "id": "88160164", "metadata": {}, "outputs": [], "source": ["vc.fit(X_train,Y_train)\nypred=vc.predict(df_test.values)\n\nid=df_test1['Id']\ndict={'ID':id,'Cover_Type':ypred}\ndfsub=pd.DataFrame(dict)\ndfsub.to_csv('./submission_ensemblevoting.csv', index=False)"]}, {"cell_type": "markdown", "id": "19fd722b", "metadata": {}, "source": ["# Hyperparameter tuning of RF And Extratrees"]}, {"cell_type": "code", "execution_count": 1, "id": "bb6c06e1", "metadata": {}, "outputs": [], "source": ["\"\"\"\n#ExtraTrees \net2= ExtraTreesClassifier()\n\n\n## Search grid for optimal parameters\nex_param_grid = {\n \n \n 'criterion': ['gini','entropy'],\n 'max_depth':[5,10,25],\n 'max_features':[1,3,7],\n 'max_samples': [0.2],\n 'min_samples_leaf': [1,2,5],\n 'min_samples_split': [2,5,7],\n 'n_estimators': [100,200,300],\n }\n\n\ngset = GridSearchCV(et2,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs=-1, verbose = 1)\n\ngset.fit(X_train,Y_train)\ngset_best = gset.best_estimator_\n\n# Best score\nprint(gset.best_score_)\nprint(gset.best_estimator_)\"\"\"\n"]}, {"cell_type": "code", "execution_count": 1, "id": "a7950f18", "metadata": {}, "outputs": [], "source": ["\"\"\"\n# RFC Parameters tunning \nRFC = RandomForestClassifier()\n\n\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\nrf_param_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [100, 200, 300, 1000]\n}\n\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsRFC.fit(X_train,Y_train)\n\nRFC_best = gsRFC.best_estimator_\n\n# Best score\ngsRFC.best_score_\"\"\""]}, {"cell_type": "code", "execution_count": 1, "id": "f9e37c7e", "metadata": {}, "outputs": [], "source": ["\"\"\"\nRFC2 = RandomForestClassifier()\nrf_param_grid = {\n    'bootstrap': [True],\n    'max_depth': [32],\n    'max_features': [2],\n    'min_samples_leaf': [1],\n    'min_samples_split': [6],\n    'n_estimators': [300]\n}\n\n\ngsRFC2 = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsRFC2.fit(X_train,Y_train)\ngsRFC2.best_score_\"\"\""]}, {"cell_type": "markdown", "id": "e5379403", "metadata": {}, "source": ["## Feature Importance"]}, {"cell_type": "code", "execution_count": 1, "id": "f5025aa6", "metadata": {}, "outputs": [], "source": ["pd.DataFrame(RFC.feature_importances_,\n             index=X_train.columns, columns=['Importance']).sort_values(\n    by='Importance', ascending=False)[:10]"]}, {"cell_type": "code", "execution_count": 1, "id": "8fca4166", "metadata": {}, "outputs": [], "source": ["pd.DataFrame(et.feature_importances_,\n             index=X_train.columns, columns=['Importance']).sort_values(\n    by='Importance', ascending=False)[:10]"]}, {"cell_type": "markdown", "id": "22b24c14", "metadata": {}, "source": ["# Plotting learning curves"]}, {"cell_type": "code", "execution_count": 1, "id": "8c251b90", "metadata": {}, "outputs": [], "source": ["def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ng = plot_learning_curve(RFC,\"Random Forest learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(et,\"Extra trees learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsRFC2,\"Random Forest tuned learning curves\",X_train,Y_train,cv=kfold)\n#g = plot_learning_curve(gsExtC.best_estimator_,\"ExtraTrees learning curves\",X_train,Y_train,cv=kfold)\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "2c8ac2e0", "metadata": {}, "outputs": [], "source": ["def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ng = plot_learning_curve(lgb,\"lgb tuned learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(lgb2,\"Normal lgb learning curves\",X_train,Y_train,cv=kfold)\n\n#g = plot_learning_curve(gsExtC.best_estimator_,\"ExtraTrees learning curves\",X_train,Y_train,cv=kfold)"]}, {"cell_type": "code", "execution_count": 1, "id": "4976fb43", "metadata": {}, "outputs": [], "source": ["#g = plot_learning_curve(vc,\"voting classifier learning curves\",X_train,Y_train,cv=kfold)"]}, {"cell_type": "markdown", "id": "48ea512d", "metadata": {}, "source": ["### Both models are little overfitting "]}, {"cell_type": "markdown", "id": "af6ff8b7", "metadata": {}, "source": ["## Submisssion File"]}, {"cell_type": "code", "execution_count": 1, "id": "2bbaf8cb", "metadata": {}, "outputs": [], "source": ["gset_best.fit(X_train,Y_train)\nypred=gset_best.predict(df_test.values)\n\nid=df_test1['Id']\ndict={'ID':id,'Cover_Type':ypred}\ndfsub=pd.DataFrame(dict)\ndfsub.to_csv('./submission_gset.csv', index=False)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}