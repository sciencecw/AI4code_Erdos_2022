{"cells": [{"cell_type": "markdown", "id": "79c3146b", "metadata": {}, "source": ["# Introducing Logistic Regression\nLogistic Regression is a classification algorithm. It is used to predict a binary outcome (1 / 0, Yes / No, True / False) given a set of independent variable/s. You can also think of logistic regression as a special case of linear regression when the outcome variable is categorical. Logistic Regression is a supervised machine learning algorithm/model.\n\n## Agenda\n*  About Dataset\n*  Loading Libraries\n*  Loading Data\n*  Understanding Data\n*  Separating Input/Independent and Output/Dependent Variables\n*  Splitting the data\n*  Building Model\n*  Prediction\n*  Model Performance\n\n## About Dataset\nThe dataset has two columns - age (age of the person/customer) and bought_insurance (whether the customer bought insurance or not). If bought_insurance = 1, the customer bought insurance and if bought_insurance = 0, the customer did not buy the insurance.\n\nDataset Link: [insurance_data](https://raw.githubusercontent.com/codebasics/py/master/ML/7_logistic_reg/insurance_data.csv)"]}, {"cell_type": "markdown", "id": "2d89be0d", "metadata": {}, "source": ["## Loading Libraries\nAll Python capabilities are not loaded to our working environment by default (even they are already installed in your system). So, we import each and every library that we want to use.\n\nIn data science, numpy and pandas are most commonly used libraries. Numpy is required for calculations like means, medians, square roots, etc. Pandas is used for data processin and data frames. We chose alias names for our libraries for the sake of our convenience (numpy --> np and pandas --> pd).\n\n**We can import all the libraries that we think might be needed or can import as we go along.**"]}, {"cell_type": "code", "execution_count": 1, "id": "36a98a99", "metadata": {}, "outputs": [], "source": ["import pandas as pd\nimport matplotlib.pyplot as plt"]}, {"cell_type": "markdown", "id": "1e3bed57", "metadata": {}, "source": ["## Loading Data\nPandas module is used for reading files. We have our data in '.csv' format. We will use 'read_csv()' function for loading the data."]}, {"cell_type": "code", "execution_count": 1, "id": "1e07881e", "metadata": {}, "outputs": [], "source": ["# In read_csv() function, we have passed the raw data link at github\ndata_location = \"https://raw.githubusercontent.com/codebasics/py/master/ML/7_logistic_reg/insurance_data.csv\"\ndata = pd.read_csv(data_location)"]}, {"cell_type": "markdown", "id": "14a1ae0b", "metadata": {}, "source": ["## Understanding Data\nLet's check how our data looks. This can be done using head() method."]}, {"cell_type": "code", "execution_count": 1, "id": "de2209c2", "metadata": {}, "outputs": [], "source": ["data.head()"]}, {"cell_type": "markdown", "id": "8a9a32bf", "metadata": {}, "source": ["There are two columns:\n\n*  age: The age of the customer\n*  bought_insurance: If the customer bought insurance (1) or not (0). This is our target variable which we are interested to know.\n\nSince our target variable has only two different classes/values, we can say it as a binary classification problem. And Logistic Regression is used for binary classification problems."]}, {"cell_type": "markdown", "id": "244af632", "metadata": {}, "source": ["Looking the relationship between age and bought_insurance using scatter plot."]}, {"cell_type": "code", "execution_count": 1, "id": "f5bf761a", "metadata": {}, "outputs": [], "source": ["plt.scatter(data.age,data.bought_insurance,marker='+',color='red')"]}, {"cell_type": "markdown", "id": "f394dbc5", "metadata": {}, "source": ["We can easily observe from the scatter plot that generally the customer who is of age less than 30 years has not bought the insurance."]}, {"cell_type": "markdown", "id": "339598af", "metadata": {}, "source": ["## Separating Input and Output Variables\nBefore building any machine learning model, we always separate the input variables and output variables. Input variables are those quantities whose values are changed naturally in an experiment, whereas output variable is the one whose values are dependent on the input variables. So, input variables are also known as independent variables as its values are not dependent on any other quantity, and output variable/s are also known as dependent variables as its values are dependent on other variable i.e. input variables. Like here in this data, we can see that whether a person will buy insurance or not is dependent on the age of that person\n\nBy convention input variables are represented with 'X' and output variables are represented with 'y'."]}, {"cell_type": "code", "execution_count": 1, "id": "bc42da52", "metadata": {}, "outputs": [], "source": ["X = data[['age']]     # input variable\n\ny = data['bought_insurance']    # output variable"]}, {"cell_type": "markdown", "id": "50cdab09", "metadata": {}, "source": ["If you notice the above code cell, I have used two square brackets while taking input variables and only one square bracket while taking output variable. Why?\n\nAll machine learning algorithm accepts input variables as a 2D array and output variable as 1D array. Using two square brackets while selecting the input variables gives you the shape of input variable/s as 2D, but if you use only one square bracket, the shape will be 1D as you can see in the case of y.\n\nLet's check the shapes of X and y."]}, {"cell_type": "code", "execution_count": 1, "id": "aa510251", "metadata": {}, "outputs": [], "source": ["print(\"Shape: \", X.shape, \"Dimension: \", X.ndim)\nprint(\"Shape: \", y.shape, \"Dimension: \", y.ndim)"]}, {"cell_type": "markdown", "id": "b1c8acac", "metadata": {}, "source": ["## Splitting the data into Train and Test Set\nWe want to check the performance of the model that we built. For this purpose, we always split (both input and output data) the given data into training set  which will be used to train the model, and test set which will be used to check how accurately the model is predicting outcomes.\n\nFor this purpose we have a class called 'train_test_split' in the 'sklearn.model_selection' module."]}, {"cell_type": "code", "execution_count": 1, "id": "7014ff82", "metadata": {}, "outputs": [], "source": ["# import train_test_split\nfrom sklearn.model_selection import train_test_split"]}, {"cell_type": "code", "execution_count": 1, "id": "70b31c84", "metadata": {}, "outputs": [], "source": ["# split the data\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state = 42)\n\n# X_train: independent/input feature data for training the model\n# y_train: dependent/output feature data for training the model\n# X_test: independent/input feature data for testing the model; will be used to predict the output values\n# y_test: original dependent/output values of X_test; We will compare this values with our predicted values to check the performance of our built model.\n \n# test_size = 0.30: 30% of the data will go for test set and 70% of the data will go for train set\n# random_state = 42: this will fix the split i.e. there will be same split for each time you run the code"]}, {"cell_type": "markdown", "id": "f13ba868", "metadata": {}, "source": ["## Building Model\n"]}, {"cell_type": "markdown", "id": "a5c8001c", "metadata": {}, "source": ["Now we are finally ready, and we can train the model.\n\nFirst, we need to import our model - Logistic Regression (again, using the sklearn library).\n\nThen we would feed the model both with the data (X_train) and the answers for that data (y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "57612f72", "metadata": {}, "outputs": [], "source": ["# import Logistic Regression from sklearn.linear_model\nfrom sklearn.linear_model import LogisticRegression"]}, {"cell_type": "code", "execution_count": 1, "id": "f9d69f3b", "metadata": {}, "outputs": [], "source": ["log_model = LogisticRegression()"]}, {"cell_type": "code", "execution_count": 1, "id": "ab21b307", "metadata": {}, "outputs": [], "source": ["# Fit the model\nlog_model.fit(X_train, y_train)"]}, {"cell_type": "markdown", "id": "c7cbce8e", "metadata": {}, "source": ["The training happens in the third line (the \"fit\" function)."]}, {"cell_type": "markdown", "id": "ae78fe1e", "metadata": {}, "source": ["## Prediction\nNow logistic regression model (i.e. log_model) is trained using X_train and y_trian data. Let's predict the target value (i.e. bought_insurance) for the X_test data. We use \"predict()\" method for prediction."]}, {"cell_type": "code", "execution_count": 1, "id": "fb101399", "metadata": {}, "outputs": [], "source": ["predictions = log_model.predict(X_test)"]}, {"cell_type": "markdown", "id": "e33079e4", "metadata": {}, "source": ["We already have actual target values (i.e. y_test) for X_test. Let's compare y_test and the predicted value for X_test by our log_model."]}, {"cell_type": "code", "execution_count": 1, "id": "85de2586", "metadata": {}, "outputs": [], "source": ["y_test.values"]}, {"cell_type": "code", "execution_count": 1, "id": "de967fa2", "metadata": {}, "outputs": [], "source": ["predictions"]}, {"cell_type": "code", "execution_count": 1, "id": "0b197a2e", "metadata": {}, "outputs": [], "source": ["res = pd.DataFrame(predictions)\nres.index = X_test.index # its important for comparison\nres.columns = [\"prediction\"]"]}, {"cell_type": "code", "execution_count": 1, "id": "49100a51", "metadata": {}, "outputs": [], "source": ["from google.colab import files\nres.to_csv('filename.csv') \nfiles.download('filename.csv')"]}, {"cell_type": "markdown", "id": "d50d6188", "metadata": {}, "source": ["There is one person who had actually bought insurance but our model predicted that the person had not bought insurance. So, there is one misclassified data by our model."]}, {"cell_type": "markdown", "id": "1ab7193c", "metadata": {}, "source": ["## Model Performance\nWe can also check how accurate our model is performing using the 'accuracy_score' class from 'sklearn.metrics'."]}, {"cell_type": "code", "execution_count": 1, "id": "fdf590c7", "metadata": {}, "outputs": [], "source": ["# The confusion matrix\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, predictions) "]}, {"cell_type": "code", "execution_count": 1, "id": "0c32ef2f", "metadata": {}, "outputs": [], "source": ["tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()   # ravel() will convert the 2D numpy array into 1D.\nprint(tn, fp, fn, tp)"]}, {"cell_type": "code", "execution_count": 1, "id": "249646cb", "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import accuracy_score"]}, {"cell_type": "code", "execution_count": 1, "id": "0945f196", "metadata": {}, "outputs": [], "source": ["accuracy_score(y_test, predictions)"]}, {"cell_type": "markdown", "id": "d1f784ac", "metadata": {}, "source": ["**Why accuracy score?**\n\nAccuracy is a great measure when you have symmetric datasets where values of false positive and false negatives are almost same. As you can see the confusion matrix above, false positives (fp = 0) and false negatives (fn = 1) are almost same. So here accuracy score is the best measure.\n\nFurther reading: https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/"]}, {"cell_type": "markdown", "id": "8397218f", "metadata": {}, "source": ["Our model is predicting 88.9% correct results."]}, {"cell_type": "markdown", "id": "7886003b", "metadata": {}, "source": ["### Thanks for reading the Notebook!!!"]}, {"cell_type": "markdown", "id": "974fd84c", "metadata": {}, "source": ["**References:**\n\nhttps://github.com/codebasics/py/blob/master/ML/7_logistic_reg/7_logistic_regression.ipynb"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}