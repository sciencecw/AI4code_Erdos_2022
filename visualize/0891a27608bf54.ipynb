{"cells": [{"cell_type": "markdown", "id": "a4283910", "metadata": {}, "source": ["# This kernel is to predict the future of customer will come back purchase or not\n* Fot train_v2 data, we have 2016/08/01 ~ 2018/04/30 period data\n* For test_v2 data, we have 2018/05/1 ~ 2018/10/15 period data\n* The Public LB  score is base on timeframe 2018/05/1~ 2018/10/15\n* The Private LB score is base on timeframe of 2018/12/1 ~ 2019/01/31 with same visitor ID that in test_v2\n* So this competition become the future prediction question ....."]}, {"cell_type": "markdown", "id": "0bdcd02c", "metadata": {}, "source": ["## Discussion topic about this idea from AmirH\nhttps://www.kaggle.com/c/ga-customer-revenue-prediction/discussion/71427\n* I use LGBM to predict the user will come back purchase or not (Classification)\n"]}, {"cell_type": "markdown", "id": "4ec9ee86", "metadata": {}, "source": ["## Training Set\n* Training period set 1==> 2016/08/01 ~ 2017/1/15 (5.5 month)\n* Target period set 1  ==> 2017/03/1 ~ 2017/04/30 (2 month)\n* Training period set 2==> 2017/06/01 ~ 2017/11/15 (5.5 month)\n* Target period set 2  ==> 2018/1/1 ~ 2018/02/30 (2 month)\n* Concate set 1 and set 2 to be training data\n* Feature engineering on training period feature\n* Target set that those come back purchased user in target period"]}, {"cell_type": "markdown", "id": "ec6c0164", "metadata": {}, "source": ["## Valid Set (1 year ago of our test set and target )\n* Valid period set ==> 2017/5/1 ~ 2017/10/15\n* Valid target period set ==> 2017/12/1 ~ 2018/1/31"]}, {"cell_type": "code", "execution_count": 1, "id": "f4c30233", "metadata": {}, "outputs": [], "source": ["import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc\nimport os\nfrom sklearn.metrics import confusion_matrix,accuracy_score, roc_curve, auc\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nprint(os.listdir(\"../input\"))"]}, {"cell_type": "code", "execution_count": 1, "id": "06698e93", "metadata": {}, "outputs": [], "source": ["import seaborn as sns\nimport json\nimport pandas.io.json as pdjson\nimport ast\n\nfrom pandas.io.json import json_normalize\ndef load_df(csv_path='../input/train_v2.csv', nrows=None):\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    \n    df = pd.read_csv(csv_path, \n                     converters={column: json.loads for column in JSON_COLUMNS}, \n                     dtype={'fullVisitorId': 'str'}, # Important!!\n                     nrows=nrows)\n    \n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    for column in JSON_COLUMNS:\n        column_as_df = pdjson.json_normalize(df[column])\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n    return df\n"]}, {"cell_type": "markdown", "id": "b0cefe3f", "metadata": {}, "source": ["## Load Data\n* use Aguiar's dataset (Many thanks): https://www.kaggle.com/jsaguiar/parse-json-v2-without-hits-column"]}, {"cell_type": "code", "execution_count": 1, "id": "3543c40e", "metadata": {}, "outputs": [], "source": ["%%time\npath = \"../input/parse-json-v2-without-hits-column/\"\ntrain_df = pd.read_pickle(path + 'train_v2_clean.pkl')\ntest_df = pd.read_pickle(path + 'test_v2_clean.pkl')"]}, {"cell_type": "markdown", "id": "f2e1d103", "metadata": {}, "source": ["## Add time feature"]}, {"cell_type": "code", "execution_count": 1, "id": "4ce88263", "metadata": {}, "outputs": [], "source": ["for df in [train_df,test_df]:\n    df['date'] = pd.to_datetime(df['visitStartTime'], unit='s')\n    df[\"day\"] = df['date'].dt.day\n    df['month'] = df['date'].dt.month\n    df['weekday'] = df['date'].dt.weekday\n    df['weekofyear'] = df['date'].dt.weekofyear"]}, {"cell_type": "code", "execution_count": 1, "id": "faf2e46a", "metadata": {}, "outputs": [], "source": ["train_df.shape, test_df.shape"]}, {"cell_type": "markdown", "id": "ef42934e", "metadata": {}, "source": ["## Feature engineering \n* mean, max, min for \"totals_pagevies\" and \"totals_hits \"\n* Change to lable encoding for categorical feature\n* Drop 'trafficSource_referralPath','trafficSource_source'"]}, {"cell_type": "code", "execution_count": 1, "id": "e5128cd8", "metadata": {}, "outputs": [], "source": ["train_df['totals_pageviews']=train_df['totals_pageviews'].astype('float')\ntrain_df['totals_hits']=train_df['totals_hits'].astype('float')\ntest_df['totals_pageviews']=test_df['totals_pageviews'].astype('float')\ntest_df['totals_hits']=test_df['totals_hits'].astype('float')"]}, {"cell_type": "code", "execution_count": 1, "id": "1445c0d2", "metadata": {}, "outputs": [], "source": ["train_df['totals_pageviews_mean']=train_df.groupby(['fullVisitorId'])['totals_pageviews'].transform('mean')\ntrain_df['totals_pageviews_max']=train_df.groupby(['fullVisitorId'])['totals_pageviews'].transform('max')\ntrain_df['totals_pageviews_min']=train_df.groupby(['fullVisitorId'])['totals_pageviews'].transform('min')\ntrain_df['totals_hits_mean']=train_df.groupby(['fullVisitorId'])['totals_hits'].transform('mean')\ntrain_df['totals_hits_max']=train_df.groupby(['fullVisitorId'])['totals_hits'].transform('max')\ntrain_df['totals_hits_min']=train_df.groupby(['fullVisitorId'])['totals_hits'].transform('min')\ntest_df['totals_pageviews_mean']=test_df.groupby(['fullVisitorId'])['totals_pageviews'].transform('mean')\ntest_df['totals_pageviews_max']=test_df.groupby(['fullVisitorId'])['totals_pageviews'].transform('max')\ntest_df['totals_pageviews_min']=test_df.groupby(['fullVisitorId'])['totals_pageviews'].transform('min')\ntest_df['totals_hits_mean']=test_df.groupby(['fullVisitorId'])['totals_hits'].transform('mean')\ntest_df['totals_hits_max']=test_df.groupby(['fullVisitorId'])['totals_hits'].transform('max')\ntest_df['totals_hits_min']=test_df.groupby(['fullVisitorId'])['totals_hits'].transform('min')"]}, {"cell_type": "code", "execution_count": 1, "id": "a91bef86", "metadata": {}, "outputs": [], "source": ["\"\"\"\ndef process_totals(data_df):\n    print(\"process totals ...\")\n    #data_df['visitNumber'] = np.log1p(data_df['visitNumber'])\n    #data_df['totals_hits'] = np.log1p(data_df['totals_hits'])\n    #data_df['totals_pageviews'] = np.log1p(data_df['totals_pageviews'].fillna(0))\n    data_df['mean_hits_per_day'] = data_df.groupby(['day'])['totals_hits'].transform('mean')\n    data_df['sum_hits_per_day'] = data_df.groupby(['day'])['totals_hits'].transform('sum')\n    data_df['max_hits_per_day'] = data_df.groupby(['day'])['totals_hits'].transform('max')\n    data_df['min_hits_per_day'] = data_df.groupby(['day'])['totals_hits'].transform('min')\n    data_df['var_hits_per_day'] = data_df.groupby(['day'])['totals_hits'].transform('var')\n    data_df['mean_pageviews_per_day'] = data_df.groupby(['day'])['totals_pageviews'].transform('mean')\n    data_df['sum_pageviews_per_day'] = data_df.groupby(['day'])['totals_pageviews'].transform('sum')\n    data_df['max_pageviews_per_day'] = data_df.groupby(['day'])['totals_pageviews'].transform('max')\n    data_df['min_pageviews_per_day'] = data_df.groupby(['day'])['totals_pageviews'].transform('min')    \n    return data_df\ntrain_df = process_totals(train_df)\ntest_df = process_totals(test_df)\n\"\"\""]}, {"cell_type": "code", "execution_count": 1, "id": "99ccbf28", "metadata": {}, "outputs": [], "source": ["train_df.drop(['trafficSource_referralPath', 'trafficSource_source'], axis=1, inplace=True)\ntest_df.drop(['trafficSource_referralPath', 'trafficSource_source'], axis=1, inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "5434cbf6", "metadata": {}, "outputs": [], "source": ["excluded_features = [\n    'date','fullVisitorId', 'sessionId','classfication_target','totals_totalTransactionRevenue','totals_transactionRevenue',\n    'visitId', 'visitStartTime', 'vis_date', 'nb_sessions', 'max_visits','next_session_1','next_session_2'\n]\ncategorical_features = [\n    _f for _f in train_df.columns\n    if (_f not in excluded_features) & (train_df[_f].dtype == 'object')\n]\none_hot_features = ['day','month','weekday']\n#'totals.totalTransactionRevenue','totals.TransactionRevenue','classfication_target'"]}, {"cell_type": "markdown", "id": "6a412089", "metadata": {}, "source": ["## Process one hot encoding on time "]}, {"cell_type": "code", "execution_count": 1, "id": "c2244a0b", "metadata": {}, "outputs": [], "source": ["\nfor i in one_hot_features:\n    print(\"Process feature =====>\"+str(i))\n    train_df[\"one_hot_feature\"] = train_df[i]\n    train_df[\"one_hot_feature\"] =  str(i) + \".\" + train_df[\"one_hot_feature\"].astype('str')\n    one_hot_combine = pd.get_dummies(train_df[\"one_hot_feature\"])\n    print(one_hot_combine.shape)\n    train_df = train_df.join(one_hot_combine)\n    del train_df[\"one_hot_feature\"]\n    del train_df[i]\n    del one_hot_combine\n    print(train_df.shape)\n"]}, {"cell_type": "markdown", "id": "cb25fb57", "metadata": {}, "source": ["### Factoriza  categorical featuers"]}, {"cell_type": "code", "execution_count": 1, "id": "6278c28c", "metadata": {}, "outputs": [], "source": ["\nfor f in categorical_features:\n    train_df[f], indexer = pd.factorize(train_df[f])\n    test_df[f] = indexer.get_indexer(test_df[f])\n"]}, {"cell_type": "code", "execution_count": 1, "id": "764f7d2c", "metadata": {}, "outputs": [], "source": ["train_df.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "9c253c75", "metadata": {}, "outputs": [], "source": ["gc.collect()"]}, {"cell_type": "markdown", "id": "a865ec2f", "metadata": {}, "source": ["## Split Validate and Train data by timeframe"]}, {"cell_type": "markdown", "id": "6a308c20", "metadata": {}, "source": ["## Training Set\n* Training period set 1==> 2016/08/01 ~ 2017/1/15 (5.5 month)\n* Target period set 1  ==> 2017/03/1 ~ 2017/04/30 (2 month)\n* Training period set 2==> 2017/06/01 ~ 2017/11/15 (5.5 month)\n* Target period set 2  ==> 2018/1/1 ~ 2018/02/30 (2 month)"]}, {"cell_type": "code", "execution_count": 1, "id": "68237128", "metadata": {}, "outputs": [], "source": ["train_df['date'].max(),train_df['date'].min()"]}, {"cell_type": "code", "execution_count": 1, "id": "c5b4319d", "metadata": {}, "outputs": [], "source": ["test_df['date'].max(),test_df['date'].min()"]}, {"cell_type": "markdown", "id": "c09a6def", "metadata": {}, "source": ["## Training period"]}, {"cell_type": "code", "execution_count": 1, "id": "c4a6c900", "metadata": {}, "outputs": [], "source": ["train_period_1 = train_df[(train_df['date']<=pd.datetime(2017,1,15)) & (train_df['date']>=pd.datetime(2016,8,1))]\ntrain_predict_preiod_1 = train_df[(train_df['date']<=pd.datetime(2017,4,30)) & (train_df['date']>=pd.datetime(2017,3,1))]\ntrain_period_2 = train_df[(train_df['date']<=pd.datetime(2017,11,15)) & (train_df['date']>=pd.datetime(2017,6,1))]\ntrain_predict_preiod_2 = train_df[(train_df['date']<=pd.datetime(2018,2,28)) & (train_df['date']>=pd.datetime(2018,1,1))]"]}, {"cell_type": "markdown", "id": "9cb19375", "metadata": {}, "source": ["## Valid period"]}, {"cell_type": "code", "execution_count": 1, "id": "2a38a5d5", "metadata": {}, "outputs": [], "source": ["valid_period = train_df[(train_df['date']<=pd.datetime(2017,10,15)) & (train_df['date']>=pd.datetime(2017,5,1))]\nvalid_predict_preiod = train_df[(train_df['date']<=pd.datetime(2018,1,31)) & (train_df['date']>=pd.datetime(2017,12,1))]"]}, {"cell_type": "code", "execution_count": 1, "id": "1ee37474", "metadata": {}, "outputs": [], "source": ["print('train_period1_shape',train_period_1.shape) \nprint('train_target1_period_shape',train_predict_preiod_1.shape)\nprint('train_period2_shape',train_period_2.shape) \nprint('train_target2_period_shape',train_predict_preiod_2.shape)\nprint('valid_period_shape',valid_period.shape) \nprint('valid_target_period_shape',valid_predict_preiod.shape)"]}, {"cell_type": "markdown", "id": "9cf3ce77", "metadata": {}, "source": ["## Add the target on training data and validation data"]}, {"cell_type": "code", "execution_count": 1, "id": "b9e09dd8", "metadata": {}, "outputs": [], "source": ["def add_target(train_period,target_period):\n    \n    train_period['totals_totalTransactionRevenue'] = train_period['totals_totalTransactionRevenue'].fillna(0).astype('float64')\n    target_period['totals_totalTransactionRevenue'] =target_period['totals_totalTransactionRevenue'].fillna(0).astype('float64')\n    train_period['totals_transactionRevenue'] = train_period['totals_transactionRevenue'].fillna(0).astype('float64')\n    target_period['totals_transactionRevenue'] = target_period['totals_transactionRevenue'].fillna(0).astype('float64')\n    #train_period['totals_transactions'] = train_period['totals_transactions'].fillna(0).astype('float64')\n    #target_period['totals_transactions'] = target_period['totals_transactions'].fillna(0).astype('float64')\n    \n    #train_pd=train_period\n    train_pd = train_period.groupby('fullVisitorId').mean().reset_index()\n    target_pd = target_period.groupby('fullVisitorId').mean().reset_index()\n    #target_pd=target_period\n    #Find the visitors those back puchased in future period\n    train_visitors = train_pd.fullVisitorId.unique()\n    train_predict_visitors = target_pd.fullVisitorId.unique()\n    same_visitors = np.intersect1d(train_visitors, train_predict_visitors)\n    \n    #Process data type\n    \n    \n    #Process back user df\n    back_user = target_pd[(target_pd.fullVisitorId.isin(same_visitors)) & (target_pd['totals_transactionRevenue'] > 0)]\n    back_user = back_user[['fullVisitorId','totals_transactionRevenue']]\n    print('we have',len(back_user['fullVisitorId'].value_counts()),'visitors back to purchase at target periods')\n    \n    #Add target\n    train_pd['classfication_target'] = train_pd['fullVisitorId'].map(lambda x: 1 if x in list(back_user['fullVisitorId']) else 0)\n    train_pd['totals_totalTransactionRevenue'] = np.log1p(train_pd['totals_totalTransactionRevenue'])\n    train_pd['totals_transactionRevenue'] = np.log1p(train_pd['totals_transactionRevenue'])\n    print (train_pd.shape)\n    return train_pd"]}, {"cell_type": "code", "execution_count": 1, "id": "3cc45260", "metadata": {}, "outputs": [], "source": ["train_pd_1=add_target(train_period_1,train_predict_preiod_1)\ntrain_pd_2=add_target(train_period_2,train_predict_preiod_2)\nvalid_pd = add_target(valid_period,valid_predict_preiod)"]}, {"cell_type": "code", "execution_count": 1, "id": "8227cb72", "metadata": {}, "outputs": [], "source": ["train_set = pd.concat([train_pd_1,train_pd_2], axis=0)"]}, {"cell_type": "code", "execution_count": 1, "id": "af7f4414", "metadata": {}, "outputs": [], "source": ["train_set.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "df3f2f0f", "metadata": {}, "outputs": [], "source": ["excluded_features = [\n    'date','fullVisitorId', 'sessionId','classfication_target',\n    'visitId', 'visitStartTime', 'vis_date', 'nb_sessions', 'max_visits','next_session_1','next_session_2'\n]\ntrain_features = [_f for _f in train_set.columns if _f not in excluded_features ]"]}, {"cell_type": "markdown", "id": "17fea43d", "metadata": {}, "source": ["## Set K fold"]}, {"cell_type": "code", "execution_count": 1, "id": "00d952d8", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import GroupKFold\ndef get_folds(df=None, n_splits=5):\n    \"\"\"Returns dataframe indices corresponding to Visitors Group KFold\"\"\"\n    # Get sorted unique visitors\n    unique_vis = np.array(sorted(df['fullVisitorId'].unique()))\n\n    # Get folds\n    folds = GroupKFold(n_splits=n_splits)\n    fold_ids = []\n    ids = np.arange(df.shape[0])\n    for trn_vis, val_vis in folds.split(X=unique_vis, y=unique_vis, groups=unique_vis):\n        fold_ids.append(\n            [\n                ids[df['fullVisitorId'].isin(unique_vis[trn_vis])],\n                ids[df['fullVisitorId'].isin(unique_vis[val_vis])]\n            ]\n        )\n\n    return fold_ids"]}, {"cell_type": "code", "execution_count": 1, "id": "013fdbd7", "metadata": {}, "outputs": [], "source": ["y_target = train_set['classfication_target']\nvalid_target = valid_pd['classfication_target']"]}, {"cell_type": "markdown", "id": "b95b4e4f", "metadata": {}, "source": ["## Start training (5 fold LightGBM)"]}, {"cell_type": "code", "execution_count": 1, "id": "c955b182", "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import TimeSeriesSplit, KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nfrom sklearn.model_selection import GroupKFold\nparams = {\n    \"max_bin\": 512,\n    \"learning_rate\": 0.02,\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"binary\",\n    \"metric\": \"binary_logloss\",\n    \"num_leaves\": 10,\n    \"min_data\": 100,\n    \"boost_from_average\": True\n}\nn_fold = 5\n#print(train_features)\nfolds = get_folds(df=train_set, n_splits=5)\n\nmodel = lgb.LGBMClassifier(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\n\noof_reg_preds = np.zeros(train_set.shape[0])\nprediction = np.zeros(valid_pd.shape[0])\n\nfor fold_n, (trn_, val_) in enumerate(folds):\n    print('Fold:', fold_n)\n    #print(f'Train samples: {len(train_index)}. Valid samples: {len(test_index)}')\n    trn_x, trn_y = train_set[train_features].iloc[trn_], y_target.iloc[trn_]\n    val_x, val_y = train_set[train_features].iloc[val_], y_target.iloc[val_]\n    \n\n    model.fit(trn_x, trn_y, \n            eval_set=[(trn_x, trn_y), (val_x, val_y)], eval_metric='AUC',\n            verbose=500, early_stopping_rounds=100)\n    \n    oof_reg_preds[val_] = model.predict(val_x, num_iteration=model.best_iteration_)\n    \n    pred = model.predict(valid_pd[train_features], num_iteration=model.best_iteration_)\n    prediction += pred\n    \nprediction /= n_fold\n#print(accuracy_score(y_target,np.float64(oof_reg_preds>=0.5)))\n#print(accuracy_score(valid_target,np.float64(prediction>=0.5)))\n"]}, {"cell_type": "markdown", "id": "c83d3516", "metadata": {}, "source": ["## Plot feature important"]}, {"cell_type": "code", "execution_count": 1, "id": "8e9bcdb5", "metadata": {}, "outputs": [], "source": ["lgb.plot_importance(model, figsize=(15, 10))\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "35ec8cf4", "metadata": {}, "outputs": [], "source": ["prediction_ans = np.where(prediction >= 0.2, 1, 0)\n#valid_ans = np.where(prediction>=0.5,1,0)"]}, {"cell_type": "code", "execution_count": 1, "id": "c65200a1", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(16,6))\nfalse_positive_rate, recall, thresholds = roc_curve(y_target, oof_reg_preds)\nroc_auc = auc(false_positive_rate, recall)\nplt.subplot(121)\nplt.title('Receiver Operating Characteristic (ROC)_train')\nplt.plot(false_positive_rate, recall, 'b', label = 'AUC = %0.3f' %roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1], [0,1], 'r--')\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.0])\nplt.ylabel('Recall')\nplt.xlabel('Fall-out (1-Specificity)')\n\nfalse_positive_rate, recall, thresholds = roc_curve(valid_target, prediction_ans)\nroc_auc = auc(false_positive_rate, recall)\nplt.subplot(122)\nplt.title('Receiver Operating Characteristic (ROC)_Valid')\nplt.plot(false_positive_rate, recall, 'b', label = 'AUC = %0.3f' %roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1], [0,1], 'r--')\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.0])\nplt.ylabel('Recall')\nplt.xlabel('Fall-out (1-Specificity)')\nplt.show()"]}, {"cell_type": "markdown", "id": "0bfc4b53", "metadata": {}, "source": ["## Plot confusion matrix of prediction"]}, {"cell_type": "code", "execution_count": 1, "id": "7054e23c", "metadata": {}, "outputs": [], "source": ["import seaborn as sns\n#Print Confusion Matrix\nplt.figure(figsize=(16,6))\ncm1 = confusion_matrix(y_target, oof_reg_preds)\nlabels = ['0', '1']\nplt.subplot(121)\nsns.heatmap(cm1, xticklabels = labels, yticklabels = labels, annot = True, fmt='d', cmap=\"Blues\", vmin = 0.2);\nplt.title('Confusion Matrix_train')\nplt.ylabel('True Class')\nplt.xlabel('Predicted Class')\n\ncm2 = confusion_matrix(valid_target, prediction_ans)\nlabels = ['0', '1']\nplt.subplot(122)\nsns.heatmap(cm2, xticklabels = labels, yticklabels = labels, annot = True, fmt='d', cmap=\"Blues\", vmin = 0.2);\nplt.title('Confusion Matrix_valid')\nplt.ylabel('True Class')\nplt.xlabel('Predicted Class')\nplt.show()\n"]}, {"cell_type": "markdown", "id": "c6999207", "metadata": {}, "source": ["## Conclusion \n* We only can see there are only 5 true positives labels....   \n* Try find the key feature, and do another feature enginnering for the future predict \n* Did anyone have better idea and improve AUC for classification?\n\n## Next Step\n* Doing regression for future revenue prediction...\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}