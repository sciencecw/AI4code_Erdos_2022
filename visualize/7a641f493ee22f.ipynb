{"cells": [{"cell_type": "markdown", "id": "1608c7dc", "metadata": {}, "source": ["# Import Libraries"]}, {"cell_type": "code", "execution_count": 1, "id": "7f58d7e8", "metadata": {}, "outputs": [], "source": ["import re\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\n# Porter stemmer\nfrom nltk.stem.porter import PorterStemmer\nporter = PorterStemmer()\n# Snowball stemmer\nfrom nltk.stem import SnowballStemmer\nsnowball = SnowballStemmer('english')\n# Wordnet lemmatizer\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\nimport warnings\nwarnings.filterwarnings('ignore')"]}, {"cell_type": "markdown", "id": "af4804a5", "metadata": {}, "source": ["# Explore textual data"]}, {"cell_type": "code", "execution_count": 1, "id": "adfe0c3e", "metadata": {}, "outputs": [], "source": ["tweets_data = \"../input/disaster-tweets/tweets.csv\""]}, {"cell_type": "code", "execution_count": 1, "id": "8c20ca02", "metadata": {}, "outputs": [], "source": ["# Read data\ntweets = pd.read_csv(tweets_data)\ntweets.head()"]}, {"cell_type": "markdown", "id": "df1fd249", "metadata": {}, "source": ["# Clean up Data "]}, {"cell_type": "code", "execution_count": 1, "id": "00b30563", "metadata": {}, "outputs": [], "source": ["def preprocessor(text):\n    text = re.sub('<[^>]*>', '', text)\n    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n        ' '.join(emoticons).replace('-', '')\n    return text"]}, {"cell_type": "code", "execution_count": 1, "id": "fb15daac", "metadata": {}, "outputs": [], "source": ["# apply the preprocess function to all tweets\ntweets['text'] = tweets['text'].apply(preprocessor)"]}, {"cell_type": "code", "execution_count": 1, "id": "734b8106", "metadata": {}, "outputs": [], "source": ["X = tweets['text']\ny = tweets['target']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"]}, {"cell_type": "markdown", "id": "2f5f00d1", "metadata": {}, "source": ["# Vectorization"]}, {"cell_type": "markdown", "id": "180d3ee6", "metadata": {}, "source": ["## Count Vectorizer"]}, {"cell_type": "code", "execution_count": 1, "id": "60d1e337", "metadata": {}, "outputs": [], "source": ["bag_of_words_vectorizer = CountVectorizer(min_df=5)\nbow_vectors = bag_of_words_vectorizer.fit_transform(tweets['text'])"]}, {"cell_type": "code", "execution_count": 1, "id": "51d0a306", "metadata": {}, "outputs": [], "source": ["#bag_of_words_vectorizer.vocabulary_"]}, {"cell_type": "code", "execution_count": 1, "id": "83b754bc", "metadata": {}, "outputs": [], "source": ["# For bag of words\npca = PCA(n_components=2)\nx_pca = pca.fit_transform(bow_vectors.todense())\nplt.figure(figsize=(8,6))\nplt.scatter(x_pca[:,0],x_pca[:,1],c=tweets['target'],cmap='rainbow')\nplt.xlabel('First principal component')\nplt.ylabel('Second Principal Component')"]}, {"cell_type": "markdown", "id": "6952140e", "metadata": {}, "source": ["## TF-IDF Vectorizer"]}, {"cell_type": "code", "execution_count": 1, "id": "f61cb145", "metadata": {}, "outputs": [], "source": ["TFIDF_vectorizer = TfidfVectorizer(min_df=5)\ntfidf_vectors = TFIDF_vectorizer.fit_transform(tweets['text'])"]}, {"cell_type": "code", "execution_count": 1, "id": "a0a1faa2", "metadata": {}, "outputs": [], "source": ["#TFIDF_vectorizer.get_feature_names()"]}, {"cell_type": "code", "execution_count": 1, "id": "985dbb0e", "metadata": {}, "outputs": [], "source": ["# For TFIDF vectors\npca_tfidf = PCA(n_components=2)\nx_pca = pca_tfidf.fit_transform(tfidf_vectors.todense())\nplt.figure(figsize=(8,6))\nplt.scatter(x_pca[:,0],x_pca[:,1],c=tweets['target'],cmap='rainbow')\nplt.xlabel('First principal component')\nplt.ylabel('Second Principal Component')"]}, {"cell_type": "markdown", "id": "6e640ff2", "metadata": {}, "source": ["# Pipeline (TFIDF + LR)"]}, {"cell_type": "code", "execution_count": 1, "id": "0a346644", "metadata": {}, "outputs": [], "source": ["def tokenizer(text):\n    return text.split()\n\ndef tokenizer_porter(text):\n    return [porter.stem(word) for word in text.split()]\n\ndef tokenizer_snowball(text):\n    return [snowball.stem(word) for word in text.split()]\n\ndef tokenizer_wordnet_lemmatizer(text):\n    return [lemmatizer.lemmatize(word) for word in text.split()]\n\nTFIDF_vectorizer = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)"]}, {"cell_type": "code", "execution_count": 1, "id": "2972961f", "metadata": {}, "outputs": [], "source": ["param_grid = [\n    {\n        'vect__ngram_range': [(1, 2)],\n        'vect__stop_words': [stop, None],\n        'vect__tokenizer': [tokenizer, tokenizer_porter, tokenizer_snowball, tokenizer_wordnet_lemmatizer],\n        'clf__penalty': ['l1', 'l2'],\n        'clf__C': [1.0, 10.0, 100.0]        \n    },\n    {\n        'vect__ngram_range': [(1, 2)],\n        'vect__stop_words': [stop, None],\n        'vect__tokenizer': [tokenizer, tokenizer_porter, tokenizer_snowball, tokenizer_wordnet_lemmatizer],\n        'vect__use_idf': [False],\n        'vect__norm': [None],\n        'clf__penalty': ['l1', 'l2'],\n        'clf__C': [1.0, 10.0, 100.0]\n    }\n]\n\nlr_tfidf = Pipeline([('vect', TFIDF_vectorizer),\n                     ('clf', LogisticRegression(random_state=0))])\n\ngs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid, scoring='accuracy',\n                           cv=5, verbose=1, n_jobs=-1)"]}, {"cell_type": "code", "execution_count": 1, "id": "957daf5b", "metadata": {}, "outputs": [], "source": ["gs_lr_tfidf.fit(X_train, y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "35920158", "metadata": {}, "outputs": [], "source": ["# Get the best parameters\ngs_lr_tfidf.best_params_"]}, {"cell_type": "code", "execution_count": 1, "id": "a8fd9d14", "metadata": {}, "outputs": [], "source": ["# Get the best score\ngs_lr_tfidf.best_score_"]}, {"cell_type": "code", "execution_count": 1, "id": "acbf777a", "metadata": {}, "outputs": [], "source": ["# Determine the score of the best model on the test set (We use here TFIDF vectorizer + LogisticRegression)\nclf = gs_lr_tfidf.best_estimator_\nclf.score(X_test, y_test)"]}, {"cell_type": "markdown", "id": "055de178", "metadata": {}, "source": ["# Test the Pipeline"]}, {"cell_type": "code", "execution_count": 1, "id": "0080cf89", "metadata": {}, "outputs": [], "source": ["print(tweets['text'][0])\nprint(clf.predict([preprocessor(tweets['text'][0])]))\nprint('True target: ', tweets['target'][0])"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}