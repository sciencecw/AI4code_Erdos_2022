{"cells": [{"cell_type": "markdown", "id": "2265e004", "metadata": {}, "source": ["## This is a simple baseline implementation for multinput pytorch pipeline to combine both Patient Features and Image Features . I am just trying it out . It might be wrong as well , so please take it with a pinch of salt . Inspiration is below , However , this version focuses on creating a simple pipeline with few features and simple network . \n\n![image.png](attachment:image.png)"]}, {"cell_type": "markdown", "id": "76bc885d", "metadata": {}, "source": ["## As you can mainly see , the maximum portion of code is copied from this two kernels. I removed the notes from Tarun's Kernel because , he has already made a great kernel , it will be redundant if i paste here .\n\nhttps://www.kaggle.com/tarunpaparaju/siim-isic-melanoma-eda-pytorch-baseline\n\n\nhttps://www.kaggle.com/tunguz/melanoma-classification-eda-and-modeling\n"]}, {"cell_type": "code", "execution_count": 1, "id": "36a35982", "metadata": {}, "outputs": [], "source": ["!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py"]}, {"cell_type": "code", "execution_count": 1, "id": "36e27a56", "metadata": {}, "outputs": [], "source": ["!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev"]}, {"cell_type": "code", "execution_count": 1, "id": "ab57d295", "metadata": {}, "outputs": [], "source": ["!export XLA_USE_BF16=1\n!pip install -q colored\n!pip install -q efficientnet_pytorch"]}, {"cell_type": "code", "execution_count": 1, "id": "6bebd616", "metadata": {}, "outputs": [], "source": ["import os\nimport gc\nimport cv2\nimport time\nimport numpy as np\nimport pandas as pd\n\nfrom colored import fg, attr\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch import FloatTensor, LongTensor, DoubleTensor\n\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\n\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import Dataset, DataLoader, sampler\nfrom torch.utils.data.distributed import DistributedSampler\n\nfrom efficientnet_pytorch import EfficientNet\nfrom albumentations import Normalize, VerticalFlip, HorizontalFlip, Compose"]}, {"cell_type": "code", "execution_count": 1, "id": "455d9fb9", "metadata": {}, "outputs": [], "source": ["W = 512\nH = 512\nFRAC = 0.25\nSPLIT = 0.8\n\nEPOCHS = 2\nLR = 1e-3, 1e-3\nBATCH_SIZE = 32\nVAL_BATCH_SIZE = 128\n\nMODEL = 'efficientnet-b3'\nTEST_IMG_PATH = '../input/siim-isic-melanoma-classification/jpeg/test/'\nTRAIN_IMG_PATH = '../input/siim-isic-melanoma-classification/jpeg/train/'"]}, {"cell_type": "code", "execution_count": 1, "id": "7b2e249e", "metadata": {}, "outputs": [], "source": ["np.random.seed(42)\ntorch.manual_seed(42)"]}, {"cell_type": "code", "execution_count": 1, "id": "56e66708", "metadata": {}, "outputs": [], "source": ["print(os.listdir('../input/siim-isic-melanoma-classification'))"]}, {"cell_type": "code", "execution_count": 1, "id": "a908696a", "metadata": {}, "outputs": [], "source": ["test_df = pd.read_csv('../input/siim-isic-melanoma-classification/test.csv')\ntrain_df = pd.read_csv('../input/siim-isic-melanoma-classification/train.csv')"]}, {"cell_type": "code", "execution_count": 1, "id": "94c2795c", "metadata": {}, "outputs": [], "source": ["test_df.head(10)"]}, {"cell_type": "code", "execution_count": 1, "id": "650a96f5", "metadata": {}, "outputs": [], "source": ["train_df.head(10)"]}, {"cell_type": "code", "execution_count": 1, "id": "7648fac2", "metadata": {}, "outputs": [], "source": ["def display_images(num):\n    sq_num = np.sqrt(num)\n    assert sq_num == int(sq_num)\n\n    sq_num = int(sq_num)\n    image_ids = os.listdir(TEST_IMG_PATH)\n    fig, ax = plt.subplots(nrows=sq_num, ncols=sq_num, figsize=(20, 20))\n\n    for i in range(sq_num):\n        for j in range(sq_num):\n            idx = i*sq_num + j\n            ax[i, j].axis('off')\n            img = cv2.cvtColor(cv2.imread(TEST_IMG_PATH + image_ids[idx]), cv2.COLOR_BGR2RGB)\n            ax[i, j].imshow(img); ax[i, j].set_title('Test Image {}'.format(idx), fontsize=12)\n\n    plt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "201a4530", "metadata": {}, "outputs": [], "source": ["display_images(36)"]}, {"cell_type": "code", "execution_count": 1, "id": "472750d7", "metadata": {}, "outputs": [], "source": ["## Create Feature \n\ntrain_df['sex_feat'] = (train_df['sex'].values == 'male')*1\ntest_df['sex_feat'] = (test_df['sex'].values == 'male')*1\n\ntrain_df['age_approx_feat'] = train_df['age_approx'].fillna(train_df['age_approx'].mean())\ntest_df['age_approx_feat'] = test_df['age_approx'].fillna(test_df['age_approx'].mean())"]}, {"cell_type": "code", "execution_count": 1, "id": "4025183b", "metadata": {}, "outputs": [], "source": ["train_df['age_approx_feat'] = train_df['age_approx_feat'] /train_df['age_approx_feat'].values.max()"]}, {"cell_type": "code", "execution_count": 1, "id": "4da712a5", "metadata": {}, "outputs": [], "source": ["test_df['age_approx_feat'] = test_df['age_approx_feat'] /test_df['age_approx_feat'].values.max()"]}, {"cell_type": "code", "execution_count": 1, "id": "c244c031", "metadata": {}, "outputs": [], "source": ["train_df['anatom_site_general_challenge_feat'] = train_df['anatom_site_general_challenge'].fillna('unknown')\ntest_df['anatom_site_general_challenge_feat'] = test_df['anatom_site_general_challenge'].fillna('unknown')"]}, {"cell_type": "code", "execution_count": 1, "id": "7cbddb2d", "metadata": {}, "outputs": [], "source": ["train_df['diagnosis'] = train_df['diagnosis'].fillna('na')\n"]}, {"cell_type": "code", "execution_count": 1, "id": "75d3d148", "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import LabelEncoder\n\nlb_make = LabelEncoder()\nfor df in [train_df,test_df]:\n    df['anatom_site_general_challenge_feat_c'] = lb_make.fit_transform(df['anatom_site_general_challenge_feat'])\n\ntrain_df['diag_aux'] = lb_make.fit_transform(train_df['diagnosis'])\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "a6bfefe6", "metadata": {}, "outputs": [], "source": ["feat_columns=['sex_feat','age_approx_feat','anatom_site_general_challenge_feat_c']"]}, {"cell_type": "code", "execution_count": 1, "id": "bbd36251", "metadata": {}, "outputs": [], "source": ["train_feat = train_df[feat_columns].copy()\ntest_feat = test_df[feat_columns].copy()"]}, {"cell_type": "code", "execution_count": 1, "id": "2701d1e4", "metadata": {}, "outputs": [], "source": ["def ToTensor(data):\n    return [FloatTensor(point) for point in data]\n\nclass SIIMFeatDataset(Dataset):\n    def __init__(self, df, aug, targ, ids, path):\n        self.df, self.targ, self.aug = df, targ, aug\n\n        self.mu = [0.485, 0.456, 0.406]\n        self.sigma = [0.229, 0.224, 0.225]\n        self.img_ids, self.img_path = ids, path\n        self.norm = Normalize(mean=self.mu, std=self.sigma, p=1)\n        self.vflip, self.hflip = VerticalFlip(p=0.5), HorizontalFlip(p=0.5)\n        \n        if self.aug: self.transformation = self.norm\n        else: self.transformation = Compose([self.norm, self.vflip, self.hflip])\n\n    def __len__(self):\n        return len(self.img_ids)\n\n    def __getitem__(self, i):\n        feat_columns=['sex_feat','age_approx_feat','anatom_site_general_challenge_feat_c'] \n        target = [self.df.target[i]] if self.targ else 0\n        feat = self.df[feat_columns].values[i]\n        image = cv2.imread(self.img_path + self.img_ids[i])\n        image = cv2.resize(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), (H, W))\n        return ToTensor([self.transformation(image=image)['image'], target,feat])"]}, {"cell_type": "code", "execution_count": 1, "id": "5e253df3", "metadata": {}, "outputs": [], "source": ["### Checking the dataset function\ntrain_ids = train_df.image_name.apply(lambda x: x + '.jpg')\n\ntrain_set = SIIMFeatDataset(train_df, True, True, train_ids, TRAIN_IMG_PATH)\nout = train_set[0]\nout"]}, {"cell_type": "code", "execution_count": 1, "id": "7aa00701", "metadata": {}, "outputs": [], "source": ["def GlobalAveragePooling(x):\n    return x.mean(axis=-1).mean(axis=-1)\n\nclass CancerNet(nn.Module):\n    def __init__(self, features):\n        super(CancerNet, self).__init__()\n        self.avgpool = GlobalAveragePooling\n        self.dense_output = nn.Linear(features, 1)\n        self.efn = EfficientNet.from_pretrained(MODEL)\n        \n    def forward(self, x):\n        x = x.view(-1, 3, H, W)\n        x = self.efn.extract_features(x)\n        return self.dense_output(self.avgpool(x))"]}, {"cell_type": "code", "execution_count": 1, "id": "d7895b53", "metadata": {}, "outputs": [], "source": ["class CancerNet2(nn.Module):\n    def __init__(self, features,num_patient_feat):\n        super(CancerNet2, self).__init__()\n        self.avgpool = GlobalAveragePooling\n        self.dense_output = nn.Linear(features, 64)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.num_patient_feat = num_patient_feat \n        self.l0=nn.Linear(self.num_patient_feat,64)\n        self.bn2 = nn.BatchNorm1d(64)\n        self.dropout = nn.Dropout(0.3)\n        self.conc_feat =128\n        self.final_output = nn.Linear(128,1)\n        self.efn = EfficientNet.from_pretrained(MODEL)\n        \n    def forward(self, x,train_patient_feat):\n        x = x.view(-1, 3, H, W)\n        x = self.efn.extract_features(x)\n\n        x = self.avgpool(x)\n       \n        x = self.dense_output(x)\n        x=  self.dropout(x)\n        x_l0 = self.bn2(F.relu(self.l0(train_patient_feat)))\n        x_l0 = self.dropout(x_l0)\n\n        x = torch.cat((x,x_l0),1)\n        \n        \n        x = self.final_output(x)\n        return x"]}, {"cell_type": "code", "execution_count": 1, "id": "30b7c8e8", "metadata": {}, "outputs": [], "source": ["## Check Net\nimport torch.nn.functional as F\ndevice = xm.xla_device()\ntrain_feat_dummy =torch.Tensor(np.random.randn(32,3)).to(device)\nimg_dummy = torch.Tensor(np.random.randn(32,3,512,512)).to(device)\nnet=CancerNet2(1536,3).to(device)\nout = net(img_dummy,train_feat_dummy)"]}, {"cell_type": "code", "execution_count": 1, "id": "18c282ff", "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import accuracy_score, roc_auc_score\n\ndef bce(y_true, y_pred):\n    return nn.BCEWithLogitsLoss()(y_pred, y_true)\n\n\ndef roc_auc(y_true, y_pred):\n    y_true = y_true.squeeze().cpu().detach().numpy()\n    y_pred = nn.Sigmoid()(y_pred.squeeze()).cpu().detach().numpy()\n    return roc_auc_score(y_true,y_pred)"]}, {"cell_type": "code", "execution_count": 1, "id": "7d4f3bf8", "metadata": {}, "outputs": [], "source": ["def print_metric(data, batch, epoch, start, end, metric, typ):\n    t = typ, metric, \"%s\", data, \"%s\"\n    if typ == \"Train\": pre = \"BATCH %s\" + str(batch-1) + \"%s  \"\n    if typ == \"Val\": pre = \"\\nEPOCH %s\" + str(epoch+1) + \"%s  \"\n    time = np.round(end - start, 1); time = \"Time: %s{}%s s\".format(time)\n    fs = [(fg(211), attr('reset')), (fg(212), attr('reset')), (fg(213), attr('reset'))]\n    xm.master_print(pre % fs[0] + \"{} {}: {}{}{}\".format(*t) % fs[1] + \"  \" + time % fs[2])"]}, {"cell_type": "code", "execution_count": 1, "id": "ce3a912d", "metadata": {}, "outputs": [], "source": ["class ImbalancedSampler(sampler.Sampler):\n\n    def __len__(self):\n        return self.num_samples\n    \n    def _get_label(self, dataset, idx):\n        return dataset.df[\"target\"][idx]\n\n    def __iter__(self):\n        return (self.indices[i] for i in self._get_probs())\n    \n    def _get_weight(self, idx, count_dict):\n        return 1.0/count_dict[self._get_label(self.dataset, idx)]\n    \n    def _get_probs(self):\n        return torch.multinomial(self.weights, self.num_samples, replacement=True)\n\n    def __init__(self, dataset, indices=None, num_samples=None):\n        self.indices = list(range(len(dataset))) if indices is None else indices\n        self.num_samples = len(self.indices) if num_samples is None else num_samples\n\n        count = {}\n        self.dataset = dataset\n        for idx in self.indices:\n            label = self._get_label(dataset, idx)\n            if label in count: count[label] += 1\n            if label not in count: count[label] = 1\n\n        self.weights = DoubleTensor([self._get_weight(idx, count) for idx in self.indices])"]}, {"cell_type": "code", "execution_count": 1, "id": "968c7e62", "metadata": {}, "outputs": [], "source": ["cut = int(FRAC*len(train_df))\ntrain_df = shuffle(train_df).reset_index(drop=True).loc[:cut]\n\nsplit = int(SPLIT*len(train_df))\ntrain_df, val_df = train_df.loc[:split], train_df.loc[split:]\ntrain_df, val_df = train_df.reset_index(drop=True), val_df.reset_index(drop=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "5bbb1e84", "metadata": {}, "outputs": [], "source": ["val_ids = val_df.image_name.apply(lambda x: x + '.jpg')\ntrain_ids = train_df.image_name.apply(lambda x: x + '.jpg')\n\nval_set = SIIMFeatDataset(val_df, False, True, val_ids, TRAIN_IMG_PATH)\ntrain_set = SIIMFeatDataset(train_df, True, True, train_ids, TRAIN_IMG_PATH)\n\ntrain_sampler = ImbalancedSampler(train_set)\nval_loader = DataLoader(val_set, VAL_BATCH_SIZE, shuffle=False)\ntrain_loader = DataLoader(train_set, BATCH_SIZE, sampler=train_sampler)\n\ndevice = xm.xla_device()\nnetwork = CancerNet2(features=1536,num_patient_feat=3).to(device)\noptimizer = Adam([{'params': network.efn.parameters(), 'lr': LR[0]},\n                  {'params': network.dense_output.parameters(), 'lr': LR[1]}])"]}, {"cell_type": "markdown", "id": "cccf4509", "metadata": {}, "source": ["#### Note : Something wrong with the metrics . May be someone can correct me ."]}, {"cell_type": "code", "execution_count": 1, "id": "cb85a597", "metadata": {}, "outputs": [], "source": ["start = time.time()\nxm.master_print(\"STARTING TRAINING ...\\n\")\n\nfor epoch in range(EPOCHS):\n    fonts = (fg(48), attr('reset'))\n    xm.master_print((\"EPOCH %s\" + str(epoch+1) + \"%s\") % fonts)\n    \n    batch = 1\n    network.train()\n    for train_batch in train_loader:\n        train_img, train_targ ,train_feat= train_batch\n        train_targ = train_targ.view(-1, 1)\n        train_img, train_targ,train_feat = train_img.to(device), train_targ.to(device),train_feat.to(device)\n            \n        train_preds = network.forward(train_img,train_feat)\n        train_auc = roc_auc(train_targ, train_preds)\n        train_loss = bce(train_targ, train_preds)\n            \n        optimizer.zero_grad()\n        train_loss.backward()\n        xm.optimizer_step(optimizer, barrier=True)\n            \n        end = time.time()\n        batch = batch + 1\n        auc = np.round(train_auc.item(), 3)\n        if batch %1 ==0:\n            print_metric(auc, batch, 0, start, end, metric=\"roc-auc\", typ=\"Train\")\n            \n    network.eval()\n    val_loss, val_auc, val_points = 0, 0, 0\n        \n    with torch.no_grad():\n        for val_batch in tqdm(val_loader):\n            val_img, val_targ,val_feat = val_batch\n            val_targ = val_targ.view(-1, 1)\n            val_img, val_targ,val_feat = val_img.to(device), val_targ.to(device),val_feat.to(device)\n\n            val_points += len(val_targ)\n            val_preds = network.forward(val_img,val_feat)\n            val_auc += roc_auc(val_targ, val_preds).item()*len(val_preds)\n            val_loss += bce(val_targ, val_preds).item()*len(val_preds)\n        \n    end = time.time()\n    val_auc /= val_points\n    val_loss /= val_points\n    auc = np.round(val_auc, 3)\n    print_metric(auc, 0, epoch, start, end, metric=\"roc-auc\", typ=\"Val\")\n    \n    xm.master_print(\"\")\n\nxm.master_print(\"\\nENDING TRAINING ...\")"]}, {"cell_type": "markdown", "id": "5ccfb1a5", "metadata": {}, "source": ["## Visualize sample test predictions\n\n* Now since the model is trained, we will visualize predictions made on unseen test images."]}, {"cell_type": "code", "execution_count": 1, "id": "88d90e97", "metadata": {}, "outputs": [], "source": ["def display_preds(num,test_df):\n    sq_num = np.sqrt(num)\n    assert sq_num == int(sq_num)\n\n    sq_num = int(sq_num)\n    image_ids = os.listdir(TEST_IMG_PATH)\n    fig, ax = plt.subplots(nrows=sq_num, ncols=sq_num, figsize=(20, 20))\n    norm = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], p=1)\n    feat_columns=['sex_feat','age_approx_feat','anatom_site_general_challenge_feat_c'] \n    \n    for i in range(sq_num):\n        for j in range(sq_num):\n            idx = i*sq_num + j\n            ax[i, j].axis('off')\n            pred_dict = {0: '\"No-Melanoma\"', 1: '\"Melanoma\"'}\n            print(image_ids[idx])\n            img = cv2.resize(cv2.cvtColor(cv2.imread(TEST_IMG_PATH + image_ids[idx]), cv2.COLOR_BGR2RGB), (H, W))\n            pred = nn.Sigmoid()(network.forward(FloatTensor(norm(image=img)['image'].reshape(1, 3, H, W)).to(device),FloatTensor(test_df.loc[test_df.image_name==image_ids[idx].split('.')[0]][feat_columns].values).to(device)))\n            ax[i, j].imshow(img); ax[i, j].set_title('Prediction: {}'.format(pred_dict[round(pred.item())]), fontsize=12)\n\n    plt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "2d17635e", "metadata": {}, "outputs": [], "source": ["display_preds(16,test_df)"]}, {"cell_type": "markdown", "id": "88479178", "metadata": {}, "source": ["## Run inference on the test data\n\n* Next I will run inference on the test data and store the test predictions in a list.\n* These predictions are logits and will be converted to probabilities later using <code>sigmoid</code>."]}, {"cell_type": "code", "execution_count": 1, "id": "0a7b9ef3", "metadata": {}, "outputs": [], "source": ["def sigmoid(x):\n    return 1/(1 + np.exp(-x))\n\ntest_ids = test_df.image_name.apply(lambda x: x + '.jpg')\ntest_set = SIIMFeatDataset(test_df, False, False, test_ids, TEST_IMG_PATH)\ntest_loader = tqdm(DataLoader(test_set, VAL_BATCH_SIZE, shuffle=False))\n\nnetwork.eval()\ntest_preds = []\nwith torch.no_grad():\n    for test_batch in test_loader:       \n        test_img,label, test_feat = test_batch\n        test_img = test_img.to(device)\n        test_feat = test_feat.to(device)\n        test_preds.extend(network.forward(test_img,test_feat).squeeze().detach().cpu().numpy())"]}, {"cell_type": "code", "execution_count": 1, "id": "3e2bb40b", "metadata": {}, "outputs": [], "source": ["path = '../input/siim-isic-melanoma-classification/'\nsample_submission = pd.read_csv(path + 'sample_submission.csv')"]}, {"cell_type": "code", "execution_count": 1, "id": "c49747a8", "metadata": {}, "outputs": [], "source": ["sample_submission.target = sigmoid(np.array(test_preds))"]}, {"cell_type": "code", "execution_count": 1, "id": "0ac25b4d", "metadata": {}, "outputs": [], "source": ["sample_submission.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "4ae61512", "metadata": {}, "outputs": [], "source": ["sample_submission.to_csv('submission.csv', index=False)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}