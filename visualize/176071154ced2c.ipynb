{"cells": [{"cell_type": "code", "execution_count": 1, "id": "63b80c4e", "metadata": {}, "outputs": [], "source": ["import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))"]}, {"cell_type": "markdown", "id": "0d8962b0", "metadata": {}, "source": ["<h1 style=\"color:MidnightBlue; opacity: 0.8; font-size:250%; text-align:center; border-radius:7px;\">RFM Customer Segmentation & Cohort Analysis</h1>\n\n\n* [<span style=\"color:LightCoral\"><font size=\"5\">Greeting the Project!</font></span>](#0)\n\n\n* [<span style=\"color:LightCoral\"><font size=\"5\">About DATA</font></span>](#1)\n\n\n* [<span style=\"color:LightCoral\"><font size=\"5\">What are We Gonna Do</font></span>](#2)\n\n\n* [<span style=\"color:LightCoral\"><font size=\"5\">Structure of Work</font></span>](#3)\n\n\n* [<span style=\"color:LightCoral\"><font size=\"5\">1. Data Cleaning & Exploratory Data Analysis</font></span>](#4)\n\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">1.1 Import Modules, Load Data & Data Review</font></span>](#4.1)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">1.2 Take a look at relationships between InvoiceNo, Quantity and UnitPrice columns.</font></span>](#4.2)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">1.3 What does the letter \"C\" in the invoiceno column mean?</font></span>](#4.3)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">1.4 Handling Missing Values</font></span>](#4.4)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">1.5 Clean the Data from the Noise and Missing Values</font></span>](#4.5)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">1.6 Explore the Orders</font></span>](#4.6)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">1.7 Explore Customers by Country</font></span>](#4.7)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">1.8 Explore the UK Market</font></span>](#4.8)\n    \n\n* [<span style=\"color:LightCoral\"><font size=\"5\">2. RFM Analysis</font></span>](#5)\n\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">2.1 What is RFM Analysis and Overview</font></span>](#5.1)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">2.2 Review \"df_uk\" DataFrame.</font></span>](#5.2)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">2.3 Calculate Recency</font></span>](#5.3)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">2.4 Calculate Frequency</font></span>](#5.4)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">2.5 Calculate Monetary Values</font></span>](#5.5)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">2.6 Create RFM Table</font></span>](#5.6)\n        \n\n* [<span style=\"color:LightCoral\"><font size=\"5\">3. Customer Segmentation with RFM Scores</font></span>](#6)\n\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">3.1 Calculate RFM Scoring</font></span>](#6.1)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">3.2 Creating the RFM Segmentation Table.</font></span>](#6.2)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">3.3 Plot RFM Segments</font></span>](#6.3)\n\n\n* [<span style=\"color:LightCoral\"><font size=\"5\">4. Applying K-Means Clustering</font></span>](#7)\n\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">4.1 Data Pre-Processing and Exploring</font></span>](#7.1)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">4.2 Define and Plot Feature Correlations</font></span>](#7.2)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">4.3 Visualize Feature Distributions</font></span>](#7.3)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">4.4 Data Normalization</font></span>](#7.4)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">4.5 K-Means Implementation</font></span>](#7.5)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">4.6 Define Optimal Cluster Number (K) by using \"Elbow Method\" and \"Silhouette Analysis\"</font></span>](#7.6)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">4.7 Visualize the Clusters</font></span>](#7.7)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">4.8 Assign the label</font></span>](#7.8)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">4.9 Conclusion</font></span>](#7.9)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">4.10 Discussion</font></span>](#7.10)\n\n\n* [<span style=\"color:LightCoral\"><font size=\"5\">5. Create Cohort and Conduct Cohort Analysis</font></span>](#8)\n\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">5.1 Future Engineering</font></span>](#8.1)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">5.2 Extract the Month of the Purchase</font></span>](#8.2)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">5.3 Calculating time offset in Months i.e. Cohort Index</font></span>](#8.3)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">5.4 Create 1st Cohort: User Number & Retention Rate</font></span>](#8.4)\n        - [<span style=\"color:#19A4E6; opacity:1\"><font size=\"3\">5.4.1 Pivot Cohort and Cohort Retention</font></span>](#8.4.1)\n        - [<span style=\"color:#19A4E6; opacity:1\"><font size=\"3\">5.4.2 Visualize analysis of cohort 1 using seaborn and matplotlib\"</font></span>](#8.4.2)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">5.5 Create 2nd Cohort: Average Quantity Sold</font></span>](#8.5)\n        - [<span style=\"color:#19A4E6; opacity:1\"><font size=\"3\">5.5.1 Pivot Cohort and Cohort Retention</font></span>](#8.5.1)\n        - [<span style=\"color:#19A4E6; opacity:1\"><font size=\"3\">5.5.2 Visualize analysis of cohort 2 using seaborn and matplotlib modules </font></span>](#8.5.2)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">5.6 Create 3rd Cohort: Average Sales</font></span>](#8.6)\n        - [<span style=\"color:#19A4E6; opacity:1\"><font size=\"3\">5.6.1 Pivot Cohort and Cohort Retention</font></span>](#8.6.1)\n        - [<span style=\"color:#19A4E6; opacity:1\"><font size=\"3\">5.6.2 Visualize analysis of cohort 3 using seaborn and matplotlib</font></span>](#8.6.2)\n \n \n* [<span style=\"color:LightCoral\"><font size=\"5\">6. Coclusion</font></span>](#9)\n\n\n* [<span style=\"color:LightCoral\"><font size=\"5\">7. References</font></span>](#10)"]}, {"cell_type": "markdown", "id": "aeed9312", "metadata": {}, "source": ["# <span style=\"color:LightCoral; background-color:#F5EEF8\"><font size=\"6\">Hello Everyone!</font></span> <a id=\"0\"></a>\n\nWelcome to \"RFM Customer Segmentation & Cohort Analysis\". \n\nIn this project, I will show you what is RFM and how to apply RFM Analysis and Customer Segmentation using K-Means Clustering. Also, with this project I will show you fundamentals of Data Analysis process such as Data Cleaning, Data Visualization and Exploratory Data Analysis capabilities. And as the name of the project suggests, I will show and create RFM , Cohort and Conduct Cohort Analysis.analysis. \n\nEnglish and Turkish resources that may be useful to read before starting:\n- [CRM Analizi (RFM Analizi ve CLTV (M\u00fc\u015fteri Ya\u015fam Boyu De\u011feri))](https://medium.com/machine-learning-t%C3%BCrkiye/crm-analizi-rfm-analizi-ve-cltv-m%C3%BC%C5%9Fteri-ya%C5%9Fam-boyu-de%C4%9Feri-36e5c3a232b1)\n- [RFM analysis for Customer Segmentation](https://clevertap.com/blog/rfm-analysis/)\n- [RFM Analysis using BigQuery ML](https://towardsdatascience.com/rfm-analysis-using-bigquery-ml-bfaa51b83086)\n- [RFM Analysis: An Effective Customer Segmentation technique using Python](https://medium.com/capillary-data-science/rfm-analysis-an-effective-customer-segmentation-technique-using-python-58804480d232)"]}, {"cell_type": "markdown", "id": "532ed01b", "metadata": {}, "source": ["# <span style=\"color:LightCoral; background-color:#F5EEF8\"><font size=\"6\">About Data</font></span> <a id=\"1\"></a>\n\nUsing the [Online Retail dataset](https://archive.ics.uci.edu/ml/datasets/Online+Retail) from the UCI Machine Learning Repository for exploratory data analysis, ***Customer Segmentation***, ***RFM Analysis***, ***K-Means Clustering*** and ***Cohort Analysis***.\n\nThis is a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail. The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\n\nFeature Information:\n\n**InvoiceNo**: Invoice number. *Nominal*, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation. \n<br>\n**StockCode**: Product (item) code. *Nominal*, a 5-digit integral number uniquely assigned to each distinct product.\n<br>\n**Description**: Product (item) name. *Nominal*. \n<br>\n**Quantity**: The quantities of each product (item) per transaction. *Numeric*.\n<br>\n**InvoiceDate**: Invoice Date and time. *Numeric*, the day and time when each transaction was generated.\n<br>\n**UnitPrice**: Unit price. *Numeric*, Product price per unit in sterling.\n<br>\n**CustomerID**: Customer number. *Nominal*, a 5-digit integral number uniquely assigned to each customer.\n<br>\n**Country**: Country name. *Nominal*, the name of the country where each customer resides."]}, {"cell_type": "markdown", "id": "23364c3e", "metadata": {}, "source": ["# <span style=\"color:LightCoral; background-color:#F5EEF8\"><font size=\"6\">What Are We Gonna Do</font></span> <a id=\"2\"></a>\n\nFirst of all, to observe the structure of the data and missing values, you can use exploratory data analysis and data visualization techniques.\n\nYou must do descriptive analysis. Because you must understand the relationship of the features to each other and clear the noise and missing values in the data. After that, the data set will be ready for RFM analysis.\n\nBefore starting the RFM Analysis, you will be asked to do some analysis regarding the distribution of *Orders*, *Customers* and *Countries*. These analyzes will help the company develop its sales policies and contribute to the correct use of resources.\n\nYou will notice that the UK not only has the most sales revenue, but also the most customers. So you will continue to analyze only UK transactions in the next RFM Analysis, Customer Segmentation and K-Means Clustering topics.\n\nNext, you will begin RFM Analysis, a customer segmentation technique based on customers' past purchasing behavior. \n\nBy using RFM Analysis, you can enable companies to develop different approaches to different customer segments so that they can get to know their customers better, observe trends better, and increase customer retention and sales revenues.\n\nYou will calculate the Recency, Frequency and Monetary values of the customers in the RFM Analysis you will make using the data consisting of UK transactions. Ultimately, you have to create an RFM table containing these values.\n\nIn the Customer Segmentation section, you will create an RFM Segmentation Table where you segment your customers by using the RFM table. For example, you can label the best customer as \"Big Spenders\" and the lost customer as \"Lost Customer\".\n\nWe will segment the customers ourselves based on their recency, frequency, and monetary values. But can an **unsupervised learning** model do this better for us? You will use the K-Means algorithm to find the answer to this question. Then you will compare the classification made by the algorithm with the classification you have made yourself.\n\nBefore applying K-Means Clustering, you should do data pre-processing. In this context, it will be useful to examine feature correlations and distributions. In addition, the data you apply for K-Means should be normalized.\n\nOn the other hand, you should inform the K-means algorithm about the number of clusters it will predict. You will also try the *** Elbow method *** and *** Silhouette Analysis *** to find the optimum number of clusters.\n\nAfter the above operations, you will have made cluster estimation with K-Means. You should visualize the cluster distribution by using a scatter plot. You can observe the properties of the resulting clusters with the help of the boxplot. Thus you will be able to tag clusters and interpret results.\n\nFinally, you will do Cohort Analysis with the data you used at the beginning, regardless of the analysis you have done before. Cohort analysis is a subset of behavioral analytics that takes the user data and breaks them into related groups for analysis. This analysis can further be used to do customer segmentation and track metrics like retention, churn, and lifetime value.\n"]}, {"cell_type": "markdown", "id": "05b53e2f", "metadata": {}, "source": ["# <span style=\"color:LightCoral; background-color:#F5EEF8\"><font size=\"6\">Structure of Work</font></span> <a id=\"3\"></a>\n\n#### - Data Cleaning & Exploratory Data Analysis\n#### - RFM Analysis\n#### - Customer Segmentation\n#### - Applying K-Means Clustering\n#### - Create Cohort and Conduct Cohort Analysis"]}, {"cell_type": "markdown", "id": "77e27dfb", "metadata": {}, "source": ["# <span style=\"color:LightCoral; background-color:#F5EEF8\"><font size=\"6\">1. Data Cleaning and Exploratory Data Analysis</font></span> <a id=\"4\"></a>"]}, {"cell_type": "markdown", "id": "4e9ba0c6", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">1.1 Import Modules, Load Data & Data Review</font></span> <a id=\"4.1\"></a>"]}, {"cell_type": "code", "execution_count": 1, "id": "c6d0438e", "metadata": {}, "outputs": [], "source": ["!pip install missingno\n!pip install pyforest\n!pip install cufflinks\n!pip install termcolor\n#!pip install wordcloud\n!pip install squarify\n!pip install pyclustertend\n!pip install yellowbrick --user"]}, {"cell_type": "code", "execution_count": 1, "id": "b4b0a75c", "metadata": {}, "outputs": [], "source": ["# 1-Import Libraies\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n%matplotlib inline\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport missingno as msno \n\nfrom sklearn.compose import make_column_transformer\n\n# Scaling\nfrom sklearn.preprocessing import scale \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures \nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import PowerTransformer \nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\n\n# Importing plotly and cufflinks in offline mode\nimport cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\n# Ignore Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.warn(\"this will not show\")\n\n# Figure&Display options\nplt.rcParams[\"figure.figsize\"] = (10,6)\npd.set_option('max_colwidth',200)\npd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 200)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\n# makes strings colored\nimport colorama\nfrom colorama import Fore, Style\nfrom termcolor import colored\n\nimport ipywidgets\nfrom ipywidgets import interact\n\n# White grid plots for dark mode users\nsns.set_style(\"whitegrid\")"]}, {"cell_type": "code", "execution_count": 1, "id": "e2398cbc", "metadata": {}, "outputs": [], "source": ["# CSV consumes less memory than Excel. --> CSV is generally faster and less complicated when compared to Excel.\n#df = pd.read_excel(\"Online Retail.xlsx\")\n#df.to_csv('Online Retail.csv') "]}, {"cell_type": "code", "execution_count": 1, "id": "9d292409", "metadata": {}, "outputs": [], "source": ["# if you don't specified \"index_col=0\", you can not drop duplicates.\ndf=pd.read_csv('../input/online-retail-dataset/Online Retail.csv',index_col=0)\ndf.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "33a2a396", "metadata": {}, "outputs": [], "source": ["## Some Useful Functions\n\n###############################################################################\n\ndef missing_values(df):\n    missing_number = df.isnull().sum().sort_values(ascending=False)\n    missing_percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])\n    return missing_values[missing_values['Missing_Number']>0]\n\n###############################################################################\n\ndef first_looking(df):\n    print(colored(\"Shape:\", attrs=['bold']), df.shape,'\\n', \n          colored('-'*79, 'red', attrs=['bold']),\n          colored(\"\\nInfo:\\n\", attrs=['bold']), sep='')\n    print(df.info(), '\\n', \n          colored('-'*79, 'red', attrs=['bold']), sep='')\n    print(colored(\"Number of Uniques:\\n\", attrs=['bold']), df.nunique(),'\\n',\n          colored('-'*79, 'red', attrs=['bold']), sep='')\n    print(colored(\"Missing Values:\\n\", attrs=['bold']), missing_values(df),'\\n', \n          colored('-'*79, 'red', attrs=['bold']), sep='')\n    print(colored(\"All Columns:\", attrs=['bold']), list(df.columns),'\\n', \n          colored('-'*79, 'red', attrs=['bold']), sep='')\n\n    df.columns= df.columns.str.lower().str.replace('&', '_').str.replace(' ', '_')\n\n    print(colored(\"Columns after rename:\", attrs=['bold']), list(df.columns),'\\n',\n              colored('-'*79, 'red', attrs=['bold']), sep='')\n    \n        \ndef multicolinearity_control(df):\n    feature =[]\n    collinear=[]\n    for col in df.corr().columns:\n        for i in df.corr().index:\n            if (abs(df.corr()[col][i])> .9 and abs(df.corr()[col][i]) < 1):\n                    feature.append(col)\n                    collinear.append(i)\n                    print(colored(f\"Multicolinearity alert in between:{col} - {i}\", \n                                  \"red\", attrs=['bold']), df.shape,'\\n',\n                                  colored('-'*79, 'red', attrs=['bold']), sep='')\n\ndef duplicate_values(df):\n    print(colored(\"Duplicate check...\", attrs=['bold']), sep='')\n    duplicate_values = df.duplicated(subset=None, keep='first').sum()\n    if duplicate_values > 0:\n        df.drop_duplicates(keep='first', inplace=True)\n        print(duplicate_values, colored(\"Duplicates were dropped!\"),'\\n',\n              colored('-'*79, 'red', attrs=['bold']), sep='')\n    else:\n        print(colored(\"There are no duplicates\"),'\\n',\n              colored('-'*79, 'red', attrs=['bold']), sep='')     \n        \ndef drop_columns(df, drop_columns):\n    if drop_columns !=[]:\n        df.drop(drop_columns, axis=1, inplace=True)\n        print(drop_columns, 'were dropped')\n    else:\n        print(colored('We will now check the missing values and if necessary will drop realted columns!', attrs=['bold']),'\\n',\n              colored('-'*79, 'red', attrs=['bold']), sep='')\n        \ndef drop_null(df, limit):\n    print('Shape:', df.shape)\n    for i in df.isnull().sum().index:\n        if (df.isnull().sum()[i]/df.shape[0]*100)>limit:\n            print(df.isnull().sum()[i], 'percent of', i ,'null and were dropped')\n            df.drop(i, axis=1, inplace=True)\n            print('new shape:', df.shape)       \n    print('New shape after missing value control:', df.shape)\n    \n###############################################################################"]}, {"cell_type": "code", "execution_count": 1, "id": "b6908a03", "metadata": {}, "outputs": [], "source": ["first_looking(df)\nduplicate_values(df)\ndrop_columns(df,[])\ndrop_null(df, 90)\n# df.head()\n# df.tail()\n# df.sample(5)\n# df.describe().T\n# df.describe(include=object).T"]}, {"cell_type": "code", "execution_count": 1, "id": "342653e7", "metadata": {}, "outputs": [], "source": ["# df_model will be created for further analysis in case of not loosing some information about customer.\ndf_model = df.copy()"]}, {"cell_type": "code", "execution_count": 1, "id": "ccfe6e5c", "metadata": {}, "outputs": [], "source": ["df.isnull().sum()"]}, {"cell_type": "code", "execution_count": 1, "id": "7b8ee463", "metadata": {}, "outputs": [], "source": ["# Now we will drop NanN values of only \"customerid\" column.\ndf = df.dropna(subset=['customerid'])\ndf.shape"]}, {"cell_type": "markdown", "id": "cb17bf15", "metadata": {}, "source": ["- Orders which do NOT have customer ID's were not made by the customers already in the dataset because the customers who in fact made some purchases already have ID's."]}, {"cell_type": "markdown", "id": "ea95c8e9", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">1.2 Take a look at relationships between InvoiceNo, Quantity and UnitPrice columns.</font></span> <a id=\"4.2\"></a>"]}, {"cell_type": "code", "execution_count": 1, "id": "62d0e5fe", "metadata": {}, "outputs": [], "source": ["df[df['quantity'] < 0].shape"]}, {"cell_type": "code", "execution_count": 1, "id": "0b111277", "metadata": {}, "outputs": [], "source": ["df[\"quantity\"].min(), df[\"quantity\"].max()"]}, {"cell_type": "code", "execution_count": 1, "id": "c719b5d6", "metadata": {}, "outputs": [], "source": ["df[df['unitprice'] < 0].shape"]}, {"cell_type": "code", "execution_count": 1, "id": "45d71c87", "metadata": {}, "outputs": [], "source": ["df[\"unitprice\"].min(), df[\"unitprice\"].max()"]}, {"cell_type": "code", "execution_count": 1, "id": "471578fc", "metadata": {}, "outputs": [], "source": ["# We did not see any negative value in \"unitprice\" but we have \"zero\" values. \n# These are not cancelled ones, because their ID's are not starting with \"C\" letter.\n# High probability these are gifts.\ndf[(df.unitprice == 0)].shape"]}, {"cell_type": "markdown", "id": "aa0a1b39", "metadata": {}, "source": ["- We see that there are negative values in the Quantity and UnitPrice columns. These are possibly canceled and returned orders. Let's check it out."]}, {"cell_type": "code", "execution_count": 1, "id": "d185d2da", "metadata": {}, "outputs": [], "source": ["df[['unitprice', 'quantity']].describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='viridis')"]}, {"cell_type": "markdown", "id": "6d09b56d", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">1.3 What does the letter \"C\" in the InvoiceNo column mean?</font></span> <a id=\"4.3\"></a>"]}, {"cell_type": "code", "execution_count": 1, "id": "c6527ce7", "metadata": {}, "outputs": [], "source": ["df[df['invoiceno'].astype(str).str.startswith('C')]"]}, {"cell_type": "markdown", "id": "942129ea", "metadata": {}, "source": ["- If the invoice number starts with the letter \"C\", it means the order was cancelled. "]}, {"cell_type": "code", "execution_count": 1, "id": "343fc194", "metadata": {}, "outputs": [], "source": ["df[\"invoiceno\"].str.startswith('C').value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "737ca522", "metadata": {}, "outputs": [], "source": ["df[\"invoiceno\"].str.startswith('C').value_counts(normalize=True)*100"]}, {"cell_type": "code", "execution_count": 1, "id": "4a9b5503", "metadata": {}, "outputs": [], "source": ["df[df[\"invoiceno\"].str.startswith('C')] [[\"invoiceno\",\"quantity\", \"unitprice\"]]"]}, {"cell_type": "code", "execution_count": 1, "id": "880c5c1d", "metadata": {}, "outputs": [], "source": ["df[df[\"invoiceno\"].str.startswith('C')] [[\"invoiceno\",\"quantity\", \"unitprice\"]].describe()"]}, {"cell_type": "markdown", "id": "7a7bb334", "metadata": {}, "source": ["- Let's do some analysis between cancelled/non-cancelled orders and quantity."]}, {"cell_type": "code", "execution_count": 1, "id": "0c5795a1", "metadata": {}, "outputs": [], "source": ["df['cancelled'] = df['invoiceno'].str.contains('C')\ndf.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "beb073b6", "metadata": {}, "outputs": [], "source": ["df['cancelled'].value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "f35e846a", "metadata": {}, "outputs": [], "source": ["# Let's replace the True's to 1 and False's to 0.\n# df['cancelled'] = df[\"cancelled\"].replace('True',1).replace('False',0) --> This is not working\ndf['cancelled'] = df['cancelled']*1\ndf['cancelled'].value_counts()"]}, {"cell_type": "markdown", "id": "84eafc5d", "metadata": {}, "source": ["- canceled orders by Quantity > 0"]}, {"cell_type": "code", "execution_count": 1, "id": "2eb5123c", "metadata": {}, "outputs": [], "source": ["df[(df.cancelled == 1) & (df.quantity > 0)]   # returns us nothing"]}, {"cell_type": "markdown", "id": "369c55d4", "metadata": {}, "source": ["- non-canceled orders by Quantity < 0"]}, {"cell_type": "code", "execution_count": 1, "id": "de674af1", "metadata": {}, "outputs": [], "source": ["df[(df.cancelled == 0) & (df.quantity < 0)]   # returns us nothing"]}, {"cell_type": "markdown", "id": "a853ab96", "metadata": {}, "source": ["- When we filter canceled orders by Quantity > 0, or non-cancelled orders by Quantity <0 we get empty output.\n- These confirm that; negative values indicate the order has been cancelled. \n<br>So lets find out how many orders were cancelled?"]}, {"cell_type": "code", "execution_count": 1, "id": "4f00f807", "metadata": {}, "outputs": [], "source": ["df['cancelled'].value_counts()  # 1 --> cancelled based on our tortured dataset (8872)"]}, {"cell_type": "code", "execution_count": 1, "id": "b5e65c3e", "metadata": {}, "outputs": [], "source": ["df_original = pd.read_csv('../input/online-retail-dataset/Online Retail.csv',index_col=0)  # True --> cancelled based on our original dataset (9288)\ndf_original[\"InvoiceNo\"].str.startswith('C').value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "911870a1", "metadata": {}, "outputs": [], "source": ["9288 / len(df_original)"]}, {"cell_type": "code", "execution_count": 1, "id": "b72a555b", "metadata": {}, "outputs": [], "source": ["8872/len(df)"]}, {"cell_type": "code", "execution_count": 1, "id": "41bdf193", "metadata": {}, "outputs": [], "source": ["# nunique(): returns number of unique elements in the specified column. It excludes NaN values by default.\n# Don\u2019t include NaN in the count.\ndf[df.cancelled == 1]['customerid'].nunique() / df['customerid'].nunique()  # ratio obtained based on our tortured dataset "]}, {"cell_type": "code", "execution_count": 1, "id": "f90243b8", "metadata": {}, "outputs": [], "source": ["# ratio obtained based on our original dataset \ndf_original[df_original[\"InvoiceNo\"].str.startswith('C')]['CustomerID'].nunique() / df_original['CustomerID'].nunique()"]}, {"cell_type": "code", "execution_count": 1, "id": "16b94cd5", "metadata": {}, "outputs": [], "source": ["# We will not use cancelled column again\ndf = df.drop(['cancelled'],axis=1)"]}, {"cell_type": "markdown", "id": "12b84baf", "metadata": {}, "source": ["- 9288 or 1.7% (Original dataset) / 8872 or 2% (Tortured dataset) orders were cancelled.  \n- 36% --> cancelled and at the same time [\"customerid\"].nunique() ratio \n- Looking deeper into why these orders were cancelled may prevent future cancellations. Now let's find out what a negative UnitPrice means.\n"]}, {"cell_type": "markdown", "id": "7b157843", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">1.4 Handling Missing Values</font></span> <a id=\"4.4\"></a>"]}, {"cell_type": "code", "execution_count": 1, "id": "bc350c23", "metadata": {}, "outputs": [], "source": ["df.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "25890686", "metadata": {}, "outputs": [], "source": ["missing_values(df)"]}, {"cell_type": "markdown", "id": "c13ea02b", "metadata": {}, "source": ["- Let's assume that the orders which do NOT have customer ID's were not made by the customers already in the dataset because the customers who in fact made some purchases already have ID's.\n- So we don't want to assign these orders to those customers because this would alter the insights we draw from the data.\n"]}, {"cell_type": "code", "execution_count": 1, "id": "a052cf74", "metadata": {}, "outputs": [], "source": ["df = df.dropna(subset=['customerid'])  # we already did at the beginning"]}, {"cell_type": "markdown", "id": "33978297", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">1.5 Clean the Data from the Noise and Missing Values</font></span> <a id=\"4.5\"></a>"]}, {"cell_type": "code", "execution_count": 1, "id": "38d8ee37", "metadata": {}, "outputs": [], "source": ["# So, with this operation, lines containing canceled orders are dropped.\ndf = df[(df.quantity > 0) & (df.unitprice > 0)]\ndf.shape"]}, {"cell_type": "markdown", "id": "53807d0a", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">1.6 Explore the Orders</font></span> <a id=\"4.6\"></a>"]}, {"cell_type": "markdown", "id": "44e3dadd", "metadata": {}, "source": ["1. Find the unique number of InvoiceNo  per customer"]}, {"cell_type": "code", "execution_count": 1, "id": "99af7cf4", "metadata": {}, "outputs": [], "source": ["df.groupby('customerid')['invoiceno'].nunique()"]}, {"cell_type": "markdown", "id": "dd0e3ff8", "metadata": {}, "source": ["2. What's the average number of unqiue items per order or per customer?"]}, {"cell_type": "markdown", "id": "e9c15353", "metadata": {}, "source": ["- *1.Way (Detailed)*"]}, {"cell_type": "code", "execution_count": 1, "id": "459b4d5d", "metadata": {}, "outputs": [], "source": ["# This code gives us very good insight but very detailed information. \n# In the next lines we get a more general and understandable output. \n# There is no difference between including the 'invoiceno' column or not. It's just a matter of details.\ndf.groupby(['customerid', \"invoiceno\", 'stockcode', 'description'])['quantity'].mean() # Length: 387843"]}, {"cell_type": "code", "execution_count": 1, "id": "706f22a2", "metadata": {}, "outputs": [], "source": ["df.groupby(['customerid', \"invoiceno\", 'stockcode'])['quantity'].mean()  # Length: 387841 --> So 2 different descriptions written"]}, {"cell_type": "markdown", "id": "4c4a0d2d", "metadata": {}, "source": ["- *2.Way(More General)*"]}, {"cell_type": "code", "execution_count": 1, "id": "8d0c75cf", "metadata": {}, "outputs": [], "source": ["df.groupby(['customerid', 'stockcode'])['quantity'].mean()"]}, {"cell_type": "code", "execution_count": 1, "id": "5f6fefb6", "metadata": {}, "outputs": [], "source": ["df.groupby(['invoiceno', 'stockcode'])['quantity'].mean()"]}, {"cell_type": "markdown", "id": "602cc258", "metadata": {}, "source": ["3. Let's see how this compares to the number of unique products per customer."]}, {"cell_type": "markdown", "id": "954573c9", "metadata": {}, "source": ["- *1.Way (Detailed)*"]}, {"cell_type": "code", "execution_count": 1, "id": "33f8a37b", "metadata": {}, "outputs": [], "source": ["# This code gives us very good insight but very detailed information. \n# In the next line we get a more general and understandable output. \n# There is no difference between including the 'invoiceno' column or not. It's just a matter of details.\ndf.groupby(['customerid', 'invoiceno', 'stockcode'])['quantity'].sum()"]}, {"cell_type": "markdown", "id": "0a233a59", "metadata": {}, "source": ["- *2.Way(More General)*"]}, {"cell_type": "code", "execution_count": 1, "id": "b0f1056e", "metadata": {}, "outputs": [], "source": ["df.groupby(['customerid', 'stockcode'])['quantity'].sum()"]}, {"cell_type": "markdown", "id": "085400d0", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">1.7 Explore Customers by Country</font></span> <a id=\"4.7\"></a>"]}, {"cell_type": "markdown", "id": "889d5f30", "metadata": {}, "source": ["1. What's the total revenue per country?"]}, {"cell_type": "markdown", "id": "53bce107", "metadata": {}, "source": ["- *Revenue means total price.*\n- *Total Price = Quantity X Unit Price*"]}, {"cell_type": "code", "execution_count": 1, "id": "70464718", "metadata": {}, "outputs": [], "source": ["# The calculation of total price or aka. revenue. \ndf['total_price'] = df['quantity'] * df['unitprice']"]}, {"cell_type": "code", "execution_count": 1, "id": "76d5f821", "metadata": {}, "outputs": [], "source": ["df.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "d983f949", "metadata": {}, "outputs": [], "source": ["# It was a very confusing outcome. Let's sort it out and get a nicer insight in the next line.\ndf.groupby('country')[[\"total_price\"]].sum().sort_values(by=\"total_price\", ascending=False)"]}, {"cell_type": "code", "execution_count": 1, "id": "827fa8c2", "metadata": {}, "outputs": [], "source": ["round(df.groupby('country')[\"total_price\"].sum().sort_values(ascending=False),3)"]}, {"cell_type": "markdown", "id": "6f57a1d2", "metadata": {}, "source": ["2. Visualize number of customer per country"]}, {"cell_type": "code", "execution_count": 1, "id": "fcdbb06d", "metadata": {}, "outputs": [], "source": ["# First let's check how many unique customerid I have\ndf[\"customerid\"].nunique()"]}, {"cell_type": "code", "execution_count": 1, "id": "03697e9b", "metadata": {}, "outputs": [], "source": ["# Hmm! But when I check again while grouping by countries, a different result comes out.\n# To understand what's that difference, read the next line.\ndf.groupby('country')[\"customerid\"].nunique().sum()"]}, {"cell_type": "code", "execution_count": 1, "id": "249f2b3a", "metadata": {}, "outputs": [], "source": ["df.groupby('customerid')['country'].nunique().value_counts()"]}, {"cell_type": "markdown", "id": "e3b91ae3", "metadata": {}, "source": ["We have 4338 unique \"customerid\". But when we are grouping them using with contry column; Our unique number of customers is increasing. <br>It means some \"customerid\" (totally 8) are included in multiple country names. For ex: *'12431.0'*"]}, {"cell_type": "code", "execution_count": 1, "id": "a5b8ce22", "metadata": {}, "outputs": [], "source": ["# MATPLOTLIB\nplt.figure(figsize=(16,9))\ndf.groupby('country')[\"customerid\"].nunique().sort_values(ascending=False).plot.bar();"]}, {"cell_type": "code", "execution_count": 1, "id": "91b280f0", "metadata": {}, "outputs": [], "source": ["# SEABORN\nplt.figure(figsize=(16,9))\nsns.barplot(y = df.groupby('country')[\"customerid\"].nunique().sort_values(ascending=False).index, \n            x = df.groupby('country')[\"customerid\"].nunique().sort_values(ascending=False).values);"]}, {"cell_type": "code", "execution_count": 1, "id": "87465062", "metadata": {}, "outputs": [], "source": ["# PLOTLY BAR PLOT\nimport plotly.express as px\nplt.figure(figsize = (16,9))\nfig = px.bar(df.groupby('country')[\"customerid\"].nunique().sort_values(ascending=False),\n             x = ['customerid'], \n             y = df.groupby('country')[\"customerid\"].nunique().sort_values(ascending=False).index,\n             title=\"Number of Customers Per Country\")\nfig.update_yaxes(categoryorder='total ascending')\nfig.update_layout(xaxis_title=\"NUMBER OF CUSTOMER\",yaxis_title=\"COUNTRIES\", autosize=False, width=1400, height=700, title_font_color=\"red\",)\nfig.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "f1b7ad41", "metadata": {}, "outputs": [], "source": ["# PLOTLY TREEMAP\nfig = px.treemap(df.groupby('country')[[\"customerid\"]].nunique().sort_values(by=\"customerid\", ascending=False),\n                 path=[df.groupby('country')[[\"customerid\"]].nunique().sort_values(by=\"customerid\", ascending=False).index], \n                 values='customerid', \n                 width=1000, \n                 height=600)\nfig.update_layout(title_text='Number of Customers Per Country',\n                  title_x = 0.5, title_font = dict(size=20)\n)\nfig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\nfig.show()"]}, {"cell_type": "markdown", "id": "b9543b5c", "metadata": {}, "source": ["- Let's see what our total unique number of customers graph would look like if we didn't include United Kingdom."]}, {"cell_type": "code", "execution_count": 1, "id": "dbb07475", "metadata": {}, "outputs": [], "source": ["# MATPLOTLIB\nplt.figure(figsize=(16,9))\nsns.set_style(\"whitegrid\")\ndf.groupby('country')[\"customerid\"].nunique().sort_values(ascending=False).iloc[1:].plot.bar();"]}, {"cell_type": "code", "execution_count": 1, "id": "17ccfa80", "metadata": {}, "outputs": [], "source": ["# SEABORN\nplt.figure(figsize=(16,9))\nsns.barplot(y = df.groupby('country')[\"customerid\"].nunique().sort_values(ascending=False).iloc[1:].index, \n            x = df.groupby('country')[\"customerid\"].nunique().sort_values(ascending=False).iloc[1:].values);"]}, {"cell_type": "code", "execution_count": 1, "id": "48ec0b22", "metadata": {}, "outputs": [], "source": ["# PLOTLY BAR PLOT\nimport plotly.express as px\nplt.figure(figsize = (16,9))\nfig = px.bar(df.groupby('country')[\"customerid\"].nunique().sort_values(ascending=False),\n             x = df.groupby('country')[\"customerid\"].nunique().sort_values(ascending=False).iloc[1:], \n             y = df.groupby('country')[\"customerid\"].nunique().sort_values(ascending=False).iloc[1:].index,\n             title=\"Number of Customers Per Country Without UK\")\nfig.update_yaxes(categoryorder='total ascending')\nfig.update_layout(xaxis_title=\"NUMBER OF CUSTOMER\",yaxis_title=\"COUNTRIES\", autosize=False, width=1400, height=700, title_font_color=\"red\",)\nfig.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "54a9f89c", "metadata": {}, "outputs": [], "source": ["# PLOTLY TREEMAP\nfig = px.treemap(df.groupby('country')[[\"customerid\"]].nunique().sort_values(by=\"customerid\", ascending=False).iloc[1:],\n                 path=[df.groupby('country')[[\"customerid\"]].nunique().sort_values(by=\"customerid\", ascending=False).iloc[1:].index], \n                 values='customerid', \n                 width=1000, \n                 height=600)\nfig.update_layout(title_text='Number of Customers Per Country Without UK',\n                  title_x = 0.5, title_font = dict(size=20)\n)\nfig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\nfig.show()"]}, {"cell_type": "markdown", "id": "3b2ae6c0", "metadata": {}, "source": ["- These plots gives us the total number of unique customers for each country on a country basis.\n- In general, we can say the following at first sight: \n<br>*According to the dataset we have, United Kingdom has an overwhelming advantage compared to other countries in terms of total number of unique customers.*"]}, {"cell_type": "markdown", "id": "1a9efe2c", "metadata": {}, "source": ["3. Visualize total cost per country"]}, {"cell_type": "code", "execution_count": 1, "id": "abd62050", "metadata": {}, "outputs": [], "source": ["# We already created total price (aka. revenue) before. Let's use it!\npd.DataFrame(df.groupby('country').total_price.sum().sort_values(ascending=False)).head()"]}, {"cell_type": "code", "execution_count": 1, "id": "a6bc19e5", "metadata": {}, "outputs": [], "source": ["# Let's save this dataframe for future analysis.\ndf_total_price = pd.DataFrame(df.groupby('country')[\"total_price\"].sum().sort_values(ascending=False))"]}, {"cell_type": "code", "execution_count": 1, "id": "05510e35", "metadata": {}, "outputs": [], "source": ["# MATPLOTLIB\n# Attention to up left corner of plot --> 1e6 means 10**6\ndf_total_price.plot(kind=\"bar\", width=0.7, color='lightgreen', edgecolor='darkgreen', figsize=(16,9));"]}, {"cell_type": "code", "execution_count": 1, "id": "27789e07", "metadata": {}, "outputs": [], "source": ["# SEABORN\nplt.figure(figsize=(16,9))\n# plt.xlim(145.920, 7285024.644) # To see and understand the_total price's range.\nsns.barplot(data = df_total_price, x = df_total_price[\"total_price\"], y = df_total_price.index);"]}, {"cell_type": "code", "execution_count": 1, "id": "e7788867", "metadata": {}, "outputs": [], "source": ["# PLOTLY BAR PLOT\nimport plotly.express as px\nplt.figure(figsize = (16,9))\nfig = px.bar(df_total_price, \n             x = \"total_price\", \n             y = df_total_price.index, \n             title=\"Total Prices Per Country\")\nfig.update_yaxes(categoryorder='total ascending')\nfig.update_layout(xaxis_title=\"TOTAL_PRICE\",yaxis_title=\"COUNTRIES\", autosize=False, width=1400, height=700, title_font_color=\"red\",)\nfig.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "b3b66041", "metadata": {}, "outputs": [], "source": ["# PLOTLY TREEMAP\nfig = px.treemap(df_total_price,\n                 path=[df_total_price.index], \n                 values='total_price', \n                 width=1000, \n                 height=600)\nfig.update_layout(title_text='Total Prices By Countries',\n                  title_x = 0.5, title_font = dict(size=20)\n)\nfig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\nfig.show()"]}, {"cell_type": "markdown", "id": "2e99d3ef", "metadata": {}, "source": ["- Let's see what our total price graph would look like if we didn't include United Kingdom."]}, {"cell_type": "code", "execution_count": 1, "id": "b55ef16e", "metadata": {}, "outputs": [], "source": ["# MATPLOTLIB\n# Attention to up left corner of plot --> 1e6 means 10**6\ndf_total_price.iloc[1:].plot(kind=\"bar\", width=0.7, color='lightgreen', edgecolor='darkgreen', figsize=(16,9));"]}, {"cell_type": "code", "execution_count": 1, "id": "bfb9a478", "metadata": {}, "outputs": [], "source": ["# SEABORN\nplt.figure(figsize = (16,9))\nsns.barplot(data = df_total_price.iloc[1:], x = df_total_price.iloc[1:][\"total_price\"], y = df_total_price.iloc[1:].index);"]}, {"cell_type": "code", "execution_count": 1, "id": "31985f0b", "metadata": {}, "outputs": [], "source": ["# PLOTLY BAR PLOT\nimport plotly.express as px\nplt.figure(figsize = (16,9))\nfig = px.bar(df_total_price.iloc[1:], x = \"total_price\", y = df_total_price.iloc[1:].index, title=\"Total Prices Per Country Without UK\")\nfig.update_yaxes(categoryorder='total ascending')\nfig.update_layout(xaxis_title=\"TOTAL_PRICE\",yaxis_title=\"COUNTRIES\", autosize=False, width=1400, height=700, title_font_color=\"red\",)\nfig.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "2f003334", "metadata": {}, "outputs": [], "source": ["# PLOTLY TREEMAP\nfig = px.treemap(df_total_price.iloc[1:],\n                 path=[df_total_price.iloc[1:].index], \n                 values='total_price', \n                 width=1000, \n                 height=600)\nfig.update_layout(title_text='Total Prices By Countries Without UK',\n                  title_x = 0.5, title_font = dict(size=20)\n)\nfig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\nfig.show()"]}, {"cell_type": "markdown", "id": "6aabf291", "metadata": {}, "source": ["- Result: \n<br>The UK is not only the country with the most sales revenue, but also the country with the most customers.\n<br>The majority of this dataset includes orders from the UK.\n<br>So we can further explore the UK market by learning which products customers buy together and other buying behaviors to improve our sales and targeting strategy."]}, {"cell_type": "markdown", "id": "f8e5bb1d", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">1.8 Explore the UK Market</font></span> <a id=\"4.8\"></a>"]}, {"cell_type": "markdown", "id": "4b0f5a37", "metadata": {}, "source": ["1. Create df_uk DataFrame"]}, {"cell_type": "code", "execution_count": 1, "id": "52c306b3", "metadata": {}, "outputs": [], "source": ["df_uk = df[df[\"country\"] == 'United Kingdom']\ndf_uk.head()"]}, {"cell_type": "markdown", "id": "cdb8c5ae", "metadata": {}, "source": ["2. What are the most popular products that are bought in the UK?"]}, {"cell_type": "markdown", "id": "ccdeb3ff", "metadata": {}, "source": ["- Let's take a look at our columns related to the product, from \"Determines\" section. Here the story of our dataset.\n    - **InvoiceNo**: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.\n    - **StockCode**: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.\n    - **Description**: Product (item) name. Nominal.\n    - **Quantity**: The quantities of each product (item) per transaction. Numeric.\n    - **InvoiceDate**: Invoice Date and time. Numeric, the day and time when each transaction was generated.\n    - **UnitPrice**: Unit price. Numeric, Product price per unit in sterling.\n    - **CustomerID**: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.\n    - **Country**: Country name. Nominal, the name of the country where each customer resides."]}, {"cell_type": "markdown", "id": "818a4afd", "metadata": {}, "source": ["- Let's see how much of each product UK has."]}, {"cell_type": "code", "execution_count": 1, "id": "9a9470ac", "metadata": {}, "outputs": [], "source": ["df_uk[\"stockcode\"].value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "56996445", "metadata": {}, "outputs": [], "source": ["# Top 15 most owned products.\ndf_uk[\"stockcode\"].value_counts().head(15).plot(kind=\"bar\", width=0.5, color='pink', edgecolor='purple', figsize=(16,9))\nplt.xticks(rotation=45);"]}, {"cell_type": "markdown", "id": "e0b06f66", "metadata": {}, "source": ["- We will continue analyzing the UK transactions with customer segmentation.\n- **Let's start the RFM analysis.**"]}, {"cell_type": "markdown", "id": "93eb0b0d", "metadata": {}, "source": ["# <span style=\"color:LightCoral; background-color:#F5EEF8\"><font size=\"6\">2. RFM Analysis</font></span> <a id=\"5\"></a>"]}, {"cell_type": "markdown", "id": "24ab0291", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">2.1 What is RFM Analysis and Overview</font></span> <a id=\"5.1\"></a>"]}, {"cell_type": "markdown", "id": "93ddacf9", "metadata": {}, "source": ["In the age of the internet and e-commerce, companies that do not expand their businesses online or utilize digital tools to reach their customers will run into issues like scalability and a lack of digital precsence. An important marketing strategy e-commerce businesses use for analyzing and predicting customer value is customer segmentation. Customer data is used to sort customers into group based on their behaviors and preferences.\n\n**[RFM](https://www.putler.com/rfm-analysis/) (Recency, Frequency, Monetary) Analysis** is a customer segmentation technique for analyzing customer value based on past buying behavior. RFM analysis was first used by the direct mail industry more than four decades ago, yet it is still an effective way to optimize your marketing.\n<br>\n<br>\nOur goal in this Notebook is to cluster the customers in our data set to:\n - Recognize who are our most valuable customers\n - Increase revenue\n - Increase customer retention\n - Learn more about the trends and behaviors of our customers\n - Define customers that are at risk\n\nWe will start with **RFM Analysis** and then compliment our findings with predictive analysis using **K-Means Clustering Algorithms.**\n\n- RECENCY (R): Time since last purchase\n- FREQUENCY (F): Total number of purchases\n- MONETARY VALUE (M): Total monetary value\n\n\n\n\nBenefits of RFM Analysis\n\n- Increased customer retention\n- Increased response rate\n- Increased conversion rate\n- Increased revenue\n\nRFM Analysis answers the following questions:\n - Who are our best customers?\n - Who has the potential to be converted into more profitable customers?\n - Which customers do we need to retain?\n - Which group of customers is most likely to respond to our marketing campaign?\n "]}, {"cell_type": "markdown", "id": "c69bb77c", "metadata": {}, "source": ["- Now we can use \"invoicedate\" column to find when the products were last received. \n- Let's do it both for UK and for the whole dataset."]}, {"cell_type": "markdown", "id": "93c8cd67", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">2.2 Review df_uk DataFrame</font></span> <a id=\"5.2\"></a>"]}, {"cell_type": "code", "execution_count": 1, "id": "568273ea", "metadata": {}, "outputs": [], "source": ["df_uk.head()"]}, {"cell_type": "markdown", "id": "4c470b0e", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">2.3 Calculate Recency</font></span> <a id=\"5.3\"></a>"]}, {"cell_type": "markdown", "id": "4fae5799", "metadata": {}, "source": ["### Recency: Days since last purchase\n\nTo calculate the recency values, follow these steps in order:\n\n1. To calculate recency, we need to choose a date as a point of reference to evaluate how many days ago was the customer's last purchase.\n2. Create a new column called Date which contains the invoice date without the timestamp\n3. Group by CustomerID and check the last date of purchase\n4. Calculate the days since last purchase\n5. Drop Last_Purchase_Date since we don't need it anymore\n6. Plot RFM distributions"]}, {"cell_type": "markdown", "id": "147f7072", "metadata": {}, "source": ["1. Choose a date as a point of reference to evaluate how many days ago was the customer's last purchase."]}, {"cell_type": "markdown", "id": "4eeb0814", "metadata": {}, "source": ["- Now find the last and first selling date of my whole dataset."]}, {"cell_type": "code", "execution_count": 1, "id": "ecc54c61", "metadata": {}, "outputs": [], "source": ["last_invoice = max(df_uk['invoicedate'])\nlast_invoice"]}, {"cell_type": "code", "execution_count": 1, "id": "3b3f2a6c", "metadata": {}, "outputs": [], "source": ["first_invoice = min(df_uk['invoicedate'])\nfirst_invoice"]}, {"cell_type": "markdown", "id": "d0de8082", "metadata": {}, "source": ["2. Create a new column called Date which contains the invoice date without the timestamp"]}, {"cell_type": "code", "execution_count": 1, "id": "24d48a07", "metadata": {}, "outputs": [], "source": ["df_uk[\"invoicedate\"].dtype.type"]}, {"cell_type": "code", "execution_count": 1, "id": "29d3b1f6", "metadata": {}, "outputs": [], "source": ["# We need to change it's dtype as datetime\ndf_uk['invoicedate'] = pd.to_datetime(df_uk['invoicedate'])\ndf_uk[\"invoicedate\"].dtype.type"]}, {"cell_type": "code", "execution_count": 1, "id": "6997ba05", "metadata": {}, "outputs": [], "source": ["# We can do the same for the whole dataset, but it is not necessary.\ndf['invoicedate'] = pd.to_datetime(df['invoicedate'])\ndf[\"invoicedate\"].dtype.type"]}, {"cell_type": "code", "execution_count": 1, "id": "15c68f03", "metadata": {}, "outputs": [], "source": ["df_uk.head()"]}, {"cell_type": "markdown", "id": "f2c64f12", "metadata": {}, "source": ["- Let's create a date column with only dates, need to extract time part (hour-min-sec etc). --invoice date without the timestamp--"]}, {"cell_type": "code", "execution_count": 1, "id": "a349f2d7", "metadata": {}, "outputs": [], "source": ["df_uk[\"date\"] = df_uk['invoicedate'].dt.date\ndf_uk.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "e6345103", "metadata": {}, "outputs": [], "source": ["# Not necessary\ndf[\"date\"] = df['invoicedate'].dt.date\ndf.head()"]}, {"cell_type": "markdown", "id": "bf17e91f", "metadata": {}, "source": ["3. Group by CustomerID and check the last date of purchase"]}, {"cell_type": "markdown", "id": "03a3cf11", "metadata": {}, "source": ["- Now we will group dates by customers.\n<br>But we can not use sum, unique, nunique etc directly. We can use transform and will get max or min or sum or count etc..\n<br>https://pbpython.com/pandas_transform.html"]}, {"cell_type": "markdown", "id": "87976f14", "metadata": {}, "source": ["- **TRANSFORM: provides to combine the output data back with the original dataframe.** Without using transform we can't get output's length same like dataframe"]}, {"cell_type": "code", "execution_count": 1, "id": "ec44c328", "metadata": {}, "outputs": [], "source": ["df_uk.groupby(\"customerid\").date.max()"]}, {"cell_type": "code", "execution_count": 1, "id": "ab4db535", "metadata": {}, "outputs": [], "source": ["df_uk.groupby(\"customerid\").date.transform(max)"]}, {"cell_type": "code", "execution_count": 1, "id": "017d370f", "metadata": {}, "outputs": [], "source": ["# Same length both of my last date and df_uk dataset.\n# Because  I have unique customer ids here and I used transform\nlen(df_uk)"]}, {"cell_type": "code", "execution_count": 1, "id": "0a0af452", "metadata": {}, "outputs": [], "source": ["# let's add this into dataset\ndf_uk[\"last_purchased_date\"] = df_uk.groupby(\"customerid\").date.transform(max)\ndf_uk.head()"]}, {"cell_type": "markdown", "id": "d65cdf7c", "metadata": {}, "source": ["4. Calculate the days since last purchase"]}, {"cell_type": "markdown", "id": "2ee63b5f", "metadata": {}, "source": ["- That's the recency : Days since last purchase"]}, {"cell_type": "code", "execution_count": 1, "id": "eb1ddb40", "metadata": {}, "outputs": [], "source": ["last_invoice"]}, {"cell_type": "code", "execution_count": 1, "id": "a2f4599c", "metadata": {}, "outputs": [], "source": ["last_invoice = pd.to_datetime(last_invoice).date()\nlast_invoice"]}, {"cell_type": "code", "execution_count": 1, "id": "f2528992", "metadata": {}, "outputs": [], "source": ["df_uk.groupby('customerid')['last_purchased_date'].apply(lambda x: last_invoice - x)"]}, {"cell_type": "code", "execution_count": 1, "id": "8a5fbf83", "metadata": {}, "outputs": [], "source": ["df_uk[\"recency\"] = df_uk.groupby('customerid')['last_purchased_date'].apply(lambda x: last_invoice - x)\ndf_uk.head()"]}, {"cell_type": "markdown", "id": "190a98c2", "metadata": {}, "source": ["5. Drop Last_Purchase_Date since we don't need it anymore"]}, {"cell_type": "code", "execution_count": 1, "id": "1480cbd7", "metadata": {}, "outputs": [], "source": ["df_uk = df_uk.drop('last_purchased_date',axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "f9796d57", "metadata": {}, "outputs": [], "source": ["df_uk.head()"]}, {"cell_type": "markdown", "id": "f7d954ad", "metadata": {}, "source": ["6. Plot RFM distributions"]}, {"cell_type": "code", "execution_count": 1, "id": "a3755058", "metadata": {}, "outputs": [], "source": ["df_uk.recency.dtype.type"]}, {"cell_type": "code", "execution_count": 1, "id": "11ccc080", "metadata": {}, "outputs": [], "source": ["df_uk['recency'] = pd.to_numeric(df_uk['recency'].dt.days, downcast='integer')\ndf_uk.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "a31d04a8", "metadata": {}, "outputs": [], "source": ["df_uk.recency.dtype.type"]}, {"cell_type": "markdown", "id": "38d47434", "metadata": {}, "source": ["- 2 differences between hist plot and distplot:\n<br>1. Dist plot adds the kde curve by default and brings the density on the vertical axis.\n<br>2. If you specified kde = True on the hist plot, it returns the kde curve and returns the count value on the vertical axis."]}, {"cell_type": "code", "execution_count": 1, "id": "b9d44d3c", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(16,9))\nsns.distplot(df_uk['recency'], bins=35);"]}, {"cell_type": "code", "execution_count": 1, "id": "126f3606", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(16,9))\nsns.histplot(df_uk['recency'], kde = True, bins=35);"]}, {"cell_type": "markdown", "id": "61da85e8", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">2.4 Calculate Frequency</font></span> <a id=\"5.4\"></a>"]}, {"cell_type": "markdown", "id": "32406f46", "metadata": {}, "source": ["### Frequency: Number of purchases\n\nTo calculate how many times a customer purchased something, we need to count how many invoices each customer has. To calculate the frequency values, follow these steps in order:"]}, {"cell_type": "markdown", "id": "674103ae", "metadata": {}, "source": ["1. Make a copy of df_uk and drop duplicates"]}, {"cell_type": "code", "execution_count": 1, "id": "70e1b68a", "metadata": {}, "outputs": [], "source": ["df_uk.duplicated().sum()"]}, {"cell_type": "code", "execution_count": 1, "id": "6a4b9795", "metadata": {}, "outputs": [], "source": ["df_uk.duplicated().any()"]}, {"cell_type": "code", "execution_count": 1, "id": "ae64356c", "metadata": {}, "outputs": [], "source": ["# Let's drop anyway\ndf_uk = df_uk.drop_duplicates()"]}, {"cell_type": "markdown", "id": "f77ac201", "metadata": {}, "source": ["2. Calculate the frequency of purchases"]}, {"cell_type": "markdown", "id": "1e2b7378", "metadata": {}, "source": ["- When we look at the definition of invoiceno, we see:\n<br>*Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.*"]}, {"cell_type": "code", "execution_count": 1, "id": "245d630e", "metadata": {}, "outputs": [], "source": ["df_uk.groupby('customerid')[\"invoiceno\"].count().sort_values(ascending = False)"]}, {"cell_type": "code", "execution_count": 1, "id": "eae5b74b", "metadata": {}, "outputs": [], "source": ["# Exact length is gotten only by using transform\ndf_uk.groupby('customerid')[\"invoiceno\"].transform('count').sort_values(ascending = False)"]}, {"cell_type": "code", "execution_count": 1, "id": "2fa7cf28", "metadata": {}, "outputs": [], "source": ["df_uk['frequency'] = df_uk.groupby('customerid')[\"invoiceno\"].transform('count')\ndf_uk.head(3)"]}, {"cell_type": "markdown", "id": "e04b9502", "metadata": {}, "source": ["3. Plot RFM distributions"]}, {"cell_type": "code", "execution_count": 1, "id": "ccefd81f", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(16,9))\nsns.distplot(df_uk['frequency'], bins=50);"]}, {"cell_type": "code", "execution_count": 1, "id": "38acc15f", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(16,9))\nsns.histplot(df_uk['frequency'], bins=50);"]}, {"cell_type": "markdown", "id": "7f03d892", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">2.5 Calculate Monetary Values</font></span> <a id=\"5.5\"></a>"]}, {"cell_type": "markdown", "id": "dd8dcd16", "metadata": {}, "source": ["### Monetary: Total amount of money spent\n\nThe monetary value is calculated by adding together the cost of the customers' purchases.\n"]}, {"cell_type": "markdown", "id": "fcf8b6e4", "metadata": {}, "source": ["1. Calculate sum total cost by customers and named \"Monetary\""]}, {"cell_type": "code", "execution_count": 1, "id": "0a272bd7", "metadata": {}, "outputs": [], "source": ["df_uk.groupby('customerid')[\"total_price\"].transform('sum')"]}, {"cell_type": "code", "execution_count": 1, "id": "e8ebdf00", "metadata": {}, "outputs": [], "source": ["df_uk[\"monetary\"] = df_uk.groupby('customerid')[\"total_price\"].transform('sum')\ndf_uk.head(3)"]}, {"cell_type": "markdown", "id": "ed23c55d", "metadata": {}, "source": ["2. Plot RFM distributions"]}, {"cell_type": "code", "execution_count": 1, "id": "baa7c46b", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(16,9))\nsns.distplot(df_uk['monetary'], bins=50);"]}, {"cell_type": "markdown", "id": "4be92589", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">2.6 Create RFM Table</font></span> <a id=\"5.6\"></a>"]}, {"cell_type": "markdown", "id": "0ab065ae", "metadata": {}, "source": ["Need to merge the recency, frequency and motetary dataframes"]}, {"cell_type": "code", "execution_count": 1, "id": "2a68d241", "metadata": {}, "outputs": [], "source": ["df_uk.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "846220a1", "metadata": {}, "outputs": [], "source": ["df_rfm_table = df_uk[['customerid', 'recency', 'frequency', 'monetary']]\ndf_rfm_table.head()"]}, {"cell_type": "markdown", "id": "d8149230", "metadata": {}, "source": ["# <span style=\"color:LightCoral; background-color:#F5EEF8\"><font size=\"6\">3. Customer Segmentation with RFM Scores</font></span> <a id=\"6\"></a>"]}, {"cell_type": "markdown", "id": "e0f9fd57", "metadata": {}, "source": ["Businesses have this ever-lasting urge to understand their customers. The better you understand the customer, the better you serve them, and the higher the financial gain you receive from that customer. Since the dawn of trade, this process of understanding customers for a strategic gain has been there practiced and this task is known majorly as [Customer Segmentation](https://clevertap.com/blog/rfm-analysis/).\nWell as the name suggests, Customer Segmentation could segment customers according to their precise needs. Some of the common ways of segmenting customers are based on their Recency-Frequency-Monatory values, their demographics like gender, region, country, etc, and some of their business-crafted scores. You will use Recency-Frequency-Monatory values for this case.\n\nIn this section, you will create an RFM Segmentation Table where you segment your customers by using the RFM table. For example, you can label the best customer as \"Big Spenders\" and the lost customer as \"Lost Customer\"."]}, {"cell_type": "markdown", "id": "bbdb9a46", "metadata": {}, "source": ["*Source : https://clevertap.com/blog/rfm-analysis/*<br>\nRFM stands for Recency, Frequency, and Monetary value, each corresponding to some key customer trait. These RFM metrics are important indicators of a customer\u2019s behavior because frequency and monetary value affects a customer\u2019s lifetime value, and recency affects retention, a measure of engagement."]}, {"cell_type": "markdown", "id": "0143d77f", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">3.1 Calculate RFM Scoring</font></span> <a id=\"6.1\"></a>"]}, {"cell_type": "markdown", "id": "9697f52d", "metadata": {}, "source": ["The simplest way to create customer segments from an RFM model is by using **Quartiles**. We will assign a score from 1 to 4 to each category (Recency, Frequency, and Monetary) with 4 being the highest/best value. The final RFM score is calculated by combining all RFM values. For Customer Segmentation, you will use the df_rfm data set resulting from the RFM analysis.\n<br>\n<br>\n**Note**: Data can be assigned into more groups for better granularity, but we will use 4 in this case."]}, {"cell_type": "markdown", "id": "015e76ec", "metadata": {}, "source": ["1. Divide the df_rfm into quarters"]}, {"cell_type": "code", "execution_count": 1, "id": "e4fd43bd", "metadata": {}, "outputs": [], "source": ["df_rfm_table"]}, {"cell_type": "code", "execution_count": 1, "id": "e3f835dd", "metadata": {}, "outputs": [], "source": ["df_rfm_table = df_rfm_table.set_index('customerid')\ndf_rfm_table.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "6db3c40a", "metadata": {}, "outputs": [], "source": ["df_rfm_table.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "0c1258aa", "metadata": {}, "outputs": [], "source": ["df_rfm_table.duplicated().any()"]}, {"cell_type": "code", "execution_count": 1, "id": "84b091bc", "metadata": {}, "outputs": [], "source": ["df_rfm_table.reset_index().duplicated().sum()"]}, {"cell_type": "code", "execution_count": 1, "id": "207751b6", "metadata": {}, "outputs": [], "source": ["df_rfm_table.drop_duplicates(inplace=True)\ndf_rfm_table.shape"]}, {"cell_type": "markdown", "id": "734436a3", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">3.2 Creating the RFM Segmentation Table</font></span> <a id=\"6.2\"></a>"]}, {"cell_type": "markdown", "id": "b74969fd", "metadata": {}, "source": ["1. Create two functions, one for Recency and one for Frequency and Monetary. For Recency, customers in the first quarter should be scored as 4, this represents the highest Recency value. Conversely, for Frequency and Monetary, customers in the last quarter should be scored as 4, representing the highest Frequency and Monetary values."]}, {"cell_type": "markdown", "id": "bedf0946", "metadata": {}, "source": ["- help(np.quantile)"]}, {"cell_type": "markdown", "id": "b4b94d4f", "metadata": {}, "source": ["2. Score customers from 1 to 4 by applying the functions you have created. Also create separate score column for each value. "]}, {"cell_type": "markdown", "id": "4930f03b", "metadata": {}, "source": ["- *Recency*"]}, {"cell_type": "code", "execution_count": 1, "id": "7aebeeb0", "metadata": {}, "outputs": [], "source": ["df_rfm_table[\"recency\"].quantile(q = [.25,.5,.75])"]}, {"cell_type": "code", "execution_count": 1, "id": "e577b97b", "metadata": {}, "outputs": [], "source": ["def recency_scoring(data):\n    if data[\"recency\"] <= 17.000:\n        return 4\n    elif data[\"recency\"] <= 50.000:\n        return 3\n    elif data[\"recency\"] <= 142.000:\n        return 2\n    else:\n        return 1"]}, {"cell_type": "code", "execution_count": 1, "id": "63de71b0", "metadata": {}, "outputs": [], "source": ["df_rfm_table['recency_quantile'] = df_rfm_table.apply(recency_scoring, axis =1)\ndf_rfm_table.head()"]}, {"cell_type": "markdown", "id": "02fd8333", "metadata": {}, "source": ["- *Frequency*"]}, {"cell_type": "code", "execution_count": 1, "id": "d6444862", "metadata": {}, "outputs": [], "source": ["df_rfm_table[\"frequency\"].quantile(q = [.25,.5,.75])"]}, {"cell_type": "code", "execution_count": 1, "id": "631ea9a1", "metadata": {}, "outputs": [], "source": ["def frequency_scoring(data):\n    if data.frequency <= 17.000:\n        return 1\n    elif data.frequency <= 40.000:\n        return 2\n    elif data.frequency <= 98.000:\n        return 3\n    else:\n        return 4"]}, {"cell_type": "code", "execution_count": 1, "id": "925dadcd", "metadata": {}, "outputs": [], "source": ["df_rfm_table['frequency_quantile'] = df_rfm_table.apply(frequency_scoring, axis =1)\ndf_rfm_table.head()"]}, {"cell_type": "markdown", "id": "2e7f471c", "metadata": {}, "source": ["- *Monetary*"]}, {"cell_type": "code", "execution_count": 1, "id": "4d590bb5", "metadata": {}, "outputs": [], "source": ["df_rfm_table[\"monetary\"].quantile(q = [.25,.5,.75])"]}, {"cell_type": "code", "execution_count": 1, "id": "2f419adc", "metadata": {}, "outputs": [], "source": ["def monetary_scoring(data):\n    if data.monetary <= 298.185:\n        return 1\n    elif data.monetary <= 644.975:\n        return 2\n    elif data.monetary <= 1571.285:\n        return 3\n    else:\n        return 4"]}, {"cell_type": "code", "execution_count": 1, "id": "5a4319a8", "metadata": {}, "outputs": [], "source": ["df_rfm_table['monetary_quantile'] = df_rfm_table.apply(monetary_scoring, axis =1)\ndf_rfm_table.head()"]}, {"cell_type": "markdown", "id": "c2794db7", "metadata": {}, "source": ["3. Now that scored each customer, you'll combine the scores for segmentation."]}, {"cell_type": "code", "execution_count": 1, "id": "4ea0797e", "metadata": {}, "outputs": [], "source": ["def rfm_scoring(data):\n    return str(int(data['recency_quantile'])) + str(int(data['frequency_quantile'])) + str(int(data['monetary_quantile']))"]}, {"cell_type": "code", "execution_count": 1, "id": "97b9666d", "metadata": {}, "outputs": [], "source": ["df_rfm_table['rfm_score'] = df_rfm_table.apply(rfm_scoring, axis=1)\ndf_rfm_table.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "75b1815e", "metadata": {}, "outputs": [], "source": ["df_rfm_table[\"rfm_score\"].dtype.type"]}, {"cell_type": "code", "execution_count": 1, "id": "3e5010e8", "metadata": {}, "outputs": [], "source": ["df_rfm_table[\"rfm_score\"].dtype.type"]}, {"cell_type": "markdown", "id": "edf6d509", "metadata": {}, "source": ["4. Define rfm_level function that tags customers by using RFM_Scrores and Create a new variable RFM_Level"]}, {"cell_type": "code", "execution_count": 1, "id": "ae5988f2", "metadata": {}, "outputs": [], "source": ["df_rfm_table['rfm_level'] = df_rfm_table['recency_quantile'] + df_rfm_table['frequency_quantile'] + df_rfm_table['monetary_quantile']\ndf_rfm_table.head()"]}, {"cell_type": "markdown", "id": "0c1cfe9b", "metadata": {}, "source": ["5. Calculate average values for each RFM_Level, and return a size of each segment "]}, {"cell_type": "code", "execution_count": 1, "id": "e12b4acb", "metadata": {}, "outputs": [], "source": ["# Let's see how many of which \"rfm_score\" here are.\ndf_rfm_table['rfm_score'].value_counts().sort_values(ascending=False).head()"]}, {"cell_type": "markdown", "id": "54614124", "metadata": {}, "source": ["- The first way to create a \"segments\" column.\n    - Our minimum rfm_level score is 3 and maximum 12.\n    - We can roughly classify these numbers."]}, {"cell_type": "code", "execution_count": 1, "id": "80e42ab4", "metadata": {}, "outputs": [], "source": ["def segments1(data):\n    if data['rfm_level'] >= 10 :\n        return 'Gold'\n    elif (data['rfm_level'] >= 6) and (data['rfm_level'] < 10 ):\n        return 'Silver'\n    else:  \n        return 'Bronze'"]}, {"cell_type": "code", "execution_count": 1, "id": "b4d45d17", "metadata": {}, "outputs": [], "source": ["df_rfm_table ['segments1'] = df_rfm_table.apply(segments1,axis=1)\ndf_rfm_table.head()"]}, {"cell_type": "markdown", "id": "9f5850f7", "metadata": {}, "source": ["- The second way to create a \"segments\" column.\n    - Will be very detailed."]}, {"cell_type": "code", "execution_count": 1, "id": "cdabd417", "metadata": {}, "outputs": [], "source": ["segments2 = {\n             'Customer Segment':\n                                ['Champions',\n                                 'Loyal Customers',\n                                 'Potential Loyalist', \n                                 'Recent Customers', \n                                 'Customers Needing Attention', \n                                 'Still Got Hope', \n                                 'Need to Get Them Back',\n                                 'Lost', 'Give it a Try'],\\\n                           'RFM':\n                                ['(3|4)-(3|4)-(3|4)', \n                                 '(2|3|4)-(3|4)-(1|2|3|4)', \n                                 '(3|4)-(2|3)-(1|2|3|4)', \n                                 '(4)-(1)-(1|2|3|4)', \n                                 '(2|3)-(2|3)-(2|3)', \n                                 '(2|3)-(1|2)-(1|2|3|4)', \n                                 '(1|2)-(3|4)-(2|3|4)', \n                                 '(1|2)-(1|2)-(1|2)',\n                                 '(1|2)-(1|2|3)-(1|2|3|4)']\n            }\npd.DataFrame(segments2)"]}, {"cell_type": "code", "execution_count": 1, "id": "f32c51b0", "metadata": {}, "outputs": [], "source": ["def categorizer(rfm):\n    if (rfm[0] in ['3', '4']) & (rfm[1] in ['3', '4']) & (rfm[2] in ['3', '4']):\n        rfm = 'Champions'\n        \n    elif (rfm[0] in ['2', '3', '4']) & (rfm[1] in ['3', '4']) & (rfm[2] in ['1', '2', '3', '4']):\n        rfm = 'Loyal Customers'\n        \n    elif (rfm[0] in ['3', '4']) & (rfm[1] in ['2', '3']) & (rfm[2] in ['1', '2', '3', '4']):\n        rfm = 'Potential Loyalist'\n    \n    elif (rfm[0] in ['4']) & (rfm[1] in ['1']) & (rfm[2] in ['1', '2', '3', '4']):\n        rfm = 'Recent Customers'\n    \n    elif (rfm[0] in ['2', '3']) & (rfm[1] in ['2', '3']) & (rfm[2] in ['2', '3']):\n        rfm = 'Customers Needing Attention'\n    \n    elif (rfm[0] in ['2', '3']) & (rfm[1] in ['1', '2']) & (rfm[2] in ['1', '2', '3', '4']):\n        rfm = 'Still Got Hope'\n    \n    elif (rfm[0] in ['1', '2']) & (rfm[1] in ['3', '4']) & (rfm[2] in ['2', '3', '4']):\n        rfm = 'Need to Get Them Back'\n                \n    elif (rfm[0] in ['1', '2']) & (rfm[1] in ['1', '2']) & (rfm[2] in ['1', '2']):\n        rfm = 'Lost'\n    \n    elif (rfm[0] in ['1', '2']) & (rfm[1] in ['1', '2', '3']) & (rfm[2] in ['1', '2', '3', '4']):\n        rfm = 'Give it a Try'\n    \n    return rfm "]}, {"cell_type": "code", "execution_count": 1, "id": "d4793749", "metadata": {}, "outputs": [], "source": ["df_rfm_table['segments2'] = df_rfm_table[\"rfm_score\"].apply(categorizer)\ndf_rfm_table.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "dcf49562", "metadata": {}, "outputs": [], "source": ["df_rfm_table[\"segments2\"].value_counts(dropna=False)"]}, {"cell_type": "markdown", "id": "85eecaa6", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">3.3 Plot RFM Segments</font></span> <a id=\"6.3\"></a>"]}, {"cell_type": "markdown", "id": "2fdf43fd", "metadata": {}, "source": ["1. Create your plot and resize it."]}, {"cell_type": "markdown", "id": "4abf9630", "metadata": {}, "source": ["- *Segments1*"]}, {"cell_type": "code", "execution_count": 1, "id": "12e42109", "metadata": {}, "outputs": [], "source": ["df_plot1 = pd.DataFrame(df_rfm_table[\"segments1\"].value_counts(dropna=False).sort_values(ascending=False)).reset_index().rename(columns={'index':'Segments', 'segments1':'Customers'})\ndf_plot1"]}, {"cell_type": "code", "execution_count": 1, "id": "296bbc68", "metadata": {}, "outputs": [], "source": ["# MATPLOTLIB\ny = df_plot1.Customers.values\nx = df_plot1.Segments.values\n\nfig, ax = plt.subplots(figsize = (16,9), dpi=72)\nax.bar(x,y,width=0.4)\n\nplt.title(\"RFM PLOT WITH SEGMENTS1\")\nplt.xlabel(\"SEGMENTS1\");\nplt.ylabel(\"CUSTOMERS\");\n\nfor index,value in enumerate(y):\n    plt.text(x=index , y =value , s=str(value) , ha=\"center\", va=\"bottom\", color = 'gray', fontweight = 'bold', fontdict=dict(fontsize=20))\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "c2bd9761", "metadata": {}, "outputs": [], "source": ["# SEABORN\n\n# Show Values on Seaborn Barplot\ndef show_values(axs, orient=\"v\", space=.01):\n    def _single(ax):\n        if orient == \"v\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() / 2\n                _y = p.get_y() + p.get_height() + (p.get_height()*0.01)\n                value = '{:.1f}'.format(p.get_height())\n                ax.text(_x, _y, value, ha=\"center\") \n        elif orient == \"h\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() + float(space)\n                _y = p.get_y() + p.get_height() - (p.get_height()*0.5)\n                value = '{:.1f}'.format(p.get_width())\n                ax.text(_x, _y, value, ha=\"left\")\n\n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _single(ax)\n    else:\n        _single(axs)\n        \nplt.figure(figsize=(16, 9))\nplt.title(\"RFM PLOT WITH SEGMENTS1\")\np = sns.barplot(data = df_plot1, x = 'Segments', y = 'Customers', palette = 'viridis')\np.set(xlabel='SEGMENTS1', ylabel='CUSTOMERS')\nshow_values(p);"]}, {"cell_type": "code", "execution_count": 1, "id": "2469a4e4", "metadata": {}, "outputs": [], "source": ["# PLOTLY\n\nfig = px.bar(df_plot1, x = \"Segments\", y = \"Customers\", width=950, height=600)\nfig.update_yaxes(categoryorder='total ascending')\nfig.update_layout(title=\"RFM PLOT WITH SEGMENTS1\", xaxis_title=\"SEGMENTS1\", yaxis_title=\"CUSTOMERS\", legend_title=\"Segments1\", title_font_color=\"red\", title_x=0.5,\n                  font=dict(family=\"Courier New, monospace\", size=20, color=\"RebeccaPurple\")\n                 )\nfig.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "c50e3c6b", "metadata": {}, "outputs": [], "source": ["# PLOTLY TREEMAP\n\nfig = px.treemap(df_plot1,\n                 path=[df_plot1.Segments], \n                 values='Customers', \n                 width=1000, \n                 height=600)\nfig.update_layout(title=\"RFM PLOT WITH SEGMENTS1\",\n                  title_x = 0.5, title_font = dict(size=20),\n                 )\nfig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\nfig.show()"]}, {"cell_type": "markdown", "id": "91ce5902", "metadata": {}, "source": ["- *Segments2*"]}, {"cell_type": "code", "execution_count": 1, "id": "4f4711a8", "metadata": {}, "outputs": [], "source": ["df_plot2 = pd.DataFrame(df_rfm_table[\"segments2\"].value_counts(dropna=False).sort_values(ascending=False)).reset_index().rename(columns={'index':'Segments', 'segments2':'Customers'})\ndf_plot2"]}, {"cell_type": "code", "execution_count": 1, "id": "a2a76869", "metadata": {}, "outputs": [], "source": ["# MATPLOTLIB\ny = df_plot2.Customers.values\nx = df_plot2.Segments.values\n\nfig, ax = plt.subplots(figsize = (16,9), dpi=72)\nax.bar(x,y,width=0.4)\n\nplt.title(\"RFM PLOT WITH SEGMENTS2\")\nplt.xlabel(\"SEGMENTS2\");\nplt.ylabel(\"CUSTOMERS\");\nplt.xticks(rotation = 45)\n\nfor index,value in enumerate(y):\n    plt.text(x=index , y =value , s=str(value) , ha=\"center\", va=\"bottom\", color = 'gray', fontweight = 'bold', fontdict=dict(fontsize=20))\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "61b28dbe", "metadata": {}, "outputs": [], "source": ["# SEABORN\n\nplt.figure(figsize=(16, 9))\nplt.title(\"RFM PLOT WITH SEGMENTS2\")\nax = sns.barplot(data = df_plot2, x = 'Segments', y = 'Customers', palette = 'viridis')\nplt.xticks(rotation = 45)\nax.set(xlabel='SEGMENTS2', ylabel='CUSTOMERS')\nshow_values(ax);"]}, {"cell_type": "code", "execution_count": 1, "id": "45ffb492", "metadata": {}, "outputs": [], "source": ["# PLOTLY\n\nfig = px.bar(df_plot2, x = \"Customers\", y = \"Segments\", width=950, height=600)\nfig.update_yaxes(categoryorder='total ascending')\nfig.update_layout(title=\"RFM PLOT WITH SEGMENTS2\", yaxis_title=\"SEGMENTS2\", xaxis_title=\"CUSTOMERS\", legend_title=\"Segments2\", title_font_color=\"red\", title_x=0.8,\n                  font=dict(family=\"Courier New, monospace\", size=20, color=\"RebeccaPurple\")\n                 )\nfig.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "a6d329ab", "metadata": {}, "outputs": [], "source": ["# PLOTLY TREEMAP\n\nfig = px.treemap(df_plot2,\n                 path=[df_plot2.Segments], \n                 values='Customers', \n                 width=1000, \n                 height=600)\nfig.update_layout(title=\"RFM PLOT WITH SEGMENTS2\",\n                  title_x = 0.5, title_font = dict(size=20),\n                 )\nfig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\nfig.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "d961c6d5", "metadata": {}, "outputs": [], "source": ["df_plot2"]}, {"cell_type": "code", "execution_count": 1, "id": "bf0b9bc4", "metadata": {}, "outputs": [], "source": ["list(df_plot2['Customers'].values)"]}, {"cell_type": "code", "execution_count": 1, "id": "bfe5419d", "metadata": {}, "outputs": [], "source": ["list(df_plot2[\"Segments\"].values)"]}, {"cell_type": "code", "execution_count": 1, "id": "8cd2e2e1", "metadata": {}, "outputs": [], "source": ["# MATPLOTLIB SQUARIFY\n\nimport squarify\n\nfig = plt.gcf()\nax = fig.add_subplot()\nfig.set_size_inches(16, 9)\nsquarify.plot(sizes=list(df_plot2['Customers'].values), \n              label=list(df_plot2['Segments'].values), alpha=0.8,\n              text_kwargs={'fontsize':12})\nplt.title(\"RFM Segments\",fontsize=22,fontweight=\"bold\")\nplt.axis('off')\nplt.show()\n"]}, {"cell_type": "markdown", "id": "a724d72b", "metadata": {}, "source": ["Using customer segmentation categories found [here](http://www.blastam.com/blog/rfm-analysis-boosts-sales) we can formulate different marketing strategies and approaches for customer engagement for each type of customer.\n\nNote: The author in the article scores 1 as the highest and 4 as the lowest"]}, {"cell_type": "markdown", "id": "83d7721c", "metadata": {}, "source": ["2. How many customers do we have in each segment?"]}, {"cell_type": "code", "execution_count": 1, "id": "336f6c78", "metadata": {}, "outputs": [], "source": ["df_rfm_table.head(2)"]}, {"cell_type": "code", "execution_count": 1, "id": "f80a1d5c", "metadata": {}, "outputs": [], "source": ["df_rfm_table[(df_rfm_table[\"segments1\"] == \"Gold\")][[\"segments1\",\"segments2\"]].sample(5)"]}, {"cell_type": "code", "execution_count": 1, "id": "c9f0f272", "metadata": {}, "outputs": [], "source": ["df_rfm_table[(df_rfm_table[\"segments1\"] == \"Silver\")][[\"segments1\",\"segments2\"]].sample(5)"]}, {"cell_type": "code", "execution_count": 1, "id": "a2213bf0", "metadata": {}, "outputs": [], "source": ["df_rfm_table[(df_rfm_table[\"segments1\"] == \"Bronze\")][[\"segments1\",\"segments2\"]].sample(5)"]}, {"cell_type": "markdown", "id": "4fa4886c", "metadata": {}, "source": ["# <span style=\"color:LightCoral; background-color:#F5EEF8\"><font size=\"6\">4. Applying K-Means Clustering</font></span> <a id=\"7\"></a>"]}, {"cell_type": "markdown", "id": "d9bb621d", "metadata": {}, "source": ["Now that we have our customers segmented into 6 different categories, we can gain further insight into customer behavior by using predictive models in conjuction with out RFM model.\nPossible algorithms include **Logistic Regression**, **K-means Clustering**, and **K-nearest Neighbor**. We will go with [K-Means](https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1) since we already have our distinct groups determined. K-means has also been widely used for market segmentation and has the advantage of being simple to implement."]}, {"cell_type": "markdown", "id": "463d5aa2", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">4.1 Data Pre-Processing and Exploring</font></span> <a id=\"7.1\"></a>"]}, {"cell_type": "code", "execution_count": 1, "id": "d7136733", "metadata": {}, "outputs": [], "source": ["from sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler"]}, {"cell_type": "code", "execution_count": 1, "id": "eb767053", "metadata": {}, "outputs": [], "source": ["df_rfm_table.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "efb77614", "metadata": {}, "outputs": [], "source": ["df_rfm_table.groupby('rfm_level').agg(\n                                 {'recency': ['mean','min','max','count'],\n                                  'frequency': ['mean','min','max','count'],\n                                  'monetary': ['mean','min','max','count']}\n                                ).round(1)"]}, {"cell_type": "markdown", "id": "296d0bf9", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">4.2 Define and Plot Feature Correlations</font></span> <a id=\"7.2\"></a>"]}, {"cell_type": "markdown", "id": "b16df906", "metadata": {}, "source": ["Create Heatmap and evaluate the results "]}, {"cell_type": "code", "execution_count": 1, "id": "57d25336", "metadata": {}, "outputs": [], "source": ["df_rfm_table.corr()"]}, {"cell_type": "code", "execution_count": 1, "id": "6cb62ca5", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(16,9))\nsns.heatmap(data = df_rfm_table.corr(), annot=True, cmap = 'viridis');"]}, {"cell_type": "markdown", "id": "aa57dd6f", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">4.3 Visualize Feature Distributions</font></span> <a id=\"7.3\"></a>"]}, {"cell_type": "markdown", "id": "6d4d970d", "metadata": {}, "source": ["To get a better understanding of the dataset, you can costruct a scatter matrix of each of the three features in the RFM data."]}, {"cell_type": "markdown", "id": "abed03c6", "metadata": {}, "source": ["- \"*segments1*\""]}, {"cell_type": "code", "execution_count": 1, "id": "8392dda1", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(20,10))\nsns.pairplot(df_rfm_table[['recency', 'frequency', 'monetary','segments1']]);"]}, {"cell_type": "code", "execution_count": 1, "id": "79689709", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(20,10))\nsns.pairplot(df_rfm_table[['recency', 'frequency', 'monetary','segments1']],hue='segments1');"]}, {"cell_type": "markdown", "id": "e26a0221", "metadata": {}, "source": ["- \"*segments2*\""]}, {"cell_type": "code", "execution_count": 1, "id": "45b5a8fe", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(20,10))\nsns.pairplot(df_rfm_table[['recency', 'frequency', 'monetary','segments2']]);"]}, {"cell_type": "code", "execution_count": 1, "id": "9444b3fd", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(20,10))\nsns.pairplot(df_rfm_table[['recency', 'frequency', 'monetary','segments2']], hue = \"segments2\");"]}, {"cell_type": "code", "execution_count": 1, "id": "aca9e07d", "metadata": {}, "outputs": [], "source": ["# plot the distribution of RFM values\nf,ax = plt.subplots(figsize=(10, 12))\nplt.subplot(3, 1, 1); sns.distplot(df_rfm_table[\"recency\"], label = 'Recency')\nplt.subplot(3, 1, 2); sns.distplot(df_rfm_table[\"frequency\"], label = 'Frequency')\nplt.subplot(3, 1, 3); sns.distplot(df_rfm_table[\"monetary\"], label = 'Monetary')\nplt.style.use('fivethirtyeight')\nplt.tight_layout()\nplt.show()"]}, {"cell_type": "markdown", "id": "df138f7b", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">4.4 Data Normalization</font></span> <a id=\"7.4\"></a>"]}, {"cell_type": "markdown", "id": "e266468b", "metadata": {}, "source": ["1. You can use the logarithm method to normalize the values in a column."]}, {"cell_type": "markdown", "id": "c7e0f48b", "metadata": {}, "source": ["- COMPARISON OF LOG() AND LOG1P()\n- For real-valued input, log1p is accurate also for x so small that 1 + x == 1 in floating-point accuracy.<br>\nThe log() function computes the value of the natural logarithm of argument x.<br>\nThe log1p() function computes the value of log(1+x) accurately even for tiny argument x."]}, {"cell_type": "code", "execution_count": 1, "id": "791f28be", "metadata": {}, "outputs": [], "source": ["compare = {'log1p': df_rfm_table['recency'].apply(np.log1p).values,\n           'log'  : (df_rfm_table['recency'] + 0.1).apply(np.log).values}\n\npd.DataFrame(compare, index = df_rfm_table.index)"]}, {"cell_type": "code", "execution_count": 1, "id": "68a4c9c6", "metadata": {}, "outputs": [], "source": ["rfm_log = df_rfm_table[['recency', 'frequency', 'monetary']].apply(np.log1p).round(3)\nrfm_log"]}, {"cell_type": "markdown", "id": "56c4863a", "metadata": {}, "source": ["2. Plot normalized data with scatter matrix or pairplot. Also evaluate results."]}, {"cell_type": "code", "execution_count": 1, "id": "b00d149f", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(20,10))\nsns.pairplot(data = rfm_log);"]}, {"cell_type": "code", "execution_count": 1, "id": "ab21a6a5", "metadata": {}, "outputs": [], "source": ["f,ax = plt.subplots(figsize=(10, 12))\nplt.subplot(3, 1, 1); sns.distplot(rfm_log.recency, label = 'Recency')\nplt.subplot(3, 1, 2); sns.distplot(rfm_log.frequency, label = 'Frequency')\nplt.subplot(3, 1, 3); sns.distplot(rfm_log.monetary, label = 'Monetary')\nplt.style.use('fivethirtyeight')\nplt.tight_layout()\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "bd183fcd", "metadata": {}, "outputs": [], "source": ["#Normalize the variables with StandardScaler\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(rfm_log)\n#Store it separately for clustering\nrfm_normalized= scaler.transform(rfm_log)"]}, {"cell_type": "markdown", "id": "cc00c04a", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">4.5 K-Means Implementation</font></span> <a id=\"7.5\"></a>"]}, {"cell_type": "markdown", "id": "2df1f958", "metadata": {}, "source": ["For k-means, you have to set k to the number of clusters you want, but figuring out how many clusters is not obvious from the beginning. We will try different cluster numbers and check their [silhouette coefficient](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html). The silhouette coefficient for a data point measures how similar it is to its assigned cluster from -1 (dissimilar) to 1 (similar). \n<br>\n<br>\n**Note**: K-means is sensitive to initializations because they are critical to qualifty of optima found. Thus, we will use smart initialization called \"Elbow Method\"."]}, {"cell_type": "code", "execution_count": 1, "id": "03b50eab", "metadata": {}, "outputs": [], "source": ["from sklearn.cluster import KMeans, AgglomerativeClustering\nfrom pyclustertend import hopkins\nfrom sklearn.preprocessing import scale\nfrom sklearn.metrics.cluster import adjusted_rand_score\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom sklearn.metrics import silhouette_samples,silhouette_score\nfrom scipy.cluster.hierarchy import linkage, dendrogram"]}, {"cell_type": "markdown", "id": "ae4cc9bd", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">4.6 Define Optimal Cluster Number (K) by using \"Elbow Method\" and \"Silhouette Analysis\"</font></span> <a id=\"7.6\"></a>"]}, {"cell_type": "markdown", "id": "ee96b8d3", "metadata": {}, "source": ["## Define the Optimal Number of Clusters\n- The Hopkins statistic (introduced by Brian Hopkins and John Gordon Skellam) is a way of measuring the cluster tendency of a data set. It belongs to the family of sparse sampling tests. It acts as a statistical hypothesis test where the null hypothesis is that the data is generated by a Poisson point process and are thus uniformly randomly distributed. \n- **A value close to 1 tends to indicate the data is highly clustered, random data will tend to result in values around 0.5, and uniformly distributed data will tend to result in values close to 0.**"]}, {"cell_type": "code", "execution_count": 1, "id": "ebee72cc", "metadata": {}, "outputs": [], "source": ["hopkins(rfm_log,rfm_log.shape[0])"]}, {"cell_type": "markdown", "id": "42f54dcd", "metadata": {}, "source": ["- Our Hopkins value is too close to 0 which means we have a dataset which has a quite small tendency to clustering. We need to further analyse with silouhette scores whether our data has a tendency to clustering or not."]}, {"cell_type": "markdown", "id": "7c7d1d29", "metadata": {}, "source": ["[The Elbow Method](https://en.wikipedia.org/wiki/Elbow_method_(clustering) "]}, {"cell_type": "code", "execution_count": 1, "id": "32b1b847", "metadata": {}, "outputs": [], "source": ["plt.rcParams['figure.facecolor'] = 'white'\nmodel = KMeans()\nvisualizer = KElbowVisualizer(model, k=(1,10))\n\nvisualizer.fit(rfm_log)        # Fit the data to the visualizer\nvisualizer.show() "]}, {"cell_type": "code", "execution_count": 1, "id": "165d3c83", "metadata": {}, "outputs": [], "source": ["#First : Get the Best KMeans \nks = range(1,10)\ninertias=[]\nfor k in ks :\n    # Create a KMeans clusters\n    kc = KMeans(n_clusters=k,random_state=1)\n    kc.fit(rfm_log)\n    inertias.append(kc.inertia_)\n\n# Plot ks vs inertias\nf, ax = plt.subplots(figsize=(15, 8))\nplt.plot(ks, inertias, '-o')\nplt.xlabel('Number of clusters, k')\nplt.ylabel('Inertia')\nplt.xticks(ks)\nplt.style.use('ggplot')\nplt.title('What is the Best Number for KMeans ?')\nplt.show()"]}, {"cell_type": "markdown", "id": "f82fab45", "metadata": {}, "source": ["[Silhouette Coefficient](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)"]}, {"cell_type": "code", "execution_count": 1, "id": "2dafe604", "metadata": {}, "outputs": [], "source": ["for k in range(2,10):\n\n    model = KMeans(n_clusters=k)  #  random_state=10\n    cluster_labels = model.fit_predict(rfm_log)\n    \n    print(\"For n_clusters =\", k,\n          \"The average silhouette_score is :\", silhouette_score(rfm_log, cluster_labels))"]}, {"cell_type": "code", "execution_count": 1, "id": "c37cd599", "metadata": {}, "outputs": [], "source": ["# ANOTHER WAY\n\nssd =[]\n\nfor k in range(2,10):\n    model = KMeans(n_clusters=k)\n    model.fit(rfm_log)\n    ssd.append(model.inertia_)\n    print(f'Silhouette Score for {k} clusters: {silhouette_score(rfm_log, model.labels_)}')"]}, {"cell_type": "markdown", "id": "c843f56c", "metadata": {}, "source": ["- Silhouette_score decreases as the level of detail increases, that is, as the number of clusters (n_clusters) increases.\n- From this, the following conclusions can be drawn: Our first finding regarding the clustering tendency is supported by the score that we obtained with the Hopkins method, which we found to be very close to zero.\n- For the clustering, we will choose the **n_clusters=3** that have an acceptable elbow score.<br>\n(The Yellowbrick Elbow method recommends 3 clusters, but the silhouette score is too low for 3 n_clouster.)"]}, {"cell_type": "markdown", "id": "ed3ff7c7", "metadata": {}, "source": ["##  Model Fitting"]}, {"cell_type": "markdown", "id": "0e795c4e", "metadata": {}, "source": ["Fit the K-Means Algorithm with the optimal number of clusters you decided and save the model to disk."]}, {"cell_type": "code", "execution_count": 1, "id": "f992d9a2", "metadata": {}, "outputs": [], "source": ["kmeans = KMeans(n_clusters = 3).fit(rfm_log)\nlabels = kmeans.labels_\nrfm_log['labels']=labels\n\nrfm_log.head()"]}, {"cell_type": "markdown", "id": "82407eb2", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">4.7 Visualize the Clusters</font></span> <a id=\"7.7\"></a>"]}, {"cell_type": "markdown", "id": "3aa7924c", "metadata": {}, "source": ["1. Create a scatter plot and select cluster centers"]}, {"cell_type": "code", "execution_count": 1, "id": "45cfa291", "metadata": {}, "outputs": [], "source": ["plt.scatter(rfm_log.iloc[:,0], rfm_log.iloc[:,1], c = labels, s = 50, cmap = \"viridis\")\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red',alpha=0.5, label = 'Centroids')"]}, {"cell_type": "code", "execution_count": 1, "id": "f1eceaee", "metadata": {}, "outputs": [], "source": ["f, (ax1, ax2) = plt.subplots(1, 2, sharey = True, figsize = (14,6)) # sharey=True and y-axis labels are used in common.\nax1.set_title('Recency-Frequency')\nax1.set_xlabel('Recency')\nax1.set_ylabel('Frequency')\nax1.scatter(rfm_log[\"recency\"], rfm_log[\"frequency\"], c = kmeans.labels_, cmap = \"viridis\")\nax1.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], s = 300, alpha = 1, label = 'Centroids')\n\nax2.set_title(\"Frequency-Monetary\")\nax2.set_xlabel('Frequency')\nax2.set_ylabel('Monetary')\nax2.scatter(rfm_log[\"frequency\"], rfm_log[\"monetary\"], c = kmeans.labels_, cmap =\"viridis\")\nax2.scatter(kmeans.cluster_centers_[:,1], kmeans.cluster_centers_[:,2], s = 300, alpha = 1, label = 'Centroids');"]}, {"cell_type": "markdown", "id": "c2ab7b82", "metadata": {}, "source": ["2. Visualize Cluster Id vs Recency, Cluster Id vs Frequency and Cluster Id vs Monetary using Box plot. Also evaluate the results. "]}, {"cell_type": "code", "execution_count": 1, "id": "f13a6663", "metadata": {}, "outputs": [], "source": ["rfm_log.columns"]}, {"cell_type": "code", "execution_count": 1, "id": "c66d007c", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize = (14,7))\n\nplt.subplot(1,3,1)\nsns.boxplot(rfm_log['labels'], rfm_log['recency'])\n\nplt.subplot(1,3,2)\nsns.boxplot(rfm_log['labels'], rfm_log['frequency'])\n\nplt.subplot(1,3,3)\nsns.boxplot(rfm_log['labels'], rfm_log['monetary'])\nplt.show()"]}, {"cell_type": "markdown", "id": "59535fb8", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">4.8 Assign the Labels</font></span> <a id=\"7.8\"></a>"]}, {"cell_type": "code", "execution_count": 1, "id": "75157f5f", "metadata": {}, "outputs": [], "source": ["rfm_log.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "e5e1bb40", "metadata": {}, "outputs": [], "source": ["rfm_log[\"labels\"].value_counts(dropna = False)"]}, {"cell_type": "code", "execution_count": 1, "id": "7a0f6e29", "metadata": {}, "outputs": [], "source": ["# rfm_log['decision'] = rfm_log['labels'].map({1:'Best_Customers',2:'Almost_Lost',0:'Lost_Customers'})\nrfm_log['decision'] = rfm_log['labels'].apply(lambda item: 'Best_Customers' if item == 1 else (\"Lost_Customers\" if item == 0 else \"Almost_Lost\"))\nrfm_log.head() "]}, {"cell_type": "markdown", "id": "0f1cc3db", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">4.9 Conclusion</font></span> <a id=\"7.9\"></a>"]}, {"cell_type": "markdown", "id": "de17f780", "metadata": {}, "source": ["<p style=\"color:#EC407A; opacity:1; background-color:#FAFAFA\"><font size=\"4.4\">Be careful when determining the number of \"n_clusters\" and the expressions corresponding to the \"labels\" and \"decision\" columns, as my cluster analysis gives different results each time I run my notebook. Please consider the analysis of your own results as I did.</font></p> <a id=\"7.9\"></a>\n"]}, {"cell_type": "markdown", "id": "767082f0", "metadata": {}, "source": ["- Cluster 1 : The first cluster belongs to the \"Best Customers\" segment which we saw earlier as they purchase recently (R=4), frequent buyers (F=4), and spent the most (M=4)\n\n- Cluster 2 : Second cluster can be interpreted as passer customers as their last purchase is long ago (R<=1),purchased very few (F>=2 & F < 4) and spent little (M>=4 & M < 4).Company has to come up with new strategies to make them permanent members. Low value customers\n- Cluster 0 : The third cluster is more related to the \"Almost Lost\" segment as they Haven\u2019t purchased for some time(R=1), but used to purchase frequently and spent a lot."]}, {"cell_type": "markdown", "id": "4387f433", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">4.10 Discussion</font></span> <a id=\"7.10\"></a>"]}, {"cell_type": "markdown", "id": "37e096b0", "metadata": {}, "source": ["How we want to continue this analysis depends on how the business plans to use the results and the level of granularity the business stakeholders want to see in the clusters. We can also ask what range of customer behavior from high to low value customers are the stakeholders interested in exploring. From those answers, various methods of clustering can be used and applied on RFM variable or directly on the transaction data set."]}, {"cell_type": "markdown", "id": "a7a96192", "metadata": {}, "source": ["**Annotation:**\n\nLimitations of K-means clustering:\n\n1. There is no assurance that it will lead to the global best solution.\n2. Can't deal with different shapes(not circular) and consider one point's probability of belonging to more than one cluster.\n\nThese disadvantages of K-means show that for many datasets (especially low-dimensional datasets), it may not perform as well as you might hope."]}, {"cell_type": "markdown", "id": "77f04579", "metadata": {}, "source": ["# <span style=\"color:LightCoral; background-color:#F5EEF8\"><font size=\"6\">5. Create Cohort and Conduct Cohort Analysis</font></span> <a id=\"8\"></a>"]}, {"cell_type": "markdown", "id": "c6b9b090", "metadata": {}, "source": ["[Cohort Analysis](https://medium.com/swlh/cohort-analysis-using-python-and-pandas-d2a60f4d0a4d) is specifically useful in analyzing user growth patterns for products. In terms of a product, a cohort can be a group of people with the same sign-up date, the same usage starts month/date, or the same traffic source.\nCohort analysis is an analytics method by which these groups can be tracked over time for finding key insights. This analysis can further be used to do customer segmentation and track metrics like retention, churn, and lifetime value.\n\nFor e-commerce organizations, cohort analysis is a unique opportunity to find out which clients are the most valuable to their business. by performing Cohort analysis you can get the following answers to the following questions:\n\n- How much effective was a marketing campaign held in a particular time period?\n- Did the strategy employ to improve the conversion rates of Customers worked?\n- Should I focus more on retention rather than acquiring new customers?\n- Are my customer nurturing strategies effective?\n- Which marketing channels bring me the best results?\n- Is there a seasonality pattern in Customer behavior?\n- Along with various performance measures/metrics for your organization."]}, {"cell_type": "markdown", "id": "ec464fbc", "metadata": {}, "source": ["Since we will be performing Cohort Analysis based on transaction records of customers, the columns we will be dealing with mainly:\n- Invoice Data\n- CustomerID\n- Price\n- Quantity\n\nThe following steps will performed to generate the Cohort Chart of Retention Rate:\n- Month Extraction from InvioceDate column\n- Assigning Cohort to Each Transaction\n- Assigning Cohort Index to each transaction\n- Calculating number of unique customers in each Group of (ChortDate,Index)\n- Creating Cohort Table for Retention Rate\n- Creating the Cohort Chart using the Cohort Table\n\nThe Detailed information about each step is given below:"]}, {"cell_type": "markdown", "id": "9d212859", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">5.1 Future Engineering</font></span> <a id=\"8.1\"></a>"]}, {"cell_type": "markdown", "id": "7a123126", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">5.2 Extract the Month of the Purchase</font></span> <a id=\"8.2\"></a>"]}, {"cell_type": "markdown", "id": "4516ee3a", "metadata": {}, "source": ["First we will create a function, which takes any date and returns the formatted date with day value as 1st of the same month and Year."]}, {"cell_type": "code", "execution_count": 1, "id": "ee5f608c", "metadata": {}, "outputs": [], "source": ["df_cohort = df\ndf_cohort.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "d004b16f", "metadata": {}, "outputs": [], "source": ["import datetime as dt\ndef take_months(x) : \n    return dt.datetime(x.year,x.month,1)"]}, {"cell_type": "markdown", "id": "ba316b20", "metadata": {}, "source": ["Now we will use the function created above to convert all the invoice dates into respective month date format."]}, {"cell_type": "code", "execution_count": 1, "id": "ab91b9c6", "metadata": {}, "outputs": [], "source": ["df_cohort['invoicemonth'] = df_cohort['invoicedate'].apply(take_months)\ndf_cohort[[\"invoicemonth\"]].head(2)"]}, {"cell_type": "code", "execution_count": 1, "id": "8d5d7057", "metadata": {}, "outputs": [], "source": ["df_cohort['cohortmonth'] = df_cohort.groupby('customerid')['invoicemonth'].transform('min')\ndf_cohort[['cohortmonth']].head(2)"]}, {"cell_type": "markdown", "id": "7da1f39b", "metadata": {}, "source": ["- 2nd Way"]}, {"cell_type": "code", "execution_count": 1, "id": "95060247", "metadata": {}, "outputs": [], "source": ["# def first_of_month(date):    \n#     formatted_date = dt.datetime.strptime(str(date), '%Y-%m-%d %H:%M:%S')\n#     return dt.datetime(formatted_date.year, formatted_date.month, 1)\n\n# df_cohort['invoicemonth'] = df_cohort['invoicedate'].apply(first_of_month)\n# df_cohort['cohortmonth'] = df_cohort.groupby('customerid')['invoicemonth'].transform('min')"]}, {"cell_type": "markdown", "id": "e4d8d449", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">5.3 Calculating time offset in Months i.e. Cohort Index:</font></span> <a id=\"8.3\"></a>"]}, {"cell_type": "markdown", "id": "57edd06b", "metadata": {}, "source": ["Calculating time offset for each transaction will allows us to report the metrics for each cohort in a comparable fashion.\nFirst, you will create 4 variables that capture the integer value of years, months for Invoice and Cohort Date using the get_date_int() function which you'll create it below."]}, {"cell_type": "code", "execution_count": 1, "id": "fa4d7bb7", "metadata": {}, "outputs": [], "source": ["def get_month_int (dframe,column):\n    year = dframe[column].dt.year\n    month = dframe[column].dt.month\n    day = dframe[column].dt.day\n    return year, month , day "]}, {"cell_type": "markdown", "id": "f7f273d1", "metadata": {}, "source": ["You will use this function to extract the integer values for Invoice as well as Cohort Date in 3 seperate series for each of the two columns"]}, {"cell_type": "code", "execution_count": 1, "id": "05ce2047", "metadata": {}, "outputs": [], "source": ["invoice_year, invoice_month, invoice_day = get_month_int(df_cohort, 'invoicemonth')\ncohort_year, cohort_month, cohort_day = get_month_int(df_cohort, 'cohortmonth')"]}, {"cell_type": "markdown", "id": "88db3144", "metadata": {}, "source": ["Use the variables created above to calcualte the difference in days and store them in cohort Index column."]}, {"cell_type": "code", "execution_count": 1, "id": "89c9ae16", "metadata": {}, "outputs": [], "source": ["year_diff = invoice_year - cohort_year \nmonth_diff = invoice_month - cohort_month \n\ndf_cohort['CohortIndex'] = year_diff * 12 + month_diff + 1 "]}, {"cell_type": "code", "execution_count": 1, "id": "23e679c7", "metadata": {}, "outputs": [], "source": ["df_cohort.head()"]}, {"cell_type": "markdown", "id": "a0c6d969", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">5.4 Create 1st Cohort: User number & Retention Rate</font></span> <a id=\"8.4\"></a>"]}, {"cell_type": "markdown", "id": "c78271d8", "metadata": {}, "source": ["### <span style=\"color:#19A4E6; opacity:1\"><font size=\"4\">5.4.1 Pivot Cohort and Cohort Retention</font></span> <a id=\"8.4.1\"></a>"]}, {"cell_type": "code", "execution_count": 1, "id": "6bf89a0b", "metadata": {}, "outputs": [], "source": ["#Count monthly active customers from each cohort\ncohort1 = df_cohort.groupby(['cohortmonth', 'CohortIndex'])['customerid'].nunique().reset_index()\ncohort1.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "06cadb48", "metadata": {}, "outputs": [], "source": ["# Return number of unique elements in the object.\npivot_cohort1 = cohort1.reset_index().pivot(index='cohortmonth',columns='CohortIndex',values='customerid').round(1)\npivot_cohort1"]}, {"cell_type": "code", "execution_count": 1, "id": "646c7e15", "metadata": {}, "outputs": [], "source": ["sizes = pivot_cohort1.iloc[:, 0]\nretention = pivot_cohort1.divide(sizes, axis=0).round(3)*100  #axis=0 to ensure the divide along the row axis\nretention"]}, {"cell_type": "markdown", "id": "1f4e8dfe", "metadata": {}, "source": ["- 2. WAY"]}, {"cell_type": "code", "execution_count": 1, "id": "278a0c54", "metadata": {}, "outputs": [], "source": ["# cohort_first = df_cohort.groupby(['cohortmonth', 'cohortindex'])['CustomerID'].nunique().reset_index()\n# pivot_first = cohort_first.pivot(index='cohortmonth', columns='cohortindex', values='customerid').round(1)\n# pivot_first"]}, {"cell_type": "markdown", "id": "941ebec9", "metadata": {}, "source": ["### <span style=\"color:#19A4E6; opacity:1\"><font size=\"4\">5.4.2 Visualize analysis of cohort 1 using seaborn and matplotlib modules</font></span> <a id=\"8.4.2\"></a>"]}, {"cell_type": "code", "execution_count": 1, "id": "301b36a4", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize = (16, 9))\nplt.title('Retention rates')\nsns.heatmap(data=retention, annot=True, fmt='.0%', cmap=\"GnBu\")\nplt.show()"]}, {"cell_type": "markdown", "id": "2ee9a07a", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">5.5 Create the 2nd Cohort: Average Quantity Sold</font></span> <a id=\"8.5\"></a>"]}, {"cell_type": "markdown", "id": "eb521d5d", "metadata": {}, "source": ["### <span style=\"color:#19A4E6; opacity:1\"><font size=\"4\">5.5.1 Pivot Cohort and Cohort Retention</font></span> <a id=\"8.5.1\"></a>"]}, {"cell_type": "code", "execution_count": 1, "id": "a385f3e1", "metadata": {}, "outputs": [], "source": ["cohort2 = df_cohort.groupby(['cohortmonth', 'CohortIndex'])['quantity'].mean().reset_index()\ncohort2.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "000d3117", "metadata": {}, "outputs": [], "source": ["pivot_cohort2 = cohort2.pivot(index='cohortmonth', columns='CohortIndex', values='quantity').round(1)\npivot_cohort2"]}, {"cell_type": "markdown", "id": "d0d0b29d", "metadata": {}, "source": ["### <span style=\"color:#19A4E6; opacity:1\"><font size=\"4\">5.5.2 Visualize analysis of cohort 2 using seaborn and matplotlib modules</font></span> <a id=\"8.5.2\"></a>"]}, {"cell_type": "code", "execution_count": 1, "id": "dd4a4375", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize = (16, 9))\nplt.title('Average Quantity')\nsns.heatmap(data=pivot_cohort2, annot=True, cmap=\"Oranges\")\nplt.show()"]}, {"cell_type": "markdown", "id": "4d213a7b", "metadata": {}, "source": ["## <span style=\"color:#9C27B0; opacity:1\"><font size=\"5\">5.6 Create the 3rd Cohort: Average Sales</font></span> <a id=\"8.6\"></a>"]}, {"cell_type": "markdown", "id": "a27c82a8", "metadata": {}, "source": ["### <span style=\"color:#19A4E6; opacity:1\"><font size=\"4\">5.6.1 Pivot Cohort and Cohort Retention</font></span> <a id=\"8.6.1\"></a>"]}, {"cell_type": "code", "execution_count": 1, "id": "187310e1", "metadata": {}, "outputs": [], "source": ["cohort3 = df_cohort.groupby(['cohortmonth', 'CohortIndex'])['total_price'].mean().reset_index()\ncohort3.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "5da1e1bd", "metadata": {}, "outputs": [], "source": ["pivot_cohort3 = cohort3.pivot(index='cohortmonth', columns='CohortIndex', values='total_price').round(1)\npivot_cohort3.head()"]}, {"cell_type": "markdown", "id": "3af16f41", "metadata": {}, "source": ["### <span style=\"color:#19A4E6; opacity:1\"><font size=\"4\">5.6.2 Visualize analysis of cohort 3 using seaborn and matplotlib modules</font></span> <a id=\"8.6.2\"></a>"]}, {"cell_type": "code", "execution_count": 1, "id": "09fefed4", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize = (16, 9))\nplt.title('Average unitprice for each cohort')\nsns.heatmap(data=pivot_cohort3 ,annot = True, vmin = 0.0, vmax =100,cmap=sns.cubehelix_palette(8))\nplt.show()"]}, {"cell_type": "markdown", "id": "3319c9ee", "metadata": {}, "source": ["# <span style=\"color:LightCoral; background-color:#F5EEF8\"><font size=\"6\">6. Conclusion</font></span> <a id=\"9\"></a>"]}, {"cell_type": "markdown", "id": "7f91cf16", "metadata": {}, "source": ["For e-commerce organizations, cohort analysis provides a tremendous opportunity to find out which customers are most valuable to their business. Cohort analysis is the best way to obtain answers to the following questions:\n\n- How effective was a marketing campaign conducted in a given time frame?\n- Has the strategy implemented to improve customers' conversion rates work?\n- Should I focus more on retention rather than acquiring new customers?\n- Are my customer nurturing strategies effective?\n- Which marketing channels bring me the best results?\n- Is there a seasonality pattern in customer behavior?"]}, {"cell_type": "markdown", "id": "61a9acb4", "metadata": {}, "source": ["# <span style=\"color:LightCoral; background-color:#F5EEF8\"><font size=\"6\">7. References</font></span> <a id=\"10\"></a>"]}, {"cell_type": "markdown", "id": "916a7710", "metadata": {}, "source": ["- [LinkedIn](https://www.linkedin.com/in/emirhan-bozdogan/)\n- [Kaggle](https://www.kaggle.com/emrhn1031)\n- [GitHub](https://github.com/emir1031)\n- [Career Karma](https://careerkarma.com/profile/ckwspn94d145393876obwft79zug/)<br>\n![2.png](attachment:2.png)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}