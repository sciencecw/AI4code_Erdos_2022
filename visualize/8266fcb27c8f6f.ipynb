{"cells": [{"cell_type": "code", "execution_count": 1, "id": "a81b2732", "metadata": {}, "outputs": [], "source": ["import numpy as np   # import numpy\nimport pandas as pd  # import pandas\nimport os\nimport gc   # for gabage collection\nimport seaborn as sns  # data visualization lib\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy.sparse import csr_matrix, hstack\nimport operator\nfrom sklearn.model_selection import train_test_split,KFold,StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import f1_score,precision_recall_fscore_support,classification_report,confusion_matrix\nimport glob\nimport lightgbm as lgb # load lightGBM model\nimport pickle\n%matplotlib inline"]}, {"cell_type": "code", "execution_count": 1, "id": "f1bb211d", "metadata": {}, "outputs": [], "source": ["DATA_PATH = '/kaggle/input/homesite-quote-conversion'\nfile_name = os.path.join(DATA_PATH,r'train.csv.zip')\nfile_name"]}, {"cell_type": "code", "execution_count": 1, "id": "64a646cd", "metadata": {}, "outputs": [], "source": ["df= pd.read_csv(file_name)\ndf.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "631cc4a4", "metadata": {}, "outputs": [], "source": ["#seperate the target \ny = df['QuoteConversion_Flag']\ny"]}, {"cell_type": "code", "execution_count": 1, "id": "7603c11c", "metadata": {}, "outputs": [], "source": ["df.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "c4d760df", "metadata": {}, "outputs": [], "source": ["# find correlation between the features and drop one of two highly correlated ones.\ndef highly_corr_col(x):\n    corr_matrix = df[x].corr().abs()\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n    to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n    return to_drop"]}, {"cell_type": "code", "execution_count": 1, "id": "79eb47c3", "metadata": {}, "outputs": [], "source": ["Field_col =[col for col in df if col.startswith('Field')]\nCoverageField_col = [col for col in df if col.startswith('CoverageField')]\nSalesField_col = [col for col in df if col.startswith('SalesField')]\npersonalField_col = [col for col in df if col.startswith('PersonalField')]\nPropertyField_col = [col for col in df if col.startswith('PropertyField')]\nGeographicField_col = [col for col in df if col.startswith('GeographicField')]\nField_col =[col for col in df if col.startswith('Field')]\nCoverageField_col = [col for col in df if col.startswith('CoverageField')]\nSalesField_col = [col for col in df if col.startswith('SalesField')]\npersonalField_col = [col for col in df if col.startswith('PersonalField')]\nPropertyField_col = [col for col in df if col.startswith('PropertyField')]\nGeographicField_col = [col for col in df if col.startswith('GeographicField')]"]}, {"cell_type": "code", "execution_count": 1, "id": "0460728f", "metadata": {}, "outputs": [], "source": ["df.drop(highly_corr_col(Field_col), axis=1, inplace=True)\ndf.drop(highly_corr_col(CoverageField_col), axis=1, inplace=True)\ndf.drop(highly_corr_col(SalesField_col), axis=1, inplace=True)\ndf.drop(highly_corr_col(personalField_col), axis=1, inplace=True)\ndf.drop(highly_corr_col(PropertyField_col), axis=1, inplace=True)\ndf.drop(highly_corr_col(GeographicField_col), axis=1, inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "de8fd7cb", "metadata": {}, "outputs": [], "source": ["df.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "cc5a83fd", "metadata": {}, "outputs": [], "source": ["# Convert str_type 'Date' into date_type\ndf['Date'] = pd.to_datetime(pd.Series(df['Original_Quote_Date']))\n\n# Drop 'Original_Quote_date'\ndf = df.drop('Original_Quote_Date', axis=1)\n\n# Extract year,month,weekday from 'Date'\ndf['Year'] = df['Date'].apply(lambda x: x.year)\ndf['Month'] = df['Date'].apply(lambda x: x.month)\ndf['weekday'] = df['Date'].apply(lambda x: x.weekday())\ndf['Quarter'] = df['Date'].apply(lambda x: x.quarter)\n\n# Drop 'Date' feature\ndf = df.drop('Date', axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "c7a62275", "metadata": {}, "outputs": [], "source": ["df.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "0d08388b", "metadata": {}, "outputs": [], "source": ["df['Month'].value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "bd981845", "metadata": {}, "outputs": [], "source": ["import seaborn as sns\nsns.distplot(df['Month'], kde=False)"]}, {"cell_type": "code", "execution_count": 1, "id": "3a3499f1", "metadata": {}, "outputs": [], "source": ["import seaborn as sns\nsns.distplot(df[\"Year\"], kde=False)"]}, {"cell_type": "code", "execution_count": 1, "id": "9ec8c405", "metadata": {}, "outputs": [], "source": ["import seaborn as sns\nsns.distplot(df[\"weekday\"], kde=False)"]}, {"cell_type": "code", "execution_count": 1, "id": "c63f3abc", "metadata": {}, "outputs": [], "source": ["import seaborn as sns\nsns.distplot(df[\"Quarter\"], kde=False)"]}, {"cell_type": "code", "execution_count": 1, "id": "58618655", "metadata": {}, "outputs": [], "source": ["# Let us organize above table and sort the table in terms of # of NAN in descending order\nnan_info = pd.DataFrame(df.isnull().sum()).reset_index()\nnan_info.columns = ['feature_name','nan_cnt']\nnan_info.sort_values(by = 'nan_cnt',ascending=False,inplace=True)\nnan_info['nan_percentage'] = nan_info['nan_cnt']/len(df)\nnan_info"]}, {"cell_type": "code", "execution_count": 1, "id": "935300e7", "metadata": {}, "outputs": [], "source": ["nan_info.head(10)"]}, {"cell_type": "code", "execution_count": 1, "id": "53011aea", "metadata": {}, "outputs": [], "source": ["features = [f for f in df.columns.values if f not in ['QuoteConversion_Flag','QuoteNumber']] # you have to customize this according to your own needs\nprint(features)"]}, {"cell_type": "code", "execution_count": 1, "id": "bae2c477", "metadata": {}, "outputs": [], "source": ["cols_with_missing = nan_info.loc[nan_info.nan_cnt>0].feature_name.values\nprint(cols_with_missing)"]}, {"cell_type": "code", "execution_count": 1, "id": "4c05a726", "metadata": {}, "outputs": [], "source": ["for f in cols_with_missing:\n    print(f,':', df[f].dtype,' nan percentage:', nan_info.loc[nan_info.feature_name==f].nan_percentage.values[0])"]}, {"cell_type": "code", "execution_count": 1, "id": "7b098732", "metadata": {}, "outputs": [], "source": ["def enc(x):\n    le=preprocessing.LabelEncoder()\n    le.fit(list(x.values))\n    x=le.transform(list(x.values))\n    return x"]}, {"cell_type": "code", "execution_count": 1, "id": "f8d1499f", "metadata": {}, "outputs": [], "source": ["for ft in cols_with_missing:\n    if df[ft].dtype == 'object':\n        df[ft].fillna('unknown',inplace=True)\n    else:\n        df[ft].fillna(-1, inplace=True)\n    \n    print(enc(df[ft]))"]}, {"cell_type": "code", "execution_count": 1, "id": "4742d3d2", "metadata": {}, "outputs": [], "source": ["#Convert all strings to equivalent numeric representations:\nfor f in df.columns:\n     if df[f].dtype=='object':\n            #print(df[f])\n            print(enc(df[f]))"]}, {"cell_type": "code", "execution_count": 1, "id": "d9f64751", "metadata": {}, "outputs": [], "source": ["category_features = []\nf_cat = []\nthreshold = 70\nfor each in features:\n\n    if df[each].nunique() < threshold:\n        category_features.append(each)\nfor each in category_features:\n    df[each] = df[each].astype('category')\n    #df_cat.append(each)\n    #print(df[each])\n    print(enc(df[each]))\n    f_cat.append(each)"]}, {"cell_type": "code", "execution_count": 1, "id": "60266c35", "metadata": {}, "outputs": [], "source": ["X = csr_matrix(pd.get_dummies(df[f_cat],drop_first=True,prefix=f_cat,sparse=True)).tocsr()\nX"]}, {"cell_type": "code", "execution_count": 1, "id": "2a35577b", "metadata": {}, "outputs": [], "source": ["df.isnull().sum().sum()"]}, {"cell_type": "markdown", "id": "98189772", "metadata": {}, "source": ["Spliting the data into Training and test"]}, {"cell_type": "code", "execution_count": 1, "id": "3b2f6327", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\nX = X.toarray()\ny = df['QuoteConversion_Flag'].values\nX.shape,len(y)\n #split 20% data as test data\nX_train,X_test,y_train,y_test = train_test_split(X,y,shuffle=True,random_state=2019,test_size=0.2)\n#print(train_X.shape,test_X.shape,len(train_y),len(test_y))\nX_train.shape,X_test.shape,len(y_train),len(y_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "8be73ee9", "metadata": {}, "outputs": [], "source": ["clf = RandomForestClassifier(n_estimators=500, max_depth=10, random_state=18, max_leaf_nodes=64, verbose=1,\n                                 n_jobs=4)\nscores_rfc = []\n# models1 = []\n# initialize KFold, we vcan use stratified KFold to keep the same imblance ratio for target\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=10)\nfor i, (train_idx, valid_idx) in enumerate(kf.split(X_train, y_train)):\n    print('...... training {}th fold \\n'.format(i + 1))\n    tr_x = X_train[train_idx]\n    tr_y = y_train[train_idx]\n\n    val_x = X_train[valid_idx]\n    val_y = y_train[valid_idx]\n    model = clf\n    model.fit(tr_x, tr_y)\n    # picking best model?\n    pred_val_y = model.predict(val_x)\n    # measuring model vs validation\n    score_rfc = roc_auc_score(val_y, pred_val_y)\n    scores_rfc.append(score_rfc)\n    print('current performance by auc:{}'.format(score_rfc))\n# auc_scores1.append(auc)\n# models1.append(model)\nbest_f1 = -np.inf\nbest_thred = 0\nv = [i * 0.01 for i in range(50)]\nfor thred in v:\n    preds = (pred_val_y > thred).astype(int)\n    f1 = f1_score(val_y, preds)\n    if f1 > best_f1:\n        best_f1 = f1\n        best_thred = thred\ny_pred_rfc = (pred_val_y > best_thred).astype(int)\nprint(confusion_matrix(val_y, y_pred_rfc))\nprint(f1_score(val_y, y_pred_rfc))\nprint('the average mean auc is:{}'.format(np.mean(scores_rfc)))"]}, {"cell_type": "code", "execution_count": 1, "id": "16b96779", "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import roc_auc_score  \nfrom sklearn.metrics import roc_curve\nmodel_lgb = lgb.LGBMClassifier(n_jobs=4, n_estimators=10000, boost_from_average='false', learning_rate=0.01,\n                                num_leaves=64, num_threads=4, max_depth=-1, tree_learner=\"serial\",\n                                feature_fraction=0.7, bagging_freq=5, bagging_fraction=0.7, min_data_in_leaf=100,\n                                silent=-1, verbose=-1, max_bin=255, bagging_seed=11, )\nauc_scores = []\nmodels = []\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=10)\nfor i, (train_idx, valid_idx) in enumerate(kf.split(X_train, y_train)):\n    print('...... training {}th fold \\n'.format(i + 1))\n    tr_x = X_train[train_idx]\n    tr_y = y_train[train_idx]\n\n    va_x = X_train[valid_idx]\n    va_y = y_train[valid_idx]\n    model = model_lgb  # you need to initialize your lgb model at each loop, otherwise it will overwrite\n    model.fit(tr_x, tr_y, eval_set=[(tr_x, tr_y), (va_x, va_y)], eval_metric='auc', verbose=500,\n                early_stopping_rounds=300)\n# calculate current auc after training the model\n    pred_va_y = model.predict_proba(va_x, num_iteration=model.best_iteration_)[:, 1]\n    auc = roc_auc_score(va_y, pred_va_y)\n    print('current best auc score is:{}'.format(auc))\n    auc_scores.append(auc)\n    models.append(model)\n\nbest_f1 = -np.inf\nbest_thred = 0\nv = [i * 0.01 for i in range(50)]\nfor thred in v:\n    preds = (pred_va_y > thred).astype(int)\n    f1 = f1_score(va_y, preds)\n    if f1 > best_f1:\n        best_f1 = f1\n        best_thred = thred\ny_pred_lgb = (pred_va_y > best_thred).astype(int)\nprint(confusion_matrix(va_y, y_pred_lgb))\nprint(f1_score(va_y, y_pred_lgb))\nprint('the average mean auc is:{}'.format(np.mean(auc_scores)))\nfpr, tpr, _ = roc_curve(va_y, pred_va_y)\n# plot model roc curve\nplt.plot(fpr, tpr, marker='.', label='LGB model')\n# axis labels\nplt.title('ROC AUC CURVE')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n# show the legend\nplt.legend()\n# show the plot\nplt.savefig('LGB ROC_auc_curve.png')\nplt.show()\n# Test data\npred_test_1 = models[0].predict_proba(X_test, num_iteration=models[0].best_iteration_)[:, 1]\npred_test_2 = models[1].predict_proba(X_test, num_iteration=models[1].best_iteration_)[:, 1]\npred_test_3 = models[2].predict_proba(X_test, num_iteration=models[2].best_iteration_)[:, 1]\npred_test_4 = models[3].predict_proba(X_test, num_iteration=models[3].best_iteration_)[:, 1]\npred_test_5 = models[4].predict_proba(X_test, num_iteration=models[4].best_iteration_)[:, 1]\npred_test = (pred_test_1 + pred_test_2 + pred_test_3 + pred_test_4 + pred_test_5) / 5.0\nprint(pred_test)        "]}, {"cell_type": "code", "execution_count": 1, "id": "10546916", "metadata": {}, "outputs": [], "source": ["# Logging for Visual Comparison\n#log_cols=[\"Classifier\", \"AUC Score\", \"f1-Score\"]\n#log = pd.DataFrame(columns=log_cols)\n\n#for clf in classifiers:\n #   clf.fit(X_train, y_train)\n  #  name = clf.__class__.__name__\n    \n   # print(\"=\"*30)\n    #print(name)\n    \n    #print('****Results****')\n   # print('current best auc score is:{}'.format(auc))\n    #train_predictions = clf.predict_proba(X_test)\n    #ll = log_loss(y_test, train_predictions)\n    #print(\"Log Loss: {}\".format(ll))"]}, {"cell_type": "code", "execution_count": 1, "id": "071ec256", "metadata": {}, "outputs": [], "source": ["#from sklearn.model_selection import cross_validate\n#from sklearn.ensemble import RandomForestClassifier\n#random_forest = RandomForestClassifier()\n\n#scoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\n#scores = cross_validate(random_forest, X_train, y_train, scoring=scoring, cv=20)\n\n#sorted(scores.keys())\n#forest_fit_time = scores['fit_time'].mean()\n#forest_score_time = scores['score_time'].mean()\n#forest_accuracy = scores['test_accuracy'].mean()\n#forest_precision = scores['test_precision_macro'].mean()\n#forest_recall = scores['test_recall_macro'].mean()\n#forest_f1 = scores['test_f1_weighted'].mean()\n#forest_roc = scores['test_roc_auc'].mean()"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}