{"cells": [{"cell_type": "markdown", "id": "6dd9cb37", "metadata": {}, "source": ["# Chronic Kidney Disease Analysis and Prediction"]}, {"cell_type": "markdown", "id": "a58b7d32", "metadata": {}, "source": ["![](https://th.bing.com/th/id/R.7508e9f5ce7c05d2f2feb20cf9ea4436?rik=ap1KNSWzuYqBqQ&riu=http%3a%2f%2fwww.assignmentpoint.com%2fwp-content%2fuploads%2f2015%2f10%2fBioinformatics.jpg&ehk=jfYuZa%2b8bJCbDO2yzjFlWxGkCkkz6U%2b%2fN05aWJ95koQ%3d&risl=&pid=ImgRaw&r=0&sres=1&sresct=1)"]}, {"cell_type": "markdown", "id": "af6f3bdf", "metadata": {}, "source": ["# Table of contents\n1. [Import packages](#1)\n1. [Import data](#2)\n1. [Data Cleaning](#3)\n1. [EDA](#4)\n1. [Variance Inflation Factor](#5)\n1. [Principal Component Analysis](#6)  \n1. [Decision Tree model](#7)   \n1. [GridSearch](#8)    \n1. [Feature Importance](#9)    \n1. [Random Forest model](#10)    \n1. [XgBoost model](#11)    \n1. [TabNet model](#12)    \n1. [TabNet + Optuna](#13)    \n1. [Results](#14)    \n1. [Conclusion](#15)    "]}, {"cell_type": "markdown", "id": "4e8c00d0", "metadata": {}, "source": ["<a id=\"1\"></a>\n# Import packages"]}, {"cell_type": "code", "execution_count": 1, "id": "86e9ff27", "metadata": {}, "outputs": [], "source": ["import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport numpy as np"]}, {"cell_type": "code", "execution_count": 1, "id": "4e0c9769", "metadata": {}, "outputs": [], "source": ["import warnings\nwarnings.filterwarnings(\"ignore\")"]}, {"cell_type": "markdown", "id": "d859d86d", "metadata": {}, "source": ["<a id=\"2\"></a>\n# Import data"]}, {"cell_type": "code", "execution_count": 1, "id": "183e9d8b", "metadata": {}, "outputs": [], "source": ["df_orig = pd.read_csv('../input/ckdisease/kidney_disease.csv')"]}, {"cell_type": "code", "execution_count": 1, "id": "a50d039f", "metadata": {}, "outputs": [], "source": ["df_orig.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "5c95f027", "metadata": {}, "outputs": [], "source": ["df_orig.info()"]}, {"cell_type": "code", "execution_count": 1, "id": "cac4f452", "metadata": {}, "outputs": [], "source": ["df_orig.describe()"]}, {"cell_type": "code", "execution_count": 1, "id": "067e7e76", "metadata": {}, "outputs": [], "source": ["df_orig.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "8ea91cc4", "metadata": {}, "outputs": [], "source": ["(df_orig.isnull().sum() / df_orig.shape[0] * 100.00).round(2)"]}, {"cell_type": "code", "execution_count": 1, "id": "9a97fe24", "metadata": {}, "outputs": [], "source": ["df_orig.shape"]}, {"cell_type": "markdown", "id": "8d22a144", "metadata": {}, "source": ["<a id=\"3\"></a>\n# Data Cleaning"]}, {"cell_type": "markdown", "id": "baca64fe", "metadata": {}, "source": ["## Transform column names"]}, {"cell_type": "code", "execution_count": 1, "id": "d29eb9d4", "metadata": {}, "outputs": [], "source": ["df_orig.drop('id', axis = 1, inplace = True)"]}, {"cell_type": "code", "execution_count": 1, "id": "650e619a", "metadata": {}, "outputs": [], "source": ["df_orig.columns = ['age', 'blood_pressure', 'specific_gravity', 'albumin', 'sugar', 'red_blood_cells', 'pus_cell',\n              'pus_cell_clumps', 'bacteria', 'blood_glucose_random', 'blood_urea', 'serum_creatinine', 'sodium',\n              'potassium', 'haemoglobin', 'packed_cell_volume', 'white_blood_cell_count', 'red_blood_cell_count',\n              'hypertension', 'diabetes_mellitus', 'coronary_artery_disease', 'appetite', 'peda_edema',\n              'aanemia', 'class']"]}, {"cell_type": "code", "execution_count": 1, "id": "d2d98a63", "metadata": {}, "outputs": [], "source": ["df_orig.head()"]}, {"cell_type": "markdown", "id": "31610707", "metadata": {}, "source": ["## Analyze categorical columns"]}, {"cell_type": "code", "execution_count": 1, "id": "a2afe040", "metadata": {}, "outputs": [], "source": ["df_orig['packed_cell_volume'] = pd.to_numeric(df_orig['packed_cell_volume'], errors='coerce')\ndf_orig['white_blood_cell_count'] = pd.to_numeric(df_orig['white_blood_cell_count'], errors='coerce')\ndf_orig['red_blood_cell_count'] = pd.to_numeric(df_orig['red_blood_cell_count'], errors='coerce')"]}, {"cell_type": "code", "execution_count": 1, "id": "7b18ff2e", "metadata": {}, "outputs": [], "source": ["cat_col=[col for col in df_orig.columns if df_orig[col].dtype=='object']\nfor col in cat_col:\n    print('{} has {} values '.format(col,df_orig[col].unique()))\n    print('\\n')"]}, {"cell_type": "code", "execution_count": 1, "id": "97d79e82", "metadata": {}, "outputs": [], "source": ["df_orig['diabetes_mellitus'].replace(to_replace = {'\\tno':'no','\\tyes':'yes',' yes':'yes'},inplace=True)\n\ndf_orig['coronary_artery_disease'] = df_orig['coronary_artery_disease'].replace(to_replace = '\\tno', value='no')\n\ndf_orig['class'] = df_orig['class'].replace(to_replace = 'ckd\\t', value = 'ckd')\n\ndf_orig['class'] = df_orig['class'].replace(to_replace = 'notckd', value = 'not ckd')\n\n\nfor col in cat_col:\n    print('{} has {} values  '.format(col, df_orig[col].unique()))\n    print('\\n')"]}, {"cell_type": "code", "execution_count": 1, "id": "58a8345d", "metadata": {}, "outputs": [], "source": ["df_orig['class'] = df_orig['class'].map({'ckd': 0, 'not ckd': 1})\ndf_orig['class'] = pd.to_numeric(df_orig['class'], errors='coerce')\ncat_cols = [col for col in df_orig.columns if df_orig[col].dtype == 'object']\nfor col in cat_cols:\n    print(f\"{col} has {df_orig[col].unique()} values\\n\")"]}, {"cell_type": "code", "execution_count": 1, "id": "fc7442a7", "metadata": {}, "outputs": [], "source": ["cat_cols = [col for col in df_orig.columns if df_orig[col].dtype == 'object']\nnum_cols = [col for col in df_orig.columns if df_orig[col].dtype != 'object']\nnum_cols = num_cols[:-1]\nprint(cat_cols)\nprint(num_cols)"]}, {"cell_type": "markdown", "id": "c3bec770", "metadata": {}, "source": ["## Replace NaN values"]}, {"cell_type": "markdown", "id": "59b63cbb", "metadata": {}, "source": ["1) replace nan with mode for categorical values and mean for numerical:"]}, {"cell_type": "code", "execution_count": 1, "id": "5d69c07e", "metadata": {}, "outputs": [], "source": ["df_orig_1 = df_orig\n\ndef mean_value_imputation(feature):\n    mean = df_orig[feature].mean()\n    df_orig[feature] = df_orig[feature].fillna(mean)\n    \n    \nfor col in num_cols:\n    mean_value_imputation(col)"]}, {"cell_type": "code", "execution_count": 1, "id": "b69f1a70", "metadata": {}, "outputs": [], "source": ["def impute_mode(feature):\n    mode = df_orig[feature].mode()[0]\n    df_orig[feature] = df_orig[feature].fillna(mode)\n    \nfor col in cat_cols:\n    impute_mode(col)"]}, {"cell_type": "code", "execution_count": 1, "id": "b37973a3", "metadata": {}, "outputs": [], "source": ["(df_orig.isnull().sum() / df_orig.shape[0] * 100.00).round(2)"]}, {"cell_type": "markdown", "id": "bb3a47dc", "metadata": {}, "source": ["2) use knn to replace nan values:"]}, {"cell_type": "code", "execution_count": 1, "id": "f11cba50", "metadata": {}, "outputs": [], "source": ["from sklearn.impute import KNNImputer\n\nimputer = KNNImputer(n_neighbors=11)\nimputer.fit_transform(df_orig_1[num_cols])\n\ndef impute_mode(feature):\n    mode = df_orig_1[feature].mode()[0]\n    df_orig_1[feature] = df_orig_1[feature].fillna(mode)\n    \nfor col in cat_cols:\n    impute_mode(col)"]}, {"cell_type": "code", "execution_count": 1, "id": "fcde1c9c", "metadata": {}, "outputs": [], "source": ["(df_orig_1.isnull().sum() / df_orig_1.shape[0] * 100.00).round(2)"]}, {"cell_type": "markdown", "id": "869f3356", "metadata": {}, "source": ["Both methods work fine, but knn would be better for model accuracy."]}, {"cell_type": "markdown", "id": "c67bfd0a", "metadata": {}, "source": ["<a id=\"4\"></a>\n# EDA"]}, {"cell_type": "code", "execution_count": 1, "id": "6ea74c40", "metadata": {}, "outputs": [], "source": ["import pandas_profiling as pp\n\nprofile = pp.ProfileReport(df_orig, title=\"Chronic Kidney Disease Dataset Profile\", html={\"style\": {\"full_width\": True}}, sort=None)\nprofile"]}, {"cell_type": "markdown", "id": "c53c954b", "metadata": {}, "source": ["Check for outliers (there is apparently a lot of them):"]}, {"cell_type": "code", "execution_count": 1, "id": "42bc9c6c", "metadata": {}, "outputs": [], "source": ["num_cols1 = num_cols[:-2]\nfig = px.box(df_orig[num_cols1], y=num_cols1)\nfig.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "ad98a7a7", "metadata": {}, "outputs": [], "source": ["fig = px.box(df_orig['white_blood_cell_count'], y='white_blood_cell_count')\nfig.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "e636c8f7", "metadata": {}, "outputs": [], "source": ["fig = px.box(df_orig['red_blood_cell_count'], y='red_blood_cell_count')\nfig.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "2570da5e", "metadata": {}, "outputs": [], "source": ["import plotly.graph_objects as go\ndf_px = df_orig[num_cols]\nfig = px.imshow(df_px.corr())\nfig.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "26051881", "metadata": {}, "outputs": [], "source": ["df_px = df_orig[['class', 'red_blood_cell_count', 'white_blood_cell_count', 'specific_gravity', 'packed_cell_volume']]\nfig = px.scatter_matrix(df_px, \n    dimensions = ['red_blood_cell_count', 'white_blood_cell_count', 'specific_gravity', 'packed_cell_volume'],\n    color=\"class\")\nfig.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "74863b77", "metadata": {}, "outputs": [], "source": ["fig = go.Figure([go.Histogram(x = df_orig['age'])])\nfig.show()"]}, {"cell_type": "markdown", "id": "46eb03f8", "metadata": {}, "source": ["From my perspective scaling age is not necessary, because the distribution is close to normal."]}, {"cell_type": "code", "execution_count": 1, "id": "aa67d166", "metadata": {}, "outputs": [], "source": ["fig = go.Figure([go.Histogram(x = df_orig['white_blood_cell_count'])])\nfig.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "00653a5e", "metadata": {}, "outputs": [], "source": ["fig = go.Figure([go.Histogram(x = df_orig['red_blood_cell_count'])])\nfig.show() "]}, {"cell_type": "code", "execution_count": 1, "id": "b15cac7c", "metadata": {}, "outputs": [], "source": ["fig = go.Figure([go.Histogram(x = df_orig['blood_glucose_random'])])\nfig.show() "]}, {"cell_type": "markdown", "id": "d92b5ab2", "metadata": {}, "source": ["Judging by the plots I would check the outliers and also use scaling for the columns mentioned above."]}, {"cell_type": "markdown", "id": "21024266", "metadata": {}, "source": ["Let's check if diabetes and kidney disease are connected:"]}, {"cell_type": "code", "execution_count": 1, "id": "536f67bb", "metadata": {}, "outputs": [], "source": ["fig = px.scatter(df_orig, \n    x = df_orig['age'], y = df_orig['diabetes_mellitus'],\n    color=\"class\")\nfig.show()"]}, {"cell_type": "markdown", "id": "e3ea98ac", "metadata": {}, "source": ["Seeing the there is no instances where there are both diabetes and ckd we can not assume that there is any correlation between them."]}, {"cell_type": "markdown", "id": "a3a4cbe6", "metadata": {}, "source": ["## Deal with Outliers"]}, {"cell_type": "code", "execution_count": 1, "id": "ef3ec9e4", "metadata": {}, "outputs": [], "source": ["import numpy as np\n# IQR\ndef IQR_outliers(col):\n\n    Q1 = np.percentile(df_orig[col], 25,\n                       interpolation = 'midpoint')\n\n    Q3 = np.percentile(df_orig[col], 75,\n                       interpolation = 'midpoint')\n    \n    per_95 = np.percentile(df_orig[col], 95,\n                       interpolation = 'midpoint')\n    \n    IQR = Q3 - Q1\n    \n    upper = Q3+1.5*IQR\n    lower = Q1-1.5*IQR\n    \n    df_orig[col] = np.where(df_orig[col] > upper, per_95, df_orig[col])\n    df_orig[col] = np.where(df_orig[col] < lower, lower, df_orig[col])\n\n    return df_orig\n\n\n\nfor col in num_cols:\n    df_orig = IQR_outliers(col)"]}, {"cell_type": "markdown", "id": "3bada2ad", "metadata": {}, "source": ["### Check outliers"]}, {"cell_type": "code", "execution_count": 1, "id": "392a4229", "metadata": {}, "outputs": [], "source": ["num_cols1 = num_cols[:-2]\nfig = px.box(df_orig[num_cols1], y=num_cols1)\nfig.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "70f486ec", "metadata": {}, "outputs": [], "source": ["fig = px.box(df_orig['white_blood_cell_count'], y='white_blood_cell_count')\nfig.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "145f12aa", "metadata": {}, "outputs": [], "source": ["fig = px.box(df_orig['red_blood_cell_count'], y='red_blood_cell_count')\nfig.show()"]}, {"cell_type": "markdown", "id": "8e6e0d7b", "metadata": {}, "source": ["## Use LabelEncoder for categorical values"]}, {"cell_type": "code", "execution_count": 1, "id": "d053e45c", "metadata": {}, "outputs": [], "source": ["for col in cat_cols:\n    print(f\"{col} has {df_orig[col].nunique()} categories\\n\")"]}, {"cell_type": "code", "execution_count": 1, "id": "3f227b97", "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\nfor col in cat_cols:\n    df_orig[col] = le.fit_transform(df_orig[col])"]}, {"cell_type": "code", "execution_count": 1, "id": "f7d577a4", "metadata": {}, "outputs": [], "source": ["df_orig.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "3fe0630c", "metadata": {}, "outputs": [], "source": ["df_orig.info()"]}, {"cell_type": "code", "execution_count": 1, "id": "ff050caa", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\ndf, df_test = train_test_split(df_orig, test_size = 0.15, random_state = 42)"]}, {"cell_type": "markdown", "id": "ab2dc9fc", "metadata": {}, "source": ["<a id=\"5\"></a>\n# Check VIF (Variance Inflation Factor)"]}, {"cell_type": "code", "execution_count": 1, "id": "517aa49a", "metadata": {}, "outputs": [], "source": ["import statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndf_vif = df_orig\ndf_vif['const'] = 1\n\nX = df_vif[num_cols+['const']]\n\nvif_info = pd.DataFrame()\nvif_info['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif_info['Column'] = X.columns\nvif_info.sort_values('VIF', ascending=False)"]}, {"cell_type": "markdown", "id": "f1ecc5d8", "metadata": {}, "source": ["All features have VIF below 5, so they do not have multicolinearity.\n\n*Not adding a column with constant leads to anomal values of VIF, the issue is described here:\nhttps://github.com/statsmodels/statsmodels/issues/2376*"]}, {"cell_type": "markdown", "id": "bed3938b", "metadata": {}, "source": ["## Split the train/test data"]}, {"cell_type": "markdown", "id": "78abbbfd", "metadata": {}, "source": ["I split train/validation/test as 65/20/15%, but because the dataset is small I would consider using 70/20/10% split."]}, {"cell_type": "code", "execution_count": 1, "id": "4419a0a4", "metadata": {}, "outputs": [], "source": ["ind_col = [col for col in df.columns if col != 'class']\ndep_col = 'class'\n\nX = df[ind_col]\ny = df[dep_col]\n\nX_test = df_test[ind_col]\ny_test = df_test[dep_col]"]}, {"cell_type": "code", "execution_count": 1, "id": "43a500c9", "metadata": {}, "outputs": [], "source": ["print(X.shape)\nprint(X_test.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "7b1e4560", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.20, random_state = 0)"]}, {"cell_type": "markdown", "id": "2136c0d4", "metadata": {}, "source": ["<a id=\"6\"></a>\n# Perform PCA on data"]}, {"cell_type": "code", "execution_count": 1, "id": "1e06969a", "metadata": {}, "outputs": [], "source": ["from sklearn.decomposition import PCA\npca = PCA(n_components=24)\nprincipalComponents = pca.fit_transform(X_train)\nprint (pca.explained_variance_ratio_.cumsum())"]}, {"cell_type": "markdown", "id": "cfedb551", "metadata": {}, "source": ["*The PCA did not improve model perfomance so I did not use it, n_components = 24 remains the best if we use it.*"]}, {"cell_type": "code", "execution_count": 1, "id": "7b805897", "metadata": {}, "outputs": [], "source": ["pca.fit(X_train)\nX_valid_new = pca.transform(X_valid)"]}, {"cell_type": "code", "execution_count": 1, "id": "7b4390e6", "metadata": {}, "outputs": [], "source": ["#pca.fit(X_train)\nX_train_new = pca.transform(X_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "98cb7b72", "metadata": {}, "outputs": [], "source": ["X_train_new.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "faba9fb4", "metadata": {}, "outputs": [], "source": ["X_valid_new.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "dd78199e", "metadata": {}, "outputs": [], "source": ["print(X_valid_new)"]}, {"cell_type": "markdown", "id": "1be267b7", "metadata": {}, "source": ["### KMeans Clustering"]}, {"cell_type": "markdown", "id": "96ddca70", "metadata": {}, "source": ["Let's check best k for kmeans clustering:"]}, {"cell_type": "code", "execution_count": 1, "id": "2aa1230b", "metadata": {}, "outputs": [], "source": ["from yellowbrick.cluster import KElbowVisualizer\nfrom sklearn.cluster import KMeans\nmodel = KElbowVisualizer(KMeans(), k=15)\nmodel.fit(X_train)\nmodel.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "75fdc034", "metadata": {}, "outputs": [], "source": ["from sklearn.cluster import KMeans\nkmeans = KMeans(\n    init=\"random\",\n    n_clusters=5,\n    n_init=10,\n    max_iter=300,\n    random_state=42\n)\nX_clustered = kmeans.fit_transform(X_train)\n                        \nfrom sklearn.metrics import silhouette_score\nscore = silhouette_score(X_clustered, kmeans.labels_)\nprint(score)"]}, {"cell_type": "markdown", "id": "f2934218", "metadata": {}, "source": ["# Modeling"]}, {"cell_type": "markdown", "id": "a06c83d1", "metadata": {}, "source": ["<a id=\"7\"></a>\n# Build the Decision Tree model"]}, {"cell_type": "code", "execution_count": 1, "id": "d0571e5c", "metadata": {}, "outputs": [], "source": ["from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\ndtc = DecisionTreeClassifier()\ndtc.fit(X_train_new, y_train)\n\n# accuracy score, confusion matrix and classification report of decision tree\n\ndtc_acc = accuracy_score(y_valid, dtc.predict(X_valid_new))\ndtc_acc_test = accuracy_score(y_test, dtc.predict(X_test))\n\nprint(f\"Training Accuracy of Decision Tree Classifier is {accuracy_score(y_train, dtc.predict(X_train_new))}\")\nprint(f\"Test Accuracy of Decision Tree Classifier is {dtc_acc_test} \\n\")\nprint(f\"Validation Accuracy of Decision Tree Classifier is {accuracy_score(y_valid, dtc.predict(X_valid_new))}\")\n\nprint(f\"Confusion Matrix :- \\n{confusion_matrix(y_valid, dtc.predict(X_valid_new))}\\n\")\nprint(f\"Classification Report :- \\n {classification_report(y_valid, dtc.predict(X_valid_new))}\")"]}, {"cell_type": "markdown", "id": "f66805fe", "metadata": {}, "source": ["<a id=\"8\"></a>\n# Use GridSearch"]}, {"cell_type": "code", "execution_count": 1, "id": "ad058c41", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import GridSearchCV\ngrid_param = {\n    'criterion' : ['gini', 'entropy'],\n    'max_depth' : [3, 5, 7, 10],\n    'splitter' : ['best', 'random'],\n    'min_samples_leaf' : [1, 2, 3, 5, 7],\n    'min_samples_split' : [1, 2, 3, 5, 7],\n    'max_features' : ['auto', 'sqrt', 'log2']\n}\n\ngrid_search_dtc = GridSearchCV(dtc, grid_param, cv = 11, n_jobs = -1, verbose = 1)\ngrid_search_dtc.fit(X_train, y_train)\nprint(grid_search_dtc.best_params_)\nprint(grid_search_dtc.best_score_)"]}, {"cell_type": "markdown", "id": "8d7217ea", "metadata": {}, "source": ["##### Best parameters after using GridSearch are: \n*{'criterion': 'gini', 'max_depth': 7, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 7, 'splitter': 'best'}*"]}, {"cell_type": "code", "execution_count": 1, "id": "1c864542", "metadata": {}, "outputs": [], "source": ["dtc = grid_search_dtc.best_estimator_\n\nprint(dtc)\n\n# accuracy score, confusion matrix and classification report of grid search\n\ndtc_gs_acc = accuracy_score(y_valid, dtc.predict(X_valid))\ndtc_gs_acc_test = accuracy_score(y_test, dtc.predict(X_test))\n\nprint(f\"Training Accuracy of Decision Tree Classifier is {accuracy_score(y_train, dtc.predict(X_train))}\")\nprint(f\"Test Accuracy of Decision Tree Classifier is {dtc_gs_acc_test}\")\nprint(f\"Validation Accuracy of Decision Tree Classifier is {accuracy_score(y_valid, dtc.predict(X_valid))} \\n\")\n\nprint(f\"Confusion Matrix :- \\n{confusion_matrix(y_valid, dtc.predict(X_valid_new))}\\n\")\nprint(f\"Classification Report :- \\n {classification_report(y_valid, dtc.predict(X_valid_new))}\")"]}, {"cell_type": "markdown", "id": "3d8a6c8e", "metadata": {}, "source": ["<a id=\"9\"></a>\n# Feature Importance"]}, {"cell_type": "markdown", "id": "ad12a01f", "metadata": {}, "source": ["According to National Health Service UK:\n> Chronic kidney disease is usually caused by other conditions that put a strain on the kidneys. Often it's the result of a combination of different problems.\n\nNHS gives further descriptions of possible CKD causes, such as:\n\n>*CKD can be caused by:*\n>* high blood pressure \u2013 over time, this can put strain on the small blood vessels in the kidneys and stop the kidneys working properly\n>* diabetes \u2013 too much glucose in your blood can damage the tiny filters in the kidneys\n>* high cholesterol \u2013 this can cause a build-up of fatty deposits in the blood vessels supplying your kidneys, which can make it harder for them to work properly\n>* kidney infections\n>* glomerulonephritis \u2013 kidney inflammation\n>* polycystic kidney disease \u2013 an inherited condition where growths called cysts develop in the kidneys\n>* blockages in the flow of urine \u2013 for example, from kidney stones that keep coming back, or an enlarged prostate long-term, regular use of certain medicines \u2013 such as lithium and non-steroidal anti-inflammatory drugs (NSAIDs)\n"]}, {"cell_type": "markdown", "id": "75896d9a", "metadata": {}, "source": ["3 methods were used to analyze feature importance for GridSearch model:\n\n---> using built-in function"]}, {"cell_type": "code", "execution_count": 1, "id": "1f7d0b2b", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(12,3))\nfeatures = X_test.columns.values.tolist()\nimportance = dtc.feature_importances_.tolist()\nft_imp = pd.DataFrame()\nft_imp['feature'] = features\nft_imp['importance'] = importance\nft_imp.sort_values(by=['importance'], ascending = False, inplace=True)\nprint(ft_imp)\nfeature_series = pd.Series(data=importance,index=features)\nfeature_series.plot.bar()\nplt.title('Feature Importance')"]}, {"cell_type": "markdown", "id": "c2891736", "metadata": {}, "source": ["---> using LIME"]}, {"cell_type": "code", "execution_count": 1, "id": "e1347bdf", "metadata": {}, "outputs": [], "source": ["import lime\nimport lime.lime_tabular\n\nexplainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names=X_test.columns.values.tolist(),\n                                                  class_names=['skd', 'not skd'], verbose=True, mode='classification')"]}, {"cell_type": "code", "execution_count": 1, "id": "5397149a", "metadata": {}, "outputs": [], "source": ["j = len(X_train) - 1\nexp = explainer.explain_instance(X_train.values[j], grid_search_dtc.predict_proba, num_features=10)"]}, {"cell_type": "code", "execution_count": 1, "id": "873ded80", "metadata": {}, "outputs": [], "source": ["exp.show_in_notebook(show_table=True)"]}, {"cell_type": "markdown", "id": "cf22537d", "metadata": {}, "source": ["---> using boruta-shap"]}, {"cell_type": "code", "execution_count": 1, "id": "3e4ddb57", "metadata": {}, "outputs": [], "source": ["!pip install borutashap"]}, {"cell_type": "code", "execution_count": 1, "id": "87277d44", "metadata": {}, "outputs": [], "source": ["from BorutaShap import BorutaShap\n\nFeature_Selector = BorutaShap(importance_measure='shap', classification=False)\n\nFeature_Selector.fit(X=X_train, y=y_train, n_trials=50, random_state=0)"]}, {"cell_type": "code", "execution_count": 1, "id": "54be4c7d", "metadata": {}, "outputs": [], "source": ["Feature_Selector.plot(which_features='all', figsize=(16,12))\n\nselected_columns = list()\nselected_columns.append(sorted(Feature_Selector.Subset().columns))\n    \nprint(f\"Selected features are: {selected_columns[-1]}\")"]}, {"cell_type": "markdown", "id": "2bba328d", "metadata": {}, "source": ["##### All methods may slightly differ in their results, but all of them correspond to scientific definition of possible CKD causes. In conlusion, anomalies in specific gravity (kidney's ability to concentrate urine), blood pressure and presence of diabetes/high glucose level can be an alarming indicator of CKD."]}, {"cell_type": "markdown", "id": "c232abcd", "metadata": {}, "source": ["<a id=\"10\"></a>\n# Random Forest"]}, {"cell_type": "code", "execution_count": 1, "id": "247fc293", "metadata": {}, "outputs": [], "source": ["from sklearn.ensemble import RandomForestClassifier\n\nrd_clf = RandomForestClassifier(criterion = 'entropy', max_depth = 10, max_features = 'auto', min_samples_leaf = 2, min_samples_split = 7, n_estimators = 12)\nrd_clf.fit(X_train_new, y_train)\n\n# accuracy score, confusion matrix and classification report of random forest\n\nrd_clf_acc = accuracy_score(y_valid, rd_clf.predict(X_valid_new))\nrd_clf_acc_test = accuracy_score(y_test, rd_clf.predict(X_test))\n\nprint(f\"Training Accuracy of Random Forest Classifier is {accuracy_score(y_train, rd_clf.predict(X_train_new))}\")\nprint(f\"Test Accuracy of Random Forest Classifier is {rd_clf_acc_test} \\n\")\nprint(f\"Validation Accuracy of Random Forest Classifier is {accuracy_score(y_valid, rd_clf.predict(X_valid_new))}\")\n\nprint(f\"Confusion Matrix :- \\n{confusion_matrix(y_valid, rd_clf.predict(X_valid_new))}\\n\")\nprint(f\"Classification Report :- \\n {classification_report(y_valid, rd_clf.predict(X_valid_new))}\")"]}, {"cell_type": "markdown", "id": "c9323ece", "metadata": {}, "source": ["<a id=\"11\"></a>\n# XGBoost"]}, {"cell_type": "code", "execution_count": 1, "id": "9196cb88", "metadata": {}, "outputs": [], "source": ["from xgboost import XGBClassifier\n\nxgb = XGBClassifier(objective = 'binary:logistic', learning_rate = 0.5, max_depth = 5, n_estimators = 150)\nxgb.fit(X_train_new, y_train)\n\n# accuracy score, confusion matrix and classification report of xgboost\n\nxgb_acc = accuracy_score(y_valid, xgb.predict(X_valid_new))\nxgb_acc_test = accuracy_score(y_test, xgb.predict(X_test))\n\nprint(f\"Training Accuracy of XgBoost is {accuracy_score(y_train, xgb.predict(X_train_new))}\")\nprint(f\"Test Accuracy of XgBoost is {xgb_acc_test}\")\nprint(f\"Validation Accuracy of XgBoost is {accuracy_score(y_valid, xgb.predict(X_valid_new))} \\n\")\n\nprint(f\"Confusion Matrix :- \\n{confusion_matrix(y_valid, xgb.predict(X_valid_new))}\\n\")\nprint(f\"Classification Report :- \\n {classification_report(y_valid, xgb.predict(X_valid_new))}\")"]}, {"cell_type": "markdown", "id": "0ce5175b", "metadata": {}, "source": ["<a id=\"12\"></a>\n# TabNet"]}, {"cell_type": "code", "execution_count": 1, "id": "cd549ecf", "metadata": {}, "outputs": [], "source": ["!pip install pytorch-tabnet"]}, {"cell_type": "code", "execution_count": 1, "id": "a3ff4488", "metadata": {}, "outputs": [], "source": ["from pytorch_tabnet.tab_model import TabNetClassifier\nfrom sklearn.model_selection import KFold\nimport torch as torch"]}, {"cell_type": "code", "execution_count": 1, "id": "2cc7e88b", "metadata": {}, "outputs": [], "source": ["clf = TabNetClassifier(verbose=1,seed=42)"]}, {"cell_type": "code", "execution_count": 1, "id": "f170230b", "metadata": {}, "outputs": [], "source": ["clf.fit(X_train=X_train_new, y_train=y_train,\n               eval_metric=['auc'])"]}, {"cell_type": "code", "execution_count": 1, "id": "407065c6", "metadata": {}, "outputs": [], "source": ["tbnt_acc = accuracy_score(y_valid, clf.predict(X_valid_new))\n#tbnt_acc_test = accuracy_score(y_test, clf.predict(X_test))\n\nprint(f\"Training Accuracy of TabNet is {accuracy_score(y_train, clf.predict(X_train_new))}\")\n#print(f\"Test Accuracy of TabNet is {tbnt_acc_test} \\n\")\nprint(f\"Validation Accuracy of TabNet is {accuracy_score(y_valid, clf.predict(X_valid_new))}\")\n\nprint(f\"Confusion Matrix :- \\n{confusion_matrix(y_valid, clf.predict(X_valid_new))}\\n\")\nprint(f\"Classification Report :- \\n {classification_report(y_valid, clf.predict(X_valid_new))}\")"]}, {"cell_type": "markdown", "id": "6bea9531", "metadata": {}, "source": ["<a id=\"13\"></a>\n# TabNet Optimization (Optuna)"]}, {"cell_type": "code", "execution_count": 1, "id": "0bff77fb", "metadata": {}, "outputs": [], "source": ["import optuna\nfrom optuna import Trial"]}, {"cell_type": "code", "execution_count": 1, "id": "5d132898", "metadata": {}, "outputs": [], "source": ["EPOCHS = 30\nBATCH_SIZE = 32\n\ndef objective(trial):\n    # parameter set by optuna\n    N_D = trial.suggest_int('N_D', 8, 32)\n    N_A = N_D\n    GAMMA = trial.suggest_float('GAMMA', 1.0, 2.0)\n    N_STEPS = trial.suggest_int('N_STEPS', 1, 3, 1)\n    LAMBDA_SPARSE = trial.suggest_loguniform(\"LAMBDA_SPARSE\", 1e-5, 1e-1)\n    \n    # changes\n    # introduced lambda-sparse\n    clf = TabNetClassifier(\n                       optimizer_fn=torch.optim.Adam,\n                       optimizer_params=dict(lr=2e-2),\n                       scheduler_params={\"step_size\":4,\n                                         \"gamma\":0.9},\n                       scheduler_fn=torch.optim.lr_scheduler.StepLR,\n                       mask_type='sparsemax',\n                          n_d = N_D,\n                          n_a = N_A,\n                          gamma = GAMMA,\n                          n_steps = N_STEPS,\n                          lambda_sparse = LAMBDA_SPARSE)\n    \n    clf.fit(X_train_new, y_train,\n        eval_set=[(X_train_new, y_train),(X_valid_new, y_valid)],\n        max_epochs = EPOCHS,\n        batch_size = BATCH_SIZE,\n        patience = 5,\n        eval_name=['train', 'valid'],\n        eval_metric=['auc']\n           )\n    \n    # changed, now score is max val_uac\n    score = np.max(clf.history['valid_auc'])\n    \n    return score"]}, {"cell_type": "code", "execution_count": 1, "id": "da366b12", "metadata": {}, "outputs": [], "source": ["study = optuna.create_study(direction='maximize', study_name = 'tabnet-study')\n\nstudy.optimize(objective, n_trials=150, timeout = 3600*8)\n\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)"]}, {"cell_type": "code", "execution_count": 1, "id": "3d526480", "metadata": {}, "outputs": [], "source": ["print('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)"]}, {"cell_type": "code", "execution_count": 1, "id": "77dfd58d", "metadata": {}, "outputs": [], "source": ["from optuna.visualization import plot_optimization_history\n\nplot_optimization_history(study)"]}, {"cell_type": "code", "execution_count": 1, "id": "78821dfc", "metadata": {}, "outputs": [], "source": ["from optuna.visualization import plot_param_importances\n\nplot_param_importances(study)"]}, {"cell_type": "markdown", "id": "bc6b8df8", "metadata": {}, "source": ["Predict on valid/test using best parameters:"]}, {"cell_type": "code", "execution_count": 1, "id": "246d18e1", "metadata": {}, "outputs": [], "source": ["params_tb = study.best_trial.params\nprint(params_tb)"]}, {"cell_type": "code", "execution_count": 1, "id": "5e684eb4", "metadata": {}, "outputs": [], "source": ["clf_opt = TabNetClassifier(verbose=1, seed=42,\n                        optimizer_fn=torch.optim.Adam,\n                       optimizer_params=dict(lr=2e-2),\n                       scheduler_params={\"step_size\":4,\n                                         \"gamma\":0.9},\n                       scheduler_fn=torch.optim.lr_scheduler.StepLR,\n                       mask_type='sparsemax',\n                          n_d = params_tb['N_D'],\n                          n_a = params_tb['N_D'],\n                          gamma = params_tb['GAMMA'],\n                          n_steps = params_tb['N_STEPS'],\n                          lambda_sparse = params_tb['LAMBDA_SPARSE'])"]}, {"cell_type": "code", "execution_count": 1, "id": "8251b5d3", "metadata": {}, "outputs": [], "source": ["clf_opt.fit(X_train=X_train_new, y_train=y_train,\n        max_epochs = 100,\n        batch_size = BATCH_SIZE,\n        patience = 5,\n        eval_metric=['auc']\n       )"]}, {"cell_type": "code", "execution_count": 1, "id": "5a1d8e4f", "metadata": {}, "outputs": [], "source": ["tbnt_opt_acc = accuracy_score(y_valid, clf_opt.predict(X_valid_new))\n#tbnt_opt_acc_test = accuracy_score(y_test, clf_opt.predict(X_test))\n\nprint(f\"Training Accuracy of TabNet is {accuracy_score(y_train, clf_opt.predict(X_train_new))}\")\n#print(f\"Test Accuracy of TabNet is {tbnt_opt_acc_test} \\n\")\nprint(f\"Validation Accuracy of TabNet is {accuracy_score(y_valid, clf_opt.predict(X_valid_new))}\")\n\nprint(f\"Confusion Matrix :- \\n{confusion_matrix(y_valid, clf_opt.predict(X_valid_new))}\\n\")\nprint(f\"Classification Report :- \\n {classification_report(y_valid, clf_opt.predict(X_valid_new))}\")"]}, {"cell_type": "markdown", "id": "265df7c0", "metadata": {}, "source": ["<a id=\"14\"></a>\n# Plot results"]}, {"cell_type": "code", "execution_count": 1, "id": "cc368242", "metadata": {}, "outputs": [], "source": ["models = pd.DataFrame({\n    'Model' : [ 'Decision Tree Classifier', 'TabNet', 'Tabnet + Optuna', 'Decision Tree + GridSearch', 'Random Forest Classifier',\n             'XgBoost'],\n    'Score' : [dtc_acc, tbnt_acc, tbnt_opt_acc, dtc_gs_acc, rd_clf_acc, xgb_acc]\n})\n\n\nmodels = models.sort_values(by = 'Score', ascending = True)"]}, {"cell_type": "code", "execution_count": 1, "id": "ecf8126e", "metadata": {}, "outputs": [], "source": ["px.bar(data_frame = models, x = 'Score', y = 'Model', color = 'Score', template = 'seaborn', \n       title = 'Models Validation Score Comparison')"]}, {"cell_type": "markdown", "id": "da779c22", "metadata": {}, "source": ["<a id=\"15\"></a>\n# Conclusions"]}, {"cell_type": "markdown", "id": "3b171007", "metadata": {}, "source": ["**In this notebook:**\n* analyzed Chronic Kidney Disease dataset\n* performed EDA\n* tried out Principal Component Analysis (PCA)\n* used different models for prediction: Decision Tree, Grid Search, Random Forest, XgBoost, TabNet (with Optuna optimization)\n* used different packages for feature importance analysis: LIME, SHAP, Boruta\n\nIn conclusion, Decision Trees and TabNet perform well with optimization (in our case GridSearch/Optuna), whereas XgBoost is capable of outperforming them singlehandedly. TabNet is proven itself to work good with tabular data, which is common among classic neural network models, but still, tree models and boosting are more effiecient in this case - even though they are easier to implement due to their simplicity, they can maintain high performance with data like used here. When analyzing feature importance we can see, that important features are the same which are considered to be main causes of CKD."]}, {"cell_type": "markdown", "id": "0610a6e3", "metadata": {}, "source": ["#### Thank you for making it till the end of my notebook! I hope you enjoyed the content above, please feel free to comment and upvote :)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}