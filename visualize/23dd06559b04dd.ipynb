{"cells": [{"cell_type": "markdown", "id": "cefe08ef", "metadata": {}, "source": ["# Task 1 - Identify if a pacient is SARS-Cov-2 positive"]}, {"cell_type": "markdown", "id": "3693d292", "metadata": {}, "source": ["Albert Einstein Hospital in S\u00e3o Paulo, Brazil has provided a dataset of its pacients with a number of exams and tests and the SARS-Cov-2 exam result.\nThe idea of this notebook is to provide a model that identify if a pacient is SARS-Cov-2 positive based on the other exams results.\nThe greatest challenges here are:\n\n1) To handle with missing values, since the set of exams held for each pacient is different, and a lot of missing values are present;\n\n2) To present good results with unbalanced classes (there are much more negative results)"]}, {"cell_type": "code", "execution_count": 1, "id": "b13e6fd3", "metadata": {}, "outputs": [], "source": ["import pandas as pd\nimport numpy as np\n\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix \nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nfrom sklearn.neural_network import MLPClassifier\n\nimport seaborn as sn\nimport pandas as pd\nimport matplotlib.pyplot as plt"]}, {"cell_type": "code", "execution_count": 1, "id": "d6e83629", "metadata": {}, "outputs": [], "source": ["df = pd.read_excel('/kaggle/input/covid19/dataset.xlsx')"]}, {"cell_type": "markdown", "id": "9ec1a01b", "metadata": {}, "source": ["## 1. Feature Selection"]}, {"cell_type": "markdown", "id": "7a82154b", "metadata": {}, "source": ["### 1.1 Selection of variables with at least 10% non NaN values"]}, {"cell_type": "code", "execution_count": 1, "id": "8c643b77", "metadata": {}, "outputs": [], "source": ["df_freq = df.count()\nvariables = df_freq.loc[df_freq>0.1*df.shape[0]].index\ndf = df[variables]"]}, {"cell_type": "markdown", "id": "a3314ad1", "metadata": {}, "source": ["### 1.2. Selection of continuous variables\nNow it is checked the correlation of the continuous variables compared to our target (Covid exam result).\nHere, it is selected only the variables with |c|>=0.1, which is not a high correlation, but it has a minimum significance."]}, {"cell_type": "code", "execution_count": 1, "id": "bb36aeb0", "metadata": {}, "outputs": [], "source": ["df2 = df.copy()\ndf2.drop(['Patient addmited to regular ward (1=yes, 0=no)','Patient addmited to semi-intensive unit (1=yes, 0=no)','Patient addmited to intensive care unit (1=yes, 0=no)'],axis=1,inplace=True)\ndf2['SARS-Cov-2 exam result'] = df2['SARS-Cov-2 exam result'].map(lambda x: 1 if x == 'positive' else 0)\ncorrs = df2.corr()['SARS-Cov-2 exam result']\ncont_out = list(corrs.loc[(corrs<0.1)|(corrs>-0.1)].index)\ncont_vars = list(corrs.loc[(corrs>=0.1)|(corrs<=-0.1)].index)"]}, {"cell_type": "markdown", "id": "2c2a051f", "metadata": {}, "source": ["Correlations (after feature selection):"]}, {"cell_type": "code", "execution_count": 1, "id": "ca794441", "metadata": {}, "outputs": [], "source": ["corrs[cont_vars]"]}, {"cell_type": "code", "execution_count": 1, "id": "fef4135b", "metadata": {}, "outputs": [], "source": ["cont_vars = list(set(cont_vars)-set(['SARS-Cov-2 exam result','Patient addmited to regular ward (1=yes, 0=no)']))\nvariables = [var for var in variables if var not in cont_out]\ncat_vars = list(set(variables) - set(cont_vars)-set(['Patient ID','Patient addmited to regular ward (1=yes, 0=no)','Patient addmited to semi-intensive unit (1=yes, 0=no)','Patient addmited to intensive care unit (1=yes, 0=no)']))\nvariables = cont_vars+cat_vars\nyCol = ['SARS-Cov-2 exam result']\ndf = df[variables+yCol]"]}, {"cell_type": "markdown", "id": "71204bf5", "metadata": {}, "source": ["Set of continuous variables:"]}, {"cell_type": "code", "execution_count": 1, "id": "3516d8fe", "metadata": {}, "outputs": [], "source": ["cont_vars"]}, {"cell_type": "markdown", "id": "8f0b9346", "metadata": {}, "source": ["Categorical variables (no feature selection is held on them):"]}, {"cell_type": "code", "execution_count": 1, "id": "a17d5a6d", "metadata": {}, "outputs": [], "source": ["cat_vars"]}, {"cell_type": "markdown", "id": "36f876ca", "metadata": {}, "source": ["## 2. Data preparation (handling missing values)\n\nI do not infer anything about missing data, so data preparation is made according the following steps:\n\n1) For the continuous variables, any row with missing data is deleted\n\n2) For the categorical variables, missing data is filled with \"not_tested\"\n\n3) One hot encoding of categorical data\n\n4) not_tested dummy columns are deleted\n\nSo, missing data in categorical variables are handled as a sequence of zeros in one hot encoding.\nExample:\n\n| Influenza_A_detected | Influenza_A_not_detected |\n| --- | --- |\n| 0 | 0 |"]}, {"cell_type": "code", "execution_count": 1, "id": "807cf2f1", "metadata": {}, "outputs": [], "source": ["df = df.loc[~df[cont_vars].isna().all(axis=1)]\ndf[cat_vars] = df[cat_vars].replace(np.nan,'not_tested')\ndf_dummies = pd.get_dummies(df[cat_vars])\n\ndumm_col = list(df_dummies.columns)\ncols = []\nfor col in dumm_col:\n    if \"not_tested\" not in col:\n        cols.append(col)\ndf_dummies = df_dummies[cols]\n\ndf = pd.concat([df,df_dummies],axis=1)\ndf.drop(columns=cat_vars,axis=1,inplace=True)\nvariables = list(set(df.columns)-set(yCol))\n\ndf.dropna(inplace=True)\ndf.shape"]}, {"cell_type": "markdown", "id": "295655ae", "metadata": {}, "source": ["The dataframe has 598 entries, which is a good number to handle with!"]}, {"cell_type": "markdown", "id": "84a24bb6", "metadata": {}, "source": ["## 3. Train and test"]}, {"cell_type": "markdown", "id": "79d09bf7", "metadata": {}, "source": ["First, data normalization is held, followed by a random under sampling (I do not want to over sample and evaluate the model over inferred data), in order to balance classes. After that, train-test split (20% testing) is performed.\n\nIt is used an ensemble of 5 MLP classifiers, with 2 hidden layers each and different number of neurons:\n- MLP1: 2 hidden layers with 28 neurons each\n- MLP2: 2 hidden layers with 32 neurons each\n- MLP3: 2 hidden layers with 36 neurons each\n- MLP4: 2 hidden layers with 40 neurons each\n- MLP5: 2 hidden layers with 44 neurons each\n\nThe predicted class is a majority voting of each network predicted label."]}, {"cell_type": "code", "execution_count": 1, "id": "5970b0d2", "metadata": {}, "outputs": [], "source": ["scaler = MinMaxScaler(copy=False, feature_range=(0, 1))\ndf[variables] = scaler.fit_transform(df[variables])\n\ndf[yCol] = df[yCol].replace('negative',0)\ndf[yCol] = df[yCol].replace('positive',1)\n\nx = df[variables]\ny = df[yCol]\nrus = RandomUnderSampler(random_state=42)\nx_rus, y_rus = rus.fit_resample(x, y)\n\nx_train, x_test, y_train, y_test = train_test_split(x_rus, y_rus, test_size=0.20, random_state=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "d4ea1302", "metadata": {}, "outputs": [], "source": ["clf = MLPClassifier(solver='lbfgs', activation='logistic', alpha=1e-3,\n                    hidden_layer_sizes=(28, 2), random_state=1)\n\nclf.fit(x_train, y_train)\ny_pred_nn1 = clf.predict(x_test)\n\nclf = MLPClassifier(solver='lbfgs', activation='logistic', alpha=1e-3,\n                    hidden_layer_sizes=(32, 2), random_state=1)\n\nclf.fit(x_train, y_train)\ny_pred_nn2 = clf.predict(x_test)\n\nclf = MLPClassifier(solver='lbfgs', activation='logistic', alpha=1e-3,\n                    hidden_layer_sizes=(36, 2), random_state=1)\n\nclf.fit(x_train, y_train)\ny_pred_nn3 = clf.predict(x_test)\n\nclf = MLPClassifier(solver='lbfgs', activation='logistic', alpha=1e-3,\n                    hidden_layer_sizes=(40, 2), random_state=1)\n\nclf.fit(x_train, y_train)\ny_pred_nn4 = clf.predict(x_test)\n\nclf = MLPClassifier(solver='lbfgs', activation='logistic', alpha=1e-3,\n                    hidden_layer_sizes=(44, 2), random_state=1)\n\nclf.fit(x_train, y_train)\ny_pred_nn5 = clf.predict(x_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "242e4a2c", "metadata": {}, "outputs": [], "source": ["df_ensemble = pd.DataFrame(columns=['class_nn1','class_nn2','class_nn3','class_nn4','class_nn5','y_true'])\ndf_ensemble['class_nn1'] = y_pred_nn1\ndf_ensemble['class_nn2'] = y_pred_nn2\ndf_ensemble['class_nn3'] = y_pred_nn3\ndf_ensemble['class_nn4'] = y_pred_nn4\ndf_ensemble['class_nn5'] = y_pred_nn5\ny_test.reset_index(inplace=True,drop=True)\ndf_ensemble['y_true'] = y_test\ndf_ensemble['y_pred'] = df_ensemble[['class_nn1','class_nn2','class_nn3','class_nn4','class_nn5']].mode(axis=1)"]}, {"cell_type": "markdown", "id": "a3832d50", "metadata": {}, "source": ["### Confusion Matrix"]}, {"cell_type": "code", "execution_count": 1, "id": "6e8dd3ea", "metadata": {}, "outputs": [], "source": ["cm = confusion_matrix(df_ensemble['y_true'],df_ensemble['y_pred'])\n\ndf_cm = pd.DataFrame(cm, index = ['negative','positive'],\n                  columns = ['negative','positive'])\nsn.set(font_scale=1.4) # for label size\nsn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size"]}, {"cell_type": "markdown", "id": "70bddb7d", "metadata": {}, "source": ["### Accuracy"]}, {"cell_type": "code", "execution_count": 1, "id": "5b301abe", "metadata": {}, "outputs": [], "source": ["accuracy_score(df_ensemble['y_true'],df_ensemble['y_pred'])"]}, {"cell_type": "markdown", "id": "ed4f3573", "metadata": {}, "source": ["### Precision"]}, {"cell_type": "code", "execution_count": 1, "id": "78968e48", "metadata": {}, "outputs": [], "source": ["precision_score(df_ensemble['y_true'],df_ensemble['y_pred'])"]}, {"cell_type": "markdown", "id": "10ba4165", "metadata": {}, "source": ["### Recall"]}, {"cell_type": "code", "execution_count": 1, "id": "56c9514c", "metadata": {}, "outputs": [], "source": ["recall_score(df_ensemble['y_true'],df_ensemble['y_pred'])"]}, {"cell_type": "markdown", "id": "c4f31a8a", "metadata": {}, "source": ["### F1 Score"]}, {"cell_type": "code", "execution_count": 1, "id": "85ede695", "metadata": {}, "outputs": [], "source": ["f1_score(df_ensemble['y_true'],df_ensemble['y_pred'], average='macro')"]}, {"cell_type": "markdown", "id": "9c5c96e2", "metadata": {}, "source": ["## 4. Conclusions"]}, {"cell_type": "markdown", "id": "3aafb8c3", "metadata": {}, "source": ["The model presents very interesting results regarding accuracy, precision, recal and F1, which means that the model, along with the undersample, handled well with the class unbalance.\n\nI hope this could be helpful somehow!"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}