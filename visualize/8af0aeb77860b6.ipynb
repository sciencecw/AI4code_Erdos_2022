{"cells": [{"cell_type": "markdown", "id": "dd62d815", "metadata": {}, "source": ["# Neural Oblivious Decision Ensembles\n\nEven though deep learning has attained trendendous success on data domains such as images, audio and texts.\nGDBT still rule the domain of tabular data.\n\nIn this notebook we will discussa deep learning architecture [Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data](https://arxiv.org/pdf/1909.06312.pdf) which is designed to work with any tabular data and generalizes ensembles of oblivious decision trees, but benefits from both end-to-end gradient-based optimization and the power of multi-layer hierarchical representation learning."]}, {"cell_type": "code", "execution_count": 1, "id": "05435b09", "metadata": {}, "outputs": [], "source": ["import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers as L\nimport tensorflow_addons as tfa\nfrom tensorflow_probability import distributions, stats\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import StratifiedKFold\nimport joblib\n"]}, {"cell_type": "markdown", "id": "35124ece", "metadata": {}, "source": ["# Data"]}, {"cell_type": "code", "execution_count": 1, "id": "e529e81e", "metadata": {}, "outputs": [], "source": ["data = pd.read_csv('../input/song-popularity-prediction/train.csv')\nprint(data.shape)\ndata.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "ec47704c", "metadata": {}, "outputs": [], "source": ["test = pd.read_csv('../input/song-popularity-prediction/test.csv')\nX_test = test.drop(['id'], axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "0b276d44", "metadata": {}, "outputs": [], "source": ["X = data.drop(['id', 'song_popularity'], axis=1)\ny = data['song_popularity']"]}, {"cell_type": "code", "execution_count": 1, "id": "eeeb1f9f", "metadata": {}, "outputs": [], "source": ["skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)"]}, {"cell_type": "markdown", "id": "675bd762", "metadata": {}, "source": ["# Model"]}, {"cell_type": "code", "execution_count": 1, "id": "15da3174", "metadata": {}, "outputs": [], "source": ["@tf.function\ndef sparsemoid(inputs):\n    return tf.clip_by_value(0.5 * inputs + 0.5, 0., 1.)\n\n\ndef get_binary_lookup_table(depth):\n    # output: binary tensor [depth, 2**depth, 2]\n    indices = tf.keras.backend.arange(0, 2**depth, 1)\n    offsets = 2 ** tf.keras.backend.arange(0, depth, 1)\n    bin_codes = (tf.reshape(indices, (1, -1)) // tf.reshape(offsets, (-1, 1)) % 2)\n    bin_codes = tf.stack([bin_codes, 1 - bin_codes], axis=-1)\n    bin_codes = tf.cast(bin_codes, 'float32')\n    binary_lut = tf.Variable(initial_value=bin_codes, trainable=False)\n    return binary_lut\n\n\ndef get_feature_selection_logits(n_trees, depth, dim):\n    initializer = tf.keras.initializers.random_uniform()\n    init_shape = (dim, n_trees, depth)\n    init_value = initializer(shape=init_shape, dtype='float32')\n    return tf.Variable(init_value, trainable=True)\n\n\ndef get_output_response(n_trees, depth, units):\n    initializer = tf.keras.initializers.random_uniform()\n    init_shape = (n_trees, units, 2**depth)\n    init_value = initializer(init_shape, dtype='float32')\n    return tf.Variable(initial_value=init_value, trainable=True)\n\n\ndef get_feature_thresholds(n_trees, depth):\n    initializer = tf.ones_initializer()\n    init_shape = (n_trees, depth)\n    init_value = initializer(shape=init_shape, dtype='float32')\n    return tf.Variable(init_value, trainable=True)\n\n\ndef get_log_temperatures(n_trees, depth):\n    initializer = tf.ones_initializer()\n    init_shape = (n_trees, depth)\n    init_value = initializer(shape=init_shape, dtype='float32')\n    return tf.Variable(initial_value=init_value, trainable=True)\n\n\ndef init_feature_thresholds(features, beta, n_trees, depth):\n    sampler = distributions.Beta(beta, beta)\n    percentiles_q = sampler.sample([n_trees * depth])\n\n    flattened_feature_values = tf.map_fn(tf.keras.backend.flatten, features)\n    percentile = stats.percentile(flattened_feature_values, 100*percentiles_q)\n    feature_thresholds = tf.reshape(percentile, (n_trees, depth))\n    return feature_thresholds\n\n\ndef init_log_temperatures(features, feature_thresholds):\n    input_threshold_diff = tf.math.abs(features - feature_thresholds)\n    log_temperatures = stats.percentile(input_threshold_diff, 50, axis=0)\n    return log_temperatures"]}, {"cell_type": "code", "execution_count": 1, "id": "df15568b", "metadata": {}, "outputs": [], "source": ["class ObliviousDecisionTree(L.Layer):\n    def __init__(self,n_trees=3, depth=4, units=1,threshold_init_beta=1.):\n        super().__init__()\n        self.initialized = False\n        self.n_trees = n_trees\n        self.depth = depth\n        self.units = units\n        self.threshold_init_beta = threshold_init_beta\n\n    def build(self, input_shape):\n        dim = input_shape[-1]\n        n_trees, depth, units = self.n_trees, self.depth, self.units\n        self.feature_selection_logits = get_feature_selection_logits(n_trees, depth, dim)\n        self.feature_thresholds = get_feature_thresholds(n_trees, depth)\n        self.log_temperatures = get_log_temperatures(n_trees, depth)\n        self.binary_lut = get_binary_lookup_table(depth)\n        self.response = get_output_response(n_trees, depth, units)\n\n    def _data_aware_initialization(self, inputs):\n        beta, n_trees, depth = self.threshold_init_beta, self.n_trees, self.depth\n        feature_values = self._get_feature_values(inputs)\n        feature_thresholds = init_feature_thresholds(feature_values, beta, n_trees, depth)\n        log_temperatures = init_log_temperatures(feature_values, feature_thresholds)\n        self.feature_thresholds.assign(feature_thresholds)\n        self.log_temperatures.assign(log_temperatures)\n\n    def _get_feature_values(self, inputs, training=None):\n        feature_selectors = tfa.activations.sparsemax(self.feature_selection_logits)\n        feature_values = tf.einsum('bi,ind->bnd', inputs, feature_selectors)\n        return feature_values\n\n    def _get_feature_gates(self, feature_values):\n        threshold_logits = (feature_values - self.feature_thresholds)\n        threshold_logits = threshold_logits * tf.math.exp(-self.log_temperatures)\n        threshold_logits = tf.stack([-threshold_logits, threshold_logits], axis=-1)\n        feature_gates = sparsemoid(threshold_logits)\n        return feature_gates\n\n    def _get_aggregated_response(self, feature_gates):\n        aggregated_gates = tf.einsum('bnds,dcs->bndc', feature_gates, self.binary_lut)\n        aggregated_gates = tf.math.reduce_prod(aggregated_gates, axis=-2)\n        aggregated_response = tf.einsum('bnc,nuc->bnu', aggregated_gates, self.response)\n        return aggregated_response\n\n    def call(self, inputs, training=None):\n        if not self.initialized:\n            self._data_aware_initialization(inputs)\n            self.initialized = True\n\n        feature_values = self._get_feature_values(inputs)\n        feature_gates = self._get_feature_gates(feature_values)\n        aggregated_response = self._get_aggregated_response(feature_gates)\n        response_averaged_over_trees = tf.reduce_mean(aggregated_response, axis=1)\n        return response_averaged_over_trees"]}, {"cell_type": "code", "execution_count": 1, "id": "1fb5d99c", "metadata": {}, "outputs": [], "source": ["class NODE(keras.Model):\n    def __init__(self, units=1, n_layers=1, link=tf.identity, n_trees=3, tree_depth=4, threshold_init_beta=1):\n        super().__init__()\n        self.units = units\n        self.n_layers = n_layers\n        self.n_trees = n_trees\n        self.tree_depth = tree_depth\n        self.units = units\n        self.threshold_init_beta = threshold_init_beta\n\n        self.bn = L.BatchNormalization()\n        self.ensemble = [ObliviousDecisionTree(\n            n_trees=n_trees, depth=tree_depth, units=units,threshold_init_beta=threshold_init_beta\n        ) for _ in range(n_layers)]\n        self.link = link\n\n    def call(self, inputs, training=None):\n        x = self.bn(inputs, training=training)\n        for tree in self.ensemble:\n            h = tree(x)\n            x = tf.concat([x, h], axis=1)\n        return self.link(h)"]}, {"cell_type": "code", "execution_count": 1, "id": "4489dde5", "metadata": {}, "outputs": [], "source": ["get_cat_pipeline = lambda: Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')), \n    ('encoder', OneHotEncoder(sparse=False))\n])\n\nget_num_pipeline = lambda: Pipeline([\n    ('imputer', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])"]}, {"cell_type": "code", "execution_count": 1, "id": "d1f6353c", "metadata": {}, "outputs": [], "source": ["class model_config:\n    NUMERIC_FEATURE_NAMES=[\n        'song_duration_ms', 'acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'loudness',\n        'speechiness', 'tempo', 'audio_valence'\n    ]\n    CATEGORICAL_FEATURE_NAMES=[\n        'key','audio_mode','time_signature'   \n    ]\n\nMAX_EPOCHS  = 250\n\nget_callbacks = lambda : [\n    keras.callbacks.EarlyStopping(min_delta=1e-4, patience=10, verbose=1, restore_best_weights=True),\n    keras.callbacks.ReduceLROnPlateau(patience=3, verbose=1)\n]"]}, {"cell_type": "markdown", "id": "13f5d301", "metadata": {}, "source": ["# Training"]}, {"cell_type": "code", "execution_count": 1, "id": "62d0e3dd", "metadata": {}, "outputs": [], "source": ["preds = []\n\nfor fold, (train_index, valid_index) in enumerate(skf.split(X, y)):\n    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n\n    num_pipeline = get_num_pipeline().fit(X_train[model_config.NUMERIC_FEATURE_NAMES])\n    cat_pipeline = get_cat_pipeline().fit(X_train[model_config.CATEGORICAL_FEATURE_NAMES])\n    \n    X_train = np.hstack((\n        num_pipeline.transform(X_train[model_config.NUMERIC_FEATURE_NAMES]),\n        cat_pipeline.transform(X_train[model_config.CATEGORICAL_FEATURE_NAMES])\n    ))\n    X_valid = np.hstack((\n        num_pipeline.transform(X_valid[model_config.NUMERIC_FEATURE_NAMES]),\n        cat_pipeline.transform(X_valid[model_config.CATEGORICAL_FEATURE_NAMES])\n    ))\n    X_test_ = np.hstack((\n        num_pipeline.transform(X_test[model_config.NUMERIC_FEATURE_NAMES]),\n        cat_pipeline.transform(X_test[model_config.CATEGORICAL_FEATURE_NAMES])\n    ))\n    \n    model = NODE()\n    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n    model.fit(\n        X_train, y_train, validation_data=(X_valid, y_valid), callbacks=get_callbacks(), \n        epochs=MAX_EPOCHS\n    )  \n    preds.append(model.predict(X_test_))"]}, {"cell_type": "markdown", "id": "51328d26", "metadata": {}, "source": ["# Submissions"]}, {"cell_type": "code", "execution_count": 1, "id": "67976ff6", "metadata": {}, "outputs": [], "source": ["submissions = pd.read_csv('../input/song-popularity-prediction/sample_submission.csv')\nsubmissions['song_popularity'] = np.array(preds).mean(axis=0)\nsubmissions.to_csv('preds.csv', index=False)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}