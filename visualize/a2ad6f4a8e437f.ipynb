{"cells": [{"cell_type": "markdown", "id": "addfcd33", "metadata": {}, "source": ["**In this , fare prediction of a cab service is done through journey details. Details like pickup and drop off locations, journey date and time and passenger details are provided. But certain other factors like if the journey was done during a weekday or a weekend or if the journey was done during daytime or nighttime etc. can also be responsible for the task. These factors are extracted from the given details and the prediction is done through a linear regression model. It is also seen how the fare prices vary with respect to the factors that are present and what are all the important factors contributing to the prediction through various hypothesis tests.**"]}, {"cell_type": "code", "execution_count": 1, "id": "bb7d700c", "metadata": {}, "outputs": [], "source": ["import pandas as pd\nimport numpy as np\nimport matplotlib as mlt\nimport matplotlib.pyplot as plt\nfrom scipy.stats import chi2_contingency\nimport seaborn as sns\nfrom random import randrange, uniform\nfrom sklearn import preprocessing"]}, {"cell_type": "markdown", "id": "b1e758b2", "metadata": {}, "source": ["**\u2022\tFrom the date and time of the journey details, the information regarding whether the journey was done during morning, afternoon, evening, or night was extracted.\n\u2022\tFrom the same details as above, features like the year, month, weekday, and the day in a month when the journey happened are also extracted.**"]}, {"cell_type": "code", "execution_count": 1, "id": "de9e9312", "metadata": {}, "outputs": [], "source": ["def daytime (row):\n    if (row['hour'] <= 6) or (row['hour'] > 22):\n        return (\"night\")\n    elif (row['hour'] > 6) and (row['hour'] <= 12):\n        return (\"morning\")\n    elif (row['hour'] > 12) and (row['hour'] <= 17):\n        return (\"afternoon\")\n    elif (row['hour'] > 17) and (row['hour'] <= 22):\n        return (\"evening\")\n\n    \ndef add_time_features(df):\n    df['year'] = df['pickup_datetime'].apply(lambda x: x.year)\n    df['month'] = df['pickup_datetime'].apply(lambda x: x.month)\n    df['day'] = df['pickup_datetime'].apply(lambda x: x.day)\n    df['hour'] = df['pickup_datetime'].apply(lambda x: x.hour)\n    df['weekday'] = df['pickup_datetime'].apply(lambda x: x.weekday())\n    df['pickup_datetime'] =  df['pickup_datetime'].apply(lambda x: str(x))\n    df['daytime'] = df.apply (lambda x: daytime(x), axis=1)\n    df = df.drop('pickup_datetime', axis=1)\n    df=df.drop('hour',axis=1)\n    df=df.drop('day',axis=1)\n    return df"]}, {"cell_type": "code", "execution_count": 1, "id": "44498937", "metadata": {}, "outputs": [], "source": ["df = pd.read_csv(\"../input/cabfare/Data.csv\")"]}, {"cell_type": "code", "execution_count": 1, "id": "87de99f9", "metadata": {}, "outputs": [], "source": ["df.info()"]}, {"cell_type": "code", "execution_count": 1, "id": "3582b6d4", "metadata": {}, "outputs": [], "source": ["df['pickup_datetime'] =  pd.to_datetime(df['pickup_datetime'], format='%Y-%m-%d %H:%M:%S %Z',errors='coerce')"]}, {"cell_type": "code", "execution_count": 1, "id": "e2f57cf9", "metadata": {}, "outputs": [], "source": ["df.isnull().sum()"]}, {"cell_type": "code", "execution_count": 1, "id": "65619ab6", "metadata": {}, "outputs": [], "source": ["df= add_time_features(df)"]}, {"cell_type": "code", "execution_count": 1, "id": "7759b9cc", "metadata": {}, "outputs": [], "source": ["df[\"year\"] = df[\"year\"].astype(object)\ndf[\"month\"] = df[\"month\"].astype(object)\ndf[\"weekday\"] = df[\"weekday\"].astype(object)"]}, {"cell_type": "markdown", "id": "d0358826", "metadata": {}, "source": ["**From the pickup and drop off latitude and longitude, the distance between the pickup and drop off points are extracted using a Python library called Geopy. Geopy makes it easy for Python developers to locate the coordinates of addresses, cities, countries, and landmarks across the globe using third-party geocoders and other data sources.**"]}, {"cell_type": "code", "execution_count": 1, "id": "32b04dd1", "metadata": {}, "outputs": [], "source": ["from geopy.distance import geodesic\nfrom geopy.distance import great_circle\ndf['great_circle']=df.apply(lambda x: great_circle((x['pickup_latitude'],x['pickup_longitude']), (x['dropoff_latitude'],   x['dropoff_longitude'])).miles, axis=1)\ndf['geodesic']=df.apply(lambda x: geodesic((x['pickup_latitude'],x['pickup_longitude']), (x['dropoff_latitude'],   x['dropoff_longitude'])).miles, axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "912c4418", "metadata": {}, "outputs": [], "source": ["df.info()"]}, {"cell_type": "markdown", "id": "1792e14c", "metadata": {}, "source": ["## Exploratory Data Analysis"]}, {"cell_type": "markdown", "id": "6fdbdfeb", "metadata": {}, "source": ["##### Cab Fare vs. Year"]}, {"cell_type": "code", "execution_count": 1, "id": "78a0d6b6", "metadata": {}, "outputs": [], "source": ["def time_analysis(df):\n    return pd.DataFrame({\"FareAverage\":np.mean(df.fare_amount),\"Count\":np.size(df.fare_amount),\"FareSum\":sum(df.fare_amount)},index=[\"Time\"] )"]}, {"cell_type": "code", "execution_count": 1, "id": "4cd521ff", "metadata": {}, "outputs": [], "source": ["df_yearly=df.groupby('year').apply(time_analysis).reset_index()\nsns.catplot(x=\"year\", y=\"FareAverage\", kind=\"bar\", data=df_yearly,color=\"c\",palette=\"dark\",height=3, aspect=1.5)\nsns.catplot(x=\"year\", y=\"Count\", kind=\"bar\", data=df_yearly,color=\"g\",palette=\"dark\",height=3, aspect=1.5)\nsns.catplot(x=\"year\", y=\"FareSum\", kind=\"bar\", data=df_yearly,color=\"m\",palette=\"dark\",height=3, aspect=1.5)"]}, {"cell_type": "markdown", "id": "0da99f85", "metadata": {}, "source": ["**The average cab fare is more or less the same over the years, but the total number of cab rides vary throughout. Thus, the corresponding revenue generated also varies accordingly and it increased initially and then decreased and another fluctuation happened later. There is no such pattern derived from the yearly analysis.**"]}, {"cell_type": "markdown", "id": "37597d95", "metadata": {}, "source": ["##### Cab Fare vs month"]}, {"cell_type": "code", "execution_count": 1, "id": "79d54d1d", "metadata": {}, "outputs": [], "source": ["df_monthly=df.groupby('month').apply(time_analysis).reset_index()\nsns.catplot(x=\"month\", y=\"FareAverage\", kind=\"bar\", data=df_monthly,color=\"c\",palette=\"dark\",height=3, aspect=1.5)\nsns.catplot(x=\"month\", y=\"Count\", kind=\"bar\", data=df_monthly,color=\"g\",palette=\"dark\",height=3, aspect=1.5)\nsns.catplot(x=\"month\", y=\"FareSum\", kind=\"bar\", data=df_monthly,color=\"m\",palette=\"dark\",height=3, aspect=1.5)"]}, {"cell_type": "markdown", "id": "4611551d", "metadata": {}, "source": ["**The average fare for all the months over the year has been more or less constant. However, it is seen that the count of rides and correspondingly the revenue generated over the years is maximum for the month of June. One reason can be educational institutes mostly start their academic sessions at that time. Plus, due to the extreme hot climate in the month of June, people usually prefer a cab ride. We can also see that during the end months the rides and total revenue generated is higher than the starting months because those are the months of festivals.**"]}, {"cell_type": "markdown", "id": "9a0154fc", "metadata": {}, "source": ["##### Cab Fare vs Weekday"]}, {"cell_type": "code", "execution_count": 1, "id": "6882c4de", "metadata": {}, "outputs": [], "source": ["df_weekly=df.groupby('weekday').apply(time_analysis).reset_index()\nsns.catplot(x=\"weekday\", y=\"FareAverage\", kind=\"bar\", data=df_weekly,color=\"c\",palette=\"dark\",height=3, aspect=1.5)\nsns.catplot(x=\"weekday\", y=\"Count\", kind=\"bar\", data=df_weekly,color=\"g\",palette=\"dark\",height=3, aspect=1.5)\nsns.catplot(x=\"weekday\", y=\"FareSum\", kind=\"bar\", data=df_weekly,color=\"m\",palette=\"dark\",height=3, aspect=1.5)"]}, {"cell_type": "markdown", "id": "e4565a39", "metadata": {}, "source": ["**Again, the average fare over the week is more or less the same. But it has been seen that the number of rides and correspondingly the revenue generated is slightly more during weekdays than during weekends mostly because of offices and educational institutes.**"]}, {"cell_type": "markdown", "id": "0286219e", "metadata": {}, "source": ["##### Cab Fare vs Daytime"]}, {"cell_type": "code", "execution_count": 1, "id": "5d5d250f", "metadata": {}, "outputs": [], "source": ["df_daily=df.groupby('daytime').apply(time_analysis).reset_index()\nsns.catplot(x=\"daytime\", y=\"FareAverage\", kind=\"bar\", data=df_daily,color=\"c\",palette=\"dark\",height=3, aspect=1)\nsns.catplot(x=\"daytime\", y=\"Count\", kind=\"bar\", data=df_daily,color=\"g\",palette=\"dark\",height=3, aspect=1)\nsns.catplot(x=\"daytime\", y=\"FareSum\", kind=\"bar\", data=df_daily,color=\"m\",palette=\"dark\",height=3, aspect=1)"]}, {"cell_type": "markdown", "id": "562b5701", "metadata": {}, "source": ["**Cab rides are much more during the morning time because people are generally going to workplaces and also during the evening time as people generally travelling back from offices, going out for dinner,movies,hanging out after college/office. Also the average cab fare is more doing night due to night fare supplement charges.**"]}, {"cell_type": "markdown", "id": "171e0b83", "metadata": {}, "source": ["##### Cab fare vs Passenger count, distance, pick up and drop off latitude and longitude."]}, {"cell_type": "code", "execution_count": 1, "id": "d5bcaced", "metadata": {}, "outputs": [], "source": ["df1=df[['passenger_count',\"pickup_longitude\",\"pickup_latitude\",\"dropoff_longitude\",\"dropoff_latitude\", 'great_circle',\"geodesic\",\"fare_amount\"]]\nsns.pairplot(df1)"]}, {"cell_type": "markdown", "id": "665b2550", "metadata": {}, "source": ["**As we can see that the fare amount is directly proportional to the distance but for other variables further analysis has to be done.**"]}, {"cell_type": "markdown", "id": "a592d66f", "metadata": {}, "source": ["###### Due to lesser number of unique values in passenger count, we will treat it as a categorical variable"]}, {"cell_type": "code", "execution_count": 1, "id": "345079a7", "metadata": {}, "outputs": [], "source": ["df[\"passenger_count\"] = df[\"passenger_count\"].astype(object)"]}, {"cell_type": "markdown", "id": "c6e4476a", "metadata": {}, "source": ["## Correlation Analysis"]}, {"cell_type": "markdown", "id": "def1bab1", "metadata": {}, "source": ["Two independent continuous variables are checked at a time if they move together directionally. If yes, one should be removed. Because that could lead to biasness in the model. (Also, one continuous independent variable is taken, checked if it is highly correlated with the target variable if it is continuous too. They should move together directionality).\n\nH0: Two variables are independent\n\nH1: Two variables are not independent\n\n\u2022 If p-value is less than 0.05 then the null hypothesis is rejected saying that 2 variables are dependent.\n\n\u2022 And if p-value is greater than 0.05 then the null hypothesis is accepted saying that 2 variables are independent."]}, {"cell_type": "code", "execution_count": 1, "id": "78b033ea", "metadata": {}, "outputs": [], "source": ["ncol=[\"great_circle\",\"geodesic\",\"pickup_longitude\",\"pickup_latitude\",\"dropoff_longitude\",\"dropoff_latitude\",\"fare_amount\"]"]}, {"cell_type": "code", "execution_count": 1, "id": "db5dc0f1", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(10,10))\n_ = sns.heatmap(df[ncol].corr(), square=True, cmap='RdYlGn',linewidths=1,linecolor='w',annot=True)\nplt.title('Correlation matrix ')\nplt.show()"]}, {"cell_type": "markdown", "id": "01ced4fe", "metadata": {}, "source": ["\"pickup_longitude\",\"pickup_latitude\",\"dropoff_longitude\",\"dropoff_latitude\" are not that correlated with fare amount. Hence, they are dropped.\nGreat circle and geodesic are highly correlated with each other, hence dropping great circle is dropped.\ngeodesic and fare amount are highly correlated with each other. Its p value is calculated. The p value for the above relations can also be calculated in the same way."]}, {"cell_type": "code", "execution_count": 1, "id": "c8eb0efb", "metadata": {}, "outputs": [], "source": ["import scipy.stats as stats\n_ = sns.jointplot(x='fare_amount',y='geodesic',data=df,kind = 'reg')\n_.annotate(stats.pearsonr)\nplt.show()"]}, {"cell_type": "markdown", "id": "74e9f50e", "metadata": {}, "source": ["Fare amount and geodesic are highly correlated with each other and p=0, hence H0 is rejected stating that they are dependent which is a must need condition for linear regression."]}, {"cell_type": "code", "execution_count": 1, "id": "126066e6", "metadata": {}, "outputs": [], "source": ["df=df.drop([\"great_circle\",\"pickup_longitude\",\"pickup_latitude\",\"dropoff_longitude\",\"dropoff_latitude\"],axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "8579bb18", "metadata": {}, "outputs": [], "source": ["df.info()"]}, {"cell_type": "markdown", "id": "2a50ad6b", "metadata": {}, "source": ["## Chi-square test of Independence for Categorical Variables/Features"]}, {"cell_type": "markdown", "id": "5bba2357", "metadata": {}, "source": ["Similar analysis is done here as it was in correlation test but with the categorical variables.\n\nHypothesis testing:\n\nNull Hypothesis: 2 variables are independent.\n\nAlternate Hypothesis: 2 variables are not independent.\n\nIf p-value is less than 0.01 then the null hypothesis is rejected saying that 2 variables are dependent.\nAnd if p-value is greater than 0.01 then the null hypothesis is accepted saying that 2 variables are independent.\nAlpha here is taken as 0.01 as majority of the variables in the data are categorical variables and it is unfair to remove them based on small amount of dependencies with others."]}, {"cell_type": "code", "execution_count": 1, "id": "2e29bbf8", "metadata": {}, "outputs": [], "source": ["# Import label encoder \ncolnames = list(df.columns)\nfrom sklearn import preprocessing \n\n# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \n  \nfor col in colnames:\n    if df[col].dtype==object:\n        df[col]= label_encoder.fit_transform(df[col])"]}, {"cell_type": "code", "execution_count": 1, "id": "cad3909e", "metadata": {}, "outputs": [], "source": ["cat_var=[\"passenger_count\",\"year\",\"month\",\"weekday\",\"daytime\"] \ncatdf=df[cat_var]"]}, {"cell_type": "code", "execution_count": 1, "id": "ccc200b0", "metadata": {}, "outputs": [], "source": ["from sklearn.feature_selection import chi2\nn= 10\nfor i in range(0,4):\n    X=catdf.iloc[:,i+1:n]\n    y=catdf.iloc[:,i]\n    chi_scores = chi2(X,y)\n    p_values = pd.Series(chi_scores[1],index = X.columns)\n    print(\"for\",i)\n    print(p_values)\n    for j in range (0, len(p_values)):\n        if (p_values[j]<0.01):\n            print(p_values[j])"]}, {"cell_type": "markdown", "id": "3337397d", "metadata": {}, "source": ["After the analysis it is seen that year, month and weekday are dependent on others, p value is less than 0.01, hence rejecting H0 for their relations and dropping them is done."]}, {"cell_type": "code", "execution_count": 1, "id": "79a7abd0", "metadata": {}, "outputs": [], "source": ["df=df.drop([\"year\",\"month\",\"weekday\"],axis=1)"]}, {"cell_type": "markdown", "id": "02d1a359", "metadata": {}, "source": ["## Anova test"]}, {"cell_type": "markdown", "id": "965bf8fa", "metadata": {}, "source": ["It is carried out to compare between each group in a categorical variable. ANOVA is done to check if the means for different groups are same or not. It does not help us to identify which mean is different.\n\nHypothesis testing:\n\nNull Hypothesis: mean of all categories in a variable are same.\n\nAlternate Hypothesis: mean of at least one category in a variable is different.\n\nIf p-value is less than 0.05 then we reject the null hypothesis.\nAnd if p-value is greater than 0.05 then we accept the null hypothesis.\n"]}, {"cell_type": "code", "execution_count": 1, "id": "367c7783", "metadata": {}, "outputs": [], "source": ["df.info()"]}, {"cell_type": "code", "execution_count": 1, "id": "f57cedf4", "metadata": {}, "outputs": [], "source": ["import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nmodel = ols('fare_amount ~ C(passenger_count)+C(daytime)',data=df).fit()\naov_table = sm.stats.anova_lm(model)\naov_table"]}, {"cell_type": "code", "execution_count": 1, "id": "3cca6fde", "metadata": {}, "outputs": [], "source": ["probanova=list(aov_table[\"PR(>F)\"])\nfor i in range(0,3):\n    if probanova[i]>0.05:\n        print(i)"]}, {"cell_type": "markdown", "id": "f935ade4", "metadata": {}, "source": ["No variable has same mean for all the categories. P value is less than 0.05, thus H0 is rejected."]}, {"cell_type": "markdown", "id": "72b90953", "metadata": {}, "source": ["## VIF Test"]}, {"cell_type": "markdown", "id": "6af36a9d", "metadata": {}, "source": ["This test is to check if there is any multicollinearity left in the data after all the above statistical tests. VIF is always greater or equal to 1.\n\nif VIF is 1 --- Not correlated to any of the variables.\n\nif VIF is between 1-5 --- Moderately correlated.\n\nif VIF is above 5 --- Highly correlated.\n\n\u2022\tVIF determines the strength of the correlation between the independent variables. It is predicted by taking a variable and regressing it against every other variable.\n\n\u2022\tVIF score of an independent variable represents how well the variable is explained by other independent variables.\n \nThe closer the R^2 value to 1, the higher the value of VIF and the higher the multicollinearity with the independent variable."]}, {"cell_type": "code", "execution_count": 1, "id": "ec3481b9", "metadata": {}, "outputs": [], "source": ["from statsmodels.stats.outliers_influence import variance_inflation_factor\ndef calc_vif(X):\n    vif = pd.DataFrame()\n    vif[\"variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    return(vif)"]}, {"cell_type": "code", "execution_count": 1, "id": "e672cebf", "metadata": {}, "outputs": [], "source": ["df1=df.drop([\"fare_amount\"],axis=1)\ncalc_vif(df1)"]}, {"cell_type": "markdown", "id": "7eb32841", "metadata": {}, "source": ["**None of the remaining variables have high multicollinearity**"]}, {"cell_type": "code", "execution_count": 1, "id": "81f1e98d", "metadata": {}, "outputs": [], "source": ["df[\"passenger_count\"] = df[\"passenger_count\"].astype(object)\ndf[\"daytime\"] = df[\"daytime\"].astype(object)"]}, {"cell_type": "markdown", "id": "6f5e3806", "metadata": {}, "source": ["## Converting passenger count and daytime to dummy variable"]}, {"cell_type": "code", "execution_count": 1, "id": "167b1c8a", "metadata": {}, "outputs": [], "source": ["df = pd.get_dummies(df, drop_first=True)\ndf.info()"]}, {"cell_type": "markdown", "id": "34796f8c", "metadata": {}, "source": ["## Multiple regression model"]}, {"cell_type": "code", "execution_count": 1, "id": "5da9501e", "metadata": {}, "outputs": [], "source": ["from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nx = df.drop('fare_amount',axis=1).values\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\nX = pd.DataFrame(x_scaled)\ny = df['fare_amount'].values"]}, {"cell_type": "code", "execution_count": 1, "id": "24812df1", "metadata": {}, "outputs": [], "source": ["model = sm.OLS(y,X).fit()\nmodel.summary()"]}, {"cell_type": "markdown", "id": "0bd3f527", "metadata": {}, "source": ["1) Here the R squared statistic value indicates that 97.3 percentage of the variance in the dependent variable is explained by independent variables collectively. So, the model does a good job explaining the changes in the dependent variable. Adjusted R square is the same as R squared stating all variables are significant.\n\n2) H0: Variables are not carrying any information towards the target variable. (b=0)\nH1: Variables are carrying info towards target variable. (b != 0)\nHere it can be seen that F-statistic value is exceptionally large and p value is less than 0.05, thus H0 is rejected stating that the variables have a linear relationship and are carrying info towards target variable. (b != 0).\n\n3) The maximum value for the log of the likelihood function is -22083, the likelihood that the process described by the model produced the data that were observed (maximise the probability of observing the data).\n\n4) Omnibus is a test of the skewness and kurtosis of the residual. The value is relatively high, and the probability of omnibus is relatively low indicating that the residual is not normally distributed.\n\n5) Even the skew value is not close to 0 confirming the above result.\n\n6) DW value suggests that there is positive autocorrelation. That is, error of a given sign tends to be followed by an error of the same sign. For example, positive errors are usually followed by positive errors, and negative errors are usually followed by negative errors.\n\n7) Kurtosis of the normal distribution is 3.0. In this case it is close to 5, validates the other results.\n\n8) A large JB value is seen and the probability of JB is 0 indicating that the errors are not normally distributed.\n\n9) In linear regression the condition number of the moment matrix can be used as a diagnostic for multicollinearity. A relatively small number (<30) is required, in this case it is.\n"]}, {"cell_type": "markdown", "id": "852f3c23", "metadata": {}, "source": ["## Recommendations\n\n## (Ways to deal with Non Normal Residual Distribution and positive autocorrelation)\n\n1.\tOne should not remove outliers just because they make the distribution of the residuals non-normal. We may examine the case that has that high residual and see if there are problems with it (the easiest would be if it is a data entry error).\n\n2.\tAssuming there is no good reason to remove that observation, one can run the regression with and without it and see if there are any large differences in the parameter estimates; if not, you can leave it and note that removing it made little difference.\n\n3.\tIf it makes a big difference, the choice of the OLS model itself may be entirely wrong for this data set. It may be needed to look at alternate models. One could try robust regression, which deals with outliers or quantile regression or any other regression model that make no assumptions about the distribution of the residuals.\n\n4.\tSome key explanatory variables might have been left out which is causing some signal to leak into the residuals in the form of autocorrelations. If one can use one residual to predict the next residual, there is some predictive information present that is not captured by the predictors. Typically, this situation involves time-ordered observations. For example, if a residual is more likely to be followed by another residual that has the same sign, adjacent residuals are positively correlated. One can include a variable that captures the relevant time-related information or use a time series analysis.\n\n5.\tMaybe one can transform the response variable to make the distribution of the random errors approximately normal, fit the model, transform the predicted values back into the original units using the inverse of the transformation applied to the response variable.\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}