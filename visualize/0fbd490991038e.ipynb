{"cells": [{"cell_type": "markdown", "id": "a5201276", "metadata": {}, "source": ["# Context\nFashion-MNIST is a dataset of Zalando's article images\u2014consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Zalando intends Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\n\nThe original MNIST dataset contains a lot of handwritten digits. Members of the AI/ML/Data Science community love this dataset and use it as a benchmark to validate their algorithms. In fact, MNIST is often the first dataset researchers try. \n\nEach image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total.\nEach pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255.\n\nEach training and test example is assigned to one of the following labels:\n0. T-shirt/top\n1. Trouser\n2. Pullover\n3. Dress\n4. Coat\n5. Sandal\n6. Shirt\n7. Sneaker\n8. Bag\n9. Ankle boot\n\nI will classify the different images of the fashion MNIST dataset in those 10 labels."]}, {"cell_type": "markdown", "id": "889233fe", "metadata": {}, "source": ["# Table of Contents\n1. Data exploration\n2. Data formatting\n3. Model Training\n4. Evaluating Model"]}, {"cell_type": "code", "execution_count": 1, "id": "e8ec1ca2", "metadata": {}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\nimport itertools\nimport os\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Activation\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers.normalization import BatchNormalization\n\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau, LearningRateScheduler\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Input data files are available in the \"../input/\" directory.\nprint(os.listdir(\"../input\"))\n\n# Setting Random Seed for Reproducibilty.\nseed = 66\nnp.random.seed(seed)"]}, {"cell_type": "markdown", "id": "7bc66b3f", "metadata": {}, "source": ["# Data exploration\nFirst, I quickly checked the data format"]}, {"cell_type": "code", "execution_count": 1, "id": "1fc57ade", "metadata": {}, "outputs": [], "source": ["data_train = pd.read_csv('../input/fashion-mnist_train.csv')\ndata_test = pd.read_csv('../input/fashion-mnist_test.csv')\n\ndata_train.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "e64c590b", "metadata": {}, "outputs": [], "source": ["data_test.shape"]}, {"cell_type": "markdown", "id": "fe69094c", "metadata": {}, "source": ["There are 60 000 examples in the train dataset and 10 000 in the test dataset."]}, {"cell_type": "code", "execution_count": 1, "id": "7b188100", "metadata": {}, "outputs": [], "source": ["data_train.head()"]}, {"cell_type": "markdown", "id": "471c8553", "metadata": {}, "source": ["The datasets are both composed of 785 columns.<br/>\n28*28 = 784 pixels per image.<br/>\nThe last column named \"label\" represents the class.<br/>\n<br/>\nNow, I verify that there isn't any null values in the datasets."]}, {"cell_type": "code", "execution_count": 1, "id": "831a7feb", "metadata": {}, "outputs": [], "source": ["data_train.isnull().any().describe()"]}, {"cell_type": "code", "execution_count": 1, "id": "a32689d7", "metadata": {}, "outputs": [], "source": ["data_test.isnull().any().describe()"]}, {"cell_type": "code", "execution_count": 1, "id": "f57eb984", "metadata": {}, "outputs": [], "source": ["data_train.label.value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "a81ff9e8", "metadata": {}, "outputs": [], "source": ["data_test.label.value_counts()"]}, {"cell_type": "markdown", "id": "6c70ac31", "metadata": {}, "source": ["We can see that the class are balanced in both datasets (10% for each class). The number of different possible labels is the same.<br/>\nTo visualize what our data represents, here is a plot of the first object."]}, {"cell_type": "code", "execution_count": 1, "id": "c8c7fc77", "metadata": {}, "outputs": [], "source": ["img = data_train.drop('label', axis=1).values[0].reshape(28,28)\nplt.imshow(img, cmap='gray')\nplt.colorbar()"]}, {"cell_type": "markdown", "id": "70f2ecaf", "metadata": {}, "source": ["# Data formatting\nI will run household cross-validation with 25% of the data in the validation dataset.<br/>\nThe dataset seems large enough (especially with data augmentation) and the data simple enough to run only a household cross-validation and not a kfold cross-validation.\n"]}, {"cell_type": "code", "execution_count": 1, "id": "a3bde090", "metadata": {}, "outputs": [], "source": ["img_rows, img_cols = 28, 28\ninput_shape = (img_rows, img_cols, 1)\n\nX = np.array(data_train.iloc[:, 1:])\ny = to_categorical(np.array(data_train.iloc[:, 0]))\n\n# I have tried running kfold cross-validation, but the running time is longer and the performances aren't increased. \n# Therefore, I believed the dataset was large enough (especially with data augmentation) to only run a household cross-validation\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=seed)\n\n#Test data\nX_test = np.array(data_test.iloc[:, 1:])\ny_test = to_categorical(np.array(data_test.iloc[:, 0]))\n\nX_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\nX_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\nX_val = X_val.reshape(X_val.shape[0], img_rows, img_cols, 1)\n\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_val = X_val.astype('float32')\nX_train /= 255\nX_test /= 255\nX_val /= 255"]}, {"cell_type": "code", "execution_count": 1, "id": "ad4edd9c", "metadata": {}, "outputs": [], "source": ["batch_size = 256\nnum_classes = 10\nepochs = 75\n\ndata_generator = ImageDataGenerator(\n        rotation_range = 3,\n        zoom_range = 0.1,\n        shear_range = 0.3,\n        width_shift_range=0.08,\n        height_shift_range=0.08,\n        vertical_flip=False)\n\ndata_generator.fit(X_train)\n\nreduce_lr = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)"]}, {"cell_type": "markdown", "id": "adc23400", "metadata": {}, "source": ["# Model training\nThe first model I used was a ConvNet (keras implementation of [Gabriel Preda](https://www.kaggle.com/gpreda/cnn-with-tensorflow-keras-for-fashion-mnist)), and the second one a miniVGGNet (keras implementation of [Adrian Rosebrock](https://www.pyimagesearch.com/2019/02/11/fashion-mnist-with-keras-and-deep-learning/)).<br/>\nConvNet has less parameters and trains way faster than the miniVGGNet.<br/>\nScores were close but miniVGGNet was getting higher scores in validation, so I chose to use this one."]}, {"cell_type": "code", "execution_count": 1, "id": "e813710d", "metadata": {}, "outputs": [], "source": ["class basicConvNet():\n    @staticmethod\n    def build(input_shape, num_classes):\n        # Builds a basic ConvNet\n        # Returns Keras model object\n        model = Sequential()\n        \n        model.add(Conv2D(32, kernel_size=(3, 3),\n                         activation='relu',\n                         kernel_initializer='he_normal',\n                         input_shape=input_shape))\n        model.add(MaxPooling2D((2, 2)))\n        model.add(Dropout(0.25))\n        model.add(Conv2D(64, (3, 3), activation='relu'))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(0.25))\n        model.add(Conv2D(128, (3, 3), activation='relu'))\n        model.add(Dropout(0.4))\n        model.add(Flatten())\n        model.add(Dense(128, activation='relu'))\n        model.add(Dropout(0.3))\n        model.add(Dense(num_classes, activation='softmax'))\n        \n        return model"]}, {"cell_type": "code", "execution_count": 1, "id": "b92a1e8c", "metadata": {}, "outputs": [], "source": ["class miniVGGNet():\n    @staticmethod\n    def build(input_shape, num_classes):\n        # Builds a MiniVGGNet\n        # Returns Keras model object\n        model = Sequential()\n\n        # first CONV => RELU => CONV => RELU => POOL layer set\n        model.add(Conv2D(32, (3, 3), padding=\"same\",\n            input_shape=input_shape))\n        model.add(Activation(\"relu\"))\n        model.add(BatchNormalization(axis=-1))\n        model.add(Conv2D(32, (3, 3), padding=\"same\"))\n        model.add(Activation(\"relu\"))\n        model.add(BatchNormalization(axis=-1))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(0.25))\n \n        # second CONV => RELU => CONV => RELU => POOL layer set\n        model.add(Conv2D(64, (3, 3), padding=\"same\"))\n        model.add(Activation(\"relu\"))\n        model.add(BatchNormalization(axis=-1))\n        model.add(Conv2D(64, (3, 3), padding=\"same\"))\n        model.add(Activation(\"relu\"))\n        model.add(BatchNormalization(axis=-1))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(0.25))\n \n        # first (and only) set of FC => RELU layers\n        model.add(Flatten())\n        model.add(Dense(512))\n        model.add(Activation(\"relu\"))\n        model.add(BatchNormalization())\n        model.add(Dropout(0.5))\n \n        # softmax classifier\n        model.add(Dense(num_classes))\n        model.add(Activation(\"softmax\"))\n \n        # return the constructed network architecture\n        return model"]}, {"cell_type": "code", "execution_count": 1, "id": "d49439a7", "metadata": {}, "outputs": [], "source": ["# model = basicConvNet.build(input_shape, num_classes)\nmodel = miniVGGNet.build(input_shape, num_classes)\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adam(),\n              metrics=['accuracy'])\nmodel.summary()"]}, {"cell_type": "code", "execution_count": 1, "id": "5b03ec0e", "metadata": {}, "outputs": [], "source": ["history = model.fit_generator(data_generator.flow(X_train, y_train, batch_size = batch_size), \n                              epochs = epochs, \n                              validation_data = (X_val, y_val),\n                              verbose=1, \n                              steps_per_epoch=X_train.shape[0] // batch_size,\n                              callbacks = [reduce_lr])"]}, {"cell_type": "markdown", "id": "cf11bc86", "metadata": {}, "source": ["Checking if the model is overfitting with loss and val_loss of the training history"]}, {"cell_type": "code", "execution_count": 1, "id": "348d46b5", "metadata": {}, "outputs": [], "source": ["%matplotlib inline\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title(\"Model Loss\")\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(['Train', 'Test'])\nplt.show()\n\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title(\"Model Accuracy\")\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(['Train', 'Test'])\nplt.show()"]}, {"cell_type": "markdown", "id": "7b027a8e", "metadata": {}, "source": ["Validation loss decreases and validation accuracy increases. Model does not seem to overfit.<br/>\nModel loss stopped decreasing and model accuracy stopped increasing. Model does not seem to underfit (dataset is also large enough)."]}, {"cell_type": "markdown", "id": "b23ea839", "metadata": {}, "source": ["# Evaluating model\nI will now apply the model to the test set to see how well it performs"]}, {"cell_type": "code", "execution_count": 1, "id": "aed99e12", "metadata": {}, "outputs": [], "source": ["scores = model.evaluate(X_test, y_test, verbose=0)\nprint('Loss on test dataset:', scores[0])\nprint('Accuracy on test dataset:', scores[1])"]}, {"cell_type": "markdown", "id": "d64341a2", "metadata": {}, "source": ["The model seems to generalize well."]}, {"cell_type": "code", "execution_count": 1, "id": "ef1cf63f", "metadata": {}, "outputs": [], "source": ["def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Oranges):\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Predict the values from the validation dataset\nY_pred = model.predict(X_test)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(y_test,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, \n            classes = ['T-shirt/Top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle Boot'])"]}, {"cell_type": "markdown", "id": "85aa46bc", "metadata": {}, "source": ["On the confusion matrix, it seems that the model is mostly having issues to classify shirts. A shirt being very close to a pullover or a t-shirt, this isn't very surprising."]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}