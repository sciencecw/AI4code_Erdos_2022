{"cells": [{"cell_type": "markdown", "id": "993de04c", "metadata": {}, "source": ["# Author of this notebook: Fanglida Yan"]}, {"cell_type": "markdown", "id": "5de01a13", "metadata": {}, "source": ["* [feature preprocessing](#section-1)\n    - [change US to us](#subsection-1)\n    - [lower case](#subsection-2)\n    - [recover abbreviations](#subsection-3)\n    - [remove everything except words, underscores and white spaces](#subsection-4)\n    - [tokenization](#subsection-5)\n    - [lemmatization](#subsection-6)\n    - [remove stop words](#subsection-7)\n* [Ideas for EDA](#section-2) \n    - [number of 1's vs 0's](#subsection-21)\n    - [Test how weekday Monday/Tuesday/etc affect the label](#subsection-22)\n    - [word map, top 1000 most frequent words](#subsection-23)\n    - [see if the most frequent words are the same for top1, top2, etc](#subsection-24)\n    - [test how presence of frequent words affect the result 1 and 0](#subsection-25)\n    - [correlation matrix of features](#subsection-26)\n    - [use tf-idf as features and repeat the analysis](#subsection-27)\n* [Simple models for benchmarks](#section-3) "]}, {"cell_type": "code", "execution_count": 1, "id": "2219f59a", "metadata": {}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]}, {"cell_type": "markdown", "id": "63fda446", "metadata": {}, "source": ["**import all necessary libraries**"]}, {"cell_type": "code", "execution_count": 1, "id": "cb8b4207", "metadata": {}, "outputs": [], "source": ["import math\nimport numpy as np\nimport pandas as pd\nimport nltk\nimport matplotlib.pyplot as plt\nimport re\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.corpus import wordnet # for pos tagging, pos is verb noun adj ect for lemmatization\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nfrom collections import Counter\nimport tensorflow as tf\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Input, Lambda, Dense, Concatenate, Dropout, Softmax\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report"]}, {"cell_type": "markdown", "id": "b43ca8e1", "metadata": {}, "source": ["# read csv as pandas dataframe"]}, {"cell_type": "code", "execution_count": 1, "id": "176bdfbd", "metadata": {}, "outputs": [], "source": ["djia_df = pd.read_csv(\"/kaggle/input/stocknews/upload_DJIA_table.csv\")\nnews_and_djia_df = pd.read_csv(\"/kaggle/input/stocknews/Combined_News_DJIA.csv\")\nnews_df = pd.read_csv(\"/kaggle/input/stocknews/RedditNews.csv\")\nnews_and_djia_df_og = pd.read_csv(\"/kaggle/input/stocknews/Combined_News_DJIA.csv\")"]}, {"cell_type": "code", "execution_count": 1, "id": "4ef22d06", "metadata": {}, "outputs": [], "source": ["#news_and_djia_df.head(3)"]}, {"cell_type": "code", "execution_count": 1, "id": "e2444c61", "metadata": {}, "outputs": [], "source": ["news_and_djia_df.info()"]}, {"cell_type": "markdown", "id": "5d1c5fad", "metadata": {}, "source": ["<a id=\"section-1\"></a>\n# 1. Feature preprocessing\n\n## (a) change US to us\n## (b) ower case \n## (c) recover abbreviations (change they'll to they will, etc)\n## (d) remove everything except words, underscores and white spaces\n## (e) tokenization\n## (f) lemmatization\n## (g) remove stop words"]}, {"cell_type": "code", "execution_count": 1, "id": "b09f406f", "metadata": {}, "outputs": [], "source": ["news_columns=news_and_djia_df.columns[2:]\nnews_columns"]}, {"cell_type": "markdown", "id": "67a398d7", "metadata": {}, "source": ["**try lowering case but found some problems with top23 top24 and top25 news**"]}, {"cell_type": "code", "execution_count": 1, "id": "e9e740bb", "metadata": {}, "outputs": [], "source": ["for column in news_columns:\n    try:\n        news_and_djia_df[column].apply(lambda x : x.lower())\n    except:\n        print(column)"]}, {"cell_type": "markdown", "id": "4954b356", "metadata": {}, "source": ["**value_counts function is very useful in this case**"]}, {"cell_type": "code", "execution_count": 1, "id": "49c027ef", "metadata": {}, "outputs": [], "source": ["news_and_djia_df['Top23'].apply(lambda x : type(x)).value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "6f984b14", "metadata": {}, "outputs": [], "source": ["news_and_djia_df['Top23'][news_and_djia_df['Top23'].apply(lambda x : type(x))!=type('something')]"]}, {"cell_type": "code", "execution_count": 1, "id": "3e024e5d", "metadata": {}, "outputs": [], "source": ["news_and_djia_df['Top24'][news_and_djia_df['Top24'].apply(lambda x : type(x))!=type('something')]"]}, {"cell_type": "code", "execution_count": 1, "id": "4436cf51", "metadata": {}, "outputs": [], "source": ["news_and_djia_df['Top25'][news_and_djia_df['Top25'].apply(lambda x : type(x))!=type('something')]"]}, {"cell_type": "markdown", "id": "5a2f92ba", "metadata": {}, "source": ["**it seems that almost every news start with b' so we are going to impute the missing values with b'**"]}, {"cell_type": "code", "execution_count": 1, "id": "0763a390", "metadata": {}, "outputs": [], "source": ["news_and_djia_df['Top23']=news_and_djia_df['Top23'].replace(float(\"nan\"),\"b'\");\nnews_and_djia_df['Top24']=news_and_djia_df['Top24'].replace(float(\"nan\"),\"b'\");\nnews_and_djia_df['Top25']=news_and_djia_df['Top25'].replace(float(\"nan\"),\"b'\");"]}, {"cell_type": "code", "execution_count": 1, "id": "769a7e6e", "metadata": {}, "outputs": [], "source": ["news_and_djia_df['Top23'][277]"]}, {"cell_type": "markdown", "id": "7f0b6043", "metadata": {}, "source": ["**make sure that we have taken care of the missing values**"]}, {"cell_type": "code", "execution_count": 1, "id": "d246ff1b", "metadata": {}, "outputs": [], "source": ["for column in news_columns:\n    try:\n        news_and_djia_df[column].apply(lambda x : x.lower())\n    except:\n        print(column)"]}, {"cell_type": "markdown", "id": "9726449f", "metadata": {}, "source": ["<a id=\"subsection-1\"></a>\n## (a) before lowering case, change US to usa (otherwise US will become us which is not a country)"]}, {"cell_type": "code", "execution_count": 1, "id": "e103ffd8", "metadata": {}, "outputs": [], "source": ["def US_to_america(news):\n    return re.sub(r'US', 'usa', news)\n    #return re.sub(r'UK', 'united kingdom', news) unlike us, uk is not a word so it's probably ok\n    #return re.sub(r'EU', 'european union', news)\n\nfor column in news_columns:\n    news_and_djia_df[column]=news_and_djia_df[column].apply(lambda x : US_to_america(x))"]}, {"cell_type": "markdown", "id": "35138242", "metadata": {}, "source": ["<a id=\"subsection-2\"></a>\n## (b) now we can lower case all news"]}, {"cell_type": "code", "execution_count": 1, "id": "6599748d", "metadata": {}, "outputs": [], "source": ["for column in news_columns:\n    news_and_djia_df[column]=news_and_djia_df[column].apply(lambda x : x.lower())"]}, {"cell_type": "markdown", "id": "044a5e96", "metadata": {}, "source": ["**Are there any news that doesn't begin with b'? It seems that the answer is yes. The news doesn't begin with b' starting from number 477 (except the rows with missing values, we imputed them with b').**"]}, {"cell_type": "code", "execution_count": 1, "id": "480b61e6", "metadata": {}, "outputs": [], "source": ["for column in news_columns[:3]:\n    mask1 = news_and_djia_df[column].apply(lambda x : x[:2])!=\"b'\"\n    mask2 = news_and_djia_df[column].apply(lambda x : x[:2])!='b\"'\n    mask = np.bitwise_and(mask1, mask2)\n    print(column)\n    print(news_and_djia_df[column][mask].head(3))\n    print()"]}, {"cell_type": "markdown", "id": "e58c5f6c", "metadata": {}, "source": ["**remove the b' and b'' at the beginning of news**"]}, {"cell_type": "code", "execution_count": 1, "id": "a29f8920", "metadata": {}, "outputs": [], "source": ["for column in news_columns:\n    mask1 = news_and_djia_df[column].apply(lambda x : x[:2])==\"b'\"\n    mask2 = news_and_djia_df[column].apply(lambda x : x[:2])=='b\"'\n    mask = np.logical_or(mask1, mask2)\n    for i in range(mask.shape[0]):\n        if mask.loc[i] == True:\n            news_and_djia_df.loc[mask.index[i],column] =  news_and_djia_df.loc[mask.index[i],column][2:]"]}, {"cell_type": "code", "execution_count": 1, "id": "5a1b9513", "metadata": {}, "outputs": [], "source": ["print(news_and_djia_df.iloc[1601,3])\nnews_and_djia_df_og.iloc[1601,3]"]}, {"cell_type": "markdown", "id": "a29b8213", "metadata": {}, "source": ["<a id=\"subsection-3\"></a>\n## (c) recover abbreviations (change they'll to they will, etc) "]}, {"cell_type": "markdown", "id": "a806ec76", "metadata": {}, "source": ["**I copied the code from the follow url by Yann Dubois** <br>\nhttps://stackoverflow.com/questions/43018030/replace-apostrophe-short-words-in-python"]}, {"cell_type": "code", "execution_count": 1, "id": "c8e838af", "metadata": {}, "outputs": [], "source": ["def decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"u\\.s\\.\", \"usa\", phrase)\n    phrase = re.sub(r\"united states of america\", \"usa\", phrase)\n    phrase = re.sub(r\"american\", \"usa\", phrase)\n    phrase = re.sub(r\"russian\", \"russia\", phrase)\n    phrase = re.sub(r\"israeli\", \"israel\", phrase)\n    phrase = re.sub(r\"united nations\", \"un\", phrase)\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n    #phrase = re.sub(r\"u\\.n\\.\", \"united nations\", phrase)\n    #phrase = re.sub(r\"un\", \"united nations\", phrase)\n    #phrase = re.sub(r\"u\\.s\\.a\\.\", \"america\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\nfor column in news_columns:\n    news_and_djia_df[column] = news_and_djia_df[column].apply(lambda x : decontracted(x))"]}, {"cell_type": "markdown", "id": "8ff62164", "metadata": {}, "source": ["<a id=\"subsection-4\"></a>\n## (d) remove everything except words and white space"]}, {"cell_type": "code", "execution_count": 1, "id": "1b8ee8a2", "metadata": {}, "outputs": [], "source": ["def remove_punc(news):\n    news = re.sub('-', ' ', news)\n    news = re.sub(r'[^\\w\\s]', '', news) # remove everything except words, digits, underscores and white spaces\n    #news = re.sub(r'\\s[^\\d]{0,}[\\d]{1,}[^\\d]{0,}\\s', '', news) #  remove words with digits e.g. $12b \n    news = re.sub(r'[\\d]', '', news) # remove all digits\n    news = re.sub('_', ' ', news) # the previous row doesn't remove underscore\n    return news\n\nfor column in news_columns:\n    news_and_djia_df[column] = news_and_djia_df[column].apply(lambda x : remove_punc(x))"]}, {"cell_type": "markdown", "id": "da69df30", "metadata": {}, "source": ["<a id=\"subsection-5\"></a>\n## (e) tokenization "]}, {"cell_type": "code", "execution_count": 1, "id": "befd5203", "metadata": {}, "outputs": [], "source": ["for column in news_columns:\n    news_and_djia_df[column] = news_and_djia_df[column].apply(lambda x: nltk.word_tokenize(x))"]}, {"cell_type": "markdown", "id": "81d80734", "metadata": {}, "source": ["<a id=\"subsection-6\"></a>\n## (f) Lemmatization"]}, {"cell_type": "code", "execution_count": 1, "id": "353af5b0", "metadata": {}, "outputs": [], "source": ["wl = WordNetLemmatizer()\nwl.lemmatize('feet','n')"]}, {"cell_type": "code", "execution_count": 1, "id": "74618bd5", "metadata": {}, "outputs": [], "source": ["tagged = nltk.pos_tag(['there','are','many','books','that','can','be','patiently','read'])\ntagged"]}, {"cell_type": "markdown", "id": "a1fb01a3", "metadata": {}, "source": ["**the function below is taken from the top answer in https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python**"]}, {"cell_type": "code", "execution_count": 1, "id": "9b9fd23a", "metadata": {}, "outputs": [], "source": ["def get_wordnet_pos(treebank_tag):\n    \n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n    \ndef lemmatize_list(lista):\n    tagged = nltk.pos_tag(lista)\n    for i, ele in enumerate(lista):\n        lista[i] = wl.lemmatize(ele, get_wordnet_pos(tagged[i][1]))\n    return lista"]}, {"cell_type": "code", "execution_count": 1, "id": "4533961e", "metadata": {}, "outputs": [], "source": ["lemmatize_list(['there','are','many','books','that','can','be','peacefully','read'])"]}, {"cell_type": "code", "execution_count": 1, "id": "fe83be70", "metadata": {}, "outputs": [], "source": ["for column in news_columns:\n    news_and_djia_df[column] = news_and_djia_df[column].apply(lambda x : lemmatize_list(x))"]}, {"cell_type": "markdown", "id": "cbe81f90", "metadata": {}, "source": ["<a id=\"subsection-7\"></a>\n## (g) remove stop words"]}, {"cell_type": "code", "execution_count": 1, "id": "38ccd21f", "metadata": {}, "outputs": [], "source": ["stop_words=stopwords.words('english')\n#stop_words.append('u') # 'u' is often short for 'you' in casual english\nstop_words.append('one'); stop_words.append('two'); stop_words.append('three')\nstop_words.append('four'); stop_words.append('five'); stop_words.append('six')\nstop_words.append('seven'); stop_words.append('eight'); stop_words.append('nine')\nstop_words.append('ten'); stop_words.append('eleven'); stop_words.append('twelve')\nstop_words.append('thirteen'); stop_words.append('fourteen'); stop_words.append('fifteen')\nstop_words.append('sixteen'); stop_words.append('seventeen'); stop_words.append('eighteen')\nstop_words.append('nineteen'); stop_words.append('twenty'); stop_words.append('million')\nstop_words.append('billion'); stop_words.append('th'); stop_words.append('m') # million\nstop_words.append('b') # billion\n\nstop_words.append('say'); stop_words.append('people'); \n\nstop_words.remove('no'); stop_words.remove('not')\nstop_words.remove('above'); stop_words.remove('below')\n#stop_words.remove('before'); stop_words.remove('after') \nstop_words.remove('up'); stop_words.remove('down') \nstop_words.remove('over'); stop_words.remove('under')\n\ndef remove_stop_words(lista):\n    pt=0 # don't use a for loop because len(lista) keeps changing as we remove stop words.\n    while pt<len(lista):\n        if lista[pt] in stop_words:\n            lista.remove(lista[pt])\n        else:\n            pt+=1\n    return lista\n\nfor column in news_columns:\n    news_and_djia_df[column] = news_and_djia_df[column].apply(lambda x: remove_stop_words(x))"]}, {"cell_type": "code", "execution_count": 1, "id": "c9f66e62", "metadata": {}, "outputs": [], "source": ["#stop_words"]}, {"cell_type": "code", "execution_count": 1, "id": "8a355570", "metadata": {}, "outputs": [], "source": ["pd.options.display.max_colwidth = 300\nhead=100\nadd=20\nfor i in range(head,head+add):\n    print(i)\n    print(news_and_djia_df_og.iloc[i, 3])\n    print(news_and_djia_df.iloc[i, 3])\n    print()\npd.options.display.max_colwidth = 50"]}, {"cell_type": "markdown", "id": "e9863b78", "metadata": {}, "source": ["<a id=\"section-2\"></a>\n# 2. Ideas for EDA \n## (a) number of 1's vs 0's\n## (b) Test how weekday Monday/Tuesday/etc affect the label \n## (c) word map, top 1000 most frequent words \n## (d) see if the most frequent words are the same for top1, top2, etc \n## (e) test how presence of frequent words affect the result 1 and 0 \n## (f) correlation matrix of features"]}, {"cell_type": "markdown", "id": "c5d3900f", "metadata": {}, "source": ["<a id=\"subsection-21\"></a>\n## (a) number of 1's vs 0's"]}, {"cell_type": "markdown", "id": "3a9ee47f", "metadata": {}, "source": ["**replace the first column with timestamp**"]}, {"cell_type": "code", "execution_count": 1, "id": "4b2df019", "metadata": {}, "outputs": [], "source": ["def str_to_timestamp(date_string):\n    return pd.Timestamp(year= int(date_string[:4]), month = int(date_string[5:7]), day = int(date_string[8:]))\n\nnews_and_djia_df['Date'] = news_and_djia_df['Date'].apply(str_to_timestamp)"]}, {"cell_type": "markdown", "id": "c0f43701", "metadata": {}, "source": ["**create a new dataframe for date and label only**"]}, {"cell_type": "code", "execution_count": 1, "id": "48241cf3", "metadata": {}, "outputs": [], "source": ["df = news_and_djia_df[['Date','Label']]\ndf = df.assign(Date = df['Date'].apply(lambda x : x.weekday()))\ndf = df.rename(columns={\"Date\": \"weekday\"})"]}, {"cell_type": "markdown", "id": "1946477d", "metadata": {}, "source": ["**count the number of 0's and 1's in the label**"]}, {"cell_type": "code", "execution_count": 1, "id": "94a366af", "metadata": {}, "outputs": [], "source": ["asdf = df['Label'].value_counts()\nprint(asdf)"]}, {"cell_type": "markdown", "id": "924fe8d2", "metadata": {}, "source": ["**the fact that there are more 1's than 0's is maybe consistent with the fact that the stock market is growing on large time scale**"]}, {"cell_type": "code", "execution_count": 1, "id": "79f6b7bd", "metadata": {}, "outputs": [], "source": ["f, ax = plt.subplots(1,1, figsize=(5,5))\nax.pie(asdf)\nax.legend(['1','0'])\nplt.title('Labels');"]}, {"cell_type": "markdown", "id": "08d8abe8", "metadata": {}, "source": ["<a id=\"subsection-22\"></a>\n## (b) Test how weekday Monday/Tuesday/etc affect the label"]}, {"cell_type": "code", "execution_count": 1, "id": "087ab39a", "metadata": {}, "outputs": [], "source": ["summary = df.groupby(by = 'weekday').sum()\nsummary = summary.rename(columns = {'Label' : 'counts of 1'})\nsummary"]}, {"cell_type": "markdown", "id": "97e4bdab", "metadata": {}, "source": ["**looks like Monday is correlated with decrease of DJIA**"]}, {"cell_type": "code", "execution_count": 1, "id": "5d8eb753", "metadata": {}, "outputs": [], "source": ["f, ax = plt.subplots(1,1, figsize=(5,5))\nplt.bar(['Mon','Tues','Wed','Thur','Fri'], summary['counts of 1'])\nax.set_xlabel('weekday')\nax.set_ylabel('days of market increase');"]}, {"cell_type": "markdown", "id": "2f5e9e88", "metadata": {}, "source": ["<a id=\"subsection-23\"></a>\n## (c) word map for all news\n<a id=\"subsection-24\"></a>\n## & (d) see if the most frequent words are the same for top1, top2, etc "]}, {"cell_type": "code", "execution_count": 1, "id": "476b01b3", "metadata": {}, "outputs": [], "source": ["f, ax = plt.subplots(3,3, figsize=(20,20))\n#ax[0,0].bar(['Mon','Tues','Wed','Thur','Fri'], summary['counts of 1'])\nfor i in range(9):\n    row = int(i/3)\n    column = i % 3\n    long_string=''\n    tot = news_and_djia_df['Top'+str(i+1)].values.shape[0]\n    for j in range(tot):\n        lista = news_and_djia_df['Top'+str(i+1)][j]\n        for ele in lista:\n            long_string = long_string + ele + ' '\n    wc = WordCloud(background_color ='white', width = 800, height = 800, min_font_size = 10).generate(long_string)\n    ax[row, column].imshow(wc)"]}, {"cell_type": "code", "execution_count": 1, "id": "793ad4e3", "metadata": {}, "outputs": [], "source": ["tot = news_and_djia_df['Top1'].values.shape[0]\nhuge_list = []\nfor i in range(tot):\n    for j in range(25):\n        huge_list = huge_list + news_and_djia_df.iloc[i,j+2]\n        \nfreq_dict = Counter(huge_list)"]}, {"cell_type": "code", "execution_count": 1, "id": "5732fa10", "metadata": {}, "outputs": [], "source": ["freq_list = []\n\nfor key in freq_dict:\n    freq_list.append([key, freq_dict[key]])\n    \ndef return_freq(lista):\n    return lista[1]\n\nfreq_list.sort(key = return_freq, reverse = True)"]}, {"cell_type": "code", "execution_count": 1, "id": "d49cab00", "metadata": {}, "outputs": [], "source": ["num =6 # num^2 is the number of most frequent words we want\nwords = [freq_list[i][0] for i in range(num**2)]\nfreqs = [freq_list[i][1] for i in range(num**2)]\nax = plt.subplots(1, 1, figsize=(23, 10))\nplt.bar(words, freqs)\nplt.ylabel(\"number of occurences\");"]}, {"cell_type": "code", "execution_count": 1, "id": "e4203636", "metadata": {}, "outputs": [], "source": ["print(news_and_djia_df.iloc[1601,3])\nnews_and_djia_df_og.iloc[1601,3]"]}, {"cell_type": "markdown", "id": "15b76493", "metadata": {}, "source": ["<a id=\"subsection-25\"></a>\n## (e) test how presence of frequent words affect the result 1 and 0 "]}, {"cell_type": "markdown", "id": "e643a291", "metadata": {}, "source": ["**create a dictionary that stores number of occurences for words in each day**<br>\n**[china, china, usa] would be {china:2, usa:1}**"]}, {"cell_type": "code", "execution_count": 1, "id": "f994297a", "metadata": {}, "outputs": [], "source": ["store_dicts=[]\nfor i in range(news_and_djia_df.shape[0]):\n    dic={}\n    for j in range(25):\n        lista=news_and_djia_df.iloc[i,j+2]\n        for ele in lista:\n            try:\n                dic[ele]+=1\n            except:\n                dic[ele]=1\n    store_dicts.append(dic)"]}, {"cell_type": "markdown", "id": "1f08e9f7", "metadata": {}, "source": ["**create a numpy array that stores the number of occurences for the most frequent words in each day**"]}, {"cell_type": "code", "execution_count": 1, "id": "ea5bf6c7", "metadata": {}, "outputs": [], "source": ["main = np.zeros((tot, num**2))\n\nfor i in range(tot):\n    for j in range(num**2):\n        try:\n            main[i,j] = store_dicts[i][words[j]]\n        except:\n            _"]}, {"cell_type": "code", "execution_count": 1, "id": "0d1812e9", "metadata": {}, "outputs": [], "source": ["main_df = pd.DataFrame(main)\nmain_df['label'] = news_and_djia_df['Label']"]}, {"cell_type": "markdown", "id": "47f6d803", "metadata": {}, "source": ["**maybe today's news will make good prediction for tomorrow's market change, so we try out this idea**"]}, {"cell_type": "code", "execution_count": 1, "id": "57ebc727", "metadata": {}, "outputs": [], "source": ["next_5_day_label = news_and_djia_df['Label']\nnext_5_day_label = next_5_day_label[5:]\nfor i in range(5):\n    next_5_day_label = np.append(next_5_day_label, float('NAN'))\nmain_df['next_5_day_label'] = next_5_day_label\nmain_df.iloc[-10:]"]}, {"cell_type": "markdown", "id": "8d5b05de", "metadata": {}, "source": ["**certain words in today's news affect today's market change, remember to normaliza the histogram because the number of market growing days are more than dropping days**"]}, {"cell_type": "code", "execution_count": 1, "id": "87ea628a", "metadata": {}, "outputs": [], "source": ["f, ax = plt.subplots(num, num, figsize=(20,20))\nbins=[0,1,2,3,4,5,6,7,8,9,10,11]\nfor i in range(num**2):\n        row = int(i/num)\n        column = i % num\n        ax[row, column].hist(main_df[main_df['label']==1][i], alpha=0.5, rwidth=0.9, bins=bins, density=True)\n        ax[row, column].hist(main_df[main_df['label']==0][i], alpha=0.5, rwidth=0.9, bins=bins, density=True)\n        ax[row, column].legend([1,0])\n        ax[row, column].set_title(words[i])"]}, {"cell_type": "markdown", "id": "cbbb050d", "metadata": {}, "source": ["**certain words in today's news affect the market change 5 days later, remember to normaliza the histogram because the number of market growing days are more than dropping days**"]}, {"cell_type": "code", "execution_count": 1, "id": "4043ffd1", "metadata": {}, "outputs": [], "source": ["f, ax = plt.subplots(num, num, figsize=(20,20))\nbins=[0,1,2,3,4,5,6,7,8,9,10,11]\nfor i in range(num**2):\n        row = int(i/num)\n        column = i % num\n        ax[row, column].hist(main_df[main_df['next_5_day_label']==1][i], alpha=0.5, rwidth=0.9, bins=bins, density=True)\n        ax[row, column].hist(main_df[main_df['next_5_day_label']==0][i], alpha=0.5, rwidth=0.9, bins=bins, density=True)\n        ax[row, column].legend([1,0])\n        ax[row, column].set_title(words[i])"]}, {"cell_type": "markdown", "id": "ca66c752", "metadata": {}, "source": ["<a id=\"subsection-26\"></a>\n## (f) correlation matrix of features"]}, {"cell_type": "code", "execution_count": 1, "id": "1ced8cde", "metadata": {}, "outputs": [], "source": ["f, ax = plt.subplots(1, 1, figsize=(10,10))\nax.matshow(main_df.corr())"]}, {"cell_type": "code", "execution_count": 1, "id": "2d2c05d8", "metadata": {}, "outputs": [], "source": ["print(words[1], words[16], ':high positive correlation')\nprint(words[13], words[30], ':high positive correlation')\nprint(words[15],words[7], ':high positive correlation')\nprint(words[0],words[10], ':high negative correlation')\nprint(words[13],words[19], ':high negative correlation')\nprint(words[13],words[20], ':high negative correlation')"]}, {"cell_type": "markdown", "id": "303f1041", "metadata": {}, "source": ["<a id=\"subsection-27\"></a>\n## (g) repeat the analysis with tfidf"]}, {"cell_type": "markdown", "id": "a6ed1a08", "metadata": {}, "source": ["**total number of words**"]}, {"cell_type": "code", "execution_count": 1, "id": "76d428a3", "metadata": {}, "outputs": [], "source": ["tot_words=[]\nfor i in range(tot):\n    tot_words.append(0)\n    for j in range(25):\n        tot_words[-1]+=len(news_and_djia_df.iloc[i,j+2])"]}, {"cell_type": "markdown", "id": "0f936d03", "metadata": {}, "source": ["**calculate tf**"]}, {"cell_type": "code", "execution_count": 1, "id": "4509a74b", "metadata": {}, "outputs": [], "source": ["tf_df=main_df\nfor i in range(tot):\n    for j in range(num**2):\n        tf_df.iloc[i,j] = tf_df.iloc[i,j]/tot_words[i]"]}, {"cell_type": "markdown", "id": "9702e1ea", "metadata": {}, "source": ["**calculate idf**"]}, {"cell_type": "code", "execution_count": 1, "id": "65d1255f", "metadata": {}, "outputs": [], "source": ["idf=[0]*(num**2)\nfor i in range(tot):\n    for j in range(num**2):\n        try:\n            store_dicts[i][words[j]]\n            idf[j]+=1\n        except:\n            None\n            \nfor i in range(num**2):\n    idf[i]=np.log(tot/idf[i])"]}, {"cell_type": "markdown", "id": "abcad334", "metadata": {}, "source": ["**tf-idf**"]}, {"cell_type": "code", "execution_count": 1, "id": "da97c946", "metadata": {}, "outputs": [], "source": ["tf_idf_df=tf_df\nfor i in range(tot):\n    for j in range(num**2):\n        tf_idf_df.iloc[i,j]=tf_idf_df.iloc[i,j] * idf[j]"]}, {"cell_type": "markdown", "id": "18ad8218", "metadata": {}, "source": ["**this tells us how today's news affect today's stock market movement**"]}, {"cell_type": "code", "execution_count": 1, "id": "c65fd806", "metadata": {}, "outputs": [], "source": ["f, ax = plt.subplots(num, num, figsize=(20,20))\nbins=[i/1000 for i in range(11)]\nfor i in range(num**2):\n    row = int(i/num)\n    column = i % num\n    ax[row, column].hist(tf_idf_df[tf_idf_df['label']==1][i], alpha=0.5, rwidth=0.9, bins=bins, density=True)\n    ax[row, column].hist(tf_idf_df[tf_idf_df['label']==0][i], alpha=0.5, rwidth=0.9, bins=bins, density=True)\n    ax[row, column].legend([1,0])\n    ax[row, column].set_title(words[i])"]}, {"cell_type": "markdown", "id": "91ef0801", "metadata": {}, "source": ["**certain words in today's news affect next 5 today's market change**"]}, {"cell_type": "code", "execution_count": 1, "id": "5cbf4c3c", "metadata": {}, "outputs": [], "source": ["f, ax = plt.subplots(num, num, figsize=(20,20))\nbins=[i/1000 for i in range(11)]\nfor i in range(num**2):\n    row = int(i/num)\n    column = i % num\n    ax[row, column].hist(tf_idf_df[tf_idf_df['next_5_day_label']==1][i], alpha=0.5, rwidth=0.9, bins=bins, density=True)\n    ax[row, column].hist(tf_idf_df[tf_idf_df['next_5_day_label']==0][i], alpha=0.5, rwidth=0.9, bins=bins, density=True)\n    ax[row, column].legend([1,0])\n    ax[row, column].set_title(words[i])"]}, {"cell_type": "markdown", "id": "e4a6f841", "metadata": {}, "source": ["<a id=\"section-3\"></a>\n# 3. Simple models for benchmark"]}, {"cell_type": "markdown", "id": "817624dc", "metadata": {}, "source": ["**use the idea of n-grams and TF-IDF, do logistic regressions**<br>\n**the idea is take from the two notebooks below**<br>\nhttps://www.kaggle.com/ndrewgele/omg-nlp-with-the-djia-and-reddit<br>\nhttps://www.kaggle.com/lseiyjg/use-news-to-predict-stock-markets"]}, {"cell_type": "code", "execution_count": 1, "id": "c68a3e4f", "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import roc_auc_score, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nimport random\nstore_null=[] # store null accuracy\nstore_result=[] # store accuracy\nstore_auc=[] # store auc roc\n\ndata = pd.read_csv('../input/stocknews/Combined_News_DJIA.csv') # read in data\n   \nfor i in range(100):\n    \n    train_X, test_X, train_Y, test_Y = train_test_split(data.iloc[:,2:], data.iloc[:,1], test_size=0.2, random_state=None) \n\n    trainheadlines = []\n    for row in range(train_X.shape[0]):\n        trainheadlines.append(' '.join(str(x) for x in train_X.iloc[row])) # join together all 25 news\n\n    testheadlines = []\n    for row in range(test_X.shape[0]):\n        testheadlines.append(' '.join(str(x) for x in test_X.iloc[row])) # join together all 25 news\n    \n    # count TF-IDF on 2-grams\n    advancedvectorizer = TfidfVectorizer(min_df=0.03, max_df=0.97, max_features = 200000, ngram_range = (2, 2)) \n    advancedtrain = advancedvectorizer.fit_transform(trainheadlines)\n    advancedtest = advancedvectorizer.transform(testheadlines)\n    \n    # C is regularization parameter, when C gets larger regularization becomes smaller\n    advancedmodel = LogisticRegression(C=1000, solver='liblinear')\n    advancedmodel.fit(advancedtrain, train_Y)\n\n    preds13 = advancedmodel.predict(advancedtest) # binary prediciton\n    preds13prob = advancedmodel.predict_proba(advancedtest)[:,1] # probablity prediction\n    acc13 = accuracy_score(test_Y, preds13)\n    \n    store_null.append(sum(test_Y)/test_Y.shape[0])\n    store_result.append(acc13)\n    store_auc.append(roc_auc_score(test_Y,preds13prob))\n\nprint('Average null accuracy: ',sum(store_null)/len(store_null))\nprint('Average accuracy: ',sum(store_result)/len(store_result))\nprint('Average AUC score: ',sum(store_auc)/len(store_auc))"]}, {"cell_type": "code", "execution_count": 1, "id": "12d25286", "metadata": {}, "outputs": [], "source": ["import statistics\n\nprint('Standard deviation of null accuracy: ', statistics.pstdev(store_null))\nprint('Standard deviation of accuracy: ', statistics.pstdev(store_result))\nprint('Standard deviation of AUC score: ', statistics.pstdev(store_auc))"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}