{"cells": [{"cell_type": "code", "execution_count": 1, "id": "0f074afd", "metadata": {}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport matplotlib.pyplot as plt\nimport os\nprint(os.listdir(\"../input\"))\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Any results you write to the current directory are saved as output."]}, {"cell_type": "markdown", "id": "6a96a154", "metadata": {}, "source": ["> Lets Import the heart.csv file into a pandas dataframe\n"]}, {"cell_type": "code", "execution_count": 1, "id": "607a5ec1", "metadata": {}, "outputs": [], "source": ["dataset = pd.read_csv('../input/heart.csv')"]}, {"cell_type": "markdown", "id": "a9e66c51", "metadata": {}, "source": ["First of all we need an insight of the data what it contains and how to interpret the data into a more meaningful statistics. So we will first check the contents using head. Head() will give 5 rows as default from the top. We can also use tail, which provides data from the bottom. For now we will stick to head(). "]}, {"cell_type": "code", "execution_count": 1, "id": "29e24527", "metadata": {}, "outputs": [], "source": ["dataset.head()"]}, {"cell_type": "markdown", "id": "dc8c0969", "metadata": {}, "source": ["\nage :            age in years\n\nsex:            (1 = male; 0 = female)\n\ncp:              chest pain type\n\ntrestbps       resting blood pressure (in mm Hg on admission to the hospital)\n\ncholserum   cholestoral in mg/dl\n\nfbs(fasting blood sugar > 120 mg/dl): (1 = true; 0 = false)\n\nrestecg       resting electrocardiographic results\n\nthalachmaximum heart rate achieved\n\nexangexercise induced angina (1 = yes; 0 = no)\n\noldpeakST depression induced by exercise relative to rest\n\nslope        the slope of the peak exercise ST segment\n\nca             number of major vessels (0-3) colored by flourosopy\n\nthal           3 = normal; 6 = fixed defect; 7 = reversable defect\n\ntarget       1 or 0\n"]}, {"cell_type": "code", "execution_count": 1, "id": "dfca8d0a", "metadata": {}, "outputs": [], "source": ["dataset.info()"]}, {"cell_type": "code", "execution_count": 1, "id": "b60dc573", "metadata": {}, "outputs": [], "source": ["dataset.describe()"]}, {"cell_type": "markdown", "id": "2cc6ce35", "metadata": {}, "source": ["Let's check the shape of the data, i.e count and columns available\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "8a685563", "metadata": {}, "outputs": [], "source": ["dataset.shape"]}, {"cell_type": "markdown", "id": "5beb66e5", "metadata": {}, "source": ["**Gender distribution in the file using Seaborn**"]}, {"cell_type": "code", "execution_count": 1, "id": "ad82e7d9", "metadata": {}, "outputs": [], "source": ["sns.countplot(x='sex',data=dataset)"]}, {"cell_type": "markdown", "id": "1237088b", "metadata": {}, "source": ["**Gender Ratio:**\n\nLets see percentage wise ratio of dataset for gender."]}, {"cell_type": "code", "execution_count": 1, "id": "be9b1e36", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(8,6))\nexplode =[0.1,0]\nlabels='Male','Female'\nplt.pie(dataset['sex'].value_counts(),explode=explode,autopct='%1.1f%%',labels=labels,shadow=True,startangle=140)"]}, {"cell_type": "markdown", "id": "bced9146", "metadata": {}, "source": ["**Chest Pain** : We can see there are different pain type, so lets build a pie chart which will show the data distribution.\n"]}, {"cell_type": "code", "execution_count": 1, "id": "c8c20461", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(10,6))\nexplode=[0.1,0,0,0]\nlabels='Pain-Type 0','Pain Type-1','Pain-Type2','Pain-Type3'\nplt.pie(dataset['cp'].value_counts(),explode=explode,labels=labels,autopct='%1.1f%%',shadow=True,startangle=140)"]}, {"cell_type": "code", "execution_count": 1, "id": "0220376f", "metadata": {}, "outputs": [], "source": ["sns.boxplot(dataset['trestbps'],orient='v',color='Magenta')"]}, {"cell_type": "code", "execution_count": 1, "id": "a14a4e37", "metadata": {}, "outputs": [], "source": ["sns.boxplot(dataset['chol'],orient='v',color='Magenta')"]}, {"cell_type": "code", "execution_count": 1, "id": "7375fe46", "metadata": {}, "outputs": [], "source": ["\n#dataset.plot.scatter(x='age',y='trestbps')\nplt.figure(figsize=(20,10))\nsns.boxplot(x='age',y='trestbps',data=dataset)"]}, {"cell_type": "code", "execution_count": 1, "id": "5962ea23", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(20,10))\nsns.boxplot(x='age',y='thalach',data=dataset)"]}, {"cell_type": "code", "execution_count": 1, "id": "56428037", "metadata": {}, "outputs": [], "source": ["sns.set()\ncol=['age','trestbps','chol','thalach']\nsns.pairplot(dataset[col])\nplt.show()"]}, {"cell_type": "markdown", "id": "190ab5ec", "metadata": {}, "source": ["Lets build a heat map to check the co relation between variables. From the below it is evident that hardly strong co realtion exists between variables. "]}, {"cell_type": "code", "execution_count": 1, "id": "d12c8e76", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(15,10))\nsns.heatmap(dataset.corr(),annot=True,cmap='YlGnBu')"]}, {"cell_type": "markdown", "id": "b60d64b3", "metadata": {}, "source": ["**Creating Dummy Variables:**\n\nFrom the above we can see there are categorical values, which includes: sex,cp,fbs etc.\n\nSo we will create dummy variables. We will also use prefix so that categorical columns when converted are recognized properly."]}, {"cell_type": "code", "execution_count": 1, "id": "f7eca554", "metadata": {}, "outputs": [], "source": ["sex = pd.get_dummies(dataset['sex'],prefix='sex',drop_first=True)\nfbs = pd.get_dummies(dataset['fbs'],prefix='fbs',drop_first=True)\nrestecg = pd.get_dummies(dataset['restecg'],prefix='restecg',drop_first=True)\nexang = pd.get_dummies(dataset['exang'],prefix='exang',drop_first=True)\ncp = pd.get_dummies(dataset['cp'],prefix='cp',drop_first=True)\nslope = pd.get_dummies(dataset['slope'],prefix='slope',drop_first=True)\nthal = pd.get_dummies(dataset['thal'],prefix='thal',drop_first=True)\n\ndataset = pd.concat([dataset,sex,fbs,restecg,exang,cp,slope,thal],axis=1)\n\n\n\n#Will do a quick check if it worked or not :P\ndataset.head()\n"]}, {"cell_type": "markdown", "id": "f0faed2d", "metadata": {}, "source": ["Dropping the columns since we have already converted the categorical data and taken care the dummy trap above"]}, {"cell_type": "code", "execution_count": 1, "id": "cd2638c1", "metadata": {}, "outputs": [], "source": ["dataset = dataset.drop(columns=['sex','fbs','restecg','exang','cp','slope','thal'])\ndataset.head()"]}, {"cell_type": "markdown", "id": "9b8b7bf4", "metadata": {}, "source": ["**Making Predictions**\n\n"]}, {"cell_type": "markdown", "id": "c4e98cb4", "metadata": {}, "source": ["Extracting the dependent (Y) and X variables.\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "60291636", "metadata": {}, "outputs": [], "source": ["X= dataset.drop('target',axis=1)\ny = dataset['target'].values"]}, {"cell_type": "markdown", "id": "4943a93f", "metadata": {}, "source": ["**Train Test Splitting**\n\nWe will split the data into train test based on 80:20 "]}, {"cell_type": "code", "execution_count": 1, "id": "a9db1f60", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)"]}, {"cell_type": "markdown", "id": "732d0fbd", "metadata": {}, "source": ["**Standard Scaler**\n\nLets Standarize the data before fitting the data into the model.Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data.\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "c568bf9c", "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test  = sc.transform(X_test)"]}, {"cell_type": "markdown", "id": "794450b9", "metadata": {}, "source": ["**PCA component **\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "3e0d8e0b", "metadata": {}, "outputs": [], "source": ["from sklearn.decomposition import PCA\npca = PCA(n_components=None,random_state=0)\nX_train = pca.fit_transform(X_train)\nX_test =pca.transform(X_test)\n\npca.explained_variance_ratio_\n"]}, {"cell_type": "markdown", "id": "6f26e47b", "metadata": {}, "source": ["**Logistic Regression**"]}, {"cell_type": "code", "execution_count": 1, "id": "34f5687f", "metadata": {}, "outputs": [], "source": ["from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\nlr_score = lr.score(X_test,y_test)\n\n"]}, {"cell_type": "markdown", "id": "11fe0f10", "metadata": {}, "source": ["**Support Vector **"]}, {"cell_type": "code", "execution_count": 1, "id": "f84a7771", "metadata": {}, "outputs": [], "source": ["from sklearn.svm import SVC\nsv = SVC(kernel ='rbf',random_state=0)\nsv.fit(X_train,y_train)\nsv_pred = sv.predict(X_test)\nsv_score = sv.score(X_test,y_test)"]}, {"cell_type": "markdown", "id": "0fc5a7c1", "metadata": {}, "source": ["**Random Forest Classifier**"]}, {"cell_type": "code", "execution_count": 1, "id": "9dab5e51", "metadata": {}, "outputs": [], "source": ["from sklearn.ensemble import RandomForestClassifier\nrf_regressor = RandomForestClassifier(n_estimators = 1000, random_state = 0)\nrf_regressor.fit(X_train, y_train)\nrf_pred = rf_regressor.predict(X_test)\nrf_score = rf_regressor.score(X_test,y_test)\n"]}, {"cell_type": "markdown", "id": "ef821e86", "metadata": {}, "source": ["**KNN**"]}, {"cell_type": "code", "execution_count": 1, "id": "95e0098e", "metadata": {}, "outputs": [], "source": ["from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=2)\nknn.fit(X_train,y_train)\nknn_score = knn.score(X_test,y_test)\n"]}, {"cell_type": "markdown", "id": "1238b8c7", "metadata": {}, "source": ["**Naive Bayes**"]}, {"cell_type": "code", "execution_count": 1, "id": "9ebded68", "metadata": {}, "outputs": [], "source": ["from sklearn.naive_bayes import GaussianNB\nnv = GaussianNB()\nnv.fit(X_train,y_train)\nnv_sc = nv.score(X_test,y_test)"]}, {"cell_type": "markdown", "id": "8da45fd2", "metadata": {}, "source": ["**Model Score**"]}, {"cell_type": "code", "execution_count": 1, "id": "102cd0c1", "metadata": {}, "outputs": [], "source": ["\nprint(\"Logistic Regression Model Score is \",round(lr_score*100))\nprint(\"SVC Model Score is \",round(sv_score*100))\n#print(\"Decision tree  Regression Model Score is \",round(tr_regressor.score(X_test,y_test)*100))\nprint(\"Random Forest Regression Model Score is \",round(rf_score*100))\n\nprint(\"KNeighbors Classifiers Model score is\",round(knn_score*100))\nprint(\"Naive Bayes model score is\",round(nv_sc*100))\n"]}, {"cell_type": "markdown", "id": "4e0565ed", "metadata": {}, "source": ["**Cross Validation Score with 10 iteration**"]}, {"cell_type": "code", "execution_count": 1, "id": "3735103d", "metadata": {}, "outputs": [], "source": ["\n\nfrom sklearn.model_selection import cross_val_score\naccuracies_lr = cross_val_score(estimator = lr,X = X_train,y = y_train,cv = 10)\naccuracies_sv = cross_val_score(estimator = sv,X = X_train,y = y_train,cv = 10)\naccuracies_rf = cross_val_score(estimator = rf_regressor,X = X_train,y = y_train,cv = 10)\n\naccuracies_knn = cross_val_score(estimator = knn,X = X_train,y = y_train,cv = 10)\naccuracies_nv = cross_val_score(estimator = nv,X = X_train,y = y_train,cv = 10)\n\nprint(\"Mean Accuracies based on cross val score for logistic regression\",round(accuracies_lr.mean()*100))\nprint(\"Mean Accuracies based on cross val score for SVM \",round(accuracies_sv.mean()*100))\nprint(\"Mean Accuracies based on cross val score for Random Forest\",round(accuracies_rf.mean()*100))\n\nprint(\"Mean Accuracies based on cross val score for KNN\",round(accuracies_knn.mean()*100))\nprint(\"Mean Accuracies based on cross val score for Naive Bayes\",round(accuracies_nv.mean()*100))\n"]}, {"cell_type": "markdown", "id": "b8dee97e", "metadata": {}, "source": ["**Confusion Matrix:**\n\nLogistic Regression and Random Forest since this performs a better model in comparison to other\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "cae1d8bb", "metadata": {}, "outputs": [], "source": ["\ncm_lr = confusion_matrix(y_test,y_pred)\ncm_lr\n"]}, {"cell_type": "markdown", "id": "d8994e29", "metadata": {}, "source": ["**Confusion Matrix** for Random Forest is as below\n"]}, {"cell_type": "code", "execution_count": 1, "id": "badbbd9d", "metadata": {}, "outputs": [], "source": ["cm_rf = confusion_matrix(y_test,rf_pred)\ncm_rf"]}, {"cell_type": "markdown", "id": "cdd5e7b9", "metadata": {}, "source": ["**Conclusion**\n\nThough there are weak co relation between variables and also exists other model, but Logistic Model and Random Forest much better than other model. \n\n\nPlease ****Upvote**** my work if you like it :)\n\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}