{"cells": [{"cell_type": "markdown", "id": "4e582c7c", "metadata": {}, "source": ["# Lets Play With Quick Draw!!"]}, {"cell_type": "markdown", "id": "f84ceb09", "metadata": {}, "source": ["![Imgur](https://i.imgur.com/l7LgSGy.png)"]}, {"cell_type": "markdown", "id": "7dadca0a", "metadata": {}, "source": ["# &#128225; What is Quick Draw?\n![Imgur](https://i.imgur.com/HyPUqwF.png)\n\u201cQuick, Draw!\u201d was a game that was initially featured at Google I/O in 2016, as a game where one player would be prompted to draw a picture of an object, and the other player would need to guess what it was. Just like pictionary.\nIn 2017, the Magenta team at Google Research took that idea a step further by using this labeled dataset to train the [Sketch-RNN](https://magenta.tensorflow.org/assets/sketch_rnn_demo/index.html) model, to try to predict what the player was drawing, in real time, instead of requiring a second player to do the guessing. The game is [available online,](https://quickdraw.withgoogle.com) and has now collected over 1 billion hand-drawn doodles!"]}, {"cell_type": "markdown", "id": "724551c0", "metadata": {}, "source": ["# &#128220; Overview Of Quick Draw DataSet\nThe team has open sourced this data, and in a variety of formats. You can learn more at their [GitHub page.](https://github.com/googlecreativelab/quickdraw-dataset)Now we can get this dataset easily from [Our Data Science House Kaggle.](https://www.kaggle.com)\n\n**Here is Orginal Google DataSet  demo picture.This picture collect from github. &#128071;** ![Imgur](https://i.imgur.com/MOziCCc.png)\n\n**Another picture of Kaggle DataSet picture. &#128071; **![Imgur](https://i.imgur.com/36cyzNX.png)"]}, {"cell_type": "markdown", "id": "49d2216a", "metadata": {}, "source": ["There are 4 formats: First up are the raw files stored in (.ndjson) format. These files encode the full set of information for each doodle. It contains timing information for each stroke of every picture drawn.\n\nThere is also a simplified version, stored in the same format (.ndjson), which has some preprocessing applied to normalize the data. The simplified version is also available as a binary format for more efficient storage and transfer. There are examples of how to read the files using both Python and NodeJS.\n\n**This picture  Google Cloud Platfrom of Quick Draw Datasets. &#128071;** ![Imgur](https://i.imgur.com/eYoCpkA.png)\n\n\nThe fourth format takes the simplified data and renders it into a 28x28 grayscale bitmap in numpy .npy format, which can be loaded using np.load().\n\nWhy is it 28x28? Well, it\u2019s a perfect replacement for any existing code you might have for processing MNIST data. So if you\u2019re looking for something fancier than 10 handwritten digits, you can try processing over 300 different classes of doodles."]}, {"cell_type": "markdown", "id": "97298f86", "metadata": {}, "source": ["#  &#128202; Data exploration and visualization of Quick Draw Game\nIf you want to explore the dataset some more, you can visualize the quickdraw dataset using Facets. The Facets team has even taken the liberty of hosting it online and giving us some presets to play around with! You can [access the page here.](https://pair-code.github.io/facets/quickdraw.html) We can load up some random chairs and see how different players drew chairs from around the world.\n\n![Quick Draw gift](https://i.imgur.com/q1h49cE.gif)\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "62819946", "metadata": {}, "outputs": [], "source": ["%%html\n<style>\n@import url('https://fonts.googleapis.com/css?family=Ewert|Roboto&effect=3d|ice|');\nbody {background-color: gainsboro;} \na {color: #37c9e1; font-family: 'Roboto';} \nh1 {color: #37c9e1; font-family: 'Orbitron'; text-shadow: 4px 4px 4px #aaa;} \nh2, h3 {color: slategray; font-family: 'Orbitron'; text-shadow: 4px 4px 4px #aaa;}\nh4 {color: #818286; font-family: 'Roboto';}\nspan {font-family:'Roboto'; color:black; text-shadow: 5px 5px 5px #aaa;}  \ndiv.output_area pre{font-family:'Roboto'; font-size:110%; color:lightblue;}      \n</style>"]}, {"cell_type": "markdown", "id": "2a93d136", "metadata": {}, "source": ["# \ud83d\udcd1 Import Libraries"]}, {"cell_type": "code", "execution_count": 1, "id": "d14d3729", "metadata": {}, "outputs": [], "source": ["import numpy as np\nimport pandas as pd\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n#deep lerning libraries\nimport keras\nfrom keras import backend as K\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\n\nimport pickle # Read/Write with Serialization\nimport requests # Makes HTTP requests\nfrom io import BytesIO # Use When expecting bytes-like objects"]}, {"cell_type": "markdown", "id": "81812a1f", "metadata": {}, "source": ["# &#128229; Load and Read DataSets"]}, {"cell_type": "code", "execution_count": 1, "id": "61b4c519", "metadata": {}, "outputs": [], "source": ["# Classes we will load\ncategories = ['cannon','eye', 'face', 'nail', 'pear','piano','radio','spider','star','sword']\n\n# Dictionary for URL and class labels\nURL_DATA = {}\nfor category in categories:\n    URL_DATA[category] = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/' + category +'.npy'"]}, {"cell_type": "code", "execution_count": 1, "id": "80d68789", "metadata": {}, "outputs": [], "source": ["classes_dict = {}\nfor key, value in URL_DATA.items():\n    response = requests.get(value)\n    classes_dict[key] = np.load(BytesIO(response.content))"]}, {"cell_type": "code", "execution_count": 1, "id": "4e2cea50", "metadata": {}, "outputs": [], "source": ["for i, (key, value) in enumerate(classes_dict.items()):\n    value = value.astype('float32')/255.\n    if i == 0:\n        classes_dict[key] = np.c_[value, np.zeros(len(value))]\n    else:\n        classes_dict[key] = np.c_[value,i*np.ones(len(value))]\n\n# Create a dict with label codes\nlabel_dict = {0:'cannon',1:'eye', 2:'face', 3:'nail', 4:'pear', \n              5:'piana',6:'radio', 7:'spider', 8:'star', 9:'sword'}"]}, {"cell_type": "code", "execution_count": 1, "id": "983d6b64", "metadata": {}, "outputs": [], "source": ["lst = []\nfor key, value in classes_dict.items():\n    lst.append(value[:3000])\ndoodles = np.concatenate(lst)"]}, {"cell_type": "code", "execution_count": 1, "id": "73354f91", "metadata": {}, "outputs": [], "source": ["# Split the data into features and class labels (X & y respectively)\ny = doodles[:,-1].astype('float32')\nX = doodles[:,:784]\n\n# Split each dataset into train/test splits\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "311413bb", "metadata": {}, "outputs": [], "source": ["# Save X_train dataset as a pickle file\nwith open('xtrain_doodle.pickle', 'wb') as f:\n    pickle.dump(X_train, f)\n    \n# Save X_test dataset as a pickle file\nwith open('xtest_doodle.pickle', 'wb') as f:\n    pickle.dump(X_test, f)\n    \n# Save y_train dataset as a pickle file\nwith open('ytrain_doodle.pickle', 'wb') as f:\n    pickle.dump(y_train, f)\n    \n# Save y_test dataset as a pickle file\nwith open('ytest_doodle.pickle', 'wb') as f:\n    pickle.dump(y_test, f)"]}, {"cell_type": "markdown", "id": "36701a44", "metadata": {}, "source": ["# &#128187; Predictive Modeling "]}, {"cell_type": "markdown", "id": "60ce3f29", "metadata": {}, "source": ["# &#128204; 1. K-Nearest Neighbors\n![Imgur](https://i.imgur.com/HyPUqwF.png)\n![Imgur](https://i.imgur.com/IO58pDy.jpg)"]}, {"cell_type": "markdown", "id": "56a6e2b3", "metadata": {}, "source": ["### &#128210;  Note:\nBy simply randomly guessing, one should be able to reach ~10% accuracy (since there are only ten class labels). A machine learning algorithm will need to obtain > 10% accuracy in order to demonstrate that it has in fact \u201clearned\u201d something (or found an underlying pattern in the data).\n\nTo start, we\u2019ll model the data with the k-Nearest Neighbor (k-NN) classifier, arguably the most simple, easy to understand machine learning algorithm. The k-NN algorithm classifies unknown data points by finding the most common class among the k-closest examples. Each data point in the k closest examples casts a vote and the category with the most votes is chosen."]}, {"cell_type": "markdown", "id": "04e05fac", "metadata": {}, "source": ["## &#128295;  Base model Of KNN\n\nNext, I will try out a KNN classifier:"]}, {"cell_type": "markdown", "id": "631c6c3d", "metadata": {}, "source": ["### Output:\n\n```Accuracy:80.5 ```"]}, {"cell_type": "markdown", "id": "3257a01a", "metadata": {}, "source": ["## &#128290; Tuning number of neighbors\nThe KNN classifier looks promising, let's test different values of K:"]}, {"cell_type": "markdown", "id": "ac101446", "metadata": {}, "source": ["![Imgur](https://i.imgur.com/vwadkbv.png)"]}, {"cell_type": "markdown", "id": "82404f26", "metadata": {}, "source": ["## &#128201; Plot results of grid search\n"]}, {"cell_type": "markdown", "id": "b4c56997", "metadata": {}, "source": ["### Output:\n![Imgur](https://i.imgur.com/DBn5Cyj.png)"]}, {"cell_type": "markdown", "id": "32c20bfd", "metadata": {}, "source": ["### &#128210;  Note:\nFrom examining our plot and using the elbow-method using 3 neighbors seems like the best choice to avoid overfitting. The main advantage of the KNN algorithm is that it performs well with multi-modal classes because the basis of its decision is based on a small neighborhood of similar objects. This is why its results were fairly high with 80%. The main disadvantage is the computational cost are very high and the results take far too long.**"]}, {"cell_type": "markdown", "id": "0f1ba5bd", "metadata": {}, "source": ["# &#127966; 2. Random Forest"]}, {"cell_type": "markdown", "id": "3d0065ef", "metadata": {}, "source": ["![Imgur](https://i.imgur.com/HyPUqwF.png)\n![Imgur](https://i.imgur.com/lREy3CV.jpg)\n![Imgur](https://i.imgur.com/lEuwiKK.jpg)"]}, {"cell_type": "markdown", "id": "1c790a09", "metadata": {}, "source": ["### &#128210; Note:\nRandom forests is an ensemble model which means that it uses the results from many different models to calculate a label"]}, {"cell_type": "markdown", "id": "437ceb14", "metadata": {}, "source": ["## &#128295; Base RFC model\n"]}, {"cell_type": "markdown", "id": "0bc7fa85", "metadata": {}, "source": ["### Output:\n```\naccuracy:74.4\n```"]}, {"cell_type": "markdown", "id": "ef67efed", "metadata": {}, "source": ["We then tuned the max_features parameters, which are the maximum number of\nfeatures Random Forest can try in individual tree. By limiting the max features to\nthe square root of total features we improved the model and made it computationally\nless expensive.\nWe then plotted the pixel importances and saw that the edges of the doodles tend to\nbe the most important."]}, {"cell_type": "markdown", "id": "74e4e91e", "metadata": {}, "source": ["## &#128295; Tuning number of estimators in the ensemble method\n"]}, {"cell_type": "markdown", "id": "39b48183", "metadata": {}, "source": ["### Output:\n![Imgur](https://i.imgur.com/Ds6OVNz.png)"]}, {"cell_type": "markdown", "id": "fa73645d", "metadata": {}, "source": ["## &#128201; Plot results of grid search\n"]}, {"cell_type": "markdown", "id": "907aa673", "metadata": {}, "source": ["### Output:\n![Imgur](https://i.imgur.com/wKOndoI.png)"]}, {"cell_type": "markdown", "id": "44c97169", "metadata": {}, "source": ["## &#128295; Tuning max features"]}, {"cell_type": "markdown", "id": "afdeffe7", "metadata": {}, "source": ["### Output:\n![Imgur](https://i.imgur.com/IbOqyhQ.png)"]}, {"cell_type": "markdown", "id": "f3d609b3", "metadata": {}, "source": ["### Output:\n![Imgur](https://i.imgur.com/MhH5Wnz.png)"]}, {"cell_type": "markdown", "id": "c3adb637", "metadata": {}, "source": ["##  Modeling RFC with best hyper-parameters"]}, {"cell_type": "markdown", "id": "0e8fbb88", "metadata": {}, "source": ["### Output:\n```\naccuracy:80.5\n```"]}, {"cell_type": "markdown", "id": "e64a7422", "metadata": {}, "source": ["### Output:\n**Seeing what pixels are the most important in deciding the label\n**![Imgur](https://i.imgur.com/aJMgZJi.png)"]}, {"cell_type": "markdown", "id": "8a085440", "metadata": {}, "source": ["### &#128210; Note:\nThe random forest had an accuracy score very close to the k-nn model. The features that are most important are on the edge and in the middle of each side."]}, {"cell_type": "markdown", "id": "e556d13b", "metadata": {}, "source": ["# &#128208; 3.Support Vector Machine\n![Imgur](https://i.imgur.com/HyPUqwF.png)\n![Imgur](https://i.imgur.com/JHwkkBg.jpg)\n![Imgur](https://i.imgur.com/mILrxTO.png)\n"]}, {"cell_type": "markdown", "id": "daa1c471", "metadata": {}, "source": ["### &#128210; Note:\nSVM classification uses planes in space to divide data points. We can compared a linear divider a non-linear divider."]}, {"cell_type": "markdown", "id": "b563afec", "metadata": {}, "source": ["## 3.1.LinearSVC"]}, {"cell_type": "markdown", "id": "e9c966fa", "metadata": {}, "source": ["### Output:\n```\nLinear SVC accuracy:71.7\n```"]}, {"cell_type": "markdown", "id": "6f47ea88", "metadata": {}, "source": ["## 3.2. Non-Linear SVM (Radial Basis Function)"]}, {"cell_type": "markdown", "id": "6cdd3168", "metadata": {}, "source": ["### Output:\n```\nGaussian Radial Basis Function SVC Accuracy:  77.15\n```"]}, {"cell_type": "markdown", "id": "20023721", "metadata": {}, "source": ["# &#128218;  4. Multi-Layer Perceptron\n\n![Imgur](https://i.imgur.com/HyPUqwF.png)\n\n![Imgur](https://i.imgur.com/1wtN0Ln.png)\n\n![Imgur](https://i.imgur.com/1S9bC75.jpg)"]}, {"cell_type": "markdown", "id": "0c2b1fd7", "metadata": {}, "source": ["### &#128210; Note\n\nA perceptron is a neural network with a very basic architecture. This will be good to compare against the convolutional neural network. Neural Networks receive an input (a single vector), and transform it through a series of hidden layers. Each hidden layer is made up of a set of neurons, where each neuron is fully connected to all neurons in the previous layer, and where neurons in a single layer function completely independently and do not share any connections. The last fully-connected layer is called the \u201coutput layer\u201d and in classification settings it represents the class scores.\n\nFor tuning the hyper-parameters for a Multi-Layer Perceptron, we can try different number of layers and number of neurons in each layer. The default activiation function (ReLu) is the most effective choice from sklearn and the the defualt optimization algorithm (adam) is the best choice due to the size of our dataset."]}, {"cell_type": "markdown", "id": "3119f29d", "metadata": {}, "source": ["### Output:\n```\nmlp accuracy:  81.9\n```"]}, {"cell_type": "markdown", "id": "fa2e0414", "metadata": {}, "source": ["### Output:\n**Mean Test Score of MLP:\n**![Imgur](https://i.imgur.com/lINgGl4.png)"]}, {"cell_type": "markdown", "id": "abbc4a6e", "metadata": {}, "source": ["### Output:\n```\nmlp accuracy:  84.6\n```"]}, {"cell_type": "markdown", "id": "5f71f4c3", "metadata": {}, "source": ["# &#128225; 5.Convolutional Neural Networks (CNN / ConvNet)\nThe Convolutional Neural Network architectures make the explicit assumption that the inputs are images, which allows us to encode certain properties into the architecture. These then make the forward function more efficient to implement and vastly reduce the amount of parameters in the network.\n\n![Imgur](https://i.imgur.com/HyPUqwF.png)\n![Imgur](https://i.imgur.com/jR5naeg.png)\n\n### How Work Convolutional Neural Networks (CNN)\n![Imgur](https://i.imgur.com/q5BBj8p.png)\n"]}, {"cell_type": "markdown", "id": "c8fcfe3d", "metadata": {}, "source": ["### Output:\n**Train on 21000 samples, validate on 9000 samples\n**![Imgur](https://i.imgur.com/QoB4Bft.png)"]}, {"cell_type": "markdown", "id": "d387a032", "metadata": {}, "source": ["### Output:\n**Model accuracy & Model loss plot:\n**![Imgur](https://i.imgur.com/csePWWo.png)"]}, {"cell_type": "markdown", "id": "6a8d6b8e", "metadata": {}, "source": ["### Output:\n```\nTest Loss: 41.8\nTest Accuracy: 91\n```"]}, {"cell_type": "markdown", "id": "0af22659", "metadata": {}, "source": ["# 8. Results"]}, {"cell_type": "markdown", "id": "9e47bfec", "metadata": {}, "source": ["### Output:\nFinal result of all\n```\nKNN accuracy:  0.819555555556 \n Random forest accuracy:  0.804555555556 \n Linear SVC accuracy:  0.717777777778 \n Gaussian Radial Basis Function SVC Accuracy:  0.781555555556 \n Multi-Layer Perceptron accuracy:  0.845666666667 \n Convolutional Neural Network Score: 0.909555555556 \n```\n "]}, {"cell_type": "markdown", "id": "9fa7ee9c", "metadata": {}, "source": ["# Conclusion:\nThis notebook a demon for this compitions. I will only use around 5000 doodles for each label since the full dataset would be too much for my computer to handle. Then I will explore the drawings and graph random sketches of each\ncategory. Finally, I will test the dataset with Random Forest, Support-Vector Machine (SVM), KNearest\nNeighbors (KNN) and Multi-Layer Perceptron (MLP) classifiers in scikit-learn as well as\na Convolutional Neural Network (CNN) in Karas.\n\nHere is some reference :\n1. [Quick Draw's Article](https://towardsdatascience.com/quick-draw-the-worlds-largest-doodle-dataset-823c22ffce6b)\n2. [Quick Draw's Demo video](https://www.youtube.com/watch?list=PLIivdWyY5sqJxnwJhe3etaK7utrBiPBQ2&time_continue=2&v=8DEjphIfeYw)\n3. [ Quick Draw's Paper](https://github.com/nolanadams1230/Doodle_Classification/blob/master/Final_Report.pdf)\n\nIf you enjoyed reading the kernel , hit the upvote button !"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}