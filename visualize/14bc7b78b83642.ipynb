{"cells": [{"cell_type": "markdown", "id": "53456fe0", "metadata": {}, "source": ["# **House Sales in King County, USA**"]}, {"cell_type": "markdown", "id": "0cb92ba0", "metadata": {}, "source": ["# **Introduction**\n\nThis dataset contains house sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015."]}, {"cell_type": "markdown", "id": "805ae8d2", "metadata": {}, "source": ["## Import Libraries"]}, {"cell_type": "code", "execution_count": 1, "id": "b0df504d", "metadata": {}, "outputs": [], "source": ["# Import Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nimport math\nimport matplotlib\nimport tensorflow as tf\n\n# Print versions of libraries\nprint(f\"Numpy version : Numpy {np.__version__}\")\nprint(f\"Pandas version : Pandas {pd.__version__}\")\nprint(f\"Matplotlib version : Matplotlib {matplotlib.__version__}\")\nprint(f\"Seaborn version : Seaborn {sns.__version__}\")\nprint(f\"Tensorflow version : Tensorflow {tf.__version__}\")\n\n#Magic function to display In-Notebook display\n%matplotlib inline\n\n# Setting seabon style\nsns.set(style='darkgrid', palette='deep')"]}, {"cell_type": "markdown", "id": "dc9c357a", "metadata": {}, "source": ["## Import the Dataset"]}, {"cell_type": "code", "execution_count": 1, "id": "3baf33c0", "metadata": {}, "outputs": [], "source": ["# df = pd.read_csv(\"kc_house_data.csv\", encoding = 'latin-1')\ndf = pd.read_csv('../input/housesalesprediction/kc_house_data.csv')"]}, {"cell_type": "code", "execution_count": 1, "id": "f320fd69", "metadata": {}, "outputs": [], "source": ["df.head(8).T"]}, {"cell_type": "markdown", "id": "05092f0b", "metadata": {}, "source": ["# **Exploratory Data Analysis**\n\nOnce the data is read into python, we need to explore/clean/filter it before processing it for machine learning It involves adding/deleting few colums or rows, joining some other data, and handling qualitative variables like dates."]}, {"cell_type": "code", "execution_count": 1, "id": "a794ce46", "metadata": {}, "outputs": [], "source": ["df.columns"]}, {"cell_type": "markdown", "id": "09e5dfff", "metadata": {}, "source": ["#### Feature Columns\n    \n* id - Unique ID for each home sold\n* date - Date of the home sale\n* price - Price of each home sold\n* bedrooms - Number of bedrooms\n* bathrooms - Number of bathrooms, where .5 accounts for a room with a toilet but no shower\n* sqft_living - Square footage of the apartments interior living space\n* sqft_lot - Square footage of the land space\n* floors - Number of floors\n* waterfront - A dummy variable for whether the apartment was overlooking the waterfront or not\n* view - An index from 0 to 4 of how good the view of the property was\n* condition - An index from 1 to 5 on the condition of the apartment,\n* grade - An index from 1 to 13, where 1-3 falls short of building construction and design, 7 has an average level of construction and design, and 11-13 have a high quality level of construction and design.\n* sqft_above - The square footage of the interior housing space that is above ground level\n* sqft_basement - The square footage of the interior housing space that is below ground level\n* yr_built - The year the house was initially built\n* yr_renovated - The year of the house\u2019s last renovation\n* zipcode - What zipcode area the house is in\n* lat - Lattitude\n* long - Longitude\n* sqft_living15 - The square footage of interior housing living space for the nearest 15 neighbors\n* sqft_lot15 - The square footage of the land lots of the nearest 15 neighbors"]}, {"cell_type": "markdown", "id": "541d4770", "metadata": {}, "source": ["## Checking concise summary of dataset\n\nIt is also a good practice to know the features and their corresponding data types,along with finding whether they contain null values or not."]}, {"cell_type": "code", "execution_count": 1, "id": "875db643", "metadata": {}, "outputs": [], "source": ["df.info()"]}, {"cell_type": "markdown", "id": "3e27c78e", "metadata": {}, "source": ["* Data has float, integer, and object type values.\n* Data type of timeStamp is object that need to conver to datetime."]}, {"cell_type": "markdown", "id": "4ef344ff", "metadata": {}, "source": ["## Generate descriptive statistics\n\nLets summarize the central tendency, dispersion and shape of a dataset's distribution."]}, {"cell_type": "code", "execution_count": 1, "id": "83e667c4", "metadata": {}, "outputs": [], "source": ["df.describe().T"]}, {"cell_type": "markdown", "id": "66dd3d2e", "metadata": {}, "source": ["### Categorical vs Continuous Features"]}, {"cell_type": "code", "execution_count": 1, "id": "42b3da00", "metadata": {}, "outputs": [], "source": ["# Finging unique values for each column\n# TO understand which column is categorical and which one is Continuous\ndf.nunique()"]}, {"cell_type": "markdown", "id": "7439eba1", "metadata": {}, "source": ["**Below are the category of the features:**\n\n1. Qualitative Features:\n    * date - Qualitative Features\n\n2. Categorical Features:\n    * floors - Categorical Nominal feature\n    * waterfront - Categorical Nominal feature\n    * view - Categorical Ordinal feature\n    * condition - Categorical Ordinal feature\n    * grade - Categorical Ordinal feature\n    * bedrooms - Categorical Nominal feature\n    * bathrooms - Categorical Nominal feature\n    \n3. Quantitative Features:\n    * Rest all are Quantitative Features."]}, {"cell_type": "markdown", "id": "aec81b91", "metadata": {}, "source": ["### Finding null values"]}, {"cell_type": "code", "execution_count": 1, "id": "dd2a2c03", "metadata": {}, "outputs": [], "source": ["# Dealing with missing data\ndf.isnull().sum()"]}, {"cell_type": "markdown", "id": "65bec66a", "metadata": {}, "source": ["There are no missing values present in the dataset. It is not necessary that missing values are present in the dataset in the form of  NA, NAN, Zeroes etc, it may be present by some other values that can be explored by analysising the each features."]}, {"cell_type": "markdown", "id": "8814aa78", "metadata": {}, "source": ["### Removing Duplicate rows"]}, {"cell_type": "code", "execution_count": 1, "id": "c58ff79a", "metadata": {}, "outputs": [], "source": ["df.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "1ee4dc6c", "metadata": {}, "outputs": [], "source": ["df = df.drop_duplicates()"]}, {"cell_type": "code", "execution_count": 1, "id": "17a79ab2", "metadata": {}, "outputs": [], "source": ["df.shape"]}, {"cell_type": "markdown", "id": "62b16d45", "metadata": {}, "source": ["There was no duplicate data in this dataset."]}, {"cell_type": "markdown", "id": "9a75ebac", "metadata": {}, "source": ["### Observe the distribution of target variable - Price\n\nIf target variable's distribution is too skewed then the predictive modeling will not be possible. Bell curve is desirable but slightly positive skew or negative skew is also fine."]}, {"cell_type": "code", "execution_count": 1, "id": "e5c4a539", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(8,6))\nsns.distplot(df['price'])\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "5b5fb799", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(8,6))\nsns.boxplot(df['price'])\nplt.show()"]}, {"cell_type": "markdown", "id": "b8ab4590", "metadata": {}, "source": ["Most the property prices falls between 0 and around $ 1.5 million and we have some outliers for really expensive houses and it may actually make sense to drop those outliers in our analysis if they are just a few\npoints that are very extreme.\n\nSo we can essentially build a model that realistically predicts the price of a house if its intended value is somewhere between let's say 0 and 2 million dollars.\n\nThere's not going to be that many houses apparently on the market that are worth more than let's say 3 million. So that's something to keep in mind here especially for applying this to a realistic situation when trying to build a model for a real estate agency since it's really not that many houses on the market that are that expensive. It may not be really useful to actually have our model train on these extreme outliers."]}, {"cell_type": "code", "execution_count": 1, "id": "4d19f623", "metadata": {}, "outputs": [], "source": ["# Create a function to return the outliers\ndef detect_outliers(x, c = 1.5):\n    q1, q3 = np.percentile(x, [25,75])\n    #print(\"q1 - \",q1, \" q3 - \", q3)\n    \n    iqr = (q3 - q1)\n    #print(\"iqr --\", iqr)\n    \n    lob = q1 - (iqr * c)\n    #print(\"lob - \",lob)\n    \n    uob = q3 + (iqr * c)\n    #print(\"uob - \",uob)\n    \n    # Generate outliers\n    indicies = np.where((x > uob) | (x < lob))\n\n    return indicies"]}, {"cell_type": "code", "execution_count": 1, "id": "ccee0528", "metadata": {}, "outputs": [], "source": ["# Detect all Outliers \npriceOutliers = detect_outliers(df['price'])\nprint(\"Total Outliers count : \",len(priceOutliers[0]))"]}, {"cell_type": "code", "execution_count": 1, "id": "a1b3a3a5", "metadata": {}, "outputs": [], "source": ["df.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "a31e1d82", "metadata": {}, "outputs": [], "source": ["# Remove outliers\ndf = df.drop(priceOutliers[0])"]}, {"cell_type": "code", "execution_count": 1, "id": "855ad066", "metadata": {}, "outputs": [], "source": ["df.shape"]}, {"cell_type": "markdown", "id": "f5022f64", "metadata": {}, "source": ["## Correlation Among Explanatory Variables\n\nHaving **too many features** in a model is not always a good thing because it might cause overfitting and worser results when we want to predict values for a new dataset. Thus, **if a feature does not improve your model a lot, not adding it may be a better choice.**\n\nAnother important thing is **correlation. If there is very high correlation between two features, keeping both of them is not a good idea most of the time not to cause overfitting.** For instance, if there is overfitting, we may remove sqt_above or sqt_living because they are highly correlated. This relation can be estimated when we look at the definitions in the dataset but to be sure the correlation matrix should be checked. However, this does not mean that you must remove one of the highly correlated features. For example bathrooms and sqrt_living. They are highly correlated but I do not think that the relation between them is the same as the relation between sqt_living and sqt_above.\n\nLets find out top 10 features which are highly correlaed with price."]}, {"cell_type": "code", "execution_count": 1, "id": "ec7f32ac", "metadata": {}, "outputs": [], "source": ["df.corr()['price'].sort_values(ascending=False).head(10)"]}, {"cell_type": "code", "execution_count": 1, "id": "0a8c600e", "metadata": {}, "outputs": [], "source": ["features = ['date', 'price', 'bedrooms', 'bathrooms', 'sqft_living',\n       'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',\n       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',\n       'lat', 'long', 'sqft_living15', 'sqft_lot15']\n\nmask = np.zeros_like(df[features].corr(), dtype=np.bool) \nmask[np.triu_indices_from(mask)] = True \n\nf, ax = plt.subplots(figsize=(16, 12))\nplt.title('Pearson Correlation Matrix',fontsize=25)\n\nsns.heatmap(df[features].corr(),linewidths=0.25,vmax=0.7,square=True,cmap=\"viridis\",\n            linecolor='w',annot=True,annot_kws={\"size\":8},mask=mask,cbar_kws={\"shrink\": .9});\n\nplt.show()"]}, {"cell_type": "markdown", "id": "212d92ef", "metadata": {}, "source": ["So the feature like sqft_living, grade, sqft_above, sqft_living15 , bathrooms, view , sqft_basement, bedrooms, lat are hightly correlated with lable price. So lets explore the highly correlated features with label Price."]}, {"cell_type": "code", "execution_count": 1, "id": "dc32bdfe", "metadata": {}, "outputs": [], "source": ["df.corr()['sqft_living'].sort_values(ascending=False).head(10)"]}, {"cell_type": "code", "execution_count": 1, "id": "a4e2b582", "metadata": {}, "outputs": [], "source": ["fig, axs = plt.subplots( nrows=2, ncols=3, sharey=True, figsize=(18,12))\n\nsns.scatterplot(y='price',x='sqft_living',data=df, hue='condition', palette='viridis', ax=axs[0,0])\nsns.scatterplot(y='price',x='sqft_above',data=df, hue='condition', palette='viridis', ax=axs[0,1])\nsns.scatterplot(y='price',x='sqft_living15',data=df, hue='condition', palette='viridis', ax=axs[0,2])\n\nsns.scatterplot(y='price',x='sqft_basement',data=df, hue='condition', palette='viridis', ax=axs[1,0])\nsns.scatterplot(y='price',x='sqft_lot',data=df, hue='condition', palette='viridis', ax=axs[1,1])\nsns.scatterplot(y='price',x='sqft_lot15',data=df, hue='condition', palette='viridis', ax=axs[1,2])\n\n\nplt.show()"]}, {"cell_type": "markdown", "id": "304602a7", "metadata": {}, "source": ["It seems there is strong positive correlation of sqft_living, sqft_above and sqft_living15 with price. However it is not the case with sqft_basement, sqft_lot, sqft_lot15. It is quite obvious that bigger houses are costiler. Graphs also shows that most of the houses do not have basement and land space, although price is not linearily increasing with bigger basement and land space. I found that the feature sqft_living is sum of sqft_above and sqft_basement. \n\nsqft_living = sqft_above + sqft_basement"]}, {"cell_type": "markdown", "id": "d651eebe", "metadata": {}, "source": ["#### View of property vs property price"]}, {"cell_type": "code", "execution_count": 1, "id": "ac5fdbec", "metadata": {}, "outputs": [], "source": ["fig, axs = plt.subplots(ncols=2, figsize=(12,6))\n\nsns.countplot(x='view',data=df, palette='Set2', ax=axs[0])\nsns.boxplot(y='price',x='view',data=df, palette='Set2', ax=axs[1])\n\nplt.tight_layout()\nplt.plot()"]}, {"cell_type": "markdown", "id": "4f9ec92b", "metadata": {}, "source": ["View ranges from 0 to 4 and it shows how good the view of the property was. Above plots graph shows that majority of houses do have good view and that's why their price range is also very less. Few houses have view index of 4 and they are actually expensive."]}, {"cell_type": "markdown", "id": "9daaae8a", "metadata": {}, "source": ["#### Grade of property vs property price"]}, {"cell_type": "code", "execution_count": 1, "id": "898c92af", "metadata": {}, "outputs": [], "source": ["fig, axs = plt.subplots(ncols=2, figsize=(12,6))\n\nax = sns.countplot(x='grade',data=df, palette='Set2', ax=axs[0])\nax.set_xticklabels(ax.get_xticklabels(), rotation=60, ha=\"right\")\nsns.boxplot(y='price',x='grade',data=df, palette='Set2', ax=axs[1])\n\nplt.tight_layout()\nplt.show()"]}, {"cell_type": "markdown", "id": "b9b3f217", "metadata": {}, "source": ["grade - An index from 1 to 13, where 1-3 falls short of building construction and design, 7 has an average level of construction and design, and 11-13 have a high quality level of construction and design.\n\nMost of the houses have average level of contrcution and design. And as the construction and design quality increases, as their prices. It actually make sense that good quality of construction and design have higher prices."]}, {"cell_type": "markdown", "id": "2acccc43", "metadata": {}, "source": ["#### Condition of property vs property price"]}, {"cell_type": "code", "execution_count": 1, "id": "3c82e55a", "metadata": {}, "outputs": [], "source": ["fig, axs = plt.subplots(ncols=2, figsize=(12,6))\n\nsns.countplot(x='condition',data=df, palette='Set2', ax=axs[0])\nsns.boxplot(y='price',x='condition',data=df, palette='Set2', ax=axs[1])\n\nplt.tight_layout()\nplt.show()"]}, {"cell_type": "markdown", "id": "dec2facf", "metadata": {}, "source": ["condition - An index from 1 to 5 on the condition of the apartment.\n\nMajority of houses have average condition, however there prices is not very less in comparision to good condition of houses. "]}, {"cell_type": "markdown", "id": "239578cd", "metadata": {}, "source": ["#### Waterfront of property vs property price"]}, {"cell_type": "code", "execution_count": 1, "id": "abf615df", "metadata": {}, "outputs": [], "source": ["fig, axs = plt.subplots(ncols=2, figsize=(12,6))\n\nsns.countplot(x='waterfront',data=df, palette='Set2', ax=axs[0])\nsns.boxplot(y='price',x='waterfront',data=df, palette='Set2', ax=axs[1])\n\nplt.tight_layout()\nplt.show()"]}, {"cell_type": "markdown", "id": "d91fbb73", "metadata": {}, "source": ["waterfront - A dummy variable for whether the apartment was overlooking the waterfront or not.\n\nIt is quite obvious that properties facing water bodies are always quite expensive, however they are only few in numbers."]}, {"cell_type": "markdown", "id": "eda50e50", "metadata": {}, "source": ["#### No of Bedroom of property vs property price"]}, {"cell_type": "code", "execution_count": 1, "id": "55bd5c99", "metadata": {}, "outputs": [], "source": ["fig, axs = plt.subplots(ncols=2, figsize=(12,6))\n\nsns.countplot(x='bedrooms',data=df, palette='Set2',ax=axs[0])\nsns.boxplot(y='price',x='bedrooms',data=df, palette='Set2', ax=axs[1])\n\nplt.tight_layout()\nplt.show()"]}, {"cell_type": "markdown", "id": "9e868132", "metadata": {}, "source": ["Vast majority of all these houses have somewhere between two to five bedrooms and it looks like there's a huge mansion that has 33 bedrooms. But we can't actually see that bar because number of rest of the bedrooms are in the thousands and this is as small as one.\n\nOther graph is showing the distribution of prices per bedrooms. There's quite a bit of variation in bedrooms ranging between 3 and 7. And that also makes sense because if we took a look at our count plot it looks like the majority of the houses have bedrooms between maybe 3 and 7. So it also makes sense that there's quite a large variety in prices there."]}, {"cell_type": "markdown", "id": "9ca1f68a", "metadata": {}, "source": ["#### No of Bathrooms of property vs property price"]}, {"cell_type": "code", "execution_count": 1, "id": "44e98a36", "metadata": {}, "outputs": [], "source": ["fig, axs = plt.subplots(ncols=2, figsize=(12,6))\n\nchart1 = sns.countplot(x='bathrooms',data=df, palette='Set2',ax=axs[0])\nchart1.set_xticklabels(chart1.get_xticklabels(), rotation=90, horizontalalignment='right')\n\nchart2 = sns.boxplot(y='price',x='bathrooms',data=df, palette='Set2', ax=axs[1])\nchart2.set_xticklabels(chart2.get_xticklabels(), rotation=90, horizontalalignment='right')\n\nplt.tight_layout()\nplt.show()"]}, {"cell_type": "markdown", "id": "77138394", "metadata": {}, "source": ["Bathrooms - Number of bathrooms, where .5 accounts for a room with a toilet but no shower.\n\nMost of the houses have 1 to 4 bathrooms and their prices is also less compared to houess have more than 4 bathrooms. Actually 'bathroom' is highly correlated with 'sqft_living' as per headmap generated above, so I think it area of the house making it expensive not the number of bathrooms."]}, {"cell_type": "markdown", "id": "e530f868", "metadata": {}, "source": ["#### No of floors of property vs property price"]}, {"cell_type": "code", "execution_count": 1, "id": "056ea0a9", "metadata": {}, "outputs": [], "source": ["fig, axs = plt.subplots(ncols=2, figsize=(12,6))\n\nsns.countplot(x='floors',data=df, palette='Set2',ax=axs[0])\nsns.boxplot(y='price',x='floors',data=df, palette='Set2', ax=axs[1])\n\nplt.tight_layout()\nplt.show()"]}, {"cell_type": "markdown", "id": "a8e5b0f2", "metadata": {}, "source": ["Large number of properies have only ground floor and few properties have penthouse also. However it does not show that high story building are more expensive, may be other factors like total size of property or location is also playing the major role in deciding the price of property."]}, {"cell_type": "markdown", "id": "0b41df97", "metadata": {}, "source": ["### Geographical Properties\n\nThis dataset contain the features latitude and longitude so it may be interesting to actually explore this by plotting. So let's first see the distribution of prices per latitude vs. longitude."]}, {"cell_type": "code", "execution_count": 1, "id": "52058281", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(12,8))\nsns.scatterplot(x='price',y='long',data=df)\nplt.show()"]}, {"cell_type": "markdown", "id": "74a37d4c", "metadata": {}, "source": ["There was no price differentiation based off longitude. But it looks like there tends to be some sort of price distribution at a certain longitude. So looks like at longitude -122 have expensive housing area. We can see the distribution quite clearly."]}, {"cell_type": "code", "execution_count": 1, "id": "21e00cba", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(12,8))\nsns.scatterplot(x='price',y='lat',data=df)\nplt.show()"]}, {"cell_type": "markdown", "id": "fe99bd54", "metadata": {}, "source": ["And the same behavior seems for latitude also. It also seems that a particular latitude there's some sort of expensive housing area. And basically what this is telling is that it looks like at a certain combination of latitude and longitude that tends to be an expensive area.\nSo if we just look at a King County map we can begin to distinguish this."]}, {"cell_type": "code", "execution_count": 1, "id": "2dbfa839", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(12,8))\nsns.scatterplot(x='long',y='lat',\n                data=df,hue='price',\n                palette='RdYlGn',edgecolor=None,alpha=0.2)\nplt.show()"]}, {"cell_type": "markdown", "id": "76e7ed4c", "metadata": {}, "source": ["If we compare above plot to our actual map of King County we can see that more or less they tend to match up.\nWe can see here kind of the shapes of Seattle and we can see <a href=\"https://goo.gl/maps/mrsCJavrJ9mywpBC7\">here the real map of King County.</a>\n\nPlot show that there are certain expensive housing areas espically between long -122 and lat 47.4.\n\nThis graph showing the expensive parts of King County. And also note the expensive houses almost on the edge of the water points which makes sense because usually a waterfront property are going to be more expensive than inland properties."]}, {"cell_type": "markdown", "id": "d5878534", "metadata": {}, "source": ["# **Working with Feature Data**"]}, {"cell_type": "code", "execution_count": 1, "id": "cc05a883", "metadata": {}, "outputs": [], "source": ["df.info()"]}, {"cell_type": "code", "execution_count": 1, "id": "223f54e2", "metadata": {}, "outputs": [], "source": ["df = df.drop('id',axis=1)"]}, {"cell_type": "markdown", "id": "70fc3ca5", "metadata": {}, "source": ["### Feature Engineering from Date"]}, {"cell_type": "code", "execution_count": 1, "id": "73d622eb", "metadata": {}, "outputs": [], "source": ["df['date'] = pd.to_datetime(df['date'])"]}, {"cell_type": "code", "execution_count": 1, "id": "6cea52f9", "metadata": {}, "outputs": [], "source": ["df['month'] = df['date'].apply(lambda date : date.month)"]}, {"cell_type": "code", "execution_count": 1, "id": "4c1be3ab", "metadata": {}, "outputs": [], "source": ["df['year'] = df['date'].apply(lambda date : date.year)"]}, {"cell_type": "code", "execution_count": 1, "id": "8140e5d4", "metadata": {}, "outputs": [], "source": ["fig, axs = plt.subplots(ncols=2, figsize=(12,6))\n\nsns.countplot(x='year',data=df, palette='Set2',ax=axs[0])\nsns.boxplot(y='price',x='year',data=df, palette='Set2', ax=axs[1])\n\nplt.tight_layout()\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "3286c86a", "metadata": {}, "outputs": [], "source": ["fig, axs = plt.subplots(ncols=2, figsize=(12,6))\n\nsns.countplot(x='month',data=df, palette='Set2',ax=axs[0])\nsns.boxplot(y='price',x='month',data=df, palette='Set2', ax=axs[1])\n\nplt.tight_layout()\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "0e656688", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(8,6))\ndf.groupby('month').mean()['price'].plot()"]}, {"cell_type": "markdown", "id": "fc6aad12", "metadata": {}, "source": ["**We do not see any significat differences in price with months.**"]}, {"cell_type": "code", "execution_count": 1, "id": "0621ead3", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(8,6))\ndf.groupby('year').mean()['price'].plot()"]}, {"cell_type": "markdown", "id": "c7db2ea5", "metadata": {}, "source": ["**Above plot definitely makes sense because if you look back at King County sales they're just increasing in price as time goes on.**"]}, {"cell_type": "code", "execution_count": 1, "id": "eb7416d7", "metadata": {}, "outputs": [], "source": ["# Drop the date columns after doing feature engineering.\ndf = df.drop('date',axis=1)"]}, {"cell_type": "markdown", "id": "5e2035b4", "metadata": {}, "source": ["### Feature Engineering from zipcode"]}, {"cell_type": "code", "execution_count": 1, "id": "a55f4b56", "metadata": {}, "outputs": [], "source": ["df['zipcode'].value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "9c186115", "metadata": {}, "outputs": [], "source": ["df = df.drop('zipcode',axis=1)"]}, {"cell_type": "markdown", "id": "732688fe", "metadata": {}, "source": ["Zipcode is actually categorical feature with 70 unique values, however it look like continues. If we keep zipcode as it is, then model will pretend that zipcode 98103 is greater than 98038, which is not the case. So apply some kind of feature engineering like by creating dummy variable, but it will create 70 more features which is too much for us. So lets do not do any more manual work here and delete this zipcode feature."]}, {"cell_type": "markdown", "id": "b4649902", "metadata": {}, "source": ["### Feature Engineering from Year of Renovation"]}, {"cell_type": "code", "execution_count": 1, "id": "6ff77bd2", "metadata": {}, "outputs": [], "source": ["# could make sense due to scaling, higher should correlate to more value\ndf['yr_renovated'].value_counts()"]}, {"cell_type": "markdown", "id": "be3a08df", "metadata": {}, "source": ["Most of the values of feature 'yr_renovated' is zero, which implies that they are not renovated at all. However zero is actually is not the year and other values are year. So actually it make more sense to categorize into renovated us not-renovated."]}, {"cell_type": "code", "execution_count": 1, "id": "4c658158", "metadata": {}, "outputs": [], "source": ["df['renovated'] = df['yr_renovated'].apply(lambda yr : 0 if yr==0 else 1)"]}, {"cell_type": "code", "execution_count": 1, "id": "efc4b4e9", "metadata": {}, "outputs": [], "source": ["df['renovated'].value_counts()"]}, {"cell_type": "markdown", "id": "984ac185", "metadata": {}, "source": ["Now we got clear picture that 19,701 houses are not renovated while only 766 properties are renovated. Below graph shows that renovated properties are expensive."]}, {"cell_type": "code", "execution_count": 1, "id": "d9db4fce", "metadata": {}, "outputs": [], "source": ["fig, axs = plt.subplots(ncols=2, figsize=(12,6))\n\nsns.countplot(x='renovated',data=df, palette='Set2',ax=axs[0])\nsns.boxplot(y='price',x='renovated',data=df, palette='Set2', ax=axs[1])\n\nplt.tight_layout()\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "2f12b6f6", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(12,8))\nsns.scatterplot(x='yr_built', y='price',hue='renovated' ,data=df)\nplt.show()"]}, {"cell_type": "markdown", "id": "b857c9e6", "metadata": {}, "source": ["### Feature Engineering from columns containing year"]}, {"cell_type": "markdown", "id": "5809072d", "metadata": {}, "source": ["Actually all the columns containing year are categorical like yr_built and yr_renovated. However, notice that it's most likely that more recent the year of build or renovation essentially the higher the value of that property. So there is a positive correlation of price with yr_built and yr_renovated. So in this case we're actually kind of lucky and due to the scaling from zero to the highest year higher should correlate with more value and we can actually just keep this as is. There is no harm if our model would pretent that years as continues values and 2015 > 2010."]}, {"cell_type": "markdown", "id": "aaa6fb17", "metadata": {}, "source": ["# **Splitting data into Training and Testing samples**\n\nWe dont use the full data for creating the model. Some data is randomly selected and kept aside for checking how good the model is. This is known as Testing Data and the remaining data is called Training data on which the model is built. Typically 70% of data is used as Training data and the rest 30% is used as Tesing data."]}, {"cell_type": "code", "execution_count": 1, "id": "37cac1ba", "metadata": {}, "outputs": [], "source": ["# Separate Target Variable and Predictor Variables\n# Also please note that to call the values here because tensor flow may work with numeric array, and it can't work with pandas dataframes.\nX = df.drop('price',axis=1).values\ny = df['price'].values"]}, {"cell_type": "code", "execution_count": 1, "id": "7ac1f620", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split"]}, {"cell_type": "code", "execution_count": 1, "id": "3f9bf737", "metadata": {}, "outputs": [], "source": ["# Split the data into training and testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"]}, {"cell_type": "code", "execution_count": 1, "id": "277815ec", "metadata": {}, "outputs": [], "source": ["# Quick sanity check with the shapes of Training and testing datasets\nprint(\"X_train - \",X_train.shape)\nprint(\"y_train - \",y_train.shape)\nprint(\"X_test - \",X_test.shape)\nprint(\"y_test - \",y_test.shape)"]}, {"cell_type": "markdown", "id": "0be4b2a2", "metadata": {}, "source": ["# Bringing features onto the same scale\n\n* While using distance based methods like KNN we must attempt to scale the data, so that the column(feature) with lesser significance might not end up dominating the objective function due to its larger range. like a column like age has a range between 0 to 80, but a column like salary has range from thousands to lakhs, hence, salary column will dominate to predict the outcome even if it may not be important.\n* In addition, features having different unit should also be scaled thus providing each feature equal initial weightage. Like Age in years and Sales in Dollars must be brought down to a common scale before feeding it to the ML algorithm\n* This will result in a better prediction model.\n\n\n### Normalization i.e. Min-Max scaling\n\nAn alternative approach to Z-score normalization (or standardization) is the so-called **Min-Max scaling** (often also simply called \"normalization\" - a common cause for confusion!).  \nIn this approach, the data is scaled to a fixed range - usually 0 to 1.  \nThe cost of having this bounded range - in contrast to standardization - is that we will end up with smaller standard deviations, which can suppress the effect of outliers.\n\nA Min-Max scaling is typically done via the following equation:\n\n\\begin{equation} X_{norm} = \\frac{X - X_{min}}{X_{max}-X_{min}} \\end{equation}"]}, {"cell_type": "code", "execution_count": 1, "id": "fd56eddd", "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nX_train= scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "64c80b15", "metadata": {}, "outputs": [], "source": ["np.min(X_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "eb3f7eec", "metadata": {}, "outputs": [], "source": ["np.max(X_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "eb570c63", "metadata": {}, "outputs": [], "source": ["X_train.shape[1]"]}, {"cell_type": "markdown", "id": "7cda9f63", "metadata": {}, "source": ["# **Creating a Neural Network Model**"]}, {"cell_type": "markdown", "id": "088f4134", "metadata": {}, "source": ["In this section we will create a baseline neural network model for the regression problem. Let\u2019s start off by including all of the functions and objects we will need."]}, {"cell_type": "code", "execution_count": 1, "id": "58242674", "metadata": {}, "outputs": [], "source": ["from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.optimizers import Adam"]}, {"cell_type": "markdown", "id": "3d5af430", "metadata": {}, "source": ["We build Sequential model that has two densely connected hidden layer to make it Deep Learning Network with the same number of neurons as input attributes (20). The network uses good practices such as the rectifier activation 'relu' function for the hidden layer. No activation function is used for the output layer because it is a regression problem and we are interested in predicting numerical values directly without transform.\n\nThe efficient ADAM optimization algorithm is used and a mean squared error loss function is optimized. This will be the same metric that we will use to evaluate the performance of the model. It is a desirable metric because by taking the square root gives us an error value we can directly understand in the context of the problem (thousands of dollars)."]}, {"cell_type": "code", "execution_count": 1, "id": "0a73aa47", "metadata": {}, "outputs": [], "source": ["model = Sequential()\n\nmodel.add(Dense(20,activation='relu'))\nmodel.add(Dense(20,activation='relu'))\nmodel.add(Dense(20,activation='relu'))\nmodel.add(Dense(20,activation='relu'))\nmodel.add(Dense(1))\n\nmodel.compile(optimizer='adam',loss='mse')"]}, {"cell_type": "markdown", "id": "531cac5a", "metadata": {}, "source": ["## Training the Model"]}, {"cell_type": "markdown", "id": "23289d66", "metadata": {}, "source": ["Train the model for 400 epochs, and record the training and validation accuracy in the history object. We also pass the validation data  \nso that after each epoch of training on the training data will quickly run the test data and check our loss on the test data, so that way we can keep a tracking of how well performing not just on our training data but also on our test data.\n\nNote that this test data will not actually affect the weights or biases of our network. So Kares isn't going to update the model based on the test data or validation data. Instead it will only use the training data as it's updating the weights and biases and continue to check how well model is doing and not just the training data but also the validation data.\n\nIt is a larger data set so we're going to feed in our data in batches and we're gonna call batch size of 128. It's very typical to do batch sizes in powers of two so 64, 128, 256.  The smaller the batch size, the longer training is going to take but the less likely we're going to overfit to data because we're not passing in the entire training set at once."]}, {"cell_type": "code", "execution_count": 1, "id": "8c27f937", "metadata": {}, "outputs": [], "source": ["model.fit(x=X_train,y=y_train,\n          validation_data=(X_test,y_test),\n          batch_size=128,epochs=400)"]}, {"cell_type": "code", "execution_count": 1, "id": "fcb1696c", "metadata": {}, "outputs": [], "source": ["model.summary()"]}, {"cell_type": "code", "execution_count": 1, "id": "e6a5fdb6", "metadata": {}, "outputs": [], "source": ["losses = pd.DataFrame(model.history.history)"]}, {"cell_type": "code", "execution_count": 1, "id": "edc5ba84", "metadata": {}, "outputs": [], "source": ["losses.plot(figsize=(12,8))"]}, {"cell_type": "markdown", "id": "0ac76afc", "metadata": {}, "source": ["# Evaluation on Test Data\n\nhttps://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics"]}, {"cell_type": "code", "execution_count": 1, "id": "be2b57cc", "metadata": {}, "outputs": [], "source": ["from sklearn import metrics\n\ndef measure_accuracy(original, predicted, train=True):  \n    mae = metrics.mean_absolute_error(original, predicted)\n    mse = metrics.mean_squared_error(original, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(original, predicted))\n    #rmsle = np.sqrt(metrics.mean_squared_log_error(original, predicted))\n    r2_square = metrics.r2_score(original, predicted)\n    evs = metrics.explained_variance_score(original,predicted)\n    \n    if train:\n        print(\"Training Result : \")\n        print('------------------')\n        print('MAE: {0:0.3f}'.format(mae))\n        print('MSE: {0:0.3f}'.format(mse))\n        print('RMSE: {0:0.3f}'.format(rmse))\n        #print('RMSLE: {0:0.3f}'.format(rmsle))\n        print('Explained Variance Score: {0:0.3f}'.format(evs))\n        print('R2 Square: {0:0.3f}'.format(r2_square))\n        print('\\n')\n    elif not train:\n        print(\"Testing Result : \")\n        print('------------------')\n        print('MAE: {0:0.3f}'.format(mae))\n        print('MSE: {0:0.3f}'.format(mse))\n        print('RMSE: {0:0.3f}'.format(rmse))\n        #print('RMSLE: {0:0.3f}'.format(rmsle))\n        print('Explained Variance Score: {0:0.3f}'.format(evs))\n        print('R2 Square: {0:0.3f}'.format(r2_square))"]}, {"cell_type": "markdown", "id": "ebfd21be", "metadata": {}, "source": ["### Predicting on Brand New Data"]}, {"cell_type": "code", "execution_count": 1, "id": "a059d283", "metadata": {}, "outputs": [], "source": ["y_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "c9b98de9", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(8,6))\n# Our predictions\nplt.scatter(y_test,y_test_pred)\n\n# Perfect predictions\nplt.plot(y_test,y_test,'r')"]}, {"cell_type": "code", "execution_count": 1, "id": "eb5526cf", "metadata": {}, "outputs": [], "source": ["errors = y_test.reshape(6141, 1) - y_test_pred"]}, {"cell_type": "code", "execution_count": 1, "id": "505d5b7b", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(8,6))\nsns.distplot(errors)"]}, {"cell_type": "code", "execution_count": 1, "id": "386142d0", "metadata": {}, "outputs": [], "source": ["measure_accuracy(y_train, y_train_pred, train=True)\nmeasure_accuracy(y_test, y_test_pred, train=False)"]}, {"cell_type": "markdown", "id": "4aafadfe", "metadata": {}, "source": ["# **Comparing with LinearRegression**"]}, {"cell_type": "code", "execution_count": 1, "id": "41374875", "metadata": {}, "outputs": [], "source": ["from sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "e7ee7521", "metadata": {}, "outputs": [], "source": ["y_train_pred = lr.predict(X_train)\ny_test_pred = lr.predict(X_test)\n\nmeasure_accuracy(y_train, y_train_pred, train=True)\nmeasure_accuracy(y_test, y_test_pred, train=False)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}