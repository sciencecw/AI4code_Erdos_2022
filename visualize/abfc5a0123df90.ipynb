{"cells": [{"cell_type": "markdown", "id": "3dcdd279", "metadata": {}, "source": ["Our world is moving towards cashless economy and so, we need to build efficient models to track fraud transactions.Here is my attempt to build one. The challenge in this dataset is that, the Class is highly imbalanced.\nI followed the following steps:\n1. EDA\n2. train test split\n3. Normal Scaling\n4. TSNE for visualization\n5. SMOTE for upsampling the minority\n6. Trained multiple models\n7. Chose the ones with low variance compared to others\n8. Blending of those models with XGBClassifier\n\n**Please upvote the kernel if you liked it. Your upvotes will motivate me to code more. Thank you  ** "]}, {"cell_type": "markdown", "id": "e70e2a28", "metadata": {}, "source": ["**Importing basic modules**"]}, {"cell_type": "code", "execution_count": 1, "id": "8a8bc35e", "metadata": {}, "outputs": [], "source": ["\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport os\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.stats import kurtosis,skew\nfrom sklearn.manifold import TSNE\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split,StratifiedKFold,GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import average_precision_score,make_scorer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier,AdaBoostClassifier,RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom scipy.stats import zscore\nfrom xgboost import XGBClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\nprint(os.listdir(\"../input\"))\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "1a1a8348", "metadata": {}, "outputs": [], "source": ["data=pd.read_csv('../input/creditcard.csv')\ndata.head()"]}, {"cell_type": "markdown", "id": "0c4ebf2e", "metadata": {}, "source": ["**Taking overview of data**"]}, {"cell_type": "code", "execution_count": 1, "id": "85318f5f", "metadata": {}, "outputs": [], "source": ["data.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "e412e30b", "metadata": {}, "outputs": [], "source": ["data.info()"]}, {"cell_type": "markdown", "id": "28b183e3", "metadata": {}, "source": ["Hushhhh!! No null values :D :D"]}, {"cell_type": "code", "execution_count": 1, "id": "16c9625f", "metadata": {}, "outputs": [], "source": ["data.isnull().sum()"]}, {"cell_type": "code", "execution_count": 1, "id": "8e6e42f5", "metadata": {}, "outputs": [], "source": ["data.describe()"]}, {"cell_type": "markdown", "id": "48b5c458", "metadata": {}, "source": ["Let us plot V1,V2,.. upto V28. V16, V18 and V19 seem to be closer to normal distribution than any other columns. We will dive a bit deeper in the next step. "]}, {"cell_type": "code", "execution_count": 1, "id": "a54d9475", "metadata": {}, "outputs": [], "source": ["f,ax=plt.subplots(7,4,figsize=(15,15))\nfor i in range(28):\n    sns.distplot(data['V'+str(i+1)],ax=ax[i//4,i%4])\n    \n\nplt.tight_layout()\nplt.show()"]}, {"cell_type": "markdown", "id": "00dbd26e", "metadata": {}, "source": ["Let us create a dataframe with columns mean, standard deviation, max, min, skewness and kurtosis of each of the V columns. Let us plot each of these features.  "]}, {"cell_type": "code", "execution_count": 1, "id": "d1e85203", "metadata": {}, "outputs": [], "source": ["stats=pd.DataFrame()\ncols=[col for col in data.columns[1:29]]\nmean=data[cols].mean(axis=0)\nstd=data[cols].std(axis=0)\nmax_val=data[cols].max(axis=0)\nmin_val=data[cols].min(axis=0)\nskew=data[cols].skew(axis=0)\nkurt=data[cols].kurt(axis=0)\nstats['mean']=mean\nstats['std']=std\nstats['max']=max_val\nstats['min']=min_val\nstats['skew']=skew\nstats['kurt']=kurt\nstats.index=cols\n"]}, {"cell_type": "code", "execution_count": 1, "id": "aec2d71b", "metadata": {}, "outputs": [], "source": ["x_ticks=np.arange(1,29,1)\nf,ax=plt.subplots(2,3,figsize=(15,8))\nfor i in range(6):\n    ax[i//3,i%3].plot(x_ticks,stats.iloc[:,i].values,'b.')\n    ax[i//3,i%3].set_title(stats.columns[i])\n    \nplt.tight_layout()\nplt.show()"]}, {"cell_type": "markdown", "id": "854b43bc", "metadata": {}, "source": ["There is atleast one outlier in each of the graphs. In 'mean' graph, mean of V3 is quite different from other columns.'std' graph looks fine and clearly shows decreasing trend in standard deviation with increasing value of column. In 'max' graph, max value of V6 and V7 is quite different. In 'min' graph, minimum value of V5 is quite different. In 'skew' aswell as 'kurtosis' graph, skewness and kurtosis of V29 are quite high. Let us understand what skewness and kurtosis actually mean. Skewness is a measure of how unsymmetric are the tails of the distribution. Normal distribution has 0 skewness because it has perfectly symmetric tails. V8 and V29 are skewed to left and right respectively to large extent. Skewed to left means the tails are longer on the left side rather than right side of mean. Kurtosis is a measure of how much probability mass is concentrated on the shoulders of distribution. Normal distribution has a kurtosis of 3 (excess kurtosis=0). As kurtosis increases, probability mass decreases from shoulder and spreads at the center and tails of the distribution. Minimum kurtosis possible is 0 (excess kurtosis= -3) and maximum possible is infinite. 0 kurtosis implies all the probability mass is concentrated on the shoulders. Many columns have very high excess kurtosis. Both skewness and kurtosis are unitless. Outliers in a sample have more effect on kurtosis than on skewness because kurtosis involves fourth moment while skewness involves third moment of distribution."]}, {"cell_type": "code", "execution_count": 1, "id": "57cf2df7", "metadata": {}, "outputs": [], "source": ["print(stats.loc[['V16','V18','V19'],:])"]}, {"cell_type": "markdown", "id": "076293a4", "metadata": {}, "source": ["Here we can see that only looking at graphs is deceptive. I had thought that V16,V18 and V19 look close to normal distributions. But kurtosis of V16 says otherwise. V18 is quite close to normal distribution."]}, {"cell_type": "code", "execution_count": 1, "id": "1c4e268d", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(15,10))\nsns.heatmap(data.corr())\nplt.show()"]}, {"cell_type": "markdown", "id": "bac30066", "metadata": {}, "source": ["None of the columns is significantly correlated with Class. Amount is kind of significantly correlated with V7 and V20.\n\nNow let us look how the Classes are distributed."]}, {"cell_type": "code", "execution_count": 1, "id": "8913baf9", "metadata": {}, "outputs": [], "source": ["sns.countplot(data['Class'])\nprint((data['Class'].value_counts()/data.shape[0])*100)"]}, {"cell_type": "markdown", "id": "70ddc2a1", "metadata": {}, "source": ["Class 1 corresponds to Fraud Transactions. We see that only 0.172% of transactions are fraud. The data is highly imbalanced. Since just upsampling with replace=True will lead to lot of duplicates. So, I shall use a technique called SMOTE. It is basically oversamplng technique which tweaks just one column a little bit and thus a new sample of minority class is created.I shall use SMOTE only on training data so that fraud transactions remain as minority in case of validation as it would be in real world scenario."]}, {"cell_type": "code", "execution_count": 1, "id": "56344719", "metadata": {}, "outputs": [], "source": ["X=data.drop(['Class','Time'],axis=1)\nY=data['Class']\ntrain_X,test_X,train_y,test_y=train_test_split(X,Y,random_state=5,test_size=0.2)"]}, {"cell_type": "code", "execution_count": 1, "id": "e7bd3cbe", "metadata": {}, "outputs": [], "source": ["sc=StandardScaler()\ntrain_X=sc.fit_transform(train_X)\ntest_X=sc.transform(test_X)\ntrain_X=pd.DataFrame(train_X,columns=X.columns)\ntest_X=pd.DataFrame(test_X,columns=X.columns)"]}, {"cell_type": "code", "execution_count": 1, "id": "c61ab205", "metadata": {}, "outputs": [], "source": ["sm=SMOTE(random_state=5)\ntrain_X_res,train_y_res=sm.fit_sample(train_X,train_y)\ntrain_X_res=pd.DataFrame(train_X_res,columns=train_X.columns)\ntrain_y_res=pd.Series(train_y_res,name='Class')"]}, {"cell_type": "markdown", "id": "8d892761", "metadata": {}, "source": ["Let us see how much separable are the two classes. If we consider all the V columns then we shall have 28 dimensional space. We cannot visualise such high dimensional data. I shall use TSNE to project the points from 28 dimensional space to 2 dimensional space. For faster computation, I shall only take 2500 points from each class.\n"]}, {"cell_type": "code", "execution_count": 1, "id": "f8a77372", "metadata": {}, "outputs": [], "source": ["train=pd.concat([train_X_res,train_y_res],axis=1)\nfraud=train[train['Class']==1].sample(2500)\nnon_fraud=train[train['Class']==0].sample(2500)\ntsne_data=pd.concat([fraud,non_fraud],axis=0)\ntsne_data_1=tsne_data.drop(['Class'],axis=1)\n"]}, {"cell_type": "code", "execution_count": 1, "id": "9eeb2fd4", "metadata": {}, "outputs": [], "source": ["tsne=TSNE(n_components=2,random_state=5,verbose=1)\ntsne_trans=tsne.fit_transform(tsne_data_1)"]}, {"cell_type": "code", "execution_count": 1, "id": "94fdffa3", "metadata": {}, "outputs": [], "source": ["tsne_data['first_tsne']=tsne_trans[:,0]\ntsne_data['second_tsne']=tsne_trans[:,1]\nplt.figure(figsize=(15,10))\nsns.scatterplot(tsne_data['first_tsne'],tsne_data['second_tsne'],hue='Class',data=tsne_data)\n"]}, {"cell_type": "code", "execution_count": 1, "id": "d60b80b5", "metadata": {}, "outputs": [], "source": ["models=[SVC(probability=True),LogisticRegression(),LinearDiscriminantAnalysis(),DecisionTreeClassifier(),\n       ExtraTreesClassifier(n_estimators=100),AdaBoostClassifier(n_estimators=100),RandomForestClassifier(n_estimators=100)]\n\nmodel_names=['SVC','LR','LDA','DTC','ETC','ABC','RFC']\ntrain_score=[]\nscore_1=[]\ntest_score=[]"]}, {"cell_type": "markdown", "id": "7f0bb258", "metadata": {}, "source": ["Defining function to train models and predict probabilities."]}, {"cell_type": "code", "execution_count": 1, "id": "e3c09a90", "metadata": {}, "outputs": [], "source": ["skf=StratifiedKFold(n_splits=5,random_state=5)\ndef get_model(train_X,train_y,test_X,test_y,model):\n    for train_index,val_index in skf.split(train_X,train_y):\n        train_X_skf,val_X_skf=train_X.iloc[train_index,:],train_X.iloc[val_index,:]\n        train_y_skf,val_y_skf=train_y.iloc[train_index],train_y.iloc[val_index]\n        clf=model\n        clf.fit(train_X_skf,train_y_skf)\n        pred=clf.predict_proba(val_X_skf)[:,1]\n        score=average_precision_score(val_y_skf,pred)\n        score_1.append(score)\n        \n    train_score.append(np.mean(score_1))\n    clf.fit(train_X,train_y)\n    pred_prob=clf.predict_proba(test_X)[:,1]\n    score_test=average_precision_score(test_y,pred_prob)\n    test_score.append(score_test)\n           "]}, {"cell_type": "markdown", "id": "d5fadf04", "metadata": {}, "source": ["To increase computational speed, I sampled only 50000 points from train_X_res and 10000 points from test_X"]}, {"cell_type": "code", "execution_count": 1, "id": "7c259ccc", "metadata": {}, "outputs": [], "source": ["train_X_sam=train_X_res.sample(10000)\ntrain_X_index=train_X_sam.index\ntrain_y_sam=train_y_res[train_X_index]\ntrain_X_sam.reset_index(drop=True,inplace=True)\ntrain_y_sam.reset_index(drop=True,inplace=True)\ntest_X_sam=test_X.sample(1000)\ntest_X_index=test_X_sam.index\ntest_y_sam=test_y[test_X_index]\ntest_X_sam.reset_index(drop=True,inplace=True)\ntest_y_sam.reset_index(drop=True,inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "eb64db5a", "metadata": {}, "outputs": [], "source": ["for model in models:\n    get_model(train_X_sam,train_y_sam,test_X,test_y,model)"]}, {"cell_type": "code", "execution_count": 1, "id": "aa83d101", "metadata": {}, "outputs": [], "source": ["result=pd.DataFrame({'models':model_names,'train_score':train_score,\n                    'test_score':test_score},index=model_names)\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "5f03a95e", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(10,6))\nplt.subplot(1,2,1)\nresult['train_score'].plot.bar()\nplt.title('Train Score')\nplt.subplot(1,2,2)\nresult['test_score'].plot.bar()\nplt.title('Test Score')\nplt.tight_layout()\nplt.show()"]}, {"cell_type": "markdown", "id": "b8e93cd7", "metadata": {}, "source": ["All models are overfitting. But Logistic Regression, ETC, ABC and RFC provide decent test scores."]}, {"cell_type": "markdown", "id": "19dd593f", "metadata": {}, "source": ["Let us try Blending the top 4 performers with XGBClassifier"]}, {"cell_type": "code", "execution_count": 1, "id": "c1372dfc", "metadata": {}, "outputs": [], "source": ["clf=LogisticRegression()\nclf.fit(train_X_sam,train_y_sam)\nlr_pred=clf.predict_proba(test_X)[:,1]\n\nclf_2=AdaBoostClassifier()\nclf_2.fit(train_X_sam,train_y_sam)\nabc_pred=clf_2.predict_proba(test_X)[:,1]\n\nclf_3=ExtraTreesClassifier()\nclf_3.fit(train_X_sam,train_y_sam)\netc_pred=clf_3.predict_proba(test_X)[:,1]\n\nclf_4=RandomForestClassifier()\nclf_4.fit(train_X_sam,train_y_sam)\nrfc_pred=clf_4.predict_proba(test_X)[:,1]\n\nxgb=XGBClassifier()\nxgb.fit(train_X_sam,train_y_sam)\nxgb_pred=xgb.predict_proba(test_X)[:,1]"]}, {"cell_type": "markdown", "id": "a01966ed", "metadata": {}, "source": ["This blending score is better than each of the models alone. "]}, {"cell_type": "code", "execution_count": 1, "id": "7b17a5fa", "metadata": {}, "outputs": [], "source": ["blending_pred=0.20*(lr_pred+etc_pred+rfc_pred+abc_pred+xgb_pred)\nblending_score=average_precision_score(test_y,blending_pred)\nprint(blending_score)\n"]}, {"cell_type": "markdown", "id": "98c96303", "metadata": {}, "source": ["**Things which you can further try** :\n1. Using different scaling of variables\n2. Using different algorithm for upsampling\n3. Using different models\n4. Hyperparamter tuning of mutiple models and then blending\n5. Ensembling"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}