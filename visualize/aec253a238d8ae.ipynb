{"cells": [{"cell_type": "markdown", "id": "74e78cb1", "metadata": {}, "source": ["# Text Classification Models - An Extensive List \n\nI won't go into the details & bore you'll with the information about \"what is text classification?\". Instead I shall go straight to implementing various models for text classification [assuming thats what you're here for :-)]. \n\nI will keep the notebook fairely organised & well commented for easy reading, please do **UPVOTE** if you find it helpful."]}, {"cell_type": "markdown", "id": "8b8bf2c4", "metadata": {}, "source": ["# Setup\n\n### Libraries"]}, {"cell_type": "code", "execution_count": 1, "id": "09c83c41", "metadata": {}, "outputs": [], "source": ["# Generic\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os, warnings, gc, string\nwarnings.filterwarnings(\"ignore\")\n\n# SKLearn\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import decomposition, ensemble\n\n# Tensorflow / Keras\nimport tensorflow as tf\nfrom keras.preprocessing import text,sequence\nfrom keras import layers,models,optimizers\nimport tensorflow_hub as hub\n\n# XGBoost & Textblob\nimport xgboost\n\n#Gensim Library for Text Processing\nimport gensim.parsing.preprocessing as gsp\nfrom gensim import utils"]}, {"cell_type": "markdown", "id": "b36e0921", "metadata": {}, "source": ["### Data Setup"]}, {"cell_type": "code", "execution_count": 1, "id": "9d727e30", "metadata": {}, "outputs": [], "source": ["'''Load'''\n\n#train\nurl = '../input/analytics-vidhya-identify-the-sentiments/train.csv'\ndf = pd.read_csv(url, header='infer')\n\n#Drop Columns\ndf.drop('id', inplace=True, axis=1)\n\n#Inspect\nprint(\"Total Records (training dataset): \", df.shape[0])"]}, {"cell_type": "code", "execution_count": 1, "id": "57757a6f", "metadata": {}, "outputs": [], "source": ["'''Tweet Data Cleaning Utility Function'''\n\nprocesses = [\n               gsp.strip_tags, \n               gsp.strip_punctuation,\n               gsp.strip_multiple_whitespaces,\n               gsp.strip_numeric,\n               gsp.remove_stopwords, \n               gsp.strip_short, \n               gsp.stem_text\n            ]\n\ndef proc_txt(txt):\n    text = txt.lower()\n    text = utils.to_unicode(text)\n    for p in processes:\n        text = p(text)\n    return text\n\n\n# Training Dataset\ndf['tweet_cln'] = df['tweet'].apply(lambda x: proc_txt(x))"]}, {"cell_type": "code", "execution_count": 1, "id": "bafb29a4", "metadata": {}, "outputs": [], "source": ["# Training Dataset\ndf.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "d9fad1f5", "metadata": {}, "outputs": [], "source": [" '''Data Split (training dataset)'''\ntrain_x, valid_x, train_y, valid_y = model_selection.train_test_split(df['tweet_cln'], df['label'])\n\n\n\n'''Feature Engineering of Training Dataset [TF-IDF Vectors] - Basic Classifiers'''\ntfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\ntfidf_vect.fit(df['tweet_cln'])\nxtrain_tfidf =  tfidf_vect.transform(train_x)\nxvalid_tfidf =  tfidf_vect.transform(valid_x)\n\n\n\n'''Feature Engineering of Training Dataset [Word Embedding] - Deep Neural'''\nembeddings_index = {}\n\nfor i, line in enumerate(open('../input/wikinews300d1mvec/wiki-news-300d-1M.vec')):  #Pretrained Word Embedding Vectors\n    values = line.split()\n    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n\n# Tokenizer \ntoken = text.Tokenizer()\ntoken.fit_on_texts(df['tweet_cln'])\nword_index = token.word_index\n\n# Text to Sequence \ntrain_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\nvalid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n\n# Token-embedding Mapping\nembedding_matrix = np.zeros((len(word_index) + 1, 300))\n\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector"]}, {"cell_type": "markdown", "id": "fb5bcc76", "metadata": {}, "source": ["# Build Model"]}, {"cell_type": "code", "execution_count": 1, "id": "e28a893b", "metadata": {}, "outputs": [], "source": ["'''Utility Function'''\n\ndef model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n    # fit the training dataset on the classifier\n    classifier.fit(feature_vector_train, label)\n    \n    # predict the labels on validation dataset\n    predictions = classifier.predict(feature_vector_valid)\n    \n    if is_neural_net:\n        predictions = predictions.argmax(axis=-1)\n   \n    #free memory\n    gc.collect()\n    \n    return metrics.accuracy_score(predictions, valid_y)"]}, {"cell_type": "markdown", "id": "f42950a8", "metadata": {}, "source": ["### Naive Bayes"]}, {"cell_type": "code", "execution_count": 1, "id": "775d6895", "metadata": {}, "outputs": [], "source": ["nb_acc = model(naive_bayes.MultinomialNB(),xtrain_tfidf, train_y, xvalid_tfidf)\nprint(\"Naive Bayes(multinomial) Accuracy Achieved: \", '{:.2%}'.format(nb_acc))"]}, {"cell_type": "markdown", "id": "0df063a2", "metadata": {}, "source": ["### Logistic Reg Classifier"]}, {"cell_type": "code", "execution_count": 1, "id": "3efa84fc", "metadata": {}, "outputs": [], "source": ["ln_acc = model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\nprint(\"Logistic Reg Accuracy Achieved: \", '{:.2%}'.format(ln_acc))"]}, {"cell_type": "markdown", "id": "f689637d", "metadata": {}, "source": ["### Random Forest"]}, {"cell_type": "code", "execution_count": 1, "id": "7a42b34b", "metadata": {}, "outputs": [], "source": ["rf_acc = model(ensemble.RandomForestClassifier(random_state=42), xtrain_tfidf, train_y, xvalid_tfidf)\nprint(\"Random Forest Accuracy Achieved: \", '{:.2%}'.format(rf_acc))"]}, {"cell_type": "markdown", "id": "285ff71c", "metadata": {}, "source": ["### XGBoost"]}, {"cell_type": "code", "execution_count": 1, "id": "7ed53777", "metadata": {}, "outputs": [], "source": ["xgb_acc = model(xgboost.XGBClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\nprint(\"XGBoost Accuracy Achieved: \", '{:.2%}'.format(xgb_acc))"]}, {"cell_type": "markdown", "id": "c24d9a6c", "metadata": {}, "source": ["### CNN (Keras)"]}, {"cell_type": "code", "execution_count": 1, "id": "6df5cd3a", "metadata": {}, "outputs": [], "source": ["'''Create Model'''\n\n# Input Layer\ninput_layer = layers.Input((70, ))\n\n# Word Embedding Layer\nembedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\nembedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n\n# Convolutional Layer\nconv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n\n# Pooling Layer\npooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n\n# Output Layers\noutput_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\noutput_layer1 = layers.Dropout(0.25)(output_layer1)\noutput_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n\n# Compile\ncnn_model = models.Model(inputs=input_layer, outputs = output_layer2)\ncnn_model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n\n\ncnn_acc = model(cnn_model, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\nprint(\"CNN Model Accuracy Achieved: \", '{:.2%}'.format(cnn_acc))"]}, {"cell_type": "markdown", "id": "ebf86444", "metadata": {}, "source": ["### RNN - LSTM"]}, {"cell_type": "code", "execution_count": 1, "id": "1932fd4e", "metadata": {}, "outputs": [], "source": ["'''Create Model'''\n\n# Input Layer\ninput_layer = layers.Input((70, ))\n\n# Word Embedding Layer\nembedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\nembedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n\n# LSTM Layer\nlstm_layer = layers.LSTM(100)(embedding_layer)\n\n# Output Layers\noutput_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\noutput_layer1 = layers.Dropout(0.25)(output_layer1)\noutput_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n\n# Compile\nrnn_model = models.Model(inputs=input_layer, outputs = output_layer2)\nrnn_model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n\n\nrnn_acc = model(rnn_model, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\nprint(\"RNN(LSTM) Model Accuracy Achieved: \", '{:.2%}'.format(rnn_acc))"]}, {"cell_type": "markdown", "id": "8454ab8a", "metadata": {}, "source": ["### RNN - GRU"]}, {"cell_type": "code", "execution_count": 1, "id": "22ea6ec4", "metadata": {}, "outputs": [], "source": ["'''Create Model'''\n\n# Input Layer\ninput_layer = layers.Input((70, ))\n\n# Word Embedding Layer\nembedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\nembedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n\n# GRU Layer\ngru_layer = layers.GRU(100)(embedding_layer)\n\n# Output Layers\noutput_layer1 = layers.Dense(50, activation=\"relu\")(gru_layer)\noutput_layer1 = layers.Dropout(0.25)(output_layer1)\noutput_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n\n# Compile\nrnngru_model = models.Model(inputs=input_layer, outputs = output_layer2)\nrnngru_model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n\n\nrnngru_acc = model(rnngru_model, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\nprint(\"RNN(GRU) Model Accuracy Achieved: \", '{:.2%}'.format(rnngru_acc))"]}, {"cell_type": "markdown", "id": "d0d3a5e5", "metadata": {}, "source": ["### RNN - BiDirectional(GRU)"]}, {"cell_type": "code", "execution_count": 1, "id": "1a704593", "metadata": {}, "outputs": [], "source": ["'''Create Model'''\n\n# Input Layer\ninput_layer = layers.Input((70, ))\n\n# Word Embedding Layer\nembedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\nembedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n\n# BiDirectional Layer\nbi_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n\n# Output Layers\noutput_layer1 = layers.Dense(50, activation=\"relu\")(bi_layer)\noutput_layer1 = layers.Dropout(0.25)(output_layer1)\noutput_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n\n# Compile\nrnnbi_model = models.Model(inputs=input_layer, outputs = output_layer2)\nrnnbi_model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n\n\nrnnbi_acc = model(rnnbi_model, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\nprint(\"RNN(BiDirectional-GRU) Model Accuracy Achieved: \", '{:.2%}'.format(rnnbi_acc))"]}, {"cell_type": "markdown", "id": "abab77ec", "metadata": {}, "source": ["### RNN - BiDirectional(LSTM)"]}, {"cell_type": "code", "execution_count": 1, "id": "a2f83648", "metadata": {}, "outputs": [], "source": ["'''Create Model'''\n\n# Input Layer\ninput_layer = layers.Input((70, ))\n\n# Word Embedding Layer\nembedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\nembedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n\n# BiDirectional Layer\nbi_layer = layers.Bidirectional(layers.LSTM(100))(embedding_layer)\n\n# Output Layers\noutput_layer1 = layers.Dense(50, activation=\"relu\")(bi_layer)\noutput_layer1 = layers.Dropout(0.25)(output_layer1)\noutput_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n\n# Compile\nrnnbil_model = models.Model(inputs=input_layer, outputs = output_layer2)\nrnnbil_model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n\n\nrnnbil_acc = model(rnnbil_model, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\nprint(\"RNN(BiDirectional-LSTM) Model Accuracy Achieved: \", '{:.2%}'.format(rnnbil_acc))"]}, {"cell_type": "markdown", "id": "8a6a48c7", "metadata": {}, "source": ["### RCNN"]}, {"cell_type": "code", "execution_count": 1, "id": "b8f87680", "metadata": {}, "outputs": [], "source": ["'''Create Model'''\n\n# Input Layer\ninput_layer = layers.Input((70, ))\n\n# Word Embedding Layer\nembedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\nembedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n\n# Recurrent Layer\nrnn_layer = layers.Bidirectional(layers.GRU(100,return_sequences=True))(embedding_layer)\n    \n# Convolutional Layer\nconv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(rnn_layer)\n\n# Pooling Layer\npooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n\n\n# Output Layers\noutput_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\noutput_layer1 = layers.Dropout(0.25)(output_layer1)\noutput_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n\n# Compile\nrcnn_model = models.Model(inputs=input_layer, outputs = output_layer2)\nrcnn_model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n\n\nrcnn_acc = model(rcnn_model, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\nprint(\"RCNN Model Accuracy Achieved: \", '{:.2%}'.format(rcnn_acc))"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}