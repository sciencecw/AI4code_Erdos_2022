{"cells": [{"cell_type": "markdown", "id": "88b963de", "metadata": {}, "source": ["Naive Bayes Classification for Phish Show Reviews V1\n\nTODO:\n* Optimize HyperParameter N_MC\n* N_MC Heuristic instead of frequency?\n* Construct ROC chart from probabilities\n* Scrape Summer 2019 Data to compare\n* Ensemble of reviews per show?"]}, {"cell_type": "code", "execution_count": 1, "id": "26e36a0f", "metadata": {}, "outputs": [], "source": ["import nltk\nimport numpy as np\nimport pandas as pd\nimport os \nimport re\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('ggplot')\n\nN_MC = 400\nCUTOFF_PROB = 0.5"]}, {"cell_type": "markdown", "id": "4fb5048c", "metadata": {}, "source": ["Phase 1: Load Data and Process text\n"]}, {"cell_type": "code", "execution_count": 1, "id": "4159b4d6", "metadata": {}, "outputs": [], "source": ["df = pd.read_csv(\"/kaggle/input/phish-reviews/Reviews.csv\")\ndf.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "5c18b97b", "metadata": {}, "outputs": [], "source": ["# Regex to remove punctuation\ndef remove_punctuation(review):\n    symbols=\"[\u2019'`!)(&@#.,/'`~:;|\\?]\"\n    return re.sub('\\r',' ', re.sub('\\n', ' ', re.sub(symbols,' ',review)))\n"]}, {"cell_type": "code", "execution_count": 1, "id": "5c91ddf6", "metadata": {}, "outputs": [], "source": ["# Remove newlines and convert all to lowercase\ndf.Reviews = df.Reviews.apply(remove_punctuation)\ndf.Reviews = df.Reviews.apply(lambda n: n.lower())"]}, {"cell_type": "code", "execution_count": 1, "id": "ea4d6159", "metadata": {}, "outputs": [], "source": ["df.Reviews"]}, {"cell_type": "code", "execution_count": 1, "id": "07a55041", "metadata": {}, "outputs": [], "source": ["# Tokenize Reviews\ndf.Reviews = df.Reviews.apply(nltk.word_tokenize)"]}, {"cell_type": "code", "execution_count": 1, "id": "10cc6b7a", "metadata": {}, "outputs": [], "source": ["# Filter Stop Words\nstops = nltk.corpus.stopwords.words('english')\ndef filter_stop_words(tk_review):\n    return [word for word in tk_review if word not in stops]"]}, {"cell_type": "code", "execution_count": 1, "id": "62df83af", "metadata": {}, "outputs": [], "source": ["df.Reviews=df.Reviews.apply(filter_stop_words)"]}, {"cell_type": "code", "execution_count": 1, "id": "3e3d5532", "metadata": {}, "outputs": [], "source": ["# POS-Tagging\ndf['Reviews2'] = df.Reviews.apply(nltk.pos_tag)"]}, {"cell_type": "code", "execution_count": 1, "id": "9300c6e6", "metadata": {}, "outputs": [], "source": ["# Lemmatization\nLemming = nltk.stem.WordNetLemmatizer()\n\ndef penn2morphy(penntag):\n    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n    morphy_tag = {'NN':'n', 'JJ':'a',\n                  'VB':'v', 'RB':'r'}\n    try:\n        return morphy_tag[penntag[:2]]\n    except:\n        return 'n'\n\ndef lemmatize_words(tag_review):\n    output = []\n    for w in tag_review:\n        try:\n            lm = Lemming.lemmatize(w[0], pos=penn2morphy(w[1]))\n        except Exception as e:\n            lm = w\n        output.append(lm)\n        \n    return output\n"]}, {"cell_type": "code", "execution_count": 1, "id": "4c1ba120", "metadata": {}, "outputs": [], "source": ["df['Reviews2'] = df.Reviews2.apply(lemmatize_words)"]}, {"cell_type": "code", "execution_count": 1, "id": "c6707922", "metadata": {}, "outputs": [], "source": ["df.head()"]}, {"cell_type": "markdown", "id": "cd3aef73", "metadata": {}, "source": ["Now, with a decent baseline amount of text preprocessing done, time to bin the scores.\n"]}, {"cell_type": "code", "execution_count": 1, "id": "5430de49", "metadata": {}, "outputs": [], "source": ["# Bin Scores and view Histogrm\ndef bin_score(row):\n    return 1 if row['Score'] >= 4 else 0\ndf['BinnedScore'] = df.apply(bin_score, axis=1)\ndf.groupby('BinnedScore').size().plot(kind='bar')"]}, {"cell_type": "markdown", "id": "e431b877", "metadata": {}, "source": ["Next, we extract Reviews2 and BinnedScore, and do final preprocessing for model-building"]}, {"cell_type": "code", "execution_count": 1, "id": "eda8d166", "metadata": {}, "outputs": [], "source": ["# Extract N most common words\ndef n_most_common(reviews, n=100):\n    words = []\n    sentences = reviews.values\n    \n    # Iterate through sentences to extract all unique words\n    for s in sentences:\n        for w in s:\n            if w not in words:\n                words.append(w)\n    \n    # Generate a counter for each word\n    counts = {w: 0 for w in words}\n    \n    # Iterate through all words, incrementing count\n    for sent in sentences:\n        for word in sent:\n            counts[word] += 1\n    \n    # Generate list of \n    words = sorted([(w, counts[w]) for w in words], key= lambda x: x[1], reverse=True)\n    return words[0:n]\n    "]}, {"cell_type": "code", "execution_count": 1, "id": "d800770c", "metadata": {}, "outputs": [], "source": ["MC = n_most_common(df.Reviews2,N_MC)"]}, {"cell_type": "code", "execution_count": 1, "id": "fa5625a4", "metadata": {}, "outputs": [], "source": ["# Create Categorical Variables\nMC = [w[0] for w in MC]\ndef create_categorical_column(dframe, word):\n    def hasword(row):\n        if word in row['Reviews2']:\n            return 1\n        else:\n            return 0\n    dframe[word] = dframe.apply(lambda row: hasword(row), axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "193e27ea", "metadata": {}, "outputs": [], "source": ["DF = df.drop(columns=['Unnamed: 0', 'Reviews', 'Score'])\nDF.head()\nDF.to_csv('CleanedDF1.csv')"]}, {"cell_type": "code", "execution_count": 1, "id": "166b7940", "metadata": {}, "outputs": [], "source": ["DF.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "d51ecbc1", "metadata": {}, "outputs": [], "source": ["#One-Hot Encoding\ndef encode_word(row, word):\n    return 1 if word in row['Reviews2'] else 0\nlenmc = len(MC)\nfor i, word in enumerate(MC):\n    DF[str(word)] = DF.apply(lambda row: encode_word(row, word), axis=1)\n    print(f'{i}/{lenmc}')\n    "]}, {"cell_type": "code", "execution_count": 1, "id": "2d204ce9", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n\n\ntrain, test = train_test_split(DF, test_size=0.2)\n\ntrain_y = train.pop('BinnedScore')\ntest_y = test.pop('BinnedScore')"]}, {"cell_type": "markdown", "id": "265ff863", "metadata": {}, "source": ["With our data prepared, we can start building a model!\n\nNOTE: Have to revise data cleaning to get rid of single-charactar tokens it looks like :/"]}, {"cell_type": "code", "execution_count": 1, "id": "a0b9913a", "metadata": {}, "outputs": [], "source": ["train.drop('Reviews2', inplace=True, axis=1)\ntest.drop('Reviews2', inplace=True, axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "5d4567f9", "metadata": {}, "outputs": [], "source": ["from sklearn.naive_bayes import ComplementNB, GaussianNB\nmodel = ComplementNB(alpha=1)\nmodel.fit(train, train_y)"]}, {"cell_type": "code", "execution_count": 1, "id": "c5319a85", "metadata": {}, "outputs": [], "source": ["import random\ndef pred_cat(prob, CUTOFF_PROB=CUTOFF_PROB):\n    if prob > CUTOFF_PROB:\n        return '1'\n    elif prob < CUTOFF_PROB:\n        return '0'\n    else:\n        return random.choice([0, 1])"]}, {"cell_type": "code", "execution_count": 1, "id": "392f5dfb", "metadata": {}, "outputs": [], "source": ["# Make Predictions!\nprobs = [round(p[1], 4) for p in model.predict_proba(test)]\npredictions = [pred_cat(p) for p in probs]\nobserved = [v for v in test_y.values]\nbenchmark_predictions = [1 for i in test_y.values]\n\nresults = pd.DataFrame([predictions, observed, benchmark_predictions, probs]).transpose()\nresults.rename({0:'Predicted', 1:'Observed', 2:'Benchmark_Predictions', 3:'Probabilities'}, axis=1, inplace=True)\n\nresults = results.apply(lambda x: x.apply(str))\nresults"]}, {"cell_type": "code", "execution_count": 1, "id": "8e1a262f", "metadata": {}, "outputs": [], "source": ["def model_right(row):\n    return 1 if row['Predicted'] == row['Observed'] else 0\n\ndef benchmark_right(row):\n    return 1 if row['Benchmark_Predictions'] == row['Observed'] else 0\n"]}, {"cell_type": "code", "execution_count": 1, "id": "267a3fdb", "metadata": {}, "outputs": [], "source": ["results['model_right'] = results.apply(model_right, axis=1)\nresults['benchmark_right'] = results.apply(benchmark_right, axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "699d1de5", "metadata": {}, "outputs": [], "source": ["results"]}, {"cell_type": "code", "execution_count": 1, "id": "962ad4e8", "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import roc_auc_score\nAUCstat = roc_auc_score(results['Observed'].apply(float).values.flatten(), results['Probabilities'].apply(float).values.flatten())"]}, {"cell_type": "code", "execution_count": 1, "id": "bde23295", "metadata": {}, "outputs": [], "source": ["print(f'model error: {1 - results[\"model_right\"].sum() / len(results)}')\nprint(f'AUC: {AUCstat}')\nprint(f'benchmark error: {1 - results[\"benchmark_right\"].sum() / len(results)}')"]}, {"cell_type": "code", "execution_count": 1, "id": "0ebf398d", "metadata": {}, "outputs": [], "source": ["Positives = results[results['Observed'] == '1']\nNegatives = results[results['Observed'] == '0']\n\nsensitivity = len(Positives[Positives['Predicted']=='1']) / len(Positives)\nspecificity = len(Negatives[Negatives['Predicted']=='0']) / len(Negatives)\n\nprint(f'Sensitivty: {round(sensitivity*100, 2)}%')\nprint(f'Specificity: {round(specificity*100, 2)}%')"]}, {"cell_type": "markdown", "id": "db894322", "metadata": {}, "source": ["Unfinished! Need to add error/benchmark type dealios and vastly improve my preprocessing... but the pipeline is taking shape!\n\nCurrent Baselines: \n\n100 words, 40.75% error, 59.45% Sensitivity, 58.96% Specificity\n\n200 words, 38.25% error, 62.46% Sensitivity, 60.77% Specificity\n\n400 words, 37.02% error, 65.82% Sensitivity, 58.94% Specificity"]}, {"cell_type": "markdown", "id": "6adbf8da", "metadata": {}, "source": ["ROC Chart"]}, {"cell_type": "code", "execution_count": 1, "id": "66086a73", "metadata": {}, "outputs": [], "source": ["results"]}, {"cell_type": "code", "execution_count": 1, "id": "71769c8a", "metadata": {}, "outputs": [], "source": ["def ROC(rdf):\n    \n    df1 = pd.concat([rdf['Probabilities'], rdf['Observed']], axis=1)\n\n    ss = []\n    sc = []    \n    \n    def populate_ss(ctf):\n        \n        df1['TempPred'] = df1['Probabilities'].apply(lambda p: pred_cat(float(p), CUTOFF_PROB=float(ctf)))\n        \n        Positives = df1[df1['Observed'] == '1']\n        Negatives = df1[df1['Observed'] == '0']\n        \n        sensi = len(Positives[Positives['TempPred'] == '1']) / len(Positives)\n        speci = len(Negatives[Negatives['TempPred'] == '0']) / len(Negatives)\n        \n        ss.append(sensi)\n        sc.append(1-speci)\n\n    cutoffs = np.arange(0, 1, 0.005)\n    for i in cutoffs:\n        populate_ss(i)\n\n    plt.title(\"ROC Chart\")\n    plt.xlabel('1-Specificity')\n    plt.ylabel('Sensitivity')\n    \n    plt.plot([0, 0, 1], [0, 1, 1], c='g', label='Ideal')\n    plt.plot(sc, ss, c='b', label='Observed')\n    plt.plot([0, 1], [0, 1], c='r', label='Benchmark')\n    \n    plt.legend()"]}, {"cell_type": "code", "execution_count": 1, "id": "e7753a2c", "metadata": {}, "outputs": [], "source": ["ROC(results)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}