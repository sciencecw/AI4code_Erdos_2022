{"cells": [{"cell_type": "markdown", "id": "1b3a1de2", "metadata": {}, "source": ["# Pedestrianspedestrians Region-based Convolutional Neural Networks (R-CNN)"]}, {"cell_type": "code", "execution_count": 1, "id": "2f70774f", "metadata": {}, "outputs": [], "source": ["import sys\nsys.path.append('../input/pytorch-vision-detection/')"]}, {"cell_type": "code", "execution_count": 1, "id": "e03e41b3", "metadata": {}, "outputs": [], "source": ["!pip install -q pycocotools"]}, {"cell_type": "markdown", "id": "2a8b9e4e", "metadata": {}, "source": ["# Create the training Pedestrian Dataset with mask, label and images"]}, {"cell_type": "code", "execution_count": 1, "id": "ea64bc58", "metadata": {}, "outputs": [], "source": ["import os\nimport numpy as np\nimport torch\nfrom PIL import Image, ImageDraw\nfrom torchvision.transforms import ToPILImage\n\nclass PennFudanDataset(object):\n    def __init__(self, root, transforms):\n        self.root = root\n        self.transforms = transforms\n        # load all image files, sorting them to\n        # ensure that they are aligned\n        self.penn_path = '../input/penn-fudan/'\n        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n\n    def __getitem__(self, idx):\n        # load images ad masks\n        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n        img = Image.open(img_path).convert(\"RGB\")\n        # note that we haven't converted the mask to RGB,\n        # because each color corresponds to a different instance\n        # with 0 being background\n        mask = Image.open(mask_path)\n        # convert the PIL Image into a numpy array\n        mask = np.array(mask)\n        # instances are encoded as different colors\n        obj_ids = np.unique(mask)\n        # first id is the background, so remove it\n        obj_ids = obj_ids[1:]\n\n        # split the color-encoded mask into a set\n        # of binary masks\n        masks = mask == obj_ids[:, None, None]\n\n        # get bounding box coordinates for each mask\n        num_objs = len(obj_ids)\n        boxes = []\n        for i in range(num_objs):\n            pos = np.where(masks[i])\n            xmin = np.min(pos[1])\n            xmax = np.max(pos[1])\n            ymin = np.min(pos[0])\n            ymax = np.max(pos[0])\n            boxes.append([xmin, ymin, xmax, ymax])\n\n        # convert everything into a torch.Tensor\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        # there is only one class\n        labels = torch.ones((num_objs,), dtype=torch.int64)\n        masks = torch.as_tensor(masks, dtype=torch.uint8)\n\n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"masks\"] = masks\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.imgs)"]}, {"cell_type": "markdown", "id": "7edb1e67", "metadata": {}, "source": ["# Prepare the model using faster R-CNN weights resnet50"]}, {"cell_type": "code", "execution_count": 1, "id": "bdc89246", "metadata": {}, "outputs": [], "source": ["import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n# load a model pre-trained pre-trained on COCO\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n# replace the classifier with a new one, that has\n# num_classes which is user-defined\nnum_classes = 2  # 1 class (person) + background\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"]}, {"cell_type": "code", "execution_count": 1, "id": "5edcfe3a", "metadata": {}, "outputs": [], "source": ["import torchvision\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\n# load a pre-trained model for classification and return\n# only the features\nbackbone = torchvision.models.mobilenet_v2(pretrained=True).features\n# FasterRCNN needs to know the number of\n# output channels in a backbone. For mobilenet_v2, it's 1280\n# so we need to add it here\nbackbone.out_channels = 1280\n\n# let's make the RPN generate 5 x 3 anchors per spatial\n# location, with 5 different sizes and 3 different aspect\n# ratios. We have a Tuple[Tuple[int]] because each feature\n# map could potentially have different sizes and\n# aspect ratios\nanchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n                                   aspect_ratios=((0.5, 1.0, 2.0),))\n\n# let's define what are the feature maps that we will\n# use to perform the region of interest cropping, as well as\n# the size of the crop after rescaling.\n# if your backbone returns a Tensor, featmap_names is expected to\n# be [0]. More generally, the backbone should return an\n# OrderedDict[Tensor], and in featmap_names you can choose which\n# feature maps to use.\nroi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n                                                output_size=7,\n                                                sampling_ratio=2)\n\n# put the pieces together inside a FasterRCNN model\nmodel = FasterRCNN(backbone,\n                   num_classes=2,\n                   rpn_anchor_generator=anchor_generator,\n                   box_roi_pool=roi_pooler)"]}, {"cell_type": "code", "execution_count": 1, "id": "aabcff47", "metadata": {}, "outputs": [], "source": ["import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n\n\ndef get_model_instance_segmentation(num_classes):\n    # load an instance segmentation model pre-trained pre-trained on COCO\n    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n\n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    # now get the number of input features for the mask classifier\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n    # and replace the mask predictor with a new one\n    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n                                                       hidden_layer,\n                                                       num_classes)\n\n    return model"]}, {"cell_type": "code", "execution_count": 1, "id": "2283f145", "metadata": {}, "outputs": [], "source": ["import transforms as T\n\ndef get_transform(train):\n    transforms = []\n    transforms.append(T.ToTensor())\n    if train:\n        transforms.append(T.RandomHorizontalFlip(0.5))\n    return T.Compose(transforms)"]}, {"cell_type": "markdown", "id": "3a07255d", "metadata": {}, "source": ["# Train the Fastern R-CNN with the new boxes, labels on the images"]}, {"cell_type": "code", "execution_count": 1, "id": "53fa994a", "metadata": {}, "outputs": [], "source": ["from engine import train_one_epoch, evaluate\nimport utils\n\n\n# train on the GPU or on the CPU, if a GPU is not available\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# our dataset has two classes only - background and person\nnum_classes = 2\n\n# root\nroot = '../input/penn-fudan/'\n\n# use our dataset and defined transformations\ndataset = PennFudanDataset(root, get_transform(train=True))\ndataset_test = PennFudanDataset(root, get_transform(train=False))\n\n# split the dataset in train and test set\nindices = torch.randperm(len(dataset)).tolist()\ndataset = torch.utils.data.Subset(dataset, indices[:-50])\ndataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=2, shuffle=True, num_workers=4,\n    collate_fn=utils.collate_fn)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n    collate_fn=utils.collate_fn)\n\n# get the model using our helper function\nmodel = get_model_instance_segmentation(num_classes)\n\n# move model to the right device\nmodel.to(device)\n\n# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005,\n                            momentum=0.9, weight_decay=0.0005)\n# and a learning rate scheduler\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                                step_size=3,\n                                                gamma=0.1)\n\n# let's train it for 10 epochs\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 59 iterations\n    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=59)\n    # update the learning rate\n    lr_scheduler.step()"]}, {"cell_type": "markdown", "id": "d6ff7bcf", "metadata": {}, "source": ["# Evaluate and Analyze Trained Data"]}, {"cell_type": "code", "execution_count": 1, "id": "ed55c750", "metadata": {}, "outputs": [], "source": ["# evaluate on the test dataset\nevaluate(model, data_loader_test, device=device)"]}, {"cell_type": "code", "execution_count": 1, "id": "ffda3255", "metadata": {}, "outputs": [], "source": ["dataset = PennFudanDataset(root, get_transform(train=True))\ndata_loader = torch.utils.data.DataLoader(\n dataset, batch_size=5, shuffle=True, num_workers=4,\n collate_fn=utils.collate_fn)\n# For Training\nimages, _ = next(iter(data_loader))\nimages = list(image.cuda() for image in images)"]}, {"cell_type": "code", "execution_count": 1, "id": "768e6fd7", "metadata": {}, "outputs": [], "source": ["with torch.no_grad():\n    predicted = model(images)"]}, {"cell_type": "code", "execution_count": 1, "id": "1b6460e7", "metadata": {}, "outputs": [], "source": ["to_img = ToPILImage()\nto_img(images[0].cpu())"]}, {"cell_type": "code", "execution_count": 1, "id": "e7dded2f", "metadata": {}, "outputs": [], "source": ["# show example of boxes predicted\npredicted[0]['boxes']"]}, {"cell_type": "code", "execution_count": 1, "id": "43100933", "metadata": {}, "outputs": [], "source": ["# draw the boxes on the image\nim = to_img(images[0].cpu())\nmask = Image.new('L', im.size, color=255)\ndraw=ImageDraw.Draw(mask)\nfor box in predicted[0]['boxes']:\n    draw.rectangle(box.cpu().numpy(), fill=250, outline=\"red\", width=2)\nim.putalpha(mask)"]}, {"cell_type": "code", "execution_count": 1, "id": "1639ac4e", "metadata": {}, "outputs": [], "source": ["im"]}, {"cell_type": "markdown", "id": "752156da", "metadata": {}, "source": ["# Resource\n\nPytorch torchvision tutorial from: [Torchvision Tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}