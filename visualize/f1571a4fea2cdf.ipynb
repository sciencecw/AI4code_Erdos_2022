{"cells": [{"cell_type": "markdown", "id": "4445feb2", "metadata": {}, "source": ["# <span style=\"color:blue\">Import Packages</span>"]}, {"cell_type": "code", "execution_count": 1, "id": "b0a5da16", "metadata": {}, "outputs": [], "source": ["# for numerical analysis\nimport numpy as np # linear algebra\n\n# to store and process in a dataframe\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Visualizations, for ploting graphs\nimport matplotlib.pyplot as plt\n\n# image processing\nimport matplotlib.image as mpimg\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n# advanced ploting\nimport seaborn as sns"]}, {"cell_type": "code", "execution_count": 1, "id": "76829b81", "metadata": {}, "outputs": [], "source": ["import warnings\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\n# Image manipulations\nfrom PIL import Image\n\n# Timing utility\nfrom timeit import default_timer as timer\n\nfrom IPython.core.interactiveshell import InteractiveShell\n\n# Printing out all outputs\nInteractiveShell.ast_node_interactivity = 'all'"]}, {"cell_type": "code", "execution_count": 1, "id": "0d121bac", "metadata": {}, "outputs": [], "source": ["# PyTorch\nimport torchvision\nfrom torchvision import transforms, datasets, models\n\nimport torch\nfrom torch import optim, cuda\nfrom torch.utils.data import DataLoader, sampler\nfrom torch.autograd import Variable\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d\nfrom torch.nn import Module, Softmax, BatchNorm2d, Dropout\n\nimport gc"]}, {"cell_type": "code", "execution_count": 1, "id": "72f0d6c7", "metadata": {}, "outputs": [], "source": ["# file operations\n\nimport shutil\nimport os\nfrom os import walk\n\n# to list files\nimport glob\n\nprint(os.listdir(\"../input\"))"]}, {"cell_type": "code", "execution_count": 1, "id": "e1c7d984", "metadata": {}, "outputs": [], "source": ["# current working directory\nos.getcwd()"]}, {"cell_type": "markdown", "id": "7de5ba82", "metadata": {}, "source": ["* F (Fusion beat) - 801 images\n* N (Normal beat) - 90,589 images\n* Q (Unknown beat) - 8,038 images\n* S (Supraventricular ectopic beat) - 2,779 images\n* V (Ventricular ectopic beat) - 7,236 images"]}, {"cell_type": "markdown", "id": "21c520a5", "metadata": {}, "source": ["# <span style=\"color:blue\">Check Files in the MITBIH_img Folder</span>"]}, {"cell_type": "code", "execution_count": 1, "id": "00ce24c1", "metadata": {}, "outputs": [], "source": ["# no. of files\n\ndef list_files(startpath):\n    \n    for root, dirs, files in os.walk(startpath):\n        \n        level = root.replace(startpath, '').count(os.sep)\n        \n        indent = ' ' * 4 * (level)\n        \n        print('{}{}'.format(indent, os.path.basename(root)), '-', len(os.listdir(root)))\n        \nfolder = '/kaggle/input'\nlist_files(folder)"]}, {"cell_type": "code", "execution_count": 1, "id": "33411638", "metadata": {}, "outputs": [], "source": ["folder = '/kaggle/working'\nlist_files(folder)"]}, {"cell_type": "code", "execution_count": 1, "id": "7935a70e", "metadata": {}, "outputs": [], "source": ["# list of files in the dataset /input/ecg-images/MITBIH_img\n\nos.listdir('../input/ecg-images/MITBIH_img')"]}, {"cell_type": "code", "execution_count": 1, "id": "b387279c", "metadata": {}, "outputs": [], "source": ["# Classes in the data\n\nECG_list = os.listdir('../input/ecg-images/MITBIH_img')\n\nn_classes = len(ECG_list)\n\nprint(f'There are {n_classes} different classes.')"]}, {"cell_type": "code", "execution_count": 1, "id": "0c7309de", "metadata": {}, "outputs": [], "source": ["ECG_list"]}, {"cell_type": "code", "execution_count": 1, "id": "e0024580", "metadata": {}, "outputs": [], "source": ["classes = ('S', 'V', 'Q', 'N', 'F')"]}, {"cell_type": "markdown", "id": "8436d688", "metadata": {}, "source": ["# <span style=\"color:blue\">Count Number of Files in the MITBIH_img Folder</span>"]}, {"cell_type": "code", "execution_count": 1, "id": "67708cf7", "metadata": {}, "outputs": [], "source": ["N_imgs = os.listdir('../input/ecg-images/MITBIH_img/N')\nprint('# of Normal beats: ',len(N_imgs))\n\nF_imgs = os.listdir('../input/ecg-images/MITBIH_img/F')\nprint('# of Fusion beats: ',len(F_imgs))\n\nQ_imgs = os.listdir('../input/ecg-images/MITBIH_img/Q')\nprint('# of Unknown beats: ',len(Q_imgs))\n\nV_imgs = os.listdir('../input/ecg-images/MITBIH_img/V')\nprint('# of Ventricular ectopic beats: ',len(V_imgs))\n\nS_imgs = os.listdir('../input/ecg-images/MITBIH_img/S')\nprint('# of Supraventricular ectopic beats: ',len(S_imgs))"]}, {"cell_type": "code", "execution_count": 1, "id": "5e383fab", "metadata": {}, "outputs": [], "source": ["#print(N_dir)\nprint(N_imgs[0])"]}, {"cell_type": "markdown", "id": "9f76483d", "metadata": {}, "source": ["# <span style=\"color:blue\">EDA</span>"]}, {"cell_type": "code", "execution_count": 1, "id": "f3dd0114", "metadata": {}, "outputs": [], "source": ["def imshow(image):\n    \"\"\"Display image\"\"\"\n    plt.figure(figsize=(6, 6))\n    plt.imshow(image)\n    plt.axis('off')\n    plt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "aca58ac3", "metadata": {}, "outputs": [], "source": ["image = mpimg.imread(os.path.join('../input/ecg-images/MITBIH_img/N', N_imgs[0]))\n\nimshow(image)"]}, {"cell_type": "code", "execution_count": 1, "id": "395709e1", "metadata": {}, "outputs": [], "source": ["print(image.shape)\nprint(type(image))"]}, {"cell_type": "markdown", "id": "b726047f", "metadata": {}, "source": ["# <span style=\"color:blue\">Show Images From Each Folder</span>"]}, {"cell_type": "code", "execution_count": 1, "id": "f935eb07", "metadata": {}, "outputs": [], "source": ["# Define a function which will plot several images\n\ndef image_shows(folder, number_of_images):\n    \n    n=number_of_images;\n    \n    folder_list = os.listdir(folder)\n    \n    fig, axes = plt.subplots(nrows = 1, ncols=n, figsize=(20, 10))\n    \n    for i in range(n):\n        \n        print(os.path.join(folder, folder_list[i]))\n        \n        image = mpimg.imread(os.path.join(folder, folder_list[i]));\n        \n        axes[i].imshow(image);"]}, {"cell_type": "code", "execution_count": 1, "id": "d802d4ad", "metadata": {}, "outputs": [], "source": ["# Examples of N\n\nimage_shows(folder = '../input/ecg-images/MITBIH_img/N', number_of_images = 6)"]}, {"cell_type": "code", "execution_count": 1, "id": "11cf80b6", "metadata": {}, "outputs": [], "source": ["# Examples of S\n\nimage_shows(folder = '../input/ecg-images/MITBIH_img/S', number_of_images = 6)"]}, {"cell_type": "code", "execution_count": 1, "id": "7ae360ff", "metadata": {}, "outputs": [], "source": ["# Examples of Q\n\nimage_shows(folder = '../input/ecg-images/MITBIH_img/Q', number_of_images = 6)"]}, {"cell_type": "code", "execution_count": 1, "id": "044a7494", "metadata": {}, "outputs": [], "source": ["# Examples of V\n\nimage_shows(folder = '../input/ecg-images/MITBIH_img/V', number_of_images = 6)"]}, {"cell_type": "code", "execution_count": 1, "id": "c13188ff", "metadata": {}, "outputs": [], "source": ["# Examples of F\n\nimage_shows(folder = '../input/ecg-images/MITBIH_img/F', number_of_images = 6)"]}, {"cell_type": "markdown", "id": "ce11c216", "metadata": {}, "source": ["## To see 3 channels I plot figures with cv 2 package"]}, {"cell_type": "code", "execution_count": 1, "id": "edd12d1f", "metadata": {}, "outputs": [], "source": ["import cv2"]}, {"cell_type": "code", "execution_count": 1, "id": "578def4c", "metadata": {}, "outputs": [], "source": ["imgcv = cv2.imread(os.path.join('../input/ecg-images/MITBIH_img/N', N_imgs[0]))\nplt.imshow(imgcv)\nplt.show();"]}, {"cell_type": "code", "execution_count": 1, "id": "5234069d", "metadata": {}, "outputs": [], "source": ["imgcv.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "c38bbf87", "metadata": {}, "outputs": [], "source": ["b = imgcv.copy()\n# set green and red channels to 0\nb[:, :, 1] = 0\nb[:, :, 2] = 0\n\n\ng = imgcv.copy()\n# set blue and red channels to 0\ng[:, :, 0] = 0\ng[:, :, 2] = 0\n\nr = imgcv.copy()\n# set blue and green channels to 0\nr[:, :, 0] = 0\nr[:, :, 1] = 0"]}, {"cell_type": "markdown", "id": "5bf37227", "metadata": {}, "source": ["# <span style=\"color:blue\">Plot RGB</span>"]}, {"cell_type": "code", "execution_count": 1, "id": "a59a81a3", "metadata": {}, "outputs": [], "source": ["# plot data\n\nfig = plt.figure(figsize=(15,15))\n\nplot_1 = plt.subplot(131)\nplot_1.imshow(r);\n\n#plt.subplot(131).imshow(b);\n\nplot_2 = plt.subplot(132, sharex=plot_1, sharey=plot_1)\nplt.setp(plot_2.get_yticklabels(), visible=False);\nplot_2.imshow(b);\n\nplot_3 = plt.subplot(133, sharex=plot_1, sharey=plot_1)\nplt.setp(plot_3.get_yticklabels(), visible=False);\nplot_3.imshow(g);\n\nplt.show();"]}, {"cell_type": "markdown", "id": "9a53f25f", "metadata": {}, "source": ["# <span style=\"color:blue\">Split Data in to Test and Train Folders</span>"]}, {"cell_type": "code", "execution_count": 1, "id": "8d7c2a61", "metadata": {}, "outputs": [], "source": ["for root, dirs, files in os.walk('/kaggle/'):\n    print(root)"]}, {"cell_type": "markdown", "id": "a838a180", "metadata": {}, "source": ["## <span style=\"color:blue\">Create Test and Train Folders</span>"]}, {"cell_type": "code", "execution_count": 1, "id": "9c37e9db", "metadata": {}, "outputs": [], "source": ["os.makedirs('../working/train/N', exist_ok = True)\nos.makedirs('../working/train/F', exist_ok = True)\nos.makedirs('../working/train/Q', exist_ok = True)\nos.makedirs('../working/train/V', exist_ok = True)\nos.makedirs('../working/train/S', exist_ok = True)\n\n\nos.makedirs('../working/test/N', exist_ok = True)\nos.makedirs('../working/test/F', exist_ok = True)\nos.makedirs('../working/test/Q', exist_ok = True)\nos.makedirs('../working/test/V', exist_ok = True)\nos.makedirs('../working/test/S', exist_ok = True)"]}, {"cell_type": "code", "execution_count": 1, "id": "0cb596ab", "metadata": {}, "outputs": [], "source": ["for root, dirs, files in os.walk('/kaggle/'):\n    print(root)"]}, {"cell_type": "markdown", "id": "4cd84590", "metadata": {}, "source": ["## I want to make %20 test set. For this reason I will copy first %20 percent of each class to test folders"]}, {"cell_type": "markdown", "id": "fdc8d492", "metadata": {}, "source": ["## <span style=\"color:blue\">Copy Files</span>"]}, {"cell_type": "markdown", "id": "e623ccf6", "metadata": {}, "source": ["## <span style=\"color:red\">Copy F files</span>"]}, {"cell_type": "code", "execution_count": 1, "id": "85aab632", "metadata": {}, "outputs": [], "source": ["for file_name in F_imgs[0:round(len(F_imgs)/5)]:\n    \n    full_file_name = os.path.join('../input/ecg-images/MITBIH_img/F', file_name);\n    \n    if os.path.isfile(full_file_name):\n        \n        a = shutil.copy(full_file_name, '../working/test/F');\n        \n        \nfor file_name in F_imgs[round(len(F_imgs)/5):]:\n    \n    full_file_name = os.path.join('../input/ecg-images/MITBIH_img/F', file_name);\n    \n    if os.path.isfile(full_file_name):\n        \n        a = shutil.copy(full_file_name, '../working/train/F');\n\nprint(\"done\")"]}, {"cell_type": "markdown", "id": "ca21633b", "metadata": {}, "source": ["## <span style=\"color:red\">Copy N files</span>"]}, {"cell_type": "code", "execution_count": 1, "id": "1382799e", "metadata": {}, "outputs": [], "source": ["# N_imgs = os.listdir('../input/ecg-images/MITBIH_img/N')\n\nfor file_name in N_imgs[0:round(len(N_imgs)/5)]:\n    \n    full_file_name = os.path.join('../input/ecg-images/MITBIH_img/N', file_name);\n    \n    if os.path.isfile(full_file_name):\n        \n        a = shutil.copy(full_file_name, '../working/test/N');\n        \nfor file_name in N_imgs[round(len(N_imgs)/5):]:\n    \n    full_file_name = os.path.join('../input/ecg-images/MITBIH_img/N', file_name);\n    \n    if os.path.isfile(full_file_name):\n        \n        a = shutil.copy(full_file_name, '../working/train/N');\n\nprint(\"done\") "]}, {"cell_type": "markdown", "id": "0548dbd0", "metadata": {}, "source": ["## <span style=\"color:red\">Copy V files</span>"]}, {"cell_type": "code", "execution_count": 1, "id": "7a480d81", "metadata": {}, "outputs": [], "source": ["for file_name in V_imgs[0:round(len(V_imgs)/5)]:\n    \n    full_file_name = os.path.join('../input/ecg-images/MITBIH_img/V', file_name);\n    \n    if os.path.isfile(full_file_name):\n        \n        a = shutil.copy(full_file_name, '../working/test/V');\n        \n        \nfor file_name in V_imgs[round(len(V_imgs)/5):]:\n    \n    full_file_name = os.path.join('../input/ecg-images/MITBIH_img/V', file_name);\n    \n    if os.path.isfile(full_file_name):\n        \n        a = shutil.copy(full_file_name, '../working/train/V');\n\nprint(\"done\")"]}, {"cell_type": "markdown", "id": "fd91b87d", "metadata": {}, "source": ["## <span style=\"color:red\">Copy Q files</span>"]}, {"cell_type": "code", "execution_count": 1, "id": "81f9cc3b", "metadata": {}, "outputs": [], "source": ["for file_name in Q_imgs[0:round(len(Q_imgs)/5)]:\n    \n    full_file_name = os.path.join('../input/ecg-images/MITBIH_img/Q', file_name);\n    \n    if os.path.isfile(full_file_name):\n        \n        a = shutil.copy(full_file_name, '../working/test/Q');\n        \n        \nfor file_name in Q_imgs[round(len(Q_imgs)/5):]:\n    \n    full_file_name = os.path.join('../input/ecg-images/MITBIH_img/Q', file_name);\n    \n    if os.path.isfile(full_file_name):\n        \n        a = shutil.copy(full_file_name, '../working/train/Q');\n\nprint(\"done\")"]}, {"cell_type": "markdown", "id": "50f6f0f2", "metadata": {}, "source": ["## <span style=\"color:red\">Copy S files</span>"]}, {"cell_type": "code", "execution_count": 1, "id": "ff298678", "metadata": {}, "outputs": [], "source": ["S_imgs = os.listdir('../input/ecg-images/MITBIH_img/S')\n\nfor file_name in S_imgs[0:round(len(S_imgs)/5)]:\n    \n    full_file_name = os.path.join('../input/ecg-images/MITBIH_img/S', file_name);\n    \n    if os.path.isfile(full_file_name):\n        \n        a = shutil.copy(full_file_name, '../working/test/S');\n        \n        \nfor file_name in S_imgs[round(len(S_imgs)/5):]:\n    \n    full_file_name = os.path.join('../input/ecg-images/MITBIH_img/S', file_name);\n    \n    if os.path.isfile(full_file_name):\n        \n        a = shutil.copy(full_file_name, '../working/train/S');\n\nprint(\"done\")"]}, {"cell_type": "code", "execution_count": 1, "id": "44b6c311", "metadata": {}, "outputs": [], "source": ["folder = '/kaggle/working'\nlist_files(folder)"]}, {"cell_type": "markdown", "id": "3f52a875", "metadata": {}, "source": ["## <span style=\"color:blue\">Check Total File Numbers in each Train and Test Folders </span>"]}, {"cell_type": "code", "execution_count": 1, "id": "6748a634", "metadata": {}, "outputs": [], "source": ["N_imgs       = len(os.listdir('../input/ecg-images/MITBIH_img/N'))\nN_train_imgs = len(os.listdir('../working/train/N'))\nN_test_imgs  = len(os.listdir('../working/test/N'))\nprint('number of N images at the original file: ', N_imgs)\nprint('total number of train and test picts:    ', N_train_imgs + N_test_imgs)"]}, {"cell_type": "code", "execution_count": 1, "id": "52937866", "metadata": {}, "outputs": [], "source": ["S_imgs       = len(os.listdir('../input/ecg-images/MITBIH_img/S'))\nS_train_imgs = len(os.listdir('../working/train/S'))\nS_test_imgs  = len(os.listdir('../working/test/S'))\nprint('number of S images at the original file: ', S_imgs)\nprint('total number of train and test picts:    ', S_train_imgs + S_test_imgs)"]}, {"cell_type": "code", "execution_count": 1, "id": "0fc062bc", "metadata": {}, "outputs": [], "source": ["F_imgs       = len(os.listdir('../input/ecg-images/MITBIH_img/F'))\nF_train_imgs = len(os.listdir('../working/train/F'))\nF_test_imgs  = len(os.listdir('../working/test/F'))\nprint('number of F images at the original file: ', F_imgs)\nprint('total number of train and test picts:    ', F_train_imgs + F_test_imgs)"]}, {"cell_type": "code", "execution_count": 1, "id": "98cdfd91", "metadata": {}, "outputs": [], "source": ["V_imgs       = len(os.listdir('../input/ecg-images/MITBIH_img/V'))\nV_train_imgs = len(os.listdir('../working/train/V'))\nV_test_imgs  = len(os.listdir('../working/test/V'))\nprint('number of V images at the original file: ', V_imgs)\nprint('total number of train and test picts:    ', V_train_imgs + V_test_imgs)"]}, {"cell_type": "code", "execution_count": 1, "id": "2fea4429", "metadata": {}, "outputs": [], "source": ["Q_imgs       = len(os.listdir('../input/ecg-images/MITBIH_img/Q'))\nQ_train_imgs = len(os.listdir('../working/train/Q'))\nQ_test_imgs  = len(os.listdir('../working/test/Q'))\nprint('number of Q images at the original file: ', Q_imgs)\nprint('total number of train and test picts:    ', Q_train_imgs + Q_test_imgs)"]}, {"cell_type": "markdown", "id": "62a38df2", "metadata": {}, "source": ["Lets load images with datasets.ImageFolder function and than read and plot"]}, {"cell_type": "markdown", "id": "ff51bcfe", "metadata": {}, "source": ["## <span style=\"color:blue\">Set Data Loader</span>"]}, {"cell_type": "code", "execution_count": 1, "id": "3df900d7", "metadata": {}, "outputs": [], "source": ["# Define default PATH\n\nTRAIN_PATH        = '../working/train'\n\ntransform         = transforms.Compose(\n                                       [transforms.Resize([120,120]),\n                                        transforms.Grayscale(), \n                                        transforms.ToTensor(),\n                                        transforms.Normalize((0.5), (0.5))\n                                       ])\n  \ntrain_data_set    = datasets.ImageFolder(root=TRAIN_PATH, transform=transform)\n\nbatch_size=32\n\ntrain_data_loader = DataLoader(train_data_set, batch_size=batch_size, shuffle=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "f5ded730", "metadata": {}, "outputs": [], "source": ["TEST_PATH        = '../working/test'\n  \ntest_data_set    = datasets.ImageFolder(root=TEST_PATH, transform=transform)\n\ntest_data_loader = DataLoader(test_data_set, batch_size=batch_size, shuffle=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "38fe73ef", "metadata": {}, "outputs": [], "source": ["# Run this to test your data loader\n\nimages, labels = next(iter(train_data_loader))"]}, {"cell_type": "code", "execution_count": 1, "id": "2d2a96b2", "metadata": {}, "outputs": [], "source": ["print(type(images))\n\nprint(images.size())\n\nprint(\"\")\nprint(\"Batch Size:   \",images.size()[0])\nprint(\"Channel Size: \",images.size()[1])\nprint(\"Image Height: \",images.size()[2])\nprint(\"Image Width:  \",images.size()[3])"]}, {"cell_type": "markdown", "id": "b9a4afcf", "metadata": {}, "source": ["The batch size is 32, image size is reduced to 120*120 and we need only one channel(Gray Scale)."]}, {"cell_type": "code", "execution_count": 1, "id": "b57bb07f", "metadata": {}, "outputs": [], "source": ["def imshow_tensor(image, ax=None, title=None, normalize=True):\n    \n    \"\"\"Imshow for Tensor.\"\"\"\n    \n    if ax is None:\n        fig, ax = plt.subplots()\n        \n    image = image.numpy().transpose((1, 2, 0))\n\n    if normalize:\n        mean = np.array([0.5])\n        std = np.array([0.5])\n        image = std * image + mean\n        image = np.clip(image, 0, 1)\n\n    ax.imshow(image)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    \n    ax.tick_params(axis='both', length=0)\n    ax.set_xticklabels('')\n    ax.set_yticklabels('')\n\n    return ax"]}, {"cell_type": "markdown", "id": "b4a6fa0f", "metadata": {}, "source": ["## <span style=\"color:blue\">Plot one Batch of Files from Dataloader</span>"]}, {"cell_type": "code", "execution_count": 1, "id": "4539c063", "metadata": {}, "outputs": [], "source": ["# show images\n\nncol = 8;\n\nimshow_tensor(torchvision.utils.make_grid(images,nrow = ncol));"]}, {"cell_type": "code", "execution_count": 1, "id": "e873f8f5", "metadata": {}, "outputs": [], "source": ["# print labels\n\nclasses = ('N', 'Q', 'F', 'S', 'V')\n\nnrow = batch_size/ncol;\n\nfor row in range(int(nrow)):\n    \n    print(' '.join('%5s' % classes[labels[(row*ncol)+j]] for j in range(ncol))) "]}, {"cell_type": "markdown", "id": "23d59733", "metadata": {}, "source": ["# <span style=\"color:blue\">Building Convolutional Neural Networks</span> "]}, {"cell_type": "markdown", "id": "dd82ef5e", "metadata": {}, "source": ["## <span style=\"color:red\">First CNN Model</span>"]}, {"cell_type": "code", "execution_count": 1, "id": "99b11e9b", "metadata": {}, "outputs": [], "source": ["# CNN Architect\n\nclass ConvNet_1(nn.Module):\n    \n    def __init__(self):\n        \n        super(ConvNet_1, self).__init__()\n\n        self.layer_1  = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, stride=1, padding=1)\n        \n        self.relu1    = nn.ReLU(inplace=True)\n        \n        self.maxpool1 = MaxPool2d(kernel_size=2)\n        \n\n        self.layer_2  = nn.Conv2d(in_channels=4, out_channels=4, kernel_size=3, stride=1, padding=1)\n        \n        self.relu2    = nn.ReLU(inplace=True)\n        \n        self.maxpool2 = MaxPool2d(kernel_size=2)\n        \n        self.drop_out = nn.Dropout()\n        \n        \n        # out_channels = 4, number of classes = 5\n        \n        # image width = 120, image height = 120 after two maxpooling 120 -> 60 -> 30\n        \n        self.fc1 = nn.Linear(4 * 30 * 30, 5)\n        \n    # Defining the forward pass\n\n    def forward(self, x):\n        \n        out = self.layer_1(x)\n        \n        out = self.relu1(out)\n        \n        out = self.maxpool1(out)\n        \n        \n        out = self.layer_2(out)\n        \n        out = self.relu2(out)\n        \n        out = self.maxpool2(out)\n        \n        \n        out = out.reshape(out.size(0), -1)\n        \n        out = self.drop_out(out)\n        \n        out = self.fc1(out)\n        \n        return out\n    \n# Define Model\n\nmodel_1 = ConvNet_1()\n\nprint(model_1)"]}, {"cell_type": "code", "execution_count": 1, "id": "42380f59", "metadata": {}, "outputs": [], "source": ["# Define Criterion\n\ncriterion = nn.CrossEntropyLoss()\n\n# Define Optimizer\n\noptimizer = optim.SGD(model_1.parameters(), lr=0.001, momentum=0.9)"]}, {"cell_type": "markdown", "id": "e3af328d", "metadata": {}, "source": ["## <span style=\"color:red\">Chech and run GPU</span>"]}, {"cell_type": "code", "execution_count": 1, "id": "19fb444d", "metadata": {}, "outputs": [], "source": ["# Whether to train on a gpu and Number of gpus\n\nif cuda.is_available(): \n    \n    print(f'{cuda.device_count()} number of gpus are detected and available.')\n    \nelse:\n        \n    print(f'Train on gpu is not available')\n        \n        "]}, {"cell_type": "markdown", "id": "0edd5224", "metadata": {}, "source": ["## <span style=\"color:red\">Train Our First Model</span>"]}, {"cell_type": "code", "execution_count": 1, "id": "fac91c03", "metadata": {}, "outputs": [], "source": ["# This part is working\n\n# Train the model\n\nif torch.cuda.is_available():\n    \n    MODEL = model_1.cuda()\n    CRITERION = criterion.cuda()\n    print(f'Model is started training on {cuda.device_count()} number of gpus.')\n    print(\"Devise is cuda\")\n    \nelse:\n    \n    MODEL = model_1\n    CRITERION = criterion\n    print(\"Devise is cpu and model is started training.\")\n\ntotal_step = len(train_data_loader)\nloss_list = []\nacc_list = []\n\nnum_epochs = 1\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nall_con_mat = torch.zeros([num_epochs, 5, 5], dtype=torch.int32, device=device)\n\nfor epoch in range(num_epochs):\n    \n    # define empty tensor 5*5 beginning of every epoch\n    # tensor [row,column]\n    con_mat = torch.zeros([5, 5], dtype=torch.int32, device=device)\n    \n    for i, data in enumerate(train_data_loader):\n        \n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        # optimization\n        optimizer.zero_grad()\n        \n        # Forward to get output\n        outputs = MODEL(inputs)\n        # Calculate Loss\n        loss = CRITERION(outputs, labels)\n        # Backward propagation\n        loss.backward()\n        # Updating parameters\n        optimizer.step()\n        \n        # Store loss\n        loss_list.append(loss.item())\n    \n        # Calculate labels size\n        total = labels.size(0)\n        \n        # Outputs.data has dimension batch size * 5\n        # torch.max returns the max value of elements(_) and their indices(predicted) in the tensor array\n        _, predicted = torch.max(outputs.data, 1)\n        \n        # Calculate total number of correct labels \n        correct = (predicted == labels).sum().item()\n        \n        # Store accuracy\n        acc_list.append(correct / total)\n        \n        \n        # Build Confusion Matrix\n        for element in range(total):\n            \n            # con_mat[row,column]\n            # con_mat[predictions, actual]\n            \n            con_mat[predicted[element].item()-1][labels[element].item()-1] += 1\n\n        if (i + 1) % 300 == 0:                             # every 300 mini-batches...\n            \n            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n                  \n                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n                          (correct / total) * 100))\n    print(con_mat)\n            \n    all_con_mat[epoch] = con_mat            \n            \nprint('Finished Training')\n\nplt.plot(loss_list);\nplt.show();\n\nplt.plot(acc_list);\nplt.show();"]}, {"cell_type": "code", "execution_count": 1, "id": "f7b52710", "metadata": {}, "outputs": [], "source": ["# SUM FOR EACH COLUMN\n\n# N Q S V F\n\nprint(torch.sum(con_mat, dim=0))"]}, {"cell_type": "code", "execution_count": 1, "id": "c5a56891", "metadata": {}, "outputs": [], "source": ["folder = '/kaggle/working'\nlist_files(folder)"]}, {"cell_type": "markdown", "id": "8d12782d", "metadata": {}, "source": ["## <span style=\"color:red\">Recall, Precision, and F1 Score for each Class</span>"]}, {"cell_type": "code", "execution_count": 1, "id": "95162951", "metadata": {}, "outputs": [], "source": ["# RECALL\n\n# PRECISION\n\n# F1-score = 2 \u00d7 (precision \u00d7 recall)/(precision + recall)\n\nclass_list = ['N', 'Q', 'S', 'V', 'F']\n\nf1_score_list=[0,0,0,0,0]\n\nprecision_list=[0,0,0,0,0]\n\nrecall_list=[0,0,0,0,0]\n\ndelta = 0.0000000000001\n\nfor i in range(torch.sum(con_mat, dim=0).size(0)): \n    \n    recall_list[i] = con_mat[i][i].item()/(torch.sum(con_mat, dim=0)[i].item()+delta)\n    \n    precision_list[i] = con_mat[i][i].item()/(torch.sum(con_mat, dim=1)[i].item()+delta)\n    \n    f1_score_list[i] = 2 * precision_list[i]*recall_list[i]/(precision_list[i]+recall_list[i]+delta)\n    \n    print('class: {:<2},total number of class: {:>5}, Correctly predicted: {:>5}, Recall: {:.2f}%, Precision: {:.2f}%, F1-Score: {:.2f}%'\n          \n                  .format(class_list[i],\n                          torch.sum(con_mat, dim=0)[i].item(),\n                          con_mat[i][i].item(), \n                          recall_list[i],\n                          precision_list[i],\n                          f1_score_list[i]\n                         ))"]}, {"cell_type": "markdown", "id": "31748a52", "metadata": {}, "source": ["## <span style=\"color:red\">Train Our First Model epoch=5</span>"]}, {"cell_type": "code", "execution_count": 1, "id": "b1af9ba2", "metadata": {}, "outputs": [], "source": ["# This part is working\n\nif torch.cuda.is_available():\n    \n    MODEL = model_1.cuda()\n    CRITERION = criterion.cuda()\n    print(\"cuda\")\n    \nelse:\n    \n    MODEL = model_1\n    CRITERION = criterion\n    print(\"cpu\")\n\n# Train the model\n\ntotal_step = len(train_data_loader)\nloss_list = []\nacc_list = []\n\nnum_epochs = 5\n\nclass_list = ['N', 'Q', 'S', 'V', 'F']\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nall_con_mat = torch.zeros([num_epochs, 5, 5], dtype=torch.int32, device=device)\n\nfor epoch in range(num_epochs):\n    \n    f1_score_list=[0,0,0,0,0]\n\n    precision_list=[0,0,0,0,0]\n\n    recall_list=[0,0,0,0,0]\n    \n    delta = 0.0000000000001 \n    \n    # define empty tensor 5*5 beginning of every epoch\n    # tensor [row,column]\n    con_mat = torch.zeros([5, 5], dtype=torch.int32, device=device)\n    \n    for i, data in enumerate(train_data_loader):\n        \n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        # optimization\n        optimizer.zero_grad()\n        \n        # Forward to get output\n        outputs = MODEL(inputs)\n        # Calculate Loss\n        loss = CRITERION(outputs, labels)\n        # Backward propagation\n        loss.backward()\n        # Updating parameters\n        optimizer.step()\n        \n        # Store loss\n        loss_list.append(loss.item())\n    \n        # Calculate labels size\n        total = labels.size(0)\n        \n        # Outputs.data has dimension batch size * 5\n        # torch.max returns the max value of elements(_) and their indices(predicted) in the tensor array\n        _, predicted = torch.max(outputs.data, 1)\n        \n        # Calculate total number of correct labels \n        correct = (predicted == labels).sum().item()\n        \n        # Store accuracy\n        acc_list.append(correct / total)\n        \n        for element in range(total):\n            \n            # con_mat[row,column]\n            # con_mat[predictions, actual]\n            con_mat[predicted[element].item()-1][labels[element].item()-1] += 1\n\n        if (i + 1) % 300 == 0:                             # every 300 mini-batches...\n            \n            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n                  \n                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n                          (correct / total) * 100))\n    print(con_mat)\n            \n    all_con_mat[epoch] = con_mat\n    \n    # Print Confusion Matrix\n    \n    for i in range(torch.sum(con_mat, dim=0).size(0)): \n    \n        recall_list[i] = con_mat[i][i].item()/(torch.sum(con_mat, dim=0)[i].item()+delta)\n    \n        precision_list[i] = con_mat[i][i].item()/(torch.sum(con_mat, dim=1)[i].item()+delta)\n    \n        f1_score_list[i] = 2 * precision_list[i]*recall_list[i]/(precision_list[i]+recall_list[i]+delta)\n        \n    \n        print('class name: {}, total number of class: {:>5}, Correctly predicted: {:>5}, Recall: {:.2f}%, Precision: {:.2f}%, F1-Score: {:.2f}%'\n          \n                  .format(class_list[i],\n                          torch.sum(con_mat, dim=0)[i].item(),\n                          con_mat[i][i].item(), \n                          recall_list[i],\n                          precision_list[i],\n                          f1_score_list[i]\n                         ))\n    \n            \nprint('Finished Training')\n\nplt.plot(loss_list);\nplt.show();\n\nplt.plot(acc_list);\nplt.show();"]}, {"cell_type": "code", "execution_count": 1, "id": "63b88ffe", "metadata": {}, "outputs": [], "source": ["all_con_mat"]}, {"cell_type": "markdown", "id": "e072e941", "metadata": {}, "source": ["## <span style=\"color:red\">Check Trained 1. Model on the Test Data</span>"]}, {"cell_type": "code", "execution_count": 1, "id": "2273b434", "metadata": {}, "outputs": [], "source": ["confusion_mat = torch.zeros([5, 5], dtype=torch.int32, device=device)\n\nwith torch.no_grad():\n    \n    for data in test_data_loader:\n        \n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        outputs = MODEL(inputs)\n        \n        _, predicted = torch.max(outputs.data, 1)\n        \n        total = labels.size(0)\n        \n        # Calculate total number of correct labels \n        correct = (predicted == labels).sum().item()\n        \n        for element in range(total):\n            \n            # confusion_mat[row,column]\n            # confusion_mat[predictions, actual]\n            confusion_mat[predicted[element].item()-1][labels[element].item()-1] += 1\n\n    print(confusion_mat)"]}, {"cell_type": "code", "execution_count": 1, "id": "8f778f2c", "metadata": {}, "outputs": [], "source": ["class_list = ['N', 'Q', 'S', 'V', 'F']\n\nf1_score_list=[0,0,0,0,0]\n\nprecision_list=[0,0,0,0,0]\n\nrecall_list=[0,0,0,0,0]\n    \ndelta = 0.0000000000001 \n\n\nfor i in range(torch.sum(confusion_mat, dim=0).size(0)): \n    \n        recall_list[i] = confusion_mat[i][i].item()/(torch.sum(confusion_mat, dim=0)[i].item()+delta)\n    \n        precision_list[i] = confusion_mat[i][i].item()/(torch.sum(confusion_mat, dim=1)[i].item()+delta)\n    \n        f1_score_list[i] = 2 * precision_list[i]*recall_list[i]/(precision_list[i]+recall_list[i]+delta)\n        \n    \n        print('class name: {}, total number of class: {:>5}, Correctly predicted: {:>5}, Recall: {:.2f}%, Precision: {:.2f}%, F1-Score: {:.2f}%'\n          \n                  .format(class_list[i],\n                          torch.sum(confusion_mat, dim=0)[i].item(),\n                          confusion_mat[i][i].item(), \n                          recall_list[i],\n                          precision_list[i],\n                          f1_score_list[i]\n                         ))\n    "]}, {"cell_type": "markdown", "id": "1e37cd80", "metadata": {}, "source": ["## <span style=\"color:red\">Built and Train Second CNN Model</span>"]}, {"cell_type": "code", "execution_count": 1, "id": "2d3bb882", "metadata": {}, "outputs": [], "source": ["# CNN Architect\n\nclass ConvNet_2(nn.Module):\n    \n    def __init__(self):\n        \n        super(ConvNet_2, self).__init__()\n\n        self.layer_1  = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, stride=1, padding=1)\n        \n        self.relu1    = nn.ReLU(inplace=True)\n        \n        self.maxpool1 = MaxPool2d(kernel_size=2)\n        \n\n        self.layer_2  = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=1, padding=1)\n        \n        self.relu2    = nn.ReLU(inplace=True)\n        \n        self.maxpool2 = MaxPool2d(kernel_size=2)\n        \n        self.drop_out = nn.Dropout()\n        \n        \n        # out_channels = 8, number of classes = 5\n        \n        # image width = 120, image height = 120 after two maxpooling 120 -> 60 -> 30\n        \n        self.fc1 = nn.Linear(8 * 30 * 30, 5)\n        \n    # Defining the forward pass\n\n    def forward(self, x):\n        \n        out = self.layer_1(x)\n        \n        out = self.relu1(out)\n        \n        out = self.maxpool1(out)\n        \n        \n        out = self.layer_2(out)\n        \n        out = self.relu2(out)\n        \n        out = self.maxpool2(out)\n        \n        \n        out = out.reshape(out.size(0), -1)\n        \n        out = self.drop_out(out)\n        \n        out = self.fc1(out)\n        \n        return out\n    \n# Define Model\n\nmodel_2 = ConvNet_2()\n\nprint(model_2)"]}, {"cell_type": "code", "execution_count": 1, "id": "adb3fa8d", "metadata": {}, "outputs": [], "source": ["if torch.cuda.is_available():\n    \n    MODEL = model_2.cuda()\n    CRITERION = criterion.cuda()\n    print(\"cuda\")\n    \nelse:\n    \n    MODEL = model_2\n    CRITERION = criterion\n    print(\"cpu\")\n\n# Train the model\n\ntotal_step = len(train_data_loader)\nloss_list = []\nacc_list = []\n\nnum_epochs = 5\n\nclass_list = ['N', 'Q', 'S', 'V', 'F']\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nall_con_mat = torch.zeros([num_epochs, 5, 5], dtype=torch.int32, device=device)\n\nfor epoch in range(num_epochs):\n    \n    f1_score_list=[0,0,0,0,0]\n\n    precision_list=[0,0,0,0,0]\n\n    recall_list=[0,0,0,0,0]\n    \n    delta = 0.0000000000001 \n    \n    # define empty tensor 5*5 beginning of every epoch\n    # tensor [row,column]\n    con_mat = torch.zeros([5, 5], dtype=torch.int32, device=device)\n    \n    for i, data in enumerate(train_data_loader):\n        \n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        # optimization\n        optimizer.zero_grad()\n        \n        # Forward to get output\n        outputs = MODEL(inputs)\n        # Calculate Loss\n        loss = CRITERION(outputs, labels)\n        # Backward propagation\n        loss.backward()\n        # Updating parameters\n        optimizer.step()\n        \n        # Store loss\n        loss_list.append(loss.item())\n    \n        # Calculate labels size\n        total = labels.size(0)\n        \n        # Outputs.data has dimension batch size * 5\n        # torch.max returns the max value of elements(_) and their indices(predicted) in the tensor array\n        _, predicted = torch.max(outputs.data, 1)\n        \n        # Calculate total number of correct labels \n        correct = (predicted == labels).sum().item()\n        \n        # Store accuracy\n        acc_list.append(correct / total)\n        \n        for element in range(total):\n            \n            # con_mat[row,column]\n            # con_mat[predictions, actual]\n            con_mat[predicted[element].item()-1][labels[element].item()-1] += 1\n\n        if (i + 1) % 300 == 0:                             # every 300 mini-batches...\n            \n            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n                  \n                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n                          (correct / total) * 100))\n    print(con_mat)\n            \n    all_con_mat[epoch] = con_mat\n    \n    # Print Confusion Matrix\n    \n    for i in range(torch.sum(con_mat, dim=0).size(0)): \n    \n        recall_list[i] = con_mat[i][i].item()/(torch.sum(con_mat, dim=0)[i].item()+delta)\n    \n        precision_list[i] = con_mat[i][i].item()/(torch.sum(con_mat, dim=1)[i].item()+delta)\n    \n        f1_score_list[i] = 2 * precision_list[i]*recall_list[i]/(precision_list[i]+recall_list[i]+delta)\n        \n    \n        print('class name: {}, total number of class: {:>5}, Correctly predicted: {:>5}, Recall: {:.2f}%, Precision: {:.2f}%, F1-Score: {:.2f}%'\n          \n                  .format(class_list[i],\n                          torch.sum(con_mat, dim=0)[i].item(),\n                          con_mat[i][i].item(), \n                          recall_list[i],\n                          precision_list[i],\n                          f1_score_list[i]\n                         ))\n    \n            \nprint('Finished Training')\n\nplt.plot(loss_list);\nplt.show();\n\nplt.plot(acc_list);\nplt.show();"]}, {"cell_type": "markdown", "id": "32436a30", "metadata": {}, "source": ["## <span style=\"color:red\">Built and Train Third CNN Model</span>"]}, {"cell_type": "code", "execution_count": 1, "id": "ffb6df5f", "metadata": {}, "outputs": [], "source": ["# CNN Architect\n\nclass ConvNet_3(nn.Module):\n    \n    def __init__(self):\n        \n        super(ConvNet_3, self).__init__()\n\n        self.conv1    = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, stride=1, padding=1)\n        \n        self.relu1    = nn.ReLU(inplace=True)\n        \n        self.pool1    = MaxPool2d(kernel_size=2)\n        \n\n        self.conv2    = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=1, padding=1)\n        \n        self.lrelu2   = nn.LeakyReLU(0.1)\n        \n        self.bn2      = nn.BatchNorm2d(8)\n        \n        self.pool2    = MaxPool2d(kernel_size=2)\n        \n        self.dropout2 = nn.Dropout(p=0.25)\n        \n        \n        # out_channels = 8, number of classes = 5\n        \n        # image width = 120, image height = 120 after two maxpooling 120 -> 60 -> 30\n        \n        self.fc3      = nn.Linear(8 * 30 * 30, 100)\n        \n        self.relu3    = nn.ReLU(inplace=True)\n        \n        self.dropout3 = nn.Dropout(p=0.5)\n        \n        self.fc4 = nn.Linear(100, 5)\n        \n    # Defining the forward pass\n\n    def forward(self, x):\n        \n        out = self.conv1(x)\n        \n        out = self.relu1(out)\n        \n        out = self.pool1(out)\n        \n        \n        \n        out = self.conv2(out)\n        \n        out = self.lrelu2(out)\n        \n        out = self.bn2(out)\n        \n        out = self.pool2(out)\n        \n        out = self.dropout2(out)\n        \n        \n        out = out.reshape(out.size(0), -1)\n        \n        \n        out = self.fc3(out)\n        \n        out = self.relu3(out)\n        \n        out = self.dropout3(out)\n        \n        out = self.fc4(out)\n        \n        return out\n    \n# Define Model\n\nmodel_3 = ConvNet_3()\n\nprint(model_3)"]}, {"cell_type": "code", "execution_count": 1, "id": "31b008df", "metadata": {}, "outputs": [], "source": ["if torch.cuda.is_available():\n    \n    MODEL = model_3.cuda()\n    CRITERION = criterion.cuda()\n    print(\"cuda\")\n    \nelse:\n    \n    MODEL = model_3\n    CRITERION = criterion\n    print(\"cpu\")\n\n# Train the model\n\ntotal_step = len(train_data_loader)\nloss_list = []\nacc_list = []\n\nnum_epochs = 5\n\nclass_list = ['N', 'Q', 'S', 'V', 'F']\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nall_con_mat = torch.zeros([num_epochs, 5, 5], dtype=torch.int32, device=device)\n\nfor epoch in range(num_epochs):\n    \n    f1_score_list=[0,0,0,0,0]\n\n    precision_list=[0,0,0,0,0]\n\n    recall_list=[0,0,0,0,0]\n    \n    delta = 0.0000000000001 \n    \n    # define empty tensor 5*5 beginning of every epoch\n    # tensor [row,column]\n    con_mat = torch.zeros([5, 5], dtype=torch.int32, device=device)\n    \n    for i, data in enumerate(train_data_loader):\n        \n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        # optimization\n        optimizer.zero_grad()\n        \n        # Forward to get output\n        outputs = MODEL(inputs)\n        # Calculate Loss\n        loss = CRITERION(outputs, labels)\n        # Backward propagation\n        loss.backward()\n        # Updating parameters\n        optimizer.step()\n        \n        # Store loss\n        loss_list.append(loss.item())\n    \n        # Calculate labels size\n        total = labels.size(0)\n        \n        # Outputs.data has dimension batch size * 5\n        # torch.max returns the max value of elements(_) and their indices(predicted) in the tensor array\n        _, predicted = torch.max(outputs.data, 1)\n        \n        # Calculate total number of correct labels \n        correct = (predicted == labels).sum().item()\n        \n        # Store accuracy\n        acc_list.append(correct / total)\n        \n        for element in range(total):\n            \n            # con_mat[row,column]\n            # con_mat[predictions, actual]\n            con_mat[predicted[element].item()-1][labels[element].item()-1] += 1\n\n        if (i + 1) % 300 == 0:                             # every 300 mini-batches...\n            \n            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n                  \n                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n                          (correct / total) * 100))\n    print(con_mat)\n            \n    all_con_mat[epoch] = con_mat\n    \n    # Print Confusion Matrix\n    \n    for i in range(torch.sum(con_mat, dim=0).size(0)): \n    \n        recall_list[i] = con_mat[i][i].item()/(torch.sum(con_mat, dim=0)[i].item()+delta)\n    \n        precision_list[i] = con_mat[i][i].item()/(torch.sum(con_mat, dim=1)[i].item()+delta)\n    \n        f1_score_list[i] = 2 * precision_list[i]*recall_list[i]/(precision_list[i]+recall_list[i]+delta)\n        \n    \n        print('class name: {}, total number of class: {:>5}, Correctly predicted: {:>5}, Recall: {:.2f}%, Precision: {:.2f}%, F1-Score: {:.2f}%'\n          \n                  .format(class_list[i],\n                          torch.sum(confusion_mat, dim=0)[i].item(),\n                          confusion_mat[i][i].item(), \n                          recall_list[i],\n                          precision_list[i],\n                          f1_score_list[i]\n                         ))\n    \n            \nprint('Finished Training')\n\nplt.plot(loss_list);\nplt.show();\n\nplt.plot(acc_list);\nplt.show();"]}, {"cell_type": "markdown", "id": "89947a0a", "metadata": {}, "source": ["## <span style=\"color:red\">Built and Train Forth CNN Model</span>"]}, {"cell_type": "code", "execution_count": 1, "id": "d5f03554", "metadata": {}, "outputs": [], "source": ["# CNN Architect\n\nclass ConvNet_4(nn.Module):\n    \n    def __init__(self):\n        \n        super(ConvNet_4, self).__init__()\n\n        self.layer_1  = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, stride=1, padding=1)\n        \n        self.relu1    = nn.ReLU(inplace=True)\n        \n        self.maxpool1 = MaxPool2d(kernel_size=2)\n        \n\n        self.layer_2  = nn.Conv2d(in_channels=4, out_channels=4, kernel_size=5, stride=1, padding=2)\n        \n        self.relu2    = nn.ReLU(inplace=True)\n        \n        self.maxpool2 = MaxPool2d(kernel_size=2)\n        \n        self.drop_out = nn.Dropout()\n        \n        \n        # out_channels = 4, number of classes = 5\n        \n        # image width = 120, image height = 120 after two maxpooling 120 -> 60 -> 30\n        \n        self.fc1 = nn.Linear(4 * 30 * 30, 5)\n        \n    # Defining the forward pass\n\n    def forward(self, x):\n        \n        out = self.layer_1(x)\n        \n        out = self.relu1(out)\n        \n        out = self.maxpool1(out)\n        \n        \n        out = self.layer_2(out)\n        \n        out = self.relu2(out)\n        \n        out = self.maxpool2(out)\n        \n        out = self.drop_out(out)\n        \n        \n        out = out.reshape(out.size(0), -1)\n        \n        out = self.fc1(out)\n        \n        return out\n\n# Define Model\n\nmodel_4 = ConvNet_4()\n\nprint(model_4)"]}, {"cell_type": "code", "execution_count": 1, "id": "ffe5cfc8", "metadata": {}, "outputs": [], "source": ["if torch.cuda.is_available():\n    \n    MODEL = model_4.cuda()\n    CRITERION = criterion.cuda()\n    print(\"cuda\")\n    \nelse:\n    \n    MODEL = model_4\n    CRITERION = criterion\n    print(\"cpu\")\n\n# Train the model\n\ntotal_step = len(train_data_loader)\nloss_list = []\nacc_list = []\n\nnum_epochs = 5\n\nclass_list = ['N', 'Q', 'S', 'V', 'F']\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nall_con_mat = torch.zeros([num_epochs, 5, 5], dtype=torch.int32, device=device)\n\nfor epoch in range(num_epochs):\n    \n    f1_score_list=[0,0,0,0,0]\n\n    precision_list=[0,0,0,0,0]\n\n    recall_list=[0,0,0,0,0]\n    \n    delta = 0.0000000000001 \n    \n    # define empty tensor 5*5 beginning of every epoch\n    # tensor [row,column]\n    con_mat = torch.zeros([5, 5], dtype=torch.int32, device=device)\n    \n    for i, data in enumerate(train_data_loader):\n        \n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        # optimization\n        optimizer.zero_grad()\n        \n        # Forward to get output\n        outputs = MODEL(inputs)\n        # Calculate Loss\n        loss = CRITERION(outputs, labels)\n        # Backward propagation\n        loss.backward()\n        # Updating parameters\n        optimizer.step()\n        \n        # Store loss\n        loss_list.append(loss.item())\n    \n        # Calculate labels size\n        total = labels.size(0)\n        \n        # Outputs.data has dimension batch size * 5\n        # torch.max returns the max value of elements(_) and their indices(predicted) in the tensor array\n        _, predicted = torch.max(outputs.data, 1)\n        \n        # Calculate total number of correct labels \n        correct = (predicted == labels).sum().item()\n        \n        # Store accuracy\n        acc_list.append(correct / total)\n        \n        for element in range(total):\n            \n            # con_mat[row,column]\n            # con_mat[predictions, actual]\n            con_mat[predicted[element].item()-1][labels[element].item()-1] += 1\n\n        if (i + 1) % 300 == 0:                             # every 300 mini-batches...\n            \n            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n                  \n                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n                          (correct / total) * 100))\n    print(con_mat)\n            \n    all_con_mat[epoch] = con_mat\n    \n    # Print Confusion Matrix\n    \n    for i in range(torch.sum(con_mat, dim=0).size(0)): \n    \n        recall_list[i] = con_mat[i][i].item()/(torch.sum(con_mat, dim=0)[i].item()+delta)\n    \n        precision_list[i] = con_mat[i][i].item()/(torch.sum(con_mat, dim=1)[i].item()+delta)\n    \n        f1_score_list[i] = 2 * precision_list[i]*recall_list[i]/(precision_list[i]+recall_list[i]+delta)\n        \n    \n        print('class name: {}, total number of class: {:>5}, Correctly predicted: {:>5}, Recall: {:.2f}%, Precision: {:.2f}%, F1-Score: {:.2f}%'\n          \n                  .format(class_list[i],\n                          torch.sum(confusion_mat, dim=0)[i].item(),\n                          confusion_mat[i][i].item(), \n                          recall_list[i],\n                          precision_list[i],\n                          f1_score_list[i]\n                         ))\n    \n            \nprint('Finished Training')\n\nplt.plot(loss_list);\nplt.show();\n\nplt.plot(acc_list);\nplt.show();"]}, {"cell_type": "markdown", "id": "8881e2d2", "metadata": {}, "source": ["## <span style=\"color:red\">Check Trained Forth Model on the Test Data</span>"]}, {"cell_type": "code", "execution_count": 1, "id": "a9729afa", "metadata": {}, "outputs": [], "source": ["confusion_mat = torch.zeros([5, 5], dtype=torch.int32, device=device)\n\nwith torch.no_grad():\n    \n    for data in test_data_loader:\n        \n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        outputs = MODEL(inputs)\n        \n        _, predicted = torch.max(outputs.data, 1)\n        \n        total = labels.size(0)\n        \n        # Calculate total number of correct labels \n        correct = (predicted == labels).sum().item()\n        \n        for element in range(total):\n            \n            # confusion_mat[row,column]\n            # confusion_mat[predictions, actual]\n            confusion_mat[predicted[element].item()-1][labels[element].item()-1] += 1\n\n    print(confusion_mat)"]}, {"cell_type": "code", "execution_count": 1, "id": "83f6ab38", "metadata": {}, "outputs": [], "source": ["class_list = ['N', 'Q', 'S', 'V', 'F']\n\nf1_score_list=[0,0,0,0,0]\n\nprecision_list=[0,0,0,0,0]\n\nrecall_list=[0,0,0,0,0]\n    \ndelta = 0.0000000000001 \n\n\nfor i in range(torch.sum(confusion_mat, dim=0).size(0)): \n    \n        recall_list[i] = confusion_mat[i][i].item()/(torch.sum(confusion_mat, dim=0)[i].item()+delta)\n    \n        precision_list[i] = confusion_mat[i][i].item()/(torch.sum(confusion_mat, dim=1)[i].item()+delta)\n    \n        f1_score_list[i] = 2 * precision_list[i]*recall_list[i]/(precision_list[i]+recall_list[i]+delta)\n        \n    \n        print('class name: {}, total number of class: {:>5}, Correctly predicted: {:>5}, Recall: {:.2f}%, Precision: {:.2f}%, F1-Score: {:.2f}%'\n          \n                  .format(class_list[i],\n                          torch.sum(confusion_mat, dim=0)[i].item(),\n                          confusion_mat[i][i].item(), \n                          recall_list[i],\n                          precision_list[i],\n                          f1_score_list[i]\n                         ))"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}