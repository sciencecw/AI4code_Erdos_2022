{"cells": [{"cell_type": "markdown", "id": "3c83d2af", "metadata": {}, "source": ["# MNIST Recognizing Digits\nMNIST is the hello world dataset for Image Recognition or Object Detection. The objective is to detect the handwritten digit numbered 0 to 9. This notebook will build 4 models\n1. XGBoost Model \n2. Multi Layer Perceptron\n3. Basic CNN\n4. Large CNN\nWe will compare these models with accuracy metric. \n\nThe following image captures the performance of these four models as submissions. We can observe that XGBoost model is not far behind but a simple MLP outperforms XGBoost and CNNs improve the performance by a good amount.\n\n\n![Kaggle Performance of Models](https://lh3.googleusercontent.com/qb8VQRuRjuzPIUp4764gfRkVaxYzdgQRWSmggiQXmyQX-M4nO7XcgwFo6nZ3FCATtSV5lxe9jrDPDsWrJ08pWgv2dyzA_HhKjTwRRchKj4TacEL8YSvYqw1HFyvVKk9f93ySWFtoqU6WZYh9UD63hxwN-xaiEZigFSzF0yvV93A4L2qeDZ8TIzN0hywpBCHOVrLfGqrpeDupdrwPo4eS96Q86WathAYspMT4TVVVdSp7tmPH00byAp4Olgb03Opry1roEp_3NPwSe6B5EFXfLr0jT3JDYY8kb__uO8kgc7w-8edkGx3jkB_Q-evPgpdCEANW6fJlP98OXt702koT329uGBIpsKUIb42H5ZKkEl7ErYIRwIsFBiCtJzxgKc66J0f6Pc_Xv_PfcghnoNH-okSgmMw4-T7xoCoyGrkmHta6VuHHqWe9_OBm8Jc6t9xss5vpN5ziFPD1eppmmB-B5AIpINmjp7-VgZ4Psjy1Ky2Eko4eLVE8bvwdXm1GTqHXhflOOYRMh4Rfaw3JUUkSHGZPqmdYdc0040HAgz1uvPfrP-rMSq05Xy7hy8RYzRf2iWV3dv1kHM94LLzImBcu3c4YfUfekUaNsU9gu9fK6ElT5qmH_oXcb6Pj8nx_hGSMmgZq3qLYoMLW7gw-kp7Zpz2uEH73O9T_fy9V5CsLOYUmRRxA7T4E1gEaeY09nx4=w1675-h814-no?authuser=0)\n\nThe details for these models are below"]}, {"cell_type": "markdown", "id": "63583117", "metadata": {}, "source": ["## Data Loadin' and Preppin'"]}, {"cell_type": "code", "execution_count": 1, "id": "d3e5ac30", "metadata": {}, "outputs": [], "source": ["import pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv('../input/digit-recognizer/train.csv')\ntest  = pd.read_csv('../input/digit-recognizer/test.csv')\nsub   = pd.read_csv('../input/digit-recognizer/sample_submission.csv')\n\nprint ('Train Shape :', train.shape)\nprint ('test Shape :' , test.shape)"]}, {"cell_type": "markdown", "id": "0db31c1a", "metadata": {}, "source": ["setting SEED values to make results reproducible"]}, {"cell_type": "code", "execution_count": 1, "id": "f6967fcd", "metadata": {}, "outputs": [], "source": ["SEED = 42\nfrom tensorflow.random import set_seed\nfrom numpy.random import seed\nseed(SEED)\nset_seed(SEED)"]}, {"cell_type": "markdown", "id": "d5b5cb2f", "metadata": {}, "source": ["## 0. Pitting against XGBoost"]}, {"cell_type": "code", "execution_count": 1, "id": "2cd79b9e", "metadata": {}, "outputs": [], "source": ["import xgboost as xgb\nX = train.drop('label', axis = 1).values\nY = train.label.values\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.1, random_state = SEED)\nprint (X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\ntrain_dm = xgb.DMatrix(data = X_train, label = y_train)\ntest_dm  = xgb.DMatrix(data = X_test,  label = y_test)\n\nparams = {\n    'max_depth'             : 5, \n    'eta'                   : 0.03, \n    'min_child_weight'      : 50, \n    'num_boost_round'       : 250, \n    'objective'             :'multi:softprob', \n    'seed'                  : SEED, \n    'num_class'             : 10,\n    'silent'                : 1,\n    'colsample_bytree'      : 0.5\n}\n%time model  = xgb.train(params, train_dm, num_boost_round = params['num_boost_round'])"]}, {"cell_type": "code", "execution_count": 1, "id": "f338f530", "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import accuracy_score, classification_report\nprint ('TRAIN ACCURACY : ', accuracy_score(y_train, [x.argmax() for x in model.predict(train_dm)]))\nprint ('VAL ACCURACY : '  , accuracy_score(y_test,  [x.argmax() for x in model.predict(test_dm)]))"]}, {"cell_type": "code", "execution_count": 1, "id": "cc7b4761", "metadata": {}, "outputs": [], "source": ["score_dm = xgb.DMatrix(data = test.values)\nsub_xgb = pd.Series([x.argmax() for x in model.predict(score_dm)], index  = np.arange(test.shape[0]) + 1).reset_index()\nsub_xgb.columns = sub.columns\nsub_xgb.to_csv('submission_xgboost.csv', index = False)"]}, {"cell_type": "markdown", "id": "88a7907e", "metadata": {}, "source": ["Reshaping dataset to 28 x 28 pixel values and standardizing the output of the images by dividing by 255"]}, {"cell_type": "markdown", "id": "42885d9d", "metadata": {}, "source": ["## 1. Basline Model - Multilayer Perceptron"]}, {"cell_type": "code", "execution_count": 1, "id": "3f06e597", "metadata": {}, "outputs": [], "source": ["X_train = train.drop('label', axis = 1).values.reshape(train.shape[0], 784).astype('float')/255\ny_train = train.label.values\nX_test  = test.values.reshape(test.shape[0], 784).astype('float')/255\nprint (X_train.shape, X_test.shape)"]}, {"cell_type": "markdown", "id": "c81c7fe9", "metadata": {}, "source": ["One hot encoding of label "]}, {"cell_type": "code", "execution_count": 1, "id": "17589be0", "metadata": {}, "outputs": [], "source": ["from tensorflow.keras.utils import to_categorical\ny_train = to_categorical(y_train)\nprint (y_train.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "b0939ecd", "metadata": {}, "outputs": [], "source": ["from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\ndef baseline_model():\n    model = Sequential([\n        Dense(784, input_dim = (784), activation = 'relu'),\n        Dense(10, activation = 'softmax'),\n    ])\n    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n    return model"]}, {"cell_type": "markdown", "id": "bba1edee", "metadata": {}, "source": ["#### Fit Model"]}, {"cell_type": "code", "execution_count": 1, "id": "d55aafe0", "metadata": {}, "outputs": [], "source": ["model = baseline_model()\nmodel.summary()"]}, {"cell_type": "code", "execution_count": 1, "id": "e7fa45ea", "metadata": {}, "outputs": [], "source": ["%time history = model.fit(X_train, y_train, validation_split = 0.1, epochs=40, batch_size=200, verbose=0)"]}, {"cell_type": "code", "execution_count": 1, "id": "ef6d0587", "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('LOSS')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()\n\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('ACCURACY')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()    "]}, {"cell_type": "code", "execution_count": 1, "id": "ffb2905a", "metadata": {}, "outputs": [], "source": ["import numpy as np\nsub_mlp = pd.Series([x.argmax() for x in model.predict(X_test)], index  = np.arange(test.shape[0]) + 1).reset_index()\nsub_mlp.columns = sub.columns\nsub_mlp.to_csv('submission_baseline.csv', index = False)"]}, {"cell_type": "markdown", "id": "0cd4be67", "metadata": {}, "source": ["## 2. Training Simple CNN"]}, {"cell_type": "code", "execution_count": 1, "id": "8d810ce6", "metadata": {}, "outputs": [], "source": ["X_train = train.drop('label', axis = 1).values.reshape(train.shape[0], 28, 28, 1).astype('float')/255\ny_train = to_categorical(train.label.values)\nX_test  = test.values.reshape(test.shape[0], 28, 28, 1).astype('float')/255\nprint (X_train.shape, X_test.shape, y_train.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "c1166bc5", "metadata": {}, "outputs": [], "source": ["from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten\n\ndef basic_cnn_model():\n    model = Sequential([\n        Conv2D(32, (5,5), input_shape = (28,28,1), activation = 'relu'),\n        MaxPooling2D(2,2),\n        Dropout(0.2),\n        Flatten(),\n        Dense(128, activation = 'relu'),\n        Dense(10, activation = 'softmax')\n    ])\n    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n    return model"]}, {"cell_type": "code", "execution_count": 1, "id": "2cfd00e1", "metadata": {}, "outputs": [], "source": ["model = basic_cnn_model()\nmodel.summary()"]}, {"cell_type": "code", "execution_count": 1, "id": "678f8fef", "metadata": {}, "outputs": [], "source": ["%time history = model.fit(X_train, y_train, validation_split = 0.1, epochs=40, batch_size=200, verbose=0)"]}, {"cell_type": "code", "execution_count": 1, "id": "6eff7920", "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('LOSS')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()\n\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('ACCURACY')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()"]}, {"cell_type": "markdown", "id": "4cac91c6", "metadata": {}, "source": ["### Note\n* The curves for these plots are much more closer for CNN version than the multilayer perceptron version\n* The model trained with less number of parameters than the MLP version\n* The training time was slightly higher than MLP version"]}, {"cell_type": "code", "execution_count": 1, "id": "a5ad69d5", "metadata": {}, "outputs": [], "source": ["# saving submission file\nsub_cnn = pd.Series([x.argmax() for x in model.predict(X_test)], index  = np.arange(test.shape[0]) + 1).reset_index()\nsub_cnn.columns = sub.columns\nsub_cnn.to_csv('submission_basic_CNN.csv', index = False)"]}, {"cell_type": "markdown", "id": "e7f0fbfb", "metadata": {}, "source": ["## 3. Training a Larger Convolutional Neural Network "]}, {"cell_type": "code", "execution_count": 1, "id": "a866a968", "metadata": {}, "outputs": [], "source": ["def large_cnn_model():\n    model = Sequential([\n        Conv2D(32, (5,5), input_shape = (28,28,1), activation = 'relu'),\n        MaxPooling2D(2,2),\n        Conv2D(32, (3,3), activation = 'relu'),\n        MaxPooling2D(2,2),\n        Dropout(0.2),\n        Flatten(),\n        Dense(128, activation = 'relu'),\n        Dense(10, activation = 'softmax')\n    ])\n    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n    return model"]}, {"cell_type": "code", "execution_count": 1, "id": "74a2900b", "metadata": {}, "outputs": [], "source": ["model = large_cnn_model()\nmodel.summary()"]}, {"cell_type": "code", "execution_count": 1, "id": "c1aa5100", "metadata": {}, "outputs": [], "source": ["%time history = model.fit(X_train, y_train, validation_split = 0.1, epochs=40, batch_size=200, verbose=0)"]}, {"cell_type": "code", "execution_count": 1, "id": "a275c0b2", "metadata": {}, "outputs": [], "source": ["# saving submission file\nsub_large_cnn = pd.Series([x.argmax() for x in model.predict(X_test)], index  = np.arange(test.shape[0]) + 1).reset_index()\nsub_large_cnn.columns = sub.columns\nsub_large_cnn.to_csv('submission_large_CNN.csv', index = False)"]}, {"cell_type": "code", "execution_count": 1, "id": "03223e6f", "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('LOSS')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()\n\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('ACCURACY')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()"]}, {"cell_type": "markdown", "id": "26dd4e57", "metadata": {}, "source": ["### Note\n* This time the loss and validation accuracy are closer for train and test sets for previous two versions\n* The model is trained with nearly 1/5th of previous CNN params and 1/6th of MLP's params\n* The training time even though the parameters were less was higher"]}, {"cell_type": "code", "execution_count": 1, "id": "10558e24", "metadata": {}, "outputs": [], "source": ["ls -l"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}