{"cells": [{"cell_type": "markdown", "id": "1796d8ba", "metadata": {}, "source": ["# \u2623\ufe0f Jigsaw - Explore Previous Competitions Datasets\n\n\n\n## This notebook  goes over the data from the 3 previous competitions to extract what is useful and what is not. It provides 2 curated (but not processed) dataframes, and reasons to argue that these are all the relevant pieces of data to use from those competitions.\n\n## The resulting dataframes are stored as a dataset here: [jigsaw-curated-raw-datasets](https://www.kaggle.com/julian3833/jigsaw-curated-raw-datasets)\n\n---\n# Input datasets (from the competitions):\n* Dataset: [jigsaw-toxic-comment-classification-challenge](https://www.kaggle.com/julian3833/jigsaw-toxic-comment-classification-challenge). \n\n   Competition: [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) (2018)\n\n* Dataset: [jigsaw-unintended-bias-in-toxicity-classification](https://www.kaggle.com/julian3833/jigsaw-unintended-bias-in-toxicity-classification). \n\n  Competition: [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification) (2019)\n\n\n\n* Dataset: [jigsaw-multilingual-toxic-comment-classification](https://www.kaggle.com/julian3833/jigsaw-multilingual-toxic-comment-classification). \n\n  Competition: [Jigsaw Multilingual Toxic Comment Classification](https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification) (2020)\n\n\n---\n\nIn this notebook we will:\n1. Extract the 2 main dataframes from previous competitions and save a curated version of them to a [dataset](https://www.kaggle.com/julian3833/jigsaw-curated-raw-datasets)\n2. Verify step-by-step that this data is all the relevant data\n3. Provide a utility function to post process the classification dataset into common formats (binary classification, regression, regression with weights)\n\n\n# Please, _DO_ upvote if you find this useful!\n\n---\n\n#  Summary / conclusions\n\nThere are two main relevant dataframes than can be read from the ported datasets:\n\n```python\n\ndf_classification = pd.read_csv(\"../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\")\n\ndf_bias_all = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/all_data.csv\")\n\n\n```\n\n## Data from the 2018 Classification Challenge\nThe first one has the training and test data from the original classification competition. It is merged and provided all together as part of the _multilingual_ competition. It can be easily assembled using the files `train.csv`, `test.csv` and `test_labels.csv` from the classification competition itself. The version of the multilingual competition is exactly the same as the assembled one (See below).\n\n* Rows: `223549`\n* Labels: `['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']` (6)\n\nThis dataset is stored as `classification-dataset.csv` in this notebook and provided in this [dataset]() as well.\n\n\n## Data from the 2019 Unintended Bias in Toxicity Classification\n\nThe second one has the training and test data from the unintended bias competitions, and is provided in that competition (it was released _after_ the deadline). This one contains the 6 labels of the original one, with some changes, plus other labels and much more columns.\n\nIt contains much more data, but they come\n\n* Rows: `1999516`\n* Labels: `['toxicity', 'severe_toxicity', 'obscene', 'sexual_explicit', 'identity_attack', 'insult', 'threat']` (7)\n* Other columns: 24 identities/flags + 14 others\n* Values are floats between `0` and `1` and not binary flags. \n\nThe version of this data in the multilingual competition is partially different to the originally assembled one (`all_data.csv`)\n\n\n"]}, {"cell_type": "markdown", "id": "fbfa7603", "metadata": {}, "source": ["# Provision of almost raw curated dataframes\n\nHere the previously mentioned datasets are stored to disk\n\nTheir names are `classification-dataset.csv` and `unintented-bias-dataset.csv`. They can be found in this dataset: [jigsaw-curated-raw-datasets](https://www.kaggle.com/julian3833/jigsaw-curated-raw-datasets).\n\nFor the one from the Unintented Bias competition, we drop the extra columns, we rename the labels to match the ones of the Classification Challenge, but the `sexual_explicit` label is kept and therefore the column shapes don't match.\n\nWe don't binarize the labels either, since that is already an important decision, and we want to provide a dataset as close to the original as it's possible."]}, {"cell_type": "code", "execution_count": 1, "id": "f651b12c", "metadata": {}, "outputs": [], "source": ["import numpy as np\nimport pandas as pd"]}, {"cell_type": "code", "execution_count": 1, "id": "452dca14", "metadata": {}, "outputs": [], "source": ["# 2018 classification challenge dataset\ndf_classification = pd.read_csv(\"../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\")\ndf_classification.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "676ed790", "metadata": {}, "outputs": [], "source": ["df_classification.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "052df267", "metadata": {}, "outputs": [], "source": ["df_classification.to_csv(\"classification-dataset.csv\", index=False)"]}, {"cell_type": "code", "execution_count": 1, "id": "f4dfedbe", "metadata": {}, "outputs": [], "source": ["# 2019 unintended bias dataset\ndf_bias_all = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/all_data.csv\")\ndf_bias_all.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "55477130", "metadata": {}, "outputs": [], "source": ["df_bias_all.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "2160e089", "metadata": {}, "outputs": [], "source": ["keep_cols = ['id', 'comment_text', 'toxicity', 'severe_toxicity', \n             'obscene', 'threat', 'insult', 'identity_attack',  'sexual_explicit']\ndf_bias_cleaned = df_bias_all[keep_cols].rename(columns={'toxicity': 'toxic', 'severe_toxicity': 'severe_toxic', 'identity_attack': 'identity_hate'})\ndf_bias_cleaned.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "111e3f68", "metadata": {}, "outputs": [], "source": ["df_bias_cleaned.to_csv(\"unintented-bias-dataset.csv\", index=False)"]}, {"cell_type": "markdown", "id": "e4f48da2", "metadata": {}, "source": ["# Detailed analysis\n\n## Verification for the `classification-dataset.csv`: \n### The data provided in the multilingual challenge is exactly the same as the original \u2705\n\n* `jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv`\n\nis equal to \n* `jigsaw-toxic-comment-classification-challenge/train.csv`\n* `jigsaw-toxic-comment-classification-challenge/test.csv`\n* `jigsaw-toxic-comment-classification-challenge/test_labels.csv`"]}, {"cell_type": "code", "execution_count": 1, "id": "143c7b19", "metadata": {}, "outputs": [], "source": ["df_train = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\nprint(df_train.shape)\ndf_train.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "ba67fcea", "metadata": {}, "outputs": [], "source": ["df_test = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv\")\ndf_test_labels = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv\")\ndf_test = df_test.merge(df_test_labels[df_test_labels['toxic'] != -1], on='id', how='inner')\nprint(df_test.shape)\ndf_test.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "8e452e0b", "metadata": {}, "outputs": [], "source": ["df_classification_full = pd.concat([df_train, df_test]).reset_index(drop=True)\ndf_classification_full.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "25128082", "metadata": {}, "outputs": [], "source": ["# The provided dataframe has the train and the test altogether for the Classification Challenge\ndf_ml = pd.read_csv(\"../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\")"]}, {"cell_type": "code", "execution_count": 1, "id": "c9fcd5c3", "metadata": {}, "outputs": [], "source": ["# Are they exactly the same?\n(df_ml == df_classification_full).all().all()"]}, {"cell_type": "markdown", "id": "02e0feeb", "metadata": {}, "source": ["# Verifications for `unintented-bias-dataset.csv`\n\n## 1. `all_data.csv` is a superset of `train.csv` (only 1 extra row) \u2705 "]}, {"cell_type": "code", "execution_count": 1, "id": "59ba3a89", "metadata": {}, "outputs": [], "source": ["# Read all_data.csv\ndf_all = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/all_data.csv\")\ndf_all.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "3f1c82d0", "metadata": {}, "outputs": [], "source": ["keep_cols = ['id', 'comment_text', 'split', 'toxicity', 'severe_toxicity', \n             'obscene', 'sexual_explicit', 'identity_attack', 'insult', 'threat']\ndf_all = df_all[keep_cols]\ndf_all.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "f0281d75", "metadata": {}, "outputs": [], "source": ["# Read train.csv\ndf_train = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\")\ndf_train.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "995489ed", "metadata": {}, "outputs": [], "source": ["keep_cols = ['id', 'comment_text', 'target',  'severe_toxicity', 'obscene',\n             'sexual_explicit', 'identity_attack', 'insult', 'threat']\ndf_train = df_train[keep_cols]\ndf_train.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "eeefcd43", "metadata": {}, "outputs": [], "source": ["# Get the traininig part of all_data.csv\ndf_all_train_split = df_all[df_all['split'] == 'train'].drop(\"split\", axis=1).sort_values(\"id\").reset_index(drop=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "76cc3cac", "metadata": {}, "outputs": [], "source": ["# Align indexes of train.csv\ndf_train_massaged = df_train.rename(columns={'target': 'toxicity'}).sort_values(\"id\").reset_index(drop=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "0fe0dd48", "metadata": {}, "outputs": [], "source": ["# There is 1 sample different WTF!!\ndf_all_train_split.shape, df_train_massaged.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "1e34b32a", "metadata": {}, "outputs": [], "source": ["# Drop the extra row in all_data.csv's train split\ndf_all_train_split = df_all_train_split[df_all_train_split['id'].isin(df_train_massaged['id'])].reset_index(drop=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "a066b1b1", "metadata": {}, "outputs": [], "source": ["# Do they exaclty match now?\n(df_all_train_split == df_train_massaged).all().all()"]}, {"cell_type": "markdown", "id": "9678f7ea", "metadata": {}, "source": ["## 2. `all_data.csv` contains the `test.csv` with labels \u2705 "]}, {"cell_type": "code", "execution_count": 1, "id": "75ff3836", "metadata": {}, "outputs": [], "source": ["df_all[df_all['split'] == 'test'].shape"]}, {"cell_type": "code", "execution_count": 1, "id": "6c0cb083", "metadata": {}, "outputs": [], "source": ["df_test = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")\ndf_test.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "503666d0", "metadata": {}, "outputs": [], "source": ["# All the df tests samples are contained in df_all. We can use df all\ndf_test['id'].isin(df_all['id']).all()"]}, {"cell_type": "code", "execution_count": 1, "id": "1c5cc72a", "metadata": {}, "outputs": [], "source": ["# Conclusion: all_data.csv seems good enough!!"]}, {"cell_type": "markdown", "id": "41922a1e", "metadata": {}, "source": ["# Verifications for the multilingual competitions:\n\n## 1. Multilingual competition doesn't have train data of their own \u2705 \n\nThis competition doesn't have train data. \n\nThe train data is the one of the two previous competions.\n\nHere we will see if anything is useful at all."]}, {"cell_type": "code", "execution_count": 1, "id": "a17db465", "metadata": {}, "outputs": [], "source": ["import os\nos.listdir(\"../input/jigsaw-multilingual-toxic-comment-classification/\")"]}, {"cell_type": "code", "execution_count": 1, "id": "cea1d563", "metadata": {}, "outputs": [], "source": ["# Is the test set useful?\ndf_test = pd.read_csv(\"../input/jigsaw-multilingual-toxic-comment-classification/test.csv\")"]}, {"cell_type": "code", "execution_count": 1, "id": "9ada4b9f", "metadata": {}, "outputs": [], "source": ["# It doesn't seem so\ndf_test['lang'].value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "69c3a1cb", "metadata": {}, "outputs": [], "source": ["# Validation doesnt' have English either\ndf_val = pd.read_csv(\"../input/jigsaw-multilingual-toxic-comment-classification/validation.csv\")\ndf_val['lang'].value_counts()"]}, {"cell_type": "markdown", "id": "16266513", "metadata": {}, "source": ["## 2. Is the unintented bias summary provided in the multilingual competition better than `all_data.csv`? No"]}, {"cell_type": "code", "execution_count": 1, "id": "07c31abe", "metadata": {}, "outputs": [], "source": ["df_bias_ml = pd.read_csv(\"../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\")"]}, {"cell_type": "code", "execution_count": 1, "id": "86d27d90", "metadata": {}, "outputs": [], "source": ["df_bias_ml.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "49ac118b", "metadata": {}, "outputs": [], "source": ["df_bias_ml['id'].isin(df_all['id']).all()"]}, {"cell_type": "code", "execution_count": 1, "id": "d73fbd28", "metadata": {}, "outputs": [], "source": ["df_all['id'].isin(df_bias_ml['id']).all()"]}, {"cell_type": "code", "execution_count": 1, "id": "ed8abb47", "metadata": {}, "outputs": [], "source": ["# all_data.csv seems to have more data than the version provided in the multilingual competition\ndf_all.head()"]}, {"cell_type": "markdown", "id": "c9e6bd30", "metadata": {}, "source": ["# Utility function `get_classification_dataset_as`: simple common postprocessing for `classification-dataset.csv` \n\nThis function allows to translate the `classification-dataset.csv` into a binary classification or a regression task.\n\nAdding a column `y` and dropping the original labels.\n\nFor `task=binary`\n  * `y=1` if any label is not zero and will be 0 otherwise.\n\nFor `task=regression`, \n * `y` will be the sum of the non-zero labels.\n * the parameter `regression_weights` can be used to control how much each label affects `y`. See an example usage below.\n > For example: `regression_weights={'obscene': 0.1, 'insult': 15}`. The default weight for labels with no keys is `1`.\n\n\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "1627de3a", "metadata": {}, "outputs": [], "source": ["def get_classification_dataset_as(task=\"binary\", regression_weights=None):\n    \"\"\"\n    Args:\n        task: ['binary', 'regression']\n        regression_weights: dictionary {label => weights} for each label or None, if task=='regression'\n        \n    \"\"\"\n    assert task in ['binary', 'regression']\n    \n    df = pd.read_csv(\"classification-dataset.csv\")\n\n    \n    labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n    \n    if task == 'binary':    \n        df['y'] = (df[labels].sum(axis=1) > 0).astype(int)\n        df = df.drop(labels, axis=1)\n    elif task == 'regression':\n        if regression_weights is None:\n            df['y'] = df[labels].sum(axis=1)\n        else:\n            weighed_columns = [regression_weights.get(l, 1) * df[l] for l in labels]\n            df['y'] = pd.concat(weighed_columns, axis=1).sum(axis=1)\n        df = df.drop(labels, axis=1)\n    return df\n    "]}, {"cell_type": "markdown", "id": "d25d8e35", "metadata": {}, "source": ["# Example usage"]}, {"cell_type": "code", "execution_count": 1, "id": "2e01b8ae", "metadata": {}, "outputs": [], "source": ["df_binary = get_classification_dataset_as('binary')"]}, {"cell_type": "code", "execution_count": 1, "id": "5809dcc1", "metadata": {}, "outputs": [], "source": ["df_regression = get_classification_dataset_as('regression')"]}, {"cell_type": "code", "execution_count": 1, "id": "166a8702", "metadata": {}, "outputs": [], "source": ["df_regression_weighted = get_classification_dataset_as('regression', regression_weights={'severe_toxic': 2})"]}, {"cell_type": "code", "execution_count": 1, "id": "a9b837a7", "metadata": {}, "outputs": [], "source": ["!mkdir postprocessed"]}, {"cell_type": "code", "execution_count": 1, "id": "6042e3a5", "metadata": {}, "outputs": [], "source": ["df_binary.to_csv(\"postprocessed/classification-dataset-binary.csv\", index=False)\ndf_regression.to_csv(\"postprocessed/classification-dataset-regression.csv\", index=False)\ndf_regression_weighted.to_csv(\"postprocessed/classification-dataset-regression-weighted.csv\", index=False)"]}, {"cell_type": "markdown", "id": "bf81d6af", "metadata": {}, "source": ["# Please, _DO_ upvote if you find this useful!"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}