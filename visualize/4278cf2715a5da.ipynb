{"cells": [{"cell_type": "markdown", "id": "2aae6230", "metadata": {}, "source": ["# **Graduate Admission Analysis for UCLA**"]}, {"cell_type": "markdown", "id": "8b3102dc", "metadata": {}, "source": ["### **Data Description**\nThe dataset contains information about a student's:\n* GRE Score\n* TOEFL Score\n* University Ratings\n* Statement of Purpose Score\n* Letter of Recomendation Score\n* CGPA\n* Whether the Student Has Done Any Research\n* Chance of Admission (What We're Trying to Predict)"]}, {"cell_type": "markdown", "id": "b0321b5c", "metadata": {}, "source": ["## **Importing Libraries and Data**\nImporting libraries and setting the default style in Seaborn."]}, {"cell_type": "code", "execution_count": 1, "id": "56def1b5", "metadata": {}, "outputs": [], "source": ["import numpy as np\nimport pandas as pd\nimport os\nfrom matplotlib import pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nsns.set(style='white')\nsns.set(style='whitegrid', color_codes=True)"]}, {"cell_type": "markdown", "id": "6f03722c", "metadata": {}, "source": ["Next, let's import our dataset and see what we're working with."]}, {"cell_type": "code", "execution_count": 1, "id": "ffe27b13", "metadata": {}, "outputs": [], "source": ["df = pd.read_csv(\"../input/Admission_Predict.csv\")\ndf.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "6865d472", "metadata": {}, "outputs": [], "source": ["df.describe()"]}, {"cell_type": "markdown", "id": "0ab14677", "metadata": {}, "source": ["## **Exploratory Analysis**\n\nFrom these charts it looks like we have no missing values! \n\nIt seems as though Serial No. is just an index for students, which we can take out. \n\nTwo columns also have an added space in the label which we'll take out"]}, {"cell_type": "code", "execution_count": 1, "id": "c829bd58", "metadata": {}, "outputs": [], "source": ["df.rename(columns = {'Chance of Admit ':'Chance of Admit', 'LOR ':'LOR'}, inplace=True)\ndf.drop(labels='Serial No.', axis=1, inplace=True)"]}, {"cell_type": "markdown", "id": "fc8c79af", "metadata": {}, "source": ["Let's plot a heatmap to see the correlation of all the features compared to Chance to Admit:"]}, {"cell_type": "code", "execution_count": 1, "id": "276f8dbf", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(df.corr(), annot=True, cmap='Blues')"]}, {"cell_type": "markdown", "id": "7c51182c", "metadata": {}, "source": ["The top three features that affect the Chance to Admit are:\n1. CGPA\n2. GRE Score\n3. TOEFL Score\n\nLet's explore these three features to get a better understanding."]}, {"cell_type": "markdown", "id": "328e2499", "metadata": {}, "source": ["### **CGPA**\n\nThe Cumulative Grade Point Average is a 10 point grading system.\n\nFrom the data shown below, it appears the submissions are normally distributed. With a mean of 8.6 and standard deviation of 0.6.\n\n### **CGPA vs Chance of Admit**\n\nIt appears as applicant's CGPA has a strong correlation with their chance of admission."]}, {"cell_type": "code", "execution_count": 1, "id": "4fb5cfe0", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(20,6))\nplt.subplot(1,2,1)\nsns.distplot(df['CGPA'])\nplt.title('CGPA Distribution of Applicants')\n\nplt.subplot(1,2,2)\nsns.regplot(df['CGPA'], df['Chance of Admit'])\nplt.title('CGPA vs Chance of Admit')"]}, {"cell_type": "markdown", "id": "b866629b", "metadata": {}, "source": ["### **GRE Score**\n\nThe Graduate Record Examination is a standarized exam, often required for admission to graduate and MBA programs globally. It's made up of three components:\n1. Analytical Writing (Scored on a 0-6 scale in half-point increments)\n2. Verbal Reasoning (Scored on a 130-170 scale)\n3. Quantitative Reasoning (Scored on a 130-170 scale)\n\nIn this dataset, the GRE Score is based on a maximum of 340 points. The mean is 317 with a standard deviation of 11.5.\n\n### **GRE Score vs Chance of Admit**\n\nGRE scores have a strong correlation with the chance of admission however not as strong as one's CGPA.\n"]}, {"cell_type": "code", "execution_count": 1, "id": "defa8830", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(20,6))\nplt.subplot(1,2,1)\nsns.distplot(df['GRE Score'])\nplt.title('Distributed GRE Scores of Applicants')\n\nplt.subplot(1,2,2)\nsns.regplot(df['GRE Score'], df['Chance of Admit'])\nplt.title('GRE Scores vs Chance of Admit')"]}, {"cell_type": "markdown", "id": "fb38f9d6", "metadata": {}, "source": ["### **TOEFL Score**\n\nThe Test of English as a Foreign Language is a standarized test for non-native English speakers that are choosing to enroll in English-speaking universities.\n\nThe test is split up into 4 sections:\n1. Reading\n2. Listening\n3. Speaking\n4. Writing\n\nAll sections are scored out of 30, giving the exam a total score of 120 marks. In this dataset, the TOEFL scores have a mean of 107 and a standard deviation of 6.\n\n### **TOEFL Score vs Chance of Admit**\n\nLike GRE scores, the scores received for the TOEFL strongly correlate to an applicants chance of admission."]}, {"cell_type": "code", "execution_count": 1, "id": "d0526e2f", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(20,6))\nplt.subplot(1,2,1)\nsns.distplot(df['TOEFL Score'])\nplt.title('Distributed TOEFL Scores of Applicants')\n\nplt.subplot(1,2,2)\nsns.regplot(df['TOEFL Score'], df['Chance of Admit'])\nplt.title('TOEFL Scores vs Chance of Admit')"]}, {"cell_type": "markdown", "id": "5fdd0f5f", "metadata": {}, "source": ["For my curiosity, I want to explore the data a little bit further regarding research and university rankings. Even though they hold a lower importance in the chance of admission, it would be nice to understand their characteristics in the dataset."]}, {"cell_type": "markdown", "id": "2b75aefe", "metadata": {}, "source": ["### **Research**\n\nLet's explore how many applicants have research experience.\n\nIt seems the majority of applicants have research experience. However, this is the least important feature, so it doesn't matter all too much if an applicant has the experience or not."]}, {"cell_type": "code", "execution_count": 1, "id": "02796305", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(8,6))\nsns.countplot(df['Research'])\nplt.title('Research Experience')\nplt.ylabel('Number of Applicants')\nax.set_xticklabels(['No Research Experience', 'Has Research Experience'])"]}, {"cell_type": "markdown", "id": "000c17a1", "metadata": {}, "source": ["### **University Rating**\n\nLet's see the distribution of applicants coming from each kind of university.\n\nMost applicants come from a tier 3 and tier 2 university."]}, {"cell_type": "code", "execution_count": 1, "id": "b9473250", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(8,6))\nsns.countplot(df['University Rating'])\nplt.title('University Rating')\nplt.ylabel('Number of Applicants')"]}, {"cell_type": "markdown", "id": "5cb9e5b5", "metadata": {}, "source": ["## **Preparing Data for Machine Learning**\n\nNow that we understand our dataset, it's time to implement machine learning methods to predict future applicant's chances of admission.\n\nFirst we have to prepare our data, by splitting it into training and testing data. We'll also scale our data, from 0 to 1, to receive more accurate predictions."]}, {"cell_type": "code", "execution_count": 1, "id": "76aa17ab", "metadata": {}, "outputs": [], "source": ["targets = df['Chance of Admit']\nfeatures = df.drop(columns = {'Chance of Admit'})\n\nX_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)"]}, {"cell_type": "code", "execution_count": 1, "id": "0fe216ec", "metadata": {}, "outputs": [], "source": ["scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)"]}, {"cell_type": "markdown", "id": "71022e2f", "metadata": {}, "source": ["## **Machine Learning **\n\nThe end goal is to determine if an applicant will be admitted to the univeristy or not. Therefore, this is a classification case. We'll use multiple techniques and eventually select the method with the best score. The methods used will be:\n\n1. Logistic Regression\n2. Decision Tree Classification\n3. Random Forest Classification"]}, {"cell_type": "markdown", "id": "e8e2998d", "metadata": {}, "source": ["### **Logistic Regression**"]}, {"cell_type": "markdown", "id": "8376c73b", "metadata": {}, "source": ["Our data contains the chance to admit, which is a float value, we should classify the chance of a student's admission as a binary value, accepted and not accepted.\n\nTo transform the chance to admit to binary values, we'll assume that if the student's chance is above the mean, they are accepted. If not, they will not be accepted.\n\nSo, let's find the mean chance of admission and transform the 'Chance to Admit' column to binary values."]}, {"cell_type": "code", "execution_count": 1, "id": "6736f3ff", "metadata": {}, "outputs": [], "source": ["mean_chance = df['Chance of Admit'].mean()"]}, {"cell_type": "code", "execution_count": 1, "id": "ef25ee0d", "metadata": {}, "outputs": [], "source": ["y_train_binary = (y_train > 0.72).astype(int)\ny_test_binary = (y_test > 0.72).astype(int)"]}, {"cell_type": "code", "execution_count": 1, "id": "dc32d75c", "metadata": {}, "outputs": [], "source": ["logreg = LogisticRegression()\nlogreg.fit(X_train, y_train_binary)\ny_predict = logreg.predict(X_test)\nlogreg_score = (logreg.score(X_test, y_test_binary))*100\nlogreg_score"]}, {"cell_type": "markdown", "id": "35520f5c", "metadata": {}, "source": ["### **Decision Trees**"]}, {"cell_type": "code", "execution_count": 1, "id": "03e27baf", "metadata": {}, "outputs": [], "source": ["dec_tree = DecisionTreeClassifier(random_state=0, max_depth=6)\ndec_tree.fit(X_train, y_train_binary)\ny_predict = dec_tree.predict(X_test)\ndec_tree_score = (dec_tree.score(X_test, y_test_binary))*100\ndec_tree_score"]}, {"cell_type": "markdown", "id": "cc9a7129", "metadata": {}, "source": ["### **Random Forests**"]}, {"cell_type": "code", "execution_count": 1, "id": "a828ad53", "metadata": {}, "outputs": [], "source": ["forest = RandomForestClassifier(n_estimators=110,max_depth=6,random_state=0)\nforest.fit(X_train, y_train_binary)\ny_predict = forest.predict(X_test)\nforest_score = (forest.score(X_test, y_test_binary))*100\nforest_score"]}, {"cell_type": "markdown", "id": "8160989f", "metadata": {}, "source": ["### **Comparing Scores**\n\nLet's put all the scores in a table and display their scores side-by-side."]}, {"cell_type": "code", "execution_count": 1, "id": "5c44e598", "metadata": {}, "outputs": [], "source": ["Methods = ['Logistic Regression', 'Decision Trees', 'Random Forests']\nScores = np.array([logreg_score, dec_tree_score, forest_score])\n\nfig, ax = plt.subplots(figsize=(8,6))\nsns.barplot(Methods, Scores)\nplt.title('Algorithm Prediction Accuracies')\nplt.ylabel('Accuracy')"]}, {"cell_type": "markdown", "id": "32733140", "metadata": {}, "source": ["### **Selecting the Best Algorithm**\n\n1. Logistic Regression - 96.25%\n2. Random Forests - 96.25%\n3. Decision Trees - 92.5%\n\nIt seems that Logistic Regression and Random Forests are the most accurate methods and will be used to predict the future applicant's chances of admission.\n\n"]}, {"cell_type": "markdown", "id": "c5fc3058", "metadata": {}, "source": ["## **Conclusion**\n\nThis was a great way to get started on Kaggle and for my first project outside of coursework. It gave me some practice some exploratory analysis and simple machine learning techniques. \n\nIt's great to see what specific variables contribute to the chance of admission and how they are weighted against eachother."]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}