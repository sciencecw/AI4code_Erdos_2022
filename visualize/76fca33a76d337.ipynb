{"cells": [{"cell_type": "markdown", "id": "34feca0e", "metadata": {}, "source": ["# This is to show some of the issues trying to use machine learning for stock price prediction\n\nConclusion: It is NOT as easy as you may think."]}, {"cell_type": "code", "execution_count": 1, "id": "97237443", "metadata": {}, "outputs": [], "source": ["import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))"]}, {"cell_type": "code", "execution_count": 1, "id": "74771746", "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom IPython.display import display\nimport pkg_resources \n\npkg_resources.get_distribution('pandas').version"]}, {"cell_type": "markdown", "id": "c59069ae", "metadata": {}, "source": ["# Import and look at data"]}, {"cell_type": "code", "execution_count": 1, "id": "97a446b6", "metadata": {}, "outputs": [], "source": ["prices = pd.read_csv('../input/nyse/prices.csv')\nPSA = pd.read_csv('../input/nyse/prices-split-adjusted.csv')  # prices-split-adjusted\nsecurities = pd.read_csv('../input/nyse/securities.csv')\nfundamentals = pd.read_csv('../input/nyse/fundamentals.csv')"]}, {"cell_type": "code", "execution_count": 1, "id": "1b0849e8", "metadata": {}, "outputs": [], "source": ["display([prices.shape, PSA.shape, securities.shape, fundamentals.shape])\ndisplay(prices.head(), PSA.head(), securities.head(), fundamentals.head())"]}, {"cell_type": "code", "execution_count": 1, "id": "8f8eafe6", "metadata": {}, "outputs": [], "source": ["display(PSA.groupby('symbol').size().sort_values(ascending=True),\n        fundamentals.groupby('Ticker Symbol').size().sort_values(ascending=True))"]}, {"cell_type": "markdown", "id": "9ef9e52e", "metadata": {}, "source": ["* PSA and prices have the same content. Use PSA hereafter\n* Not all symbols have the same time lenght of data, but most have data from 2010 to 2016. \n* The fundamentals has at most 4 data points for any company and do not seem useful. Ignore.\n* data in securities are not useful for us. \n\nUse data of APPLE for further analysis"]}, {"cell_type": "markdown", "id": "ce14792b", "metadata": {}, "source": ["# Feature engineering"]}, {"cell_type": "code", "execution_count": 1, "id": "3822166b", "metadata": {}, "outputs": [], "source": ["apple = (PSA.loc[PSA['symbol']=='AAPL']\n         .drop(columns='symbol')\n         .sort_values(by='date',ascending=True)\n         .reset_index(drop=True)\n         .assign(**{'average': lambda df: df.loc[:,['open','high','low','close']].mean(axis=1), \n                    'EMA20': lambda df: df['average'].ewm(span=20, adjust=False).mean(), \n                    'EMA5': lambda df: df['average'].ewm(span=5, adjust=False).mean(), \n                    'dist_EMA20': lambda df: (df['average'] - df['EMA20'])/df['EMA20']*100, \n                    'dist_EMA5': lambda df: (df['average'] - df['EMA5'])/df['EMA5']*100}))\napple.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "10a309c1", "metadata": {}, "outputs": [], "source": ["# use volume weighted averaged OHLC.mean to represent market average\n# comparison with SPY500 shows that market_average calculated this way is representative of overall market\n\ntickers_with_all_dates = PSA.groupby('symbol').size().loc[lambda s: s.values==s.values.max()].index.to_list()\nmarket = (PSA.loc[PSA['symbol'].isin(tickers_with_all_dates)]\n          .assign(**{'average': lambda df: df.loc[:,['open','high','low','close']].mean(axis=1), \n                     'price x volume': lambda df: df['average']*df['volume']})\n          .groupby('date')\n          .agg(**{'price x volume sum': pd.NamedAgg(column='price x volume', aggfunc=np.sum), \n                  'volume sum': pd.NamedAgg(column='volume', aggfunc=np.sum)})\n          .assign(**{'market_average': lambda df: df['price x volume sum']/df['volume sum']})\n          .sort_index(ascending=True))\n\napple['market'] = market['market_average'].values\napple.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "9dc63406", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(10,5))\nax.plot(apple.index[:120], apple['dist_EMA20'][:120], label='dist_EMA20')\nax.plot(apple.index[:120], apple['dist_EMA5'][:120], label='dist_EMA5')\nax.set_xlabel('day')\nax.set_ylabel('distance from moving mean')\nax.legend(loc='best')\n\nfig, ax = plt.subplots(figsize=(10,5))\nax.plot(apple.index[:120], apple['open'][:120], label='open')\nax.plot(apple.index[:120], apple['close'][:120], label='close')\nax.set_xlabel('day')\nax.set_ylabel('apple')\nax.legend(loc='best')"]}, {"cell_type": "markdown", "id": "ba617599", "metadata": {}, "source": ["# Create sample of sequences and normalize within each sample\n- will this make each sample independent from each other? -- No, you will see why later."]}, {"cell_type": "code", "execution_count": 1, "id": "a504e6c4", "metadata": {}, "outputs": [], "source": ["# use previous 120 days' data to predict price in 5 days\nlookback = 120  \nlookahead = 5\n\n# Create sequence samples\n# note: normalization is done with information within a sequence\nfeatures = ['open', 'close', 'low', 'high', 'volume', 'dist_EMA20', 'dist_EMA5', 'market']\ndata = apple[features].values\nX = np.array([data[i:i+lookback].copy() for i in range(len(data) - lookback - lookahead)])\nX[:,:,0:4] = X[:,:,0:4]/X[:,0,0, None, None]\nX[:,:,4] = X[:,:,4]/X[:,0,None,4]\nX[:,:,-1] = X[:,:,-1]/X[:,0,None,-1]\ny = np.array([(data[i+lookback+lookahead-1,1] - data[i+lookback-1,1])/data[i+lookback-1,1]*100 for i in range(len(data) - lookback - lookahead)])"]}, {"cell_type": "code", "execution_count": 1, "id": "3ee1dacd", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(10,5))\nax.plot(range(X[0].shape[0]), X[0,:,0], label='open')\nax.plot(range(X[0].shape[0]), X[0,:,-1], label='market')\nax.set_xlabel('day')\nax.set_ylabel('apple')\nax.legend(loc='best')"]}, {"cell_type": "markdown", "id": "a594afc3", "metadata": {}, "source": ["# RNN model"]}, {"cell_type": "code", "execution_count": 1, "id": "853e224c", "metadata": {}, "outputs": [], "source": ["import keras\nimport tensorflow as tf\nfrom keras.models import Model\nfrom keras.layers import Dense, Dropout, LSTM, Input, Activation, concatenate\nfrom keras import optimizers\n\nnp.random.seed(4)\n\nlstm_input = Input(shape=(lookback, len(features)), name='lstm_input')\nx = LSTM(units=64, return_sequences=False, return_state=False)(lstm_input)  \nx = Dropout(0.2)(x)\nx = Dense(units=32, activation='relu')(x)\noutput = Dense(1, activation='linear')(x)\nmodel = Model(inputs=lstm_input, outputs=output)\n\nadam = optimizers.Adam(lr=0.0005)\n\nmodel.compile(optimizer=adam, loss='mse')\n\nweights = model.get_weights()  # needed to reset the model \n\nmodel.summary()\nfrom keras.utils import plot_model\nplot_model(model) "]}, {"cell_type": "markdown", "id": "868acc3e", "metadata": {}, "source": ["# Train model"]}, {"cell_type": "code", "execution_count": 1, "id": "8a9a1273", "metadata": {}, "outputs": [], "source": ["# train test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\nprint(\"{} train samples, {} test samples\".format(X_train.shape[0], X_test.shape[0]))\n\nmodel.fit(X_train, y_train, epochs=80, verbose=0)"]}, {"cell_type": "markdown", "id": "3eec3b0c", "metadata": {}, "source": ["# test model"]}, {"cell_type": "code", "execution_count": 1, "id": "8e117528", "metadata": {}, "outputs": [], "source": ["y_pred = model.predict(X_test)\n\nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(range(len(y_test)), y_test, label='y_test')\nax.plot(range(len(y_test)), y_pred, label='y_pred')"]}, {"cell_type": "code", "execution_count": 1, "id": "4a570bf3", "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import r2_score\nr2_score(y_test, y_pred)"]}, {"cell_type": "code", "execution_count": 1, "id": "6cf35546", "metadata": {}, "outputs": [], "source": ["num_right = (np.sign(y_test) == np.sign(y_pred).squeeze()).sum()\nnum_right/len(y_test)"]}, {"cell_type": "markdown", "id": "83c92960", "metadata": {}, "source": ["The accuracy is too high. You have to question if the sample are truly independent.\nThe fact is that they are not because we create the samples by moving forward one step at a time. \nSo for each sample, the samples before and after it will be very similar to it. \nWhen we split train and test samples randomly, for each test sample, there may be several train sample similar to it, which gives the fake high accuracy. \n<font color=red>Do NOT split samples randomly for time-series problem</font>"]}, {"cell_type": "markdown", "id": "4bf217ef", "metadata": {}, "source": ["# Split train and test without randomness and retrain model"]}, {"cell_type": "code", "execution_count": 1, "id": "fafa5841", "metadata": {}, "outputs": [], "source": ["test_size = 0.2\nk = np.round(len(X)*(1-test_size)).astype(int)\nX_train, X_test, y_train, y_test = X[:k,:,:], X[k:,:,:], y[:k], y[k:]\nprint(\"{} train samples, {} test samples\".format(X_train.shape[0], X_test.shape[0]))"]}, {"cell_type": "code", "execution_count": 1, "id": "fa2bf37f", "metadata": {}, "outputs": [], "source": ["model.set_weights(weights)\n\ntrain_history = model.fit(X_train, y_train, epochs=50, verbose=0)"]}, {"cell_type": "code", "execution_count": 1, "id": "ec67b387", "metadata": {}, "outputs": [], "source": ["y_pred = model.predict(X_test)\n\nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(range(len(y_test)), y_test, label='y_test')\nax.plot(range(len(y_test)), y_pred, label='y_pred')"]}, {"cell_type": "code", "execution_count": 1, "id": "4e4f9c15", "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import r2_score\nr2_score(y_test, y_pred)"]}, {"cell_type": "code", "execution_count": 1, "id": "996bd921", "metadata": {}, "outputs": [], "source": ["num_right = (np.sign(y_test) == np.sign(y_pred).squeeze()).sum()\nnum_right/len(y_test)"]}, {"cell_type": "markdown", "id": "359eff6d", "metadata": {}, "source": ["After removing randomness in spliting train and test samples, the model accuracy becomes <font color=red>drastically worse</font>. Close to random guess!"]}, {"cell_type": "markdown", "id": "899ab6bf", "metadata": {}, "source": ["# Predict into unknown future (use predicted output as input) \n\nFor this to work, the output must have same shape as the input and lookahead must be 1 -- so the output can be used as input. Need to modify model."]}, {"cell_type": "markdown", "id": "018c4c60", "metadata": {}, "source": ["## Still train model with 1 day lookahead"]}, {"cell_type": "code", "execution_count": 1, "id": "7c558983", "metadata": {}, "outputs": [], "source": ["lookback = 120\nlookahead = 1\n\n# Create sequence samples\nfeatures = ['open', 'close', 'low', 'high', 'volume', 'dist_EMA20', 'dist_EMA5', 'market']\ndata = apple[features].values\nX = np.array([data[i:i+lookback].copy() for i in range(len(data) - lookback - lookahead)])\ny = np.array([data[i+lookback+lookahead-1] for i in range(len(data) - lookback - lookahead)])\n\n# must normalize y first\ny[:,0:4] = y[:,0:4]/X[:,0,0, None]\ny[:,4] = y[:,4]/X[:,0,4]\ny[:,-1] = y[:,-1]/X[:,0,-1]\n\nX[:,:,0:4] = X[:,:,0:4]/X[:,0,0, None, None]\nX[:,:,4] = X[:,:,4]/X[:,0,None,4]\nX[:,:,-1] = X[:,:,-1]/X[:,0,None,-1]"]}, {"cell_type": "code", "execution_count": 1, "id": "3734ebd6", "metadata": {}, "outputs": [], "source": ["lstm_input = Input(shape=(lookback, len(features)), name='lstm_input')\nx = LSTM(units=64, return_sequences=False, return_state=False)(lstm_input)  \nx = Dropout(0.2)(x)\nx = Dense(units=32, activation='relu')(x)\noutput = Dense(8, activation='linear')(x)\nmodel_1 = Model(inputs=lstm_input, outputs=output)\n\nadam = optimizers.Adam(lr=0.0005)\n\nmodel_1.compile(optimizer=adam, loss='mse')\n\nmodel_1.summary()\nfrom keras.utils import plot_model\nplot_model(model_1) #, to_file='model.png')"]}, {"cell_type": "code", "execution_count": 1, "id": "d08aacde", "metadata": {}, "outputs": [], "source": ["test_size = 0.2\nk = np.round(len(X)*(1-test_size)).astype(int)\nX_train, X_test, y_train, y_test = X[:k,:,:], X[k:,:,:], y[:k], y[k:]\nprint(\"{} train samples, {} test samples\".format(X_train.shape[0], X_test.shape[0]))"]}, {"cell_type": "code", "execution_count": 1, "id": "0cdd58ff", "metadata": {}, "outputs": [], "source": ["train_history = model_1.fit(X_train, y_train, epochs=40, verbose=0, validation_data=(X_test, y_test))"]}, {"cell_type": "markdown", "id": "812b0c2c", "metadata": {}, "source": ["## apply model to predict 328 days' price movement in a row"]}, {"cell_type": "code", "execution_count": 1, "id": "52580aa6", "metadata": {}, "outputs": [], "source": ["# function to put the new output at the end of input\ndef insert_end(Xin,new_input):\n    for i in range(lookback-1):\n        Xin[:,i,:] = Xin[:,i+1,:]\n    Xin[:,lookback-1,:] = new_input\n    return Xin"]}, {"cell_type": "code", "execution_count": 1, "id": "576c54db", "metadata": {}, "outputs": [], "source": ["first =0   \nfuture=X_test.shape[0]\nforcast = []\nXin = X_test[first:first+1,:,:]\nfor i in range(future):\n    out = model_1.predict(Xin, batch_size=1)    \n    forcast.append(out) \n    Xin = insert_end(Xin,out) \n\nforcast_output = np.array(forcast).squeeze()"]}, {"cell_type": "code", "execution_count": 1, "id": "a2c76216", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(10,5))\nplt.plot(y_test[:,1] , 'black', linewidth=4)\nplt.plot(forcast_output[:,1],'r' , linewidth=4)\nplt.legend(('close, test','close, Forcasted'))\nplt.show()"]}, {"cell_type": "markdown", "id": "57fc603f", "metadata": {}, "source": ["Apparently, predicting many days of price movement does NOT work well. "]}, {"cell_type": "markdown", "id": "c57b4cdd", "metadata": {}, "source": ["## apply model to predict 1 day at a time"]}, {"cell_type": "code", "execution_count": 1, "id": "26f6711e", "metadata": {}, "outputs": [], "source": ["y_pred = model_1.predict(X_test)\n\nfor i in range(y_test.shape[1]):\n    print(r2_score(y_test[:, i], y_pred[:, i]))"]}, {"cell_type": "code", "execution_count": 1, "id": "bbf4f2e2", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(10,5))\nplt.plot(y_test[:,1] , 'black', linewidth=4)\nplt.plot(y_pred[:,1],'r' , linewidth=4)\nplt.legend(('close, test','close, Forcasted'))\nplt.show()"]}, {"cell_type": "markdown", "id": "02d9ad89", "metadata": {}, "source": ["The R2 scores are not too bade, and two curves seem to follow the same overall trend. But don't let them fool you. "]}, {"cell_type": "code", "execution_count": 1, "id": "1e5ec0a9", "metadata": {}, "outputs": [], "source": ["# calculate the changes relative the prior day\ny_test_change = y_test - X_test[:, -1, :]\ny_pred_change = y_pred - X_test[:, -1, :]\n\nplt.figure(figsize=(10,5))\nplt.plot(y_test_change[:,1] , 'black', linewidth=4)\nplt.plot(y_pred_change[:,1],'r' , linewidth=4)\nplt.legend(('test','Forcasted'))\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "80faaa31", "metadata": {}, "outputs": [], "source": ["y_test_close_up = y_test_change[:,1] > 0\ny_pred_close_up = y_pred_change[:,1] > 0\nprint(\"number of close up days, actural = {}, number of close up days, predicted = {}\".format(y_test_close_up.sum(), y_pred_close_up.sum()))\naccuracy = (y_pred_close_up == y_test_close_up).sum() / len(y_pred_close_up)\nprint(\"percentage of actual and predict have same direction = {:2.2%}\".format(accuracy))"]}, {"cell_type": "markdown", "id": "29b74480", "metadata": {}, "source": ["The accuray is close to 50% --> No better than random guess! "]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}