{"cells": [{"cell_type": "code", "execution_count": 1, "id": "d5533858", "metadata": {}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]}, {"cell_type": "code", "execution_count": 1, "id": "48b9a7ca", "metadata": {}, "outputs": [], "source": ["#for data wrangling and manipulation\n\nimport pandas as pd\nimport numpy as np\n\n#for NLP text processing and formatting\n\nimport re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n\n# For word lemmitization\nimport nltk\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\n\n# for word Stemming\nfrom nltk.stem.porter import PorterStemmer\n\n# for Machine Learning process\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\n# for Machine Learning model evaluation\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\n\n# Global Parameters\nstop_words = set(stopwords.words('english'))"]}, {"cell_type": "markdown", "id": "06581eec", "metadata": {}, "source": ["# Defining a function which will process the tweets before using it in the models"]}, {"cell_type": "code", "execution_count": 1, "id": "90e553ef", "metadata": {}, "outputs": [], "source": ["def preprocess_tweet_text(tweet):\n    \"\"\"\n    Function to process the the tweet text and tranform it into format usable by Machine learning models\n    \"\"\"\n    \n    # to convert all the characters of the tweet into lower case alphabets\n    tweet.lower()\n    \n    # Remove urls from the tweets\n    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)\n    \n    # Remove user related references from the tweets:: '@' and '#' \n    tweet = re.sub(r'\\@\\w+|\\#','', tweet)\n    \n    # Remove punctuations from the tweets\n    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n    \n    # Remove stopwords from the tweets\n    tweet_tokens = word_tokenize(tweet)\n    filtered_words = [w for w in tweet_tokens if not w in stop_words]\n    joined_text = \" \".join(filtered_words)\n    \n    return joined_text"]}, {"cell_type": "markdown", "id": "8dc3af4c", "metadata": {}, "source": ["# Defining Vectorizer  to convert the string into corresponding TF IDF vector matrix"]}, {"cell_type": "code", "execution_count": 1, "id": "aad901fa", "metadata": {}, "outputs": [], "source": ["def get_feature_vector(train_fit):\n    \"\"\"\n    Function to Convert a collection of raw documents to a matrix of TF-IDF features.\n    TF-IDF - Term Frequency Inverse Documnet Frequency\n    \"\"\"\n    \n    vector = TfidfVectorizer(sublinear_tf=True)      # Defining the vector\n    vector.fit(train_fit)                            # fitting the data into the vector\n    return vector                                    # returning the vector as function call"]}, {"cell_type": "code", "execution_count": 1, "id": "3f222fd9", "metadata": {}, "outputs": [], "source": ["# read data\ndataset = pd.read_csv(\"../input/sentiment-analysis-of-tweets/train.txt\",  sep = \",\")\nprint(\"Train Data has been read\")\ntest = pd.read_csv(\"../input/sentiment-analysis-of-tweets/test_samples.txt\",  sep = \",\")\nprint(\"Test Data has been read\")"]}, {"cell_type": "code", "execution_count": 1, "id": "7569b7d6", "metadata": {}, "outputs": [], "source": ["# Preprocessing data before feeding it to ML models\n\nprocessed_text = dataset['tweet_text'].apply(preprocess_tweet_text)\n\nprint(\"Processed text :: \\n\\n\", processed_text)"]}, {"cell_type": "markdown", "id": "16079de0", "metadata": {}, "source": ["# **Stemming ::**\n\n    It may be defined as the process to remove the inflectional forms of a word and bring them to a base form called the stem.\n\n    The chopped-off pieces are referred to as affixes\n\n    The two most common algorithms/methods employed for stemming include the ::\n*         Porter Stemmer\n*         Snowball Stemmer\n\nWe will be using **Porter Stemmer** in our process.\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "046772bf", "metadata": {}, "outputs": [], "source": ["stemmer = PorterStemmer()\n\nstemmed_words = [stemmer.stem(i) for i in processed_text]"]}, {"cell_type": "markdown", "id": "b1fc4e55", "metadata": {}, "source": ["# Lemmatization ::\n\n    It is a process wherein the context is used to convert a word to its meaningful base form.\n    It helps in grouping together words that have a common base form and so can be identified as a single item.\n    The base form is referred to as the lemma of the word and is also sometimes known as the dictionary form.\n    The most commonly used lemmatizers are the\n1.         WordNet Lemmatizer\n1.         Spacy Lemmatizer\n1.         TextBlob Lemmatizer\n\nWe will be using **WordNet Lemmatizer** in our process.\n"]}, {"cell_type": "code", "execution_count": 1, "id": "db8d51fb", "metadata": {}, "outputs": [], "source": ["lemmatizer = WordNetLemmatizer()\nlemma_words = [lemmatizer.lemmatize(w, pos='a') for w in stemmed_words]\n"]}, {"cell_type": "markdown", "id": "a25fcf91", "metadata": {}, "source": ["# Vectorization ::\n\nProcessing natural language text and extract useful information from the given word or a sentence using machine learning and deep learning techniques requires the string/text needs to be converted into a set of real numbers (a vector) \u2014 Word Embeddings.\n\nWord Embeddings or Word vectorization is a methodology in NLP to map words or phrases from vocabulary to a corresponding vector of real numbers which used to find word predictions, word similarities/semantics.\n\nThe process of converting words into numbers are called **Vectorization**\n"]}, {"cell_type": "code", "execution_count": 1, "id": "0a39ef8e", "metadata": {}, "outputs": [], "source": ["tf_vector = get_feature_vector(np.array(dataset[\"tweet_text\"]).ravel())"]}, {"cell_type": "markdown", "id": "8e39e37d", "metadata": {}, "source": ["# Defining the Variables :"]}, {"cell_type": "code", "execution_count": 1, "id": "226f6fbc", "metadata": {}, "outputs": [], "source": ["X = tf_vector.transform(np.array(dataset[\"tweet_text\"]).ravel())     # Predictor Variable\ny = np.array(dataset[\"sentiment\"]).ravel()                           # Target varaible"]}, {"cell_type": "code", "execution_count": 1, "id": "8ed72787", "metadata": {}, "outputs": [], "source": ["# SPlitting the data into training and testing data\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=30)"]}, {"cell_type": "code", "execution_count": 1, "id": "4f171ef3", "metadata": {}, "outputs": [], "source": ["# Using Naive Bayes Model :\n\nNB_model = MultinomialNB()\n\nNB_model.fit(X_train, y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "d96efc6a", "metadata": {}, "outputs": [], "source": ["# Predicting the values and the Accuracy Score\n\ny_predict_nb = NB_model.predict(X_test)\n\nprint(\"Accuracy Score for Naive Bayes Model is :: \", accuracy_score(y_test, y_predict_nb))"]}, {"cell_type": "code", "execution_count": 1, "id": "c9840c5a", "metadata": {}, "outputs": [], "source": ["# Classification Report :\n\nprint(\"Classification_Report :: \\n\\n\", classification_report(y_test, y_predict_nb))"]}, {"cell_type": "markdown", "id": "a0efc200", "metadata": {}, "source": ["# Using Logistic Regression Model :"]}, {"cell_type": "code", "execution_count": 1, "id": "3b90253e", "metadata": {}, "outputs": [], "source": ["# Training Logistics Regression model\nLR_model = LogisticRegression(solver='lbfgs')\nLR_model.fit(X_train, y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "f62c7c56", "metadata": {}, "outputs": [], "source": ["# Predicting the Values :\n\ny_predict_lr = LR_model.predict(X_test)\n\nprint(\"Accuracy Score for Logistic Regression Model is :: \",accuracy_score(y_test, y_predict_lr))"]}, {"cell_type": "code", "execution_count": 1, "id": "bc4ce79b", "metadata": {}, "outputs": [], "source": ["# Classification Report\n\nfrom sklearn.metrics import classification_report\n\nprint(\"Classification_Report :: \\n\\n\", classification_report(y_test, y_predict_lr))"]}, {"cell_type": "code", "execution_count": 1, "id": "d001339a", "metadata": {}, "outputs": [], "source": ["# Creating text feature of test data :\n\ntest.tweet_text = test[\"tweet_text\"].apply(preprocess_tweet_text)\n\ntest_feature = tf_vector.transform(np.array(test['tweet_text']).ravel())\n"]}, {"cell_type": "code", "execution_count": 1, "id": "9fd17ec3", "metadata": {}, "outputs": [], "source": ["# Using Naive Bayes Model for Prediction ::\n\ntest_prediction_nb = NB_model.predict(test_feature)\n\ntest_prediction_nb"]}, {"cell_type": "code", "execution_count": 1, "id": "a48993fa", "metadata": {}, "outputs": [], "source": ["# Creating a Dataframe consising tweets and sentiment in a submission format\n\nsubmission_result_nb = pd.DataFrame({'tweet_id': test.tweet_id, 'sentiment':test_prediction_nb})\nsubmission_result_nb"]}, {"cell_type": "code", "execution_count": 1, "id": "1ccd2160", "metadata": {}, "outputs": [], "source": ["# Total number os tweets grouped according sentiment\n\ntest_result = submission_result_nb['sentiment'].value_counts()\ntest_result"]}, {"cell_type": "code", "execution_count": 1, "id": "5f1dcb0d", "metadata": {}, "outputs": [], "source": ["#Using Logistic Regression Model for Prediction ::\n\ntest_prediction_lr = LR_model.predict(test_feature)\n\ntest_prediction_lr"]}, {"cell_type": "code", "execution_count": 1, "id": "90c189c0", "metadata": {}, "outputs": [], "source": ["# Creating a Dataframe consising tweets and sentiment\n\nsubmission_result_lr = pd.DataFrame({'tweet_id': test.tweet_id, 'sentiment':test_prediction_lr})\nsubmission_result_lr"]}, {"cell_type": "code", "execution_count": 1, "id": "50d0620a", "metadata": {}, "outputs": [], "source": ["# Total number os tweets grouped according sentiment\n\ntest_result2 = submission_result_lr['sentiment'].value_counts()\ntest_result2"]}, {"cell_type": "markdown", "id": "2a797a85", "metadata": {}, "source": ["# Visualizing the data :: "]}, {"cell_type": "code", "execution_count": 1, "id": "49a3836f", "metadata": {}, "outputs": [], "source": ["import seaborn as sns\nsns.countplot(submission_result_lr['sentiment'])"]}, {"cell_type": "code", "execution_count": 1, "id": "8bc3690a", "metadata": {}, "outputs": [], "source": ["import seaborn as sns\nsns.countplot(submission_result_nb['sentiment'])"]}, {"cell_type": "markdown", "id": "dbe7d3f5", "metadata": {}, "source": ["# **Logistic Regression** model gives an accuracy score of 0.64103\n# **Naive Bayes** model gives an accuracy score of 0.60050"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}