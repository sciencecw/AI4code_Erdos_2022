{"cells": [{"cell_type": "code", "execution_count": 1, "id": "57ce4ff0", "metadata": {}, "outputs": [], "source": ["! pip install tf-models-official==2.4.0 -q\n! pip install tensorflow-gpu==2.4.1 -q\n! pip install tensorflow-text==2.4.1 -q\n! python -m spacy download en_core_web_sm -q\n! pip install dataprep | grep -v 'already satisfied'"]}, {"cell_type": "code", "execution_count": 1, "id": "8c2a7b67", "metadata": {}, "outputs": [], "source": ["import pandas as pd\nimport numpy as np\nnp.set_printoptions(precision=4)\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom dataprep.eda import plot, plot_diff, plot_correlation, create_report\nfrom dataprep.clean import clean_text\n\n# Preprocessing and Modelling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport spacy\nimport tensorflow_text as text\nimport tensorflow_hub as hub\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Dropout, concatenate \nfrom tensorflow.keras import Model, regularizers \nfrom tensorflow.keras.metrics import BinaryAccuracy\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom official.nlp.optimization import create_optimizer # AdamW optimizer\n# Warning\nimport warnings\nwarnings.filterwarnings('ignore')"]}, {"cell_type": "code", "execution_count": 1, "id": "70927cf5", "metadata": {}, "outputs": [], "source": ["tf.__version__"]}, {"cell_type": "code", "execution_count": 1, "id": "7644aaa1", "metadata": {}, "outputs": [], "source": ["# Random seeds\nimport random\nimport numpy as np\nimport tensorflow as tf\nrandom.seed(319)\nnp.random.seed(319)\ntf.random.set_seed(319)"]}, {"cell_type": "markdown", "id": "0fc4b419", "metadata": {}, "source": ["<a id=0></a>\n## <p style=\"background-color:lightblue; font-family:newtimeroman; font-size:120%; text-align:left; border-radius: 15px 50px;\">Table of Content</p>\n* [0. Introduction and updates](#0)\n* [1. Loading Data \ud83d\udc8e](#1)\n* [2. EDA \ud83d\udcca](#2)\n* [3. Data Preprocessing](#3)\n* [4. Vectorization](#4)\n    * [4.1 Common Vectorizer Usage](#4.1)\n    * [4.2 If-Idf Term Weightings](#4.2)\n* [5. Transfer Learning with Hugging Face](#5)\n    * [5.1 Tokenization](#5.1)\n    * [5.2 Defining a Model Architecture](#5.2)\n    * [5.3 Training Classification Layer Weights](#5.3)\n    * [5.4 Fine-tuning DistilBert and Training All Weights](#5.4)\n* [6. Make a Submission](#6)\n* [7. References](#7)"]}, {"cell_type": "markdown", "id": "6fa166ad", "metadata": {}, "source": ["<a id=0></a>\n<font size=\"+3\" color=\"#5bc0de\"><b>Introduction </b></font><br>\n[Content](#0)\n\nIn this kernel, beside the general steps working with text data as EDA, preprocessing. The workflow in Modelling can divided into 2 main stages:\n1. Defining a Model Architecture with concatenation a keyword column into BERT model\n2. Training Classification Layer Weights.\n\n<a id=1.2 ></a>\n<font size=\"+3\" color=\"#5bc0de\"><b>1.2. Update via Versions </b></font><br>\n[Content](#0)\n\n### Current Version\n* Adding 1 hidden layer in Model to incease accuracy.\n\n\n[Content](#0)"]}, {"cell_type": "markdown", "id": "154c2784", "metadata": {}, "source": ["<a id='1'></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">1. Loading Data \ud83d\udc8e</p>\n\nJust load the dataset and global variables for colors and so on.\n\n[Content](#0)"]}, {"cell_type": "code", "execution_count": 1, "id": "bf773cdd", "metadata": {}, "outputs": [], "source": ["train_full = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_full = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n\nprint('Training Set Shape = {}'.format(train_full.shape))\nprint('Training Set Memory Usage = {:.2f}MB'.format(train_full.memory_usage().sum()/2**20))\n\nprint('Test Set Shape = {}'.format(test_full.shape))\nprint('Test Set Memory Usage = {:.2f}MB'.format(test_full.memory_usage().sum()/2**20))"]}, {"cell_type": "markdown", "id": "28f2ea01", "metadata": {}, "source": ["<a id='2'></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">2. EDA \ud83d\udcca</p>\n\n\n[Content](#0)"]}, {"cell_type": "code", "execution_count": 1, "id": "a8391a81", "metadata": {}, "outputs": [], "source": ["plot(train_full)"]}, {"cell_type": "code", "execution_count": 1, "id": "f8725e89", "metadata": {}, "outputs": [], "source": ["create_report(train_full)"]}, {"cell_type": "code", "execution_count": 1, "id": "c25bca10", "metadata": {}, "outputs": [], "source": ["plot(train_full, 'text')"]}, {"cell_type": "code", "execution_count": 1, "id": "7e0aa115", "metadata": {}, "outputs": [], "source": ["train_full.text"]}, {"cell_type": "markdown", "id": "fc4e7e0a", "metadata": {}, "source": ["### Range from 120 to 140 characters is the most common in tweet."]}, {"cell_type": "markdown", "id": "8c2f754c", "metadata": {}, "source": ["### Dataset is balanced"]}, {"cell_type": "code", "execution_count": 1, "id": "30b83acc", "metadata": {}, "outputs": [], "source": ["plot(train_full, \"text\", \"target\")"]}, {"cell_type": "code", "execution_count": 1, "id": "0d4942e4", "metadata": {}, "outputs": [], "source": ["df1 = train_full.text[train_full.target == 0]\ndf2 = train_full.text[train_full.target == 1]\nplot_diff([df1, df2])"]}, {"cell_type": "markdown", "id": "9e373845", "metadata": {}, "source": ["<a id='3'></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">3. Data Pre-processing </p>\n\nNow we are going to engineering the data to make it easier for the model to clasiffy.\n\nThis section is very important to reduce the dimensions of the problem.\n\n\n\n\n[Content](#0)"]}, {"cell_type": "markdown", "id": "8b0fc391", "metadata": {}, "source": ["# Main technics I used in this data\n    * [3.1] Remove 157 duplicated rows\n    * [3.2] Cleaning text\n    * [3.3] Spelling Checker\n    * [3.4] Remove Stemming\n #### Step 3.3 spends a lot time (around 4000s in 4536s in total). \n #### So, I splits Data Preprocessing into [another kernel](https://www.kaggle.com/phanttan/disastertweet-prepareddata). \n #### And the prepared data to save in to [new dataset](https://www.kaggle.com/phanttan/disastertweet-prepared2)\n #### I am so appreciate to you for using/upvoting it.\n"]}, {"cell_type": "code", "execution_count": 1, "id": "cb5346a6", "metadata": {}, "outputs": [], "source": ["# Read commited-dataset\ndf_train = pd.read_csv(\"/kaggle/input/disastertweet-prepared2/train_prepared.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/disastertweet-prepared2/test_prepared.csv\")"]}, {"cell_type": "code", "execution_count": 1, "id": "3277d9a8", "metadata": {}, "outputs": [], "source": ["# Only apply 'keyword' columns in full data, because other features cleaned in df_train/test\ntrain_full = clean_text(train_full,'keyword')\ntest_full = clean_text(test_full, 'keyword')"]}, {"cell_type": "code", "execution_count": 1, "id": "da58aafd", "metadata": {}, "outputs": [], "source": ["# Adding cleaned data into df_train/test\ndf_train['keyword'] = train_full['keyword']\ndf_test['keyword'] = test_full['keyword']"]}, {"cell_type": "code", "execution_count": 1, "id": "69604267", "metadata": {}, "outputs": [], "source": ["# Load Spacy Library\nnlp_spacy = spacy.load('en_core_web_sm')\n# Load the sentence encoder\nsentence_enc = hub.load('https://tfhub.dev/google/universal-sentence-encoder/4')"]}, {"cell_type": "code", "execution_count": 1, "id": "e47bce1d", "metadata": {}, "outputs": [], "source": ["def extract_keywords(text):\n    potential_keywords = []\n    TOP_KEYWORD = -1\n    # Create a list for keyword parts of speech\n    pos_tag = ['ADJ', 'NOUN', 'PROPN']\n    doc = nlp_spacy(text)\n    \n    for i in doc:\n        if i.pos_ in pos_tag:\n            potential_keywords.append(i.text)\n\n    document_embed = sentence_enc([text])\n    potential_embed = sentence_enc(potential_keywords)    \n    \n    vector_distances = cosine_similarity(document_embed, potential_embed)\n    keyword = [potential_keywords[i] for i in vector_distances.argsort()[0][TOP_KEYWORD:]]\n\n    return keyword\n\ndef keyword_filler(keyword, text):\n    if pd.isnull(keyword):\n        try:\n            keyword = extract_keywords(text)[0]\n        except:\n            keyword = '' \n        \n    return keyword"]}, {"cell_type": "code", "execution_count": 1, "id": "2483c880", "metadata": {}, "outputs": [], "source": ["df_train.keyword = pd.DataFrame(list(map(keyword_filler, df_train.keyword, df_train.text))).astype(str)\ndf_test.keyword = pd.DataFrame(list(map(keyword_filler, df_test.keyword, df_test.text))).astype(str)\n\nprint('Null Training Keywords => ', df_train['keyword'].isnull().any())\nprint('Null Test Keywords => ', df_test['keyword'].isnull().any())"]}, {"cell_type": "code", "execution_count": 1, "id": "41d7df75", "metadata": {}, "outputs": [], "source": ["df_train"]}, {"cell_type": "markdown", "id": "173bc7f4", "metadata": {}, "source": ["# Visualization the Keyword Frequency"]}, {"cell_type": "code", "execution_count": 1, "id": "d1cb5ac8", "metadata": {}, "outputs": [], "source": ["keyword_non_disaster = df_train.keyword[df_train.target==0].value_counts().reset_index()\nsns.barplot(data=keyword_non_disaster[:10], x='keyword', y='index')\nplt.title('Non-Disaster Keyword Frequency (0)')\nplt.xlabel('Frequency')\nplt.ylabel('Top 10 Keywords')\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "34f3148b", "metadata": {}, "outputs": [], "source": ["keyword_disaster = df_train.keyword[df_train.target==1].value_counts().reset_index()\nsns.barplot(data=keyword_non_disaster[:10], x='keyword', y='index')\nplt.title('Non-Disaster Keyword Frequency (0)')\nplt.xlabel('Frequency')\nplt.ylabel('Top 10 Keywords')\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "52b66674", "metadata": {}, "outputs": [], "source": ["# Spilt data\nX_train, X_val, y_train, y_val = train_test_split(df_train[['text','keyword']],\n                                                    df_train.target, \n                                                    test_size=0.2, \n                                                    random_state=42)\nX_train.shape, X_val.shape"]}, {"cell_type": "markdown", "id": "cbb496f8", "metadata": {}, "source": ["# Create TensorFlow Datasets"]}, {"cell_type": "code", "execution_count": 1, "id": "4d029d8a", "metadata": {}, "outputs": [], "source": ["train_ds = tf.data.Dataset.from_tensor_slices((dict(X_train), y_train))\nval_ds = tf.data.Dataset.from_tensor_slices((dict(X_val), y_val))\ntest_ds = tf.data.Dataset.from_tensor_slices(dict(df_test[['text','keyword']]))"]}, {"cell_type": "code", "execution_count": 1, "id": "519afa9d", "metadata": {}, "outputs": [], "source": ["AUTOTUNE = tf.data.experimental.AUTOTUNE\n\nBUFFER_SIZE = 1000\nBATCH_SIZE = 32\nRANDOM_SEED = 319\n\ndef configure_dataset(dataset, shuffle=False, test=False):\n    if shuffle:\n        dataset = dataset.cache()\\\n                        .shuffle(BUFFER_SIZE, seed=RANDOM_SEED, reshuffle_each_iteration=True)\\\n                        .batch(BATCH_SIZE, drop_remainder=True)\\\n                        .prefetch(AUTOTUNE)\n    elif test:\n        dataset = dataset.cache()\\\n                        .batch(BATCH_SIZE, drop_remainder=False)\\\n                        .prefetch(AUTOTUNE)\n    else:\n        dataset = dataset.cache()\\\n                        .batch(BATCH_SIZE, drop_remainder=True)\\\n                        .prefetch(AUTOTUNE)\n    return dataset"]}, {"cell_type": "code", "execution_count": 1, "id": "111278dc", "metadata": {}, "outputs": [], "source": ["a3 = configure_dataset(train_ds, shuffle=True)\ndict3 = []\nfor elem in a3:\n    dict3.append(elem[0]['text'][0])\ndict3[:10]"]}, {"cell_type": "code", "execution_count": 1, "id": "7f145e2c", "metadata": {}, "outputs": [], "source": ["# Configure the datasets\ntrain_ds = configure_dataset(train_ds, shuffle=True)\nval_ds = configure_dataset(val_ds)\ntest_ds = configure_dataset(test_ds, test=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "de9b970e", "metadata": {}, "outputs": [], "source": ["# Free memory\ndel X_train, X_val, y_train, y_val, df_train, df_test, train_full, test_full"]}, {"cell_type": "markdown", "id": "45bd3179", "metadata": {}, "source": ["# Classifier Model"]}, {"cell_type": "code", "execution_count": 1, "id": "b8366f59", "metadata": {}, "outputs": [], "source": ["# Bidirectional Encoder Representations from Transformers (BERT).\nbert_encoder_path = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\"\n# Text preprocessing for BERT.\nbert_preprocessor_path = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n# Token based text embedding trained on English Google News 200B corpus.\nkeyword_embedding_path = \"https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2\""]}, {"cell_type": "code", "execution_count": 1, "id": "165c973e", "metadata": {}, "outputs": [], "source": ["bert_encoder = hub.KerasLayer(bert_encoder_path, trainable=True, name=\"BERT_Encoder\")\nbert_preprocessor = hub.KerasLayer(bert_preprocessor_path, name=\"BERT_Preprocessor\")\nnnlm_embed = hub.KerasLayer(keyword_embedding_path, name=\"NNLM_Embedding\")"]}, {"cell_type": "code", "execution_count": 1, "id": "9e4a714b", "metadata": {}, "outputs": [], "source": ["kernel_initializer = tf.keras.initializers.GlorotNormal(seed=319)\n# Model function\ndef create_model():\n    # Keyword Branch\n    text_input = Input(shape=(), dtype=tf.string, name=\"text\")\n    encoder_inputs = bert_preprocessor(text_input)\n    encoder_outputs = bert_encoder(encoder_inputs)\n    # Pooled output\n    pooled_output = encoder_outputs[\"pooled_output\"]\n    bert_branch = Dropout(0.1,\n                          seed=319,\n                          name=\"BERT_Dropout\")(pooled_output)\n    # Construct keyword layers\n    keyword_input = Input(shape=(), dtype=tf.string, name='keyword')\n    keyword_embed = nnlm_embed(keyword_input)\n    keyword_flat = Flatten(name=\"Keyword_Flatten\")(keyword_embed)\n    keyword_dense1 = Dense(128, \n                          activation='relu',\n                          kernel_initializer=kernel_initializer,\n                          kernel_regularizer=regularizers.l2(1e-4),\n                          name=\"Keyword_Dense1\"\n                         )(keyword_flat)\n    keyword_branch1 = Dropout(0.5,\n                             seed=319,\n                             name='Keyword_dropout1'\n                            )(keyword_dense1)\n    keyword_dense2 = Dense(128, \n                          activation='relu',\n                          kernel_initializer=kernel_initializer,\n                          kernel_regularizer=regularizers.l2(1e-4),\n                          name=\"Keyword_Dense2\"\n                         )(keyword_branch1)\n    keyword_branch2 = Dropout(0.5,\n                             seed=319,\n                             name='Keyword_dropout2'\n                            )(keyword_dense2)\n    keyword_dense3 = Dense(128, \n                          activation='relu',\n                          kernel_initializer=kernel_initializer,\n                          kernel_regularizer=regularizers.l2(1e-4),\n                          name=\"Keyword_Dense3\"\n                         )(keyword_branch2)\n    keyword_branch3 = Dropout(0.5,\n                             seed=319,\n                             name='Keyword_dropout3'\n                            )(keyword_dense3)\n    \n    # Merge the layers and classify\n    merge = concatenate([bert_branch, keyword_branch3], name=\"Concatenate\")\n    dense = Dense(128, \n                  activation='relu',\n                  kernel_initializer=kernel_initializer,\n                  kernel_regularizer=regularizers.l2(1e-4), \n                  name=\"Merged_Dense\")(merge)\n    dropout = Dropout(0.5,\n                      seed=319,\n                      name=\"Merged_Dropout\"\n                     )(dense)\n    clf = Dense(1,\n                activation=\"sigmoid\", \n                kernel_initializer=kernel_initializer,\n                name=\"Classifier\"\n               )(dropout)\n    return Model([text_input, keyword_input], \n                 clf, \n                 name=\"BERT_Classifier\")"]}, {"cell_type": "code", "execution_count": 1, "id": "61d1279b", "metadata": {}, "outputs": [], "source": ["bert_classifier = create_model()\nbert_classifier.summary()"]}, {"cell_type": "code", "execution_count": 1, "id": "13ed21d9", "metadata": {}, "outputs": [], "source": ["keras.utils.plot_model(bert_classifier, \n                      show_shapes=False)"]}, {"cell_type": "markdown", "id": "0ecd7922", "metadata": {}, "source": ["# AdamW Optimizer"]}, {"cell_type": "code", "execution_count": 1, "id": "153ce88c", "metadata": {}, "outputs": [], "source": ["EPOCHS = 3\nLEARNING_RATE = 5e-5\n\nSTEPS_PER_EPOCH = int(train_ds.unbatch().cardinality().numpy() / BATCH_SIZE)\nVAL_STEPS = int(val_ds.unbatch().cardinality().numpy() / BATCH_SIZE)\n# Calculate the train and warmup steps for the optimizer\nTRAIN_STEPS = STEPS_PER_EPOCH * EPOCHS\nWARMUP_STEPS = int(TRAIN_STEPS * 0.1)\n\nadamw_optimizer = create_optimizer(\n    init_lr=LEARNING_RATE,\n    num_train_steps=TRAIN_STEPS,\n    num_warmup_steps=WARMUP_STEPS,\n    optimizer_type='adamw'\n)"]}, {"cell_type": "code", "execution_count": 1, "id": "c01adfd3", "metadata": {}, "outputs": [], "source": ["STEPS_PER_EPOCH, VAL_STEPS, TRAIN_STEPS, WARMUP_STEPS"]}, {"cell_type": "markdown", "id": "419bf400", "metadata": {}, "source": ["## Training the model again"]}, {"cell_type": "code", "execution_count": 1, "id": "9b44425a", "metadata": {}, "outputs": [], "source": ["bert_classifier.compile(loss=BinaryCrossentropy(from_logits=True),\n                   optimizer=adamw_optimizer, \n                   metrics=[BinaryAccuracy(name=\"accuracy\")]\n                  )\nhistory = bert_classifier.fit(train_ds, \n                         epochs=EPOCHS,\n                         steps_per_epoch=STEPS_PER_EPOCH,\n                         validation_data=val_ds,\n                         validation_steps=VAL_STEPS\n                        )"]}, {"cell_type": "markdown", "id": "d00fdf0f", "metadata": {}, "source": ["<a id=6 ></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">6. Make a Submission</p>\n\n[Content](#0)"]}, {"cell_type": "code", "execution_count": 1, "id": "e60e8484", "metadata": {}, "outputs": [], "source": ["def submission(model, test):\n    sample_sub = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\n    predictions =  model.predict(test)\n    y_preds = [ int(i) for i in np.rint(predictions)]\n    sub = pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_preds})\n    sub.to_csv('submission.csv', index=False)"]}, {"cell_type": "code", "execution_count": 1, "id": "74399d71", "metadata": {}, "outputs": [], "source": ["submission(bert_classifier, test_ds)"]}, {"cell_type": "markdown", "id": "1bed1e46", "metadata": {}, "source": ["<a id=7 ></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">7. References</p>\n\n[Content](#0)"]}, {"cell_type": "markdown", "id": "5d8cb7c9", "metadata": {}, "source": ["[Hugging Face Transformers Fine-Tunning DistilBert for Binary Classification Tasks](https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379)\n\n[Keras functional API](https://keras.io/guides/functional_api/)\n\n[Distil Bert](https://huggingface.co/transformers/model_doc/distilbert.html)\n\n[Tensorflow Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)\n\n[BERT in TFHub](https://tfhub.dev/google/collections/bert)\n\n[TensorFlow NLP Modelling Toolkit](https://github.com/tensorflow/models/tree/master/official/nlp)\n\n[NLP With BERT from Tendorflow](https://www.tensorflow.org/text/tutorials/fine_tune_bert)\n\n[NLP Optimization](https://github.com/tensorflow/models/blob/master/official/nlp/optimization.py)"]}, {"cell_type": "markdown", "id": "7b7df243", "metadata": {}, "source": ["# If you like this kernel, please upvote and tell me your thought. Thank you @@"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}