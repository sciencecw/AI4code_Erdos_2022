{"cells": [{"cell_type": "markdown", "id": "cc337dc3", "metadata": {}, "source": ["# Dataset [River Water Quality EDA and Forecasting](https://www.kaggle.com/vbmokin/wq-southern-bug-river-01052021)"]}, {"cell_type": "markdown", "id": "8e0ae12f", "metadata": {}, "source": ["Dataset contains data on river water quality for 8 indicators for 22 monitoring stations.\n\nData for 2000-2021 for the Southern Bug (or Pivdennyi Booh) river."]}, {"cell_type": "markdown", "id": "f192b8fb", "metadata": {}, "source": ["### Possible Tasks:\n\n1. Analysis of data dependences, including EDA.\n\n2. Prediction of the data in the certain station by data from upstream stations with the highest accuracy."]}, {"cell_type": "markdown", "id": "2d575499", "metadata": {}, "source": ["### Map of the stations:\nhttp://monitoring.davr.gov.ua/EcoWaterMon/GDKMap/Index\n\n![image.png](attachment:7d210839-9bcd-46a8-a58d-ce90f29fb294.png)\n\nThe water quality state monitoring stations of the Southern Bug (or Pivdennyi Booh) river."]}, {"cell_type": "markdown", "id": "5dcdd0fc", "metadata": {}, "source": ["## Acknowledgements\n* [Data Science for tabular data: Advanced Techniques](https://www.kaggle.com/vbmokin/data-science-for-tabular-data-advanced-techniques)\n* [EDA for tabular data: Advanced Techniques](https://www.kaggle.com/vbmokin/eda-for-tabular-data-advanced-techniques)\n* [Datasets for river water quality prediction](https://www.kaggle.com/vbmokin/datasets-for-river-water-quality-prediction)\n* [AI-ML-DS Training. L1T : Titanic - Decision Tree](https://www.kaggle.com/vbmokin/ai-ml-ds-training-l1t-titanic-decision-tree)\n* [AI-ML-DS Training. L1T : NH4 - linear regression](https://www.kaggle.com/vbmokin/ai-ml-ds-training-l1t-nh4-linear-regression)\n* [Heart Disease - Automatic AdvEDA & FE & 20 models](https://www.kaggle.com/vbmokin/heart-disease-automatic-adveda-fe-20-models)\n* [BOD prediction in river - 15 regression models](https://www.kaggle.com/vbmokin/bod-prediction-in-river-15-regression-models)\n* [The system \"MONITORING AND ENVIRONMENTAL ASSESSMENT OF WATER RESOURCES OF UKRAINE\", State Agency of Water Resources of Ukraine](http://monitoring.davr.gov.ua/EcoWaterMon/GDKMap/Index)\n* [WQ SB river : EDA and Forecasting](https://www.kaggle.com/vbmokin/wq-sb-river-eda-and-forecasting)"]}, {"cell_type": "markdown", "id": "59e69061", "metadata": {}, "source": ["<a class=\"anchor\" id=\"0.1\"></a>\n## Table of Contents\n\n1. [Import libraries](#1)\n1. [Download data](#2)\n1. [EDA & FE & Preprocessing data](#3)\n    - [Statistics & FE](#3.1)\n    - [Data standartization](#3.2)\n    - [Training data splitting](#3.3)\n    - [Cross-validation of training data](#3.4)\n1. [Modeling](#4)\n    - [Linear Regression](#4.1)\n    - [Random Forest Regressor](#4.2)\n    - [XGBoost Regressor](#4.3)    \n1. [Test prediction](#5)\n1. [Results visualization](#6)\n1. [Select the best model](#6)"]}, {"cell_type": "markdown", "id": "7aa3c023", "metadata": {}, "source": ["## 1. Import libraries<a class=\"anchor\" id=\"1\"></a>\n\n[Back to Table of Contents](#0.1)"]}, {"cell_type": "code", "execution_count": 1, "id": "11617f1e", "metadata": {}, "outputs": [], "source": ["# Work with Data - the main Python libraries\nimport numpy as np\nimport pandas as pd\nimport pandas_profiling as pp\n\n# Visualization\nimport matplotlib.pyplot as plt\n\n# Preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, ShuffleSplit, GridSearchCV\n\n# Modeling\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBRegressor\n\n# Metrics\nfrom sklearn.metrics import r2_score\n\nimport warnings\nwarnings.simplefilter('ignore')"]}, {"cell_type": "code", "execution_count": 1, "id": "d2255a2a", "metadata": {}, "outputs": [], "source": ["pd.set_option('max_colwidth', 200)"]}, {"cell_type": "markdown", "id": "63ffa2ad", "metadata": {}, "source": ["## 2. Download data<a class=\"anchor\" id=\"2\"></a>\n\n[Back to Table of Contents](#0.1)"]}, {"cell_type": "code", "execution_count": 1, "id": "e66676d0", "metadata": {}, "outputs": [], "source": ["# Download data\ndata = pd.read_csv('../input/wq-southern-bug-river-01052021/PB_All_2000_2021.csv', sep=';', header=0)\ndata"]}, {"cell_type": "code", "execution_count": 1, "id": "b153bc74", "metadata": {}, "outputs": [], "source": ["# Information for training data\ndata.info()"]}, {"cell_type": "code", "execution_count": 1, "id": "0179c6d6", "metadata": {}, "outputs": [], "source": ["# Download data about monitoring stations\ndata_about = pd.read_csv('../input/wq-southern-bug-river-01052021/PB_stations.csv', sep=';', header=0, encoding='cp1251')\ndata_about.sort_values(by=['length'], ascending=False)"]}, {"cell_type": "markdown", "id": "aacefc54", "metadata": {}, "source": ["## 3. EDA & FE & Preprocessing data<a class=\"anchor\" id=\"3\"></a>\n\n[Back to Table of Contents](#0.1)"]}, {"cell_type": "code", "execution_count": 1, "id": "d202d488", "metadata": {}, "outputs": [], "source": ["# Amount data observations of stations\ndata['id'].value_counts().sort_values().plot(kind='barh')"]}, {"cell_type": "code", "execution_count": 1, "id": "f9bed870", "metadata": {}, "outputs": [], "source": ["# Determination the year of observations\ndata['ds'] = pd.to_datetime(data['date'])\ndata['year'] = data['ds'].dt.year\ndata.info()"]}, {"cell_type": "code", "execution_count": 1, "id": "b4e24ddc", "metadata": {}, "outputs": [], "source": ["# Determination the start year of observations for all stations\ndata[['id', 'year']].groupby(by=['id']).min().sort_values(by=['year'], ascending=False)"]}, {"cell_type": "markdown", "id": "7ab67864", "metadata": {}, "source": ["As you can see, the stations 5 & 13 have little data."]}, {"cell_type": "code", "execution_count": 1, "id": "7862d790", "metadata": {}, "outputs": [], "source": ["# Determination the final year of observations for all stations\ndata[['id', 'year']].groupby(by=['id']).max().sort_values(by=['year'], ascending=False)"]}, {"cell_type": "markdown", "id": "fad7054e", "metadata": {}, "source": ["As you can see, only stations 3, 5, 10, 14, 15, 16 and 22 have modern data."]}, {"cell_type": "markdown", "id": "49de713f", "metadata": {}, "source": ["\nAlthough, if you limit yourself to 2018, then you can take all the stations."]}, {"cell_type": "markdown", "id": "7cf6e4ab", "metadata": {}, "source": ["\u0421onsider only stations 14, 15, 16."]}, {"cell_type": "code", "execution_count": 1, "id": "9065e371", "metadata": {}, "outputs": [], "source": ["# Information about stations 14, 15, 16\nstations_good = [14, 15, 16]\ndata_about[data_about['id'].isin(stations_good)]"]}, {"cell_type": "code", "execution_count": 1, "id": "bdb91c08", "metadata": {}, "outputs": [], "source": ["# Set target indicator\ntarget_data_name = 'O2'\n#feature_target_all = ['NH4', 'BSK5', 'NO3', 'NO2', 'SO4', 'PO4', 'CL']\nfeature_target_all = ['BSK5', 'NH4']\nfeature_data_all = feature_target_all + [target_data_name]\nfeature_data_all"]}, {"cell_type": "code", "execution_count": 1, "id": "7cb59b79", "metadata": {}, "outputs": [], "source": ["# Data sampling only for good stations\ndf_indicator = data[['id', 'ds'] + feature_data_all]\ndf_indicator = df_indicator[df_indicator['id'].isin(stations_good)].dropna().reset_index(drop=True)\ndf_indicator"]}, {"cell_type": "code", "execution_count": 1, "id": "6657f170", "metadata": {}, "outputs": [], "source": ["cols = []\nfor station in stations_good:\n    for feature in feature_data_all:\n        cols.append(str(station) + \"_\" + feature)\ncols"]}, {"cell_type": "code", "execution_count": 1, "id": "f1c5d2a7", "metadata": {}, "outputs": [], "source": ["df = pd.pivot_table(df_indicator, index=[\"ds\"], columns=[\"id\"], values=feature_data_all).dropna()\ndf.columns = cols\ndf"]}, {"cell_type": "code", "execution_count": 1, "id": "d29c314e", "metadata": {}, "outputs": [], "source": ["# Data visualization\ndf.plot(figsize=(12,10))"]}, {"cell_type": "code", "execution_count": 1, "id": "1e0b5c26", "metadata": {}, "outputs": [], "source": ["# EDA with Pandas Profiling\npp.ProfileReport(df)"]}, {"cell_type": "code", "execution_count": 1, "id": "08a8bcb4", "metadata": {}, "outputs": [], "source": ["df.describe([.05, .5, .95])"]}, {"cell_type": "code", "execution_count": 1, "id": "a8d45fdc", "metadata": {}, "outputs": [], "source": ["print(len(df))\nfor col in df.columns.tolist():\n    df = df[df[col] <= float(df.quantile([.96])[col])]\ndf = df.reset_index(drop=True)\nprint(len(df))\ndf.describe()"]}, {"cell_type": "code", "execution_count": 1, "id": "d7dbfbad", "metadata": {}, "outputs": [], "source": ["# Data visualization\ndf.plot(figsize=(12,10))"]}, {"cell_type": "code", "execution_count": 1, "id": "c44c1eae", "metadata": {}, "outputs": [], "source": ["# df = df[(df['15_O2'] <= 0.4648) & (df['16_NH4'] <= 15.648) & (df['16_O2'] <= 15.78)].reset_index(drop=True)\n# df.describe([.05, .5, .96])"]}, {"cell_type": "code", "execution_count": 1, "id": "9b84f029", "metadata": {}, "outputs": [], "source": ["#len(df)"]}, {"cell_type": "code", "execution_count": 1, "id": "e4190a36", "metadata": {}, "outputs": [], "source": ["# Set target data\ntarget_name = '16_' + target_data_name\ntarget_data = df.pop(target_name)\ntarget_data"]}, {"cell_type": "code", "execution_count": 1, "id": "da237e7f", "metadata": {}, "outputs": [], "source": ["# Dividing data into training and test\ntrain, test, target, target_test = train_test_split(df, target_data, test_size=0.4, random_state=0)\nprint(train.shape, test.shape)"]}, {"cell_type": "markdown", "id": "448ea61b", "metadata": {}, "source": ["### 3.1. Statistics & FE<a class=\"anchor\" id=\"3.1\"></a>\n\n[Back to Table of Contents](#0.1)"]}, {"cell_type": "markdown", "id": "61b1c4b8", "metadata": {}, "source": ["The analysis showed that many values are only available in stations 1 and 2, while others have much less data. I propose select only these two stations."]}, {"cell_type": "code", "execution_count": 1, "id": "5444b6d9", "metadata": {}, "outputs": [], "source": ["# Display the statistics for training data\ntrain.describe([.05, .5, .96])"]}, {"cell_type": "code", "execution_count": 1, "id": "bfeb2c2a", "metadata": {}, "outputs": [], "source": ["# Display the statistics for test data\ntest.describe()"]}, {"cell_type": "markdown", "id": "d346779e", "metadata": {}, "source": ["### 3.2. Data standartization<a class=\"anchor\" id=\"3.2\"></a>\n\n[Back to Table of Contents](#0.1)"]}, {"cell_type": "code", "execution_count": 1, "id": "6ac5375d", "metadata": {}, "outputs": [], "source": ["# Standartization data\nscaler = StandardScaler()\ntrain = pd.DataFrame(scaler.fit_transform(train), columns = train.columns)\n\n# Display training data\ntrain"]}, {"cell_type": "code", "execution_count": 1, "id": "4c87b6be", "metadata": {}, "outputs": [], "source": ["# Standartization data\ntest = pd.DataFrame(scaler.transform(test), columns = test.columns)"]}, {"cell_type": "markdown", "id": "c1fbdc87", "metadata": {}, "source": ["### 3.3. Training data splitting<a class=\"anchor\" id=\"3.3\"></a>\n\n[Back to Table of Contents](#0.1)"]}, {"cell_type": "code", "execution_count": 1, "id": "21fd0257", "metadata": {}, "outputs": [], "source": ["# Training data splitting to new training (part of the all training) and validation data\ntrain_all = train.copy()\ntarget_all = target.copy()\ntrain, valid, target_train, target_valid = train_test_split(train_all, target_all, test_size=0.2, random_state=0)"]}, {"cell_type": "code", "execution_count": 1, "id": "85ed9aeb", "metadata": {}, "outputs": [], "source": ["# Display information about new training data\ntrain.info()"]}, {"cell_type": "code", "execution_count": 1, "id": "d426adce", "metadata": {}, "outputs": [], "source": ["# Display information about validation data\nvalid.info()"]}, {"cell_type": "markdown", "id": "1b21befb", "metadata": {}, "source": ["### 3.4. Cross-validation of training data<a class=\"anchor\" id=\"3.4\"></a>\n\n[Back to Table of Contents](#0.1)"]}, {"cell_type": "code", "execution_count": 1, "id": "9c3994a6", "metadata": {}, "outputs": [], "source": ["# Cross-validation of training data with shuffle\ncv_train = ShuffleSplit(n_splits=5, test_size=0.4, random_state=0)"]}, {"cell_type": "markdown", "id": "2cd391e0", "metadata": {}, "source": ["**ADDITIONAL TASK:** \n1. Set number of splitting = 5, 7, 10 and to compare of results.\n2. Try use another method for cross-validation of training data (without shuffle):\n\n        KFold(n_splits=5, shuffle=False, random_state=0)"]}, {"cell_type": "markdown", "id": "3ff25990", "metadata": {}, "source": ["## 4. Modeling<a class=\"anchor\" id=\"4\"></a>\n\n[Back to Table of Contents](#0.1)"]}, {"cell_type": "code", "execution_count": 1, "id": "a1011e73", "metadata": {}, "outputs": [], "source": ["# Creation the dataframe with the resulting score of all models\nresult = pd.DataFrame({'model' : ['Linear Regression', 'Random Forest Regressor', 'XGBoost Regressor'], \n                       'train_score': 0, 'valid_score': 0})\nresult"]}, {"cell_type": "markdown", "id": "82a46bda", "metadata": {}, "source": ["### 4.1. Linear Regression<a class=\"anchor\" id=\"4.1\"></a>\n\n[Back to Table of Contents](#0.1)"]}, {"cell_type": "code", "execution_count": 1, "id": "673be6d7", "metadata": {}, "outputs": [], "source": ["# Linear Regression\nlr = LinearRegression()\nlr.fit(train, target_train)\n\n# Prediction for training data\ny_train_lr = lr.predict(train)\n\n# Accuracy of model\nr2_score_acc = round(r2_score(target_train, y_train_lr), 2)\nprint(f'Accuracy of Linear Regression model training is {r2_score_acc}')\n\n# Save to result dataframe\nresult.loc[result['model'] == 'Linear Regression', 'train_score'] = r2_score_acc"]}, {"cell_type": "code", "execution_count": 1, "id": "8e54f127", "metadata": {}, "outputs": [], "source": ["# Print rounded r2_lr = lr.predict(valid)\ny_val_lr = lr.predict(valid)\nr2_score_acc_valid = round(r2_score(target_valid, y_val_lr),2)\nresult.loc[result['model'] == 'Linear Regression', 'valid_score'] = r2_score_acc_valid\nprint(f'Accuracy of Linear Regression model prediction for valid dataset is {r2_score_acc_valid}')"]}, {"cell_type": "markdown", "id": "87756193", "metadata": {}, "source": ["### 4.2. Random Forest Regressor<a class=\"anchor\" id=\"4.2\"></a>\n\n[Back to Table of Contents](#0.1)"]}, {"cell_type": "code", "execution_count": 1, "id": "37f98ddd", "metadata": {}, "outputs": [], "source": ["%%time\n# Random Forest Regressor\nrf = RandomForestRegressor()\nparam_grid = {'n_estimators': [50, 100], 'min_samples_leaf': [i for i in range(3,7)], \n              'max_features': ['auto'], 'max_depth': [i for i in range(3,6)], \n              'criterion': ['mse'], 'bootstrap': [False]}\n\n# Training model\nrf_CV = GridSearchCV(rf, param_grid=param_grid, cv=cv_train, verbose=False)\nrf_CV.fit(train, target_train)\nprint(rf_CV.best_params_)\n\n# Prediction for training data\ny_train_rf = rf_CV.predict(train)\n\n# Accuracy of model\nr2_score_acc = round(r2_score(target_train, y_train_rf),2)\nprint(f'Accuracy of RandomForestRegressor model training is {r2_score_acc}')\n\n# Save to result dataframe\nresult.loc[result['model'] == 'Random Forest Regressor', 'train_score'] = r2_score_acc"]}, {"cell_type": "code", "execution_count": 1, "id": "8d7d204c", "metadata": {}, "outputs": [], "source": ["# Print rounded r2_score_acc to 2 decimal values after the text\ny_val_rf = rf_CV.predict(valid)\nr2_score_acc_valid = round(r2_score(target_valid, y_val_rf),2)\nresult.loc[result['model'] == 'Random Forest Regressor', 'valid_score'] = r2_score_acc_valid\nprint(f'Accuracy of RandomForestRegressor model prediction for valid dataset is {r2_score_acc_valid}')"]}, {"cell_type": "markdown", "id": "438a34f8", "metadata": {}, "source": ["### 4.3. XGBoost Regressor<a class=\"anchor\" id=\"4.3\"></a>\n\n[Back to Table of Contents](#0.1)"]}, {"cell_type": "code", "execution_count": 1, "id": "1d9df175", "metadata": {}, "outputs": [], "source": ["%%time\n# XGBoost Regressor\nxgbr = xgb.XGBRegressor() \n# parameters = {'n_estimators': [60, 70, 80, 90, 95, 100, 105, 110, 120, 130, 140], \n#               'learning_rate': [0.005, 0.01, 0.05, 0.075, 0.1],\n#               'max_depth': [3, 5, 7, 9],\n#               'reg_lambda': [0.1, 0.3, 0.5]}\n\nparameters = {'n_estimators': [50, 100], \n              'learning_rate': [0.05, 0.01],\n              'max_depth': [4, 5],\n              'reg_lambda': [0.3]}\n\n# Training model\nxgb_CV = GridSearchCV(estimator=xgbr, param_grid=parameters, cv=cv_train, n_jobs=-1)\nxgb_CV.fit(train, target_train)\nprint(\"Best score: %0.3f\" % xgb_CV.best_score_)\nprint(\"Best parameters set:\", xgb_CV.best_params_)\n\n# Prediction for training data\ny_train_xgb = xgb_CV.predict(train)\n\n# Accuracy of model\nr2_score_acc = round(r2_score(target_train, y_train_xgb),2)\nprint(f'Accuracy of XGBoost Regressor model training is {r2_score_acc}')\n\n# Save to result dataframe\nresult.loc[result['model'] == 'XGBoost Regressor', 'train_score'] = r2_score_acc"]}, {"cell_type": "code", "execution_count": 1, "id": "4bbf5097", "metadata": {}, "outputs": [], "source": ["# Print rounded r2_score_acc to 2 decimal values after the text\ny_val_xgb = xgb_CV.predict(valid)\nr2_score_acc_valid = round(r2_score(target_valid, y_val_xgb),2)\nresult.loc[result['model'] == 'XGBoost Regressor', 'valid_score'] = r2_score_acc_valid\nprint(f'Accuracy of XGBoost Regressor model prediction for valid dataset is {r2_score_acc_valid}')"]}, {"cell_type": "code", "execution_count": 1, "id": "79535684", "metadata": {}, "outputs": [], "source": ["# Feature importance diagram\nxgbr = xgb.XGBRegressor(**xgb_CV.best_params_)\nxgbr.fit(train, target_train)\nfig =  plt.figure(figsize = (10,8))\naxes = fig.add_subplot(111)\nxgb.plot_importance(xgbr,ax = axes,height = 0.5)\nplt.show();\nplt.close()"]}, {"cell_type": "markdown", "id": "1648952e", "metadata": {}, "source": ["**ADDITIONAL TASKS:** \n1. Try to change the parameters (see examples above).\n2. Try deleting anomalous data. \n3. Add to dataframe result also calculated array: y_train, y_val.\n4. Creation the function with all commands and output information (in each section of this chapter 4) for all models:\n\n        result = get_model(train, valid, target_train, target_valid, model_name, param_grid, cv_train, result)"]}, {"cell_type": "markdown", "id": "b49e5cb3", "metadata": {}, "source": ["## 5. Test prediction<a class=\"anchor\" id=\"5\"></a>\n\n[Back to Table of Contents](#0.1)"]}, {"cell_type": "code", "execution_count": 1, "id": "61d8a048", "metadata": {}, "outputs": [], "source": ["# Prediction of target for test data for all models\ny_test_lr = lr.predict(test)\ny_test_rf = rf_CV.predict(test)\ny_test_xgb = xgb_CV.predict(test)"]}, {"cell_type": "markdown", "id": "1385a9c8", "metadata": {}, "source": ["## 6. Visualization<a class=\"anchor\" id=\"6\"></a>\n\n[Back to Table of Contents](#0.1)"]}, {"cell_type": "code", "execution_count": 1, "id": "d3b0b803", "metadata": {}, "outputs": [], "source": ["# Building plot for prediction for the training data \nx = np.arange(len(train))\nplt.figure(figsize=(16,10))\nplt.scatter(x, target_train, label = \"Target training data\", color = 'g')\nplt.scatter(x, y_train_lr, label = \"Linear Regression prediction\", color = 'b')\nplt.scatter(x, y_train_rf, label = \"Random Forest prediction\", color = 'y')\nplt.scatter(x, y_train_xgb, label = \"XGBoost Regressor prediction\", color = 'brown')\nplt.plot(x, np.full(len(train), 0.5), label = \"Maximum allowable value\", color = 'r')\nplt.title('Prediction for the training data')\nplt.legend(loc='best')\nplt.grid(True)"]}, {"cell_type": "code", "execution_count": 1, "id": "380be290", "metadata": {}, "outputs": [], "source": ["# Building plot for prediction for the valid data \nx = np.arange(len(valid))\nplt.figure(figsize=(16,10))\nplt.scatter(x, target_valid, label = \"Target valid data\", color = 'g')\nplt.scatter(x, y_val_lr, label = \"Linear Regression prediction\", color = 'b')\nplt.scatter(x, y_val_rf, label = \"Random Forest prediction\", color = 'y')\nplt.scatter(x, y_val_xgb, label = \"XGBoost Regressor prediction\", color = 'brown')\nplt.plot(x, np.full(len(valid), 0.5), label = \"Maximum allowable value\", color = 'r')\nplt.title('Prediction for the valid data')\nplt.legend(loc='best')\nplt.grid(True)"]}, {"cell_type": "code", "execution_count": 1, "id": "a6cb7705", "metadata": {}, "outputs": [], "source": ["# Building plot for prediction for the test data \nx = np.arange(len(test))\nplt.figure(figsize=(16,10))\nplt.scatter(x, target_test, label = \"Target test data\", color = 'g')\nplt.scatter(x, y_test_lr, label = \"Linear Regression prediction\", color = 'b')\nplt.scatter(x, y_test_rf, label = \"Random Forest prediction\", color = 'y')\nplt.scatter(x, y_test_xgb, label = \"XGBoost Regressor prediction\", color = 'brown')\nplt.plot(x, np.full(len(test), 0.5), label = \"Maximum allowable value\", color = 'r')\nplt.title('Prediction for the test data')\nplt.legend(loc='best')\nplt.grid(True)"]}, {"cell_type": "markdown", "id": "187e0e4f", "metadata": {}, "source": ["## 7. Select the best model <a class=\"anchor\" id=\"7\"></a>\n\n[Back to Table of Contents](#0.1)"]}, {"cell_type": "code", "execution_count": 1, "id": "18ac408d", "metadata": {}, "outputs": [], "source": ["# Display results of modeling\nresult.sort_values(by=['valid_score', 'train_score'], ascending=False)"]}, {"cell_type": "code", "execution_count": 1, "id": "6068dd45", "metadata": {}, "outputs": [], "source": ["# Select models with good training results\nresult_best = result[(result['train_score'] > result['valid_score'])]"]}, {"cell_type": "code", "execution_count": 1, "id": "14059a05", "metadata": {}, "outputs": [], "source": ["# Select models with minimal overfitting\nresult_best = result_best[(result_best['train_score'] - result_best['valid_score']).abs() < 0.15]\nresult_best.sort_values(by=['valid_score', 'train_score'], ascending=False)"]}, {"cell_type": "code", "execution_count": 1, "id": "27017d87", "metadata": {}, "outputs": [], "source": ["# Select the best model\nresult_best.nlargest(1, 'valid_score')"]}, {"cell_type": "code", "execution_count": 1, "id": "6a4acd6a", "metadata": {}, "outputs": [], "source": ["# Find a name of the best model (with maximal valid score)\nbest_model_name = result_best.loc[result_best['valid_score'].idxmax(result_best['valid_score'].max()), 'model']"]}, {"cell_type": "code", "execution_count": 1, "id": "94131288", "metadata": {}, "outputs": [], "source": ["print(f'The best model is \"{best_model_name}\"')"]}, {"cell_type": "markdown", "id": "efa50a73", "metadata": {}, "source": ["I hope you find this notebook useful and enjoyable.\n\nYour comments and feedback are most welcome.\n\n[Go to Top](#0)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}