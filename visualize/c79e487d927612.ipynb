{"cells": [{"cell_type": "markdown", "id": "62b1bd7d", "metadata": {}, "source": ["# Using XGBoost\nXGBoost is an implementation of the **Gradient Boosted Decision Trees algorithm**"]}, {"cell_type": "code", "execution_count": 1, "id": "905f94b4", "metadata": {}, "outputs": [], "source": ["import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import Imputer\nimport warnings\n\nwarnings.simplefilter(action='ignore', category=DeprecationWarning) \n#so pandas doesn't spit out a warning everytime\n\n# Loading in Iowa housing data\ndata = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ndata.dropna(axis=0, subset=['SalePrice'], inplace=True) #drops data with missing SalePrice value\ny = data.SalePrice\nX = data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object'])\n# this is the path to the Iowa data we will use\ntrain_X, test_X, train_y, test_y = train_test_split(X.as_matrix(), y.as_matrix(), test_size=0.25)\n\n# now we create our imputer\nmy_imputer = Imputer()\ntrain_X = my_imputer.fit_transform(train_X)\ntest_X = my_imputer.fit_transform(test_X)"]}, {"cell_type": "markdown", "id": "fb62c393", "metadata": {}, "source": ["Now we are going to fit a model just like we would in sckikit-learn."]}, {"cell_type": "code", "execution_count": 1, "id": "6f3e63ac", "metadata": {}, "outputs": [], "source": ["from xgboost import XGBRegressor\n\nmy_model = XGBRegressor()\n# we can add silent=True to avoid printing out updates with each cycle\nmy_model.fit(train_X, train_y, verbose=False)"]}, {"cell_type": "markdown", "id": "29740b15", "metadata": {}, "source": ["Now we will evaluate our model by making a prediction and see what the MAE is, just like in scikit."]}, {"cell_type": "code", "execution_count": 1, "id": "9a1be925", "metadata": {}, "outputs": [], "source": ["# make predictions\npredictions = my_model.predict(test_X)\n\nfrom sklearn.metrics import mean_absolute_error\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y)))"]}, {"cell_type": "markdown", "id": "1dbec6cd", "metadata": {}, "source": ["# Model Tuning\n\nXGBoost has a few parameters that can dramatically affect your model's accuracy and training speed. Now that we have our baseline, we can tune the parameters to make our model better."]}, {"cell_type": "code", "execution_count": 1, "id": "6ccae7a8", "metadata": {}, "outputs": [], "source": ["my_model = XGBRegressor(n_estimator = 1000)\nmy_model.fit(train_X, train_y, early_stopping_rounds = 5, eval_set = [(test_X, test_y)], verbose = False)"]}, {"cell_type": "markdown", "id": "c07694cd", "metadata": {}, "source": ["Now we'll add a small learning rate to hopefully increase the accuracy of our model, although this will also increase the time it takes to train our model which may be problematic when we encounter larger datasets."]}, {"cell_type": "code", "execution_count": 1, "id": "4ed40a4a", "metadata": {}, "outputs": [], "source": ["my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nmy_model.fit(train_X, train_y, early_stopping_rounds=100, \n             eval_set=[(test_X, test_y)], verbose=False)\n\n# make our final predictions\npredictions = my_model.predict(test_X)\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y)))\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}