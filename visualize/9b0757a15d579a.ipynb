{"cells": [{"cell_type": "markdown", "id": "45803dad", "metadata": {}, "source": ["# **Introduction**"]}, {"cell_type": "markdown", "id": "31d23820", "metadata": {}, "source": ["In this notebook we'll explore the following questions:\n\n1. How effective was the test-preparation course for the students' revision?\n2. How does a student's parental education level affect their exam scores?\n3. Can we predict a student's gender based on their exam scores and other attributes?"]}, {"cell_type": "markdown", "id": "8d1b936c", "metadata": {}, "source": ["# **Importing Packages & Data**"]}, {"cell_type": "code", "execution_count": 1, "id": "b6f599f7", "metadata": {}, "outputs": [], "source": ["import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline"]}, {"cell_type": "code", "execution_count": 1, "id": "c48f1386", "metadata": {}, "outputs": [], "source": ["df = pd.read_csv('../input/StudentsPerformance.csv')\ndf.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "1880831e", "metadata": {}, "outputs": [], "source": ["print('Shape of dataframe:', df.shape)"]}, {"cell_type": "markdown", "id": "c4500e39", "metadata": {}, "source": ["# **Data Cleaning & Feature Engineering**"]}, {"cell_type": "markdown", "id": "a78252d8", "metadata": {}, "source": ["First we'll check if there are any missing entries that need to be dealt with. Luckily in this dataset there aren't."]}, {"cell_type": "code", "execution_count": 1, "id": "bc106e67", "metadata": {}, "outputs": [], "source": ["df.isnull().sum()"]}, {"cell_type": "markdown", "id": "832ffd79", "metadata": {}, "source": ["Next, there are a few changes we can make to help prepare the data for analysis, and to make it slightly more readable:\n\n1. Rename columns to a simpler form.\n2. Remove unnecessary information from 'race' column.\n3. Create an average score column to use in later analysis."]}, {"cell_type": "code", "execution_count": 1, "id": "70ffecff", "metadata": {}, "outputs": [], "source": ["# 1\ndf.columns = ['gender', 'race', 'parent_education', 'lunch', 'test_prep', 'math_score', 'reading_score', 'writing_score']\n\n# 2\ndf['race'] = df.race.apply(lambda x: x[-1])\n\n# 3\ndf['avg_score'] = (df['math_score'] + df['reading_score'] + df['writing_score']) / 3\n\ndf.head()"]}, {"cell_type": "markdown", "id": "f0636b08", "metadata": {}, "source": ["The last piece of the dataset that needs cleaning is the 'parent_education' column."]}, {"cell_type": "code", "execution_count": 1, "id": "67838e09", "metadata": {}, "outputs": [], "source": ["# count of each parent_education entry\ndf.groupby(['parent_education']).gender.count()"]}, {"cell_type": "markdown", "id": "112c78ed", "metadata": {}, "source": ["Notice that we have entries labelled both 'high school' and 'some high school'. These entries can be grouped into the same category.\n\nWe'll also rename 'some college' entries to 'college'."]}, {"cell_type": "code", "execution_count": 1, "id": "56375c69", "metadata": {}, "outputs": [], "source": ["df['parent_education'] = df.parent_education.apply(lambda x: 'high school' if x == 'some high school' else ('college' if x == 'some college' else x))\ndf.head()"]}, {"cell_type": "markdown", "id": "8502325c", "metadata": {}, "source": ["# **Initial Analysis**"]}, {"cell_type": "markdown", "id": "a88f00cd", "metadata": {}, "source": ["**The data is now ready for us to perform some initial analysis.**"]}, {"cell_type": "markdown", "id": "43268c43", "metadata": {}, "source": ["First, we use scatterplots to see the correlation between each of the subject scores. We can also separate these plots by gender to see how they differ."]}, {"cell_type": "code", "execution_count": 1, "id": "538b61de", "metadata": {}, "outputs": [], "source": ["fig, axs = plt.subplots(figsize=(22,6), ncols=3)\nfig.subplots_adjust(wspace=0.23)\n\nsns.scatterplot(x='math_score', y='reading_score', hue='gender', data=df, ax=axs[0])\nsns.scatterplot(x='math_score', y='writing_score', hue='gender', data=df, ax=axs[1])\nsns.scatterplot(x='reading_score', y='writing_score', hue='gender', data=df, ax=axs[2])"]}, {"cell_type": "markdown", "id": "4a432384", "metadata": {}, "source": ["We see that men tend to have a slightly higher math score than reading or writing score, and women tend to be the opposite. \n\nWe also see that no students performed extremely well in one subject and badly in another."]}, {"cell_type": "markdown", "id": "92b9bc3b", "metadata": {}, "source": ["Next we check the distributions of the scores achieved in each exam."]}, {"cell_type": "code", "execution_count": 1, "id": "7d4a1887", "metadata": {}, "outputs": [], "source": ["fig, axs = plt.subplots(figsize=(18,5), ncols=3)\nfig.subplots_adjust(wspace=0.3)\n\nsns.distplot(df.math_score, ax=axs[0])\nsns.distplot(df.reading_score, ax=axs[1])\nsns.distplot(df.writing_score, ax=axs[2])"]}, {"cell_type": "markdown", "id": "3df14b6f", "metadata": {}, "source": ["Below we use a boxplot to compare how much of an impact the test-preparation exam had for each subject."]}, {"cell_type": "code", "execution_count": 1, "id": "a08ae348", "metadata": {}, "outputs": [], "source": ["fig, axs = plt.subplots(figsize=(15,6), ncols=3)\nfig.subplots_adjust(wspace=0.5)\n\nsns.boxplot(x='test_prep', y='math_score', data=df, ax=axs[0], fliersize=2)\nsns.boxplot(x='test_prep', y='reading_score', data=df, ax=axs[1], fliersize=2)\nsns.boxplot(x='test_prep', y='writing_score', data=df, ax=axs[2], fliersize=2)"]}, {"cell_type": "markdown", "id": "cfd078fc", "metadata": {}, "source": ["Overall it seems that the test-preparation exam was effective revision for every subject, most affecting the scores in the writing exam."]}, {"cell_type": "markdown", "id": "09db5199", "metadata": {}, "source": ["Lastly, we use another boxplot to compare how a student's parental education level affects their average exam score."]}, {"cell_type": "code", "execution_count": 1, "id": "f15ee83f", "metadata": {}, "outputs": [], "source": ["fig, axs = plt.subplots(figsize=(9,7))\n\nsns.boxplot(x='parent_education', y='avg_score', data=df, fliersize=0)\nsns.swarmplot(x='parent_education', y='avg_score', data=df, color='0')"]}, {"cell_type": "markdown", "id": "3fd868a0", "metadata": {}, "source": ["# **Logistic Regression**"]}, {"cell_type": "markdown", "id": "6d92eba5", "metadata": {}, "source": ["**Now we'll try to predict a student's gender based on their exam scores and other attributes.**"]}, {"cell_type": "markdown", "id": "a11cf014", "metadata": {}, "source": ["There are a few changes we must make to the data before we are ready to fit the model."]}, {"cell_type": "code", "execution_count": 1, "id": "e112a70c", "metadata": {}, "outputs": [], "source": ["log_df = df.copy()\nlog_df.head()"]}, {"cell_type": "markdown", "id": "d92ffe69", "metadata": {}, "source": ["First, we'll convert the 'gender', 'lunch' and 'test_prep' columns into binary. "]}, {"cell_type": "code", "execution_count": 1, "id": "3e2710be", "metadata": {}, "outputs": [], "source": ["log_df['gender'] = log_df.gender.apply(lambda x: 1 if x == 'male' else 0)\nlog_df['reduced_lunch'] = log_df.lunch.apply(lambda x: 1 if x == 'free/reduced' else 0)\nlog_df['test_prep'] = log_df.test_prep.apply(lambda x: 1 if x == 'completed' else 0)\n\n# removing 'lunch' and 'avg_score' columns\nlog_df = log_df.drop(['lunch', 'avg_score'], axis=1)\n\nlog_df.head(3)"]}, {"cell_type": "markdown", "id": "a2ff140b", "metadata": {}, "source": ["Next we need to use one-hot encoding on the 'race' and 'parent_education' columns so that the data can effectively be fed into the model."]}, {"cell_type": "code", "execution_count": 1, "id": "b348b694", "metadata": {}, "outputs": [], "source": ["race_df = pd.get_dummies(log_df.race)\ned_df = pd.get_dummies(log_df.parent_education)\nlog_df = pd.concat([log_df, race_df, ed_df], axis=1)\n\nlog_df = log_df.drop(['race', 'parent_education'], axis=1)\n\nlog_df.columns = ['gender', 'test_prep', 'math_score', 'reading_score', 'writing_score', \n                  'reduced_lunch', 'race_A', 'race_B', 'race_C', 'race_D', 'race_E', \n                  'p_associates', 'p_bachelors', 'p_college', 'p_high_school', 'p_masters']\n\nlog_df.head(3)"]}, {"cell_type": "markdown", "id": "7266b7fb", "metadata": {}, "source": ["We'll also convert the scores into decimals so that their scale is in keeping with the rest of the data. This stops the regression model from assigning skewed weight towards the score variables."]}, {"cell_type": "code", "execution_count": 1, "id": "abda743d", "metadata": {}, "outputs": [], "source": ["scores = ['math_score', 'reading_score', 'writing_score']\nfor i in scores:\n    log_df[i] = log_df[i]/100\n\nlog_df.head(3)"]}, {"cell_type": "markdown", "id": "ad06a503", "metadata": {}, "source": ["Our target variable is 'gender', and the other features are our predictor variables.\n\nWe split the dataframe into training data (80%), and test data (20%)."]}, {"cell_type": "code", "execution_count": 1, "id": "1e14aa65", "metadata": {}, "outputs": [], "source": ["predictors = list(log_df.columns)\npredictors.remove('gender')\n\nX_train, X_test, y_train, y_test = train_test_split(log_df[predictors], log_df['gender'], \n                                                    test_size=0.2, random_state=38)\nprint('Training data:', X_train.shape, '\\nTest data:', X_test.shape)"]}, {"cell_type": "markdown", "id": "bfdf1c1b", "metadata": {}, "source": ["Now it's time to fit the model. We use scikit-learn's GridSearchCV function to find the best hyperparameters to use.\n\nThe model is trained using our training data, and then evaluated on the test data to obtain an accuracy score."]}, {"cell_type": "code", "execution_count": 1, "id": "f7c28d18", "metadata": {}, "outputs": [], "source": ["log = LogisticRegression()\n\n# parameter space\npenalty = ['l1', 'l2']\nC = np.logspace(0, 4, 20)\nlog_params = dict(C=C, penalty=penalty)\n\n# grid search\nlog_clf = GridSearchCV(log, log_params, cv=5, verbose=0)\nbest_log_model = log_clf.fit(X_train, y_train)\nlog_score = best_log_model.score(X_test, y_test)\n\n# tuned parameters\nprint('Best Penalty:', best_log_model.best_estimator_.get_params()['penalty'])\nprint('Best C:', best_log_model.best_estimator_.get_params()['C'])\nprint('\\nModel Accuracy:', log_score)"]}, {"cell_type": "markdown", "id": "f4eaa358", "metadata": {}, "source": ["Below is a table showing how many of the model's predictions were correct."]}, {"cell_type": "code", "execution_count": 1, "id": "bdaf609f", "metadata": {}, "outputs": [], "source": ["log_predictions = best_log_model.predict(X_test)\npd.crosstab(y_test, log_predictions, rownames=['Actual'], colnames=['Predicted'])"]}, {"cell_type": "markdown", "id": "304e373e", "metadata": {}, "source": ["Lastly, let's compare which of our features the model assigned most weight to. This will give us an idea of what features were most influential in our model's decisions.\n\nThe metric we'll use for this comparison is the magnitude of the feature's coefficient in our model, multiplied by the standard deviation of the data in the corresponding column."]}, {"cell_type": "code", "execution_count": 1, "id": "e84a50cb", "metadata": {}, "outputs": [], "source": ["logistic = LogisticRegression(penalty = best_log_model.best_estimator_.get_params()['penalty'], \n                              C = best_log_model.best_estimator_.get_params()['C'])\nlogistic.fit(X_train, y_train)\n\nfeature_importance = abs(np.std(X_train, 0) * list(logistic.coef_[0]))\nn = len(feature_importance)\n\nfig, axs = plt.subplots(figsize=(12,5))\nsns.barplot(x = feature_importance.nlargest(n).index, \n            y = feature_importance.nlargest(n))\naxs.set_xticklabels(axs.get_xticklabels(), rotation=90)\naxs.set(xlabel='Feature', ylabel='Feature Importance')"]}, {"cell_type": "markdown", "id": "a71a10b1", "metadata": {}, "source": ["As expected, the exam scores were the most influential features, followed by the completion of the test-preparation course."]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}