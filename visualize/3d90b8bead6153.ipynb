{"cells": [{"cell_type": "markdown", "id": "bca98b1d", "metadata": {}, "source": ["# FastAI Text Classification - Beginner Tutorial\n\nBuilding on the knowledge & experience gained from the [FastAI Image Classification](https://www.kaggle.com/kkhandekar/fastai-beginner-tutorial) tutorial, we shall attempt to perform a text classification in this notebook.\n\nAnd to keep things interesting, we shall be using the [Kick Starter NLP Dataset](https://www.kaggle.com/oscarvilla/kickstarter-nlp).\n\nOnwards with the scripting ...\n\nCourse of action:\n\n* Libraries\n* Load, Prep & Explore Data\n* Text Data Pre-Processing\n* Build & Train Model\n* Predictions\n\n"]}, {"cell_type": "markdown", "id": "2df5e59a", "metadata": {}, "source": ["## Libraries"]}, {"cell_type": "code", "execution_count": 1, "id": "fc29e9c8", "metadata": {}, "outputs": [], "source": ["!pip install contractions -q"]}, {"cell_type": "code", "execution_count": 1, "id": "c465c615", "metadata": {}, "outputs": [], "source": ["#Generic\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re,string,unicodedata\nimport contractions #import contractions_dict\n\n#FastAI\nimport fastai\nfrom fastai import *\nfrom fastai.text import * \n\n#Functional Tool\nfrom functools import partial\n\n#Garbage\nimport gc\n\n#NLTK\nimport nltk\nfrom nltk.tokenize.toktok import ToktokTokenizer\n\n#SK Learn libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn import metrics\nfrom sklearn.compose import ColumnTransformer\n\n#Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")"]}, {"cell_type": "markdown", "id": "fffb3680", "metadata": {}, "source": ["## Load, Prep & Explore Data"]}, {"cell_type": "code", "execution_count": 1, "id": "db721369", "metadata": {}, "outputs": [], "source": ["#Load data\nurl = '../input/kickstarter-nlp/df_text_eng.csv'\nraw_data = pd.read_csv(url, header='infer')"]}, {"cell_type": "code", "execution_count": 1, "id": "3bcd864a", "metadata": {}, "outputs": [], "source": ["#checking the columns\nraw_data.columns"]}, {"cell_type": "markdown", "id": "e24ee266", "metadata": {}, "source": ["For the purpose of this tutorial, we are only interested in the \"blurb\" & the \"state\" columns. So, we shall create a seperate dataframe that will only consist these 2 columns."]}, {"cell_type": "code", "execution_count": 1, "id": "670e83fa", "metadata": {}, "outputs": [], "source": ["#creating a seperate dataframe\ndata = raw_data[['blurb','state']]"]}, {"cell_type": "code", "execution_count": 1, "id": "4916ec11", "metadata": {}, "outputs": [], "source": ["#inspect the shape of the dataframe\ndata.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "163c6da4", "metadata": {}, "outputs": [], "source": ["#Check for null/missing values in the new dataframe\ndata.isna().sum()"]}, {"cell_type": "code", "execution_count": 1, "id": "7a717de6", "metadata": {}, "outputs": [], "source": ["#Dropping the records with null/missing values\ndata = data.dropna()"]}, {"cell_type": "code", "execution_count": 1, "id": "cdfcf58c", "metadata": {}, "outputs": [], "source": ["#Checking the records per state\ndata.groupby('state').size()"]}, {"cell_type": "code", "execution_count": 1, "id": "1730c79b", "metadata": {}, "outputs": [], "source": ["#Encoding the State label to convert them to numerical values\nlabel_encoder = LabelEncoder() \n\n#Applying to the dataset\ndata['state']= label_encoder.fit_transform(data['state']) "]}, {"cell_type": "code", "execution_count": 1, "id": "4f75294f", "metadata": {}, "outputs": [], "source": ["#inspect the newly created dataframe\ndata.head()"]}, {"cell_type": "markdown", "id": "f2753f5b", "metadata": {}, "source": ["## Text Data Pre-Processing\n\nWe all have the habit of cleaning our fruits/veggies before eating them, so in the similar way it is always a good practice to clean text data before feeding it to the model. This will stop the Model from spewing incorrect results. \n\nIn this step we will only focus on cleaning/pre-processing the \"blurb\" column since the \"state\" columns is a categorical column.\n"]}, {"cell_type": "code", "execution_count": 1, "id": "379cbcc8", "metadata": {}, "outputs": [], "source": ["#Remove special characters & retain alphabets\ndata['blurb'] = data['blurb'].str.replace(\"[^a-zA-Z]\", \" \")"]}, {"cell_type": "code", "execution_count": 1, "id": "c6940f4e", "metadata": {}, "outputs": [], "source": ["#Lowering the case\ndata['blurb'] = data['blurb'].str.lower()\n\n#stripping leading spaces (if any)\ndata['blurb'] = data['blurb'].str.strip()"]}, {"cell_type": "code", "execution_count": 1, "id": "9ae0bc34", "metadata": {}, "outputs": [], "source": ["#removing punctuations\nfrom string import punctuation\n\ndef remove_punct(text):\n  for punctuations in punctuation:\n    text = text.replace(punctuations, '')\n  return text\n\n#apply to the dataset\ndata['blurb'] = data['blurb'].apply(remove_punct)"]}, {"cell_type": "code", "execution_count": 1, "id": "7d5778f0", "metadata": {}, "outputs": [], "source": ["#function to remove macrons & accented characters\ndef remove_accented_chars(text):\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return text\n\n#applying the function on the clean dataset\ndata['blurb'] = data['blurb'].apply(remove_accented_chars)"]}, {"cell_type": "code", "execution_count": 1, "id": "1d81fd72", "metadata": {}, "outputs": [], "source": ["#Function to expand contractions\ndef expand_contractions(con_text):\n  con_text = contractions.fix(con_text)\n  return con_text\n\n#applying the function on the clean dataset\ndata['blurb'] = data['blurb'].apply(expand_contractions) "]}, {"cell_type": "code", "execution_count": 1, "id": "da99a5b8", "metadata": {}, "outputs": [], "source": ["#Removing Stopwords\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords \n#stop_words = stopwords.words('english')\nstopword_list = set(stopwords.words('english'))"]}, {"cell_type": "markdown", "id": "165fddef", "metadata": {}, "source": ["The stopword remover function ingests the text in bite size portions. To achieve this we will have to tokenize (split) our text. Tokenization can be done in 2 ways\n\n1. Using the Split function\n2. Using a Tokenizer function\n\nWe shall implement the tokenization using the second option. NLTK provides a functions for doing just that (check the libraries section above!)\n"]}, {"cell_type": "code", "execution_count": 1, "id": "4e970633", "metadata": {}, "outputs": [], "source": ["#instantiating the tokenizer function\ntokenizer = ToktokTokenizer()"]}, {"cell_type": "code", "execution_count": 1, "id": "6705479d", "metadata": {}, "outputs": [], "source": ["#function to remove stopwords\ndef remove_stopwords(text, is_lower_case=False):\n    tokens = tokenizer.tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    if is_lower_case:\n        filtered_tokens = [token for token in tokens if token not in stopword_list]\n    else:\n        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)    \n    return filtered_text\n\n#applying the function\ndata['blurb_norm'] = data['blurb'].apply(remove_stopwords) "]}, {"cell_type": "code", "execution_count": 1, "id": "a96e0c0c", "metadata": {}, "outputs": [], "source": ["#Dropping the \"blurb\" column\ndata = data.drop(['blurb'], axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "126f5710", "metadata": {}, "outputs": [], "source": ["#Inspect the dataframe after stopword removal\ndata.head()"]}, {"cell_type": "markdown", "id": "50543027", "metadata": {}, "source": ["At this stage, our \"blurb\" column is cleaned and ready to be fed to the Model. We will take a backup of this dataset just incase something goes wrong !"]}, {"cell_type": "code", "execution_count": 1, "id": "c1b26be9", "metadata": {}, "outputs": [], "source": ["#Databack\ndata_bkup = data.copy()"]}, {"cell_type": "markdown", "id": "307fb230", "metadata": {}, "source": ["## Build & Train Model\n\nBefore building the model we need to split the dataset into Training & Test/Validation data. We will split the data into 90:10 ratio where 90% of the data will be used for training & remaining 10% for test/validation.\n"]}, {"cell_type": "code", "execution_count": 1, "id": "d6f2fea6", "metadata": {}, "outputs": [], "source": ["#data split\ntrain_data, test_data = train_test_split(data, test_size = 0.1, random_state = 12, stratify=data['state'])"]}, {"cell_type": "code", "execution_count": 1, "id": "5fce2388", "metadata": {}, "outputs": [], "source": ["train_data.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "f1b653b3", "metadata": {}, "outputs": [], "source": ["#reseting index for test_data\ntest_data.reset_index(drop=True, inplace=True)\n\n#resting index for train_data\ntrain_data.reset_index(drop=True, inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "0007dc55", "metadata": {}, "outputs": [], "source": ["#Shape of train & test data\nprint(\"Training Data Shape - \",train_data.shape, \" Test Data Shape - \", test_data.shape)"]}, {"cell_type": "markdown", "id": "d7ecc6ce", "metadata": {}, "source": ["Now, here comes the fun part ..\n\n![](https://i2.wp.com/neptune.ai/wp-content/uploads/fastai_logo.png?fit=406%2C194&ssl=1)\n\n\nWe need to prep our text data for 2 different models i.e. **Language & Classification Model**. This can be done using the FastAI libraries\n"]}, {"cell_type": "code", "execution_count": 1, "id": "9f2006a0", "metadata": {}, "outputs": [], "source": ["#Language Model\nlang_mod = TextLMDataBunch.from_df(train_df= train_data, valid_df=test_data, path='')\n\n#Classification Model\nclass_mod = TextClasDataBunch.from_df(path='', train_df=train_data, valid_df=test_data, vocab=lang_mod.train_ds.vocab, bs=32)"]}, {"cell_type": "markdown", "id": "f97025f9", "metadata": {}, "source": ["Creating a language learner based on the language model (lang_mod) created above."]}, {"cell_type": "code", "execution_count": 1, "id": "72462beb", "metadata": {}, "outputs": [], "source": ["lang_learner = language_model_learner(lang_mod, arch = AWD_LSTM, pretrained = True, drop_mult=0.3)"]}, {"cell_type": "code", "execution_count": 1, "id": "1df9db10", "metadata": {}, "outputs": [], "source": ["#finding the learning rate for language learner\nlang_learner.lr_find()"]}, {"cell_type": "code", "execution_count": 1, "id": "ae8f9353", "metadata": {}, "outputs": [], "source": ["#Plotting the Recorder Plot\nlang_learner.recorder.plot()"]}, {"cell_type": "markdown", "id": "e416caf6", "metadata": {}, "source": ["And just like in the FastAI Image Classification tutorial we will use the **One Cycle** approach to train our language learner.  The learning rate is chosen based on the plot above.\nThe learning rate = 1e-2"]}, {"cell_type": "code", "execution_count": 1, "id": "88b90419", "metadata": {}, "outputs": [], "source": ["#Training the language learner model\nlang_learner.fit_one_cycle(1, 1e-2, moms=(0.8,0.7))"]}, {"cell_type": "markdown", "id": "e1dea386", "metadata": {}, "source": ["Note: Observe that we have achieved an accuracy of ~ 14% , which really bad."]}, {"cell_type": "code", "execution_count": 1, "id": "26975c2e", "metadata": {}, "outputs": [], "source": ["#Saving the language learner encoder\nlang_learner.save_encoder('fai_langlrn_enc')"]}, {"cell_type": "markdown", "id": "fe3ba52f", "metadata": {}, "source": ["Now, let's use the \"class_mod\" object created above to build a classifier and then fine-tune our language learner."]}, {"cell_type": "code", "execution_count": 1, "id": "3511ed0e", "metadata": {}, "outputs": [], "source": ["class_learner = text_classifier_learner(class_mod, drop_mult=0.3, arch = AWD_LSTM, pretrained = True)\nclass_learner.load_encoder('fai_langlrn_enc')"]}, {"cell_type": "code", "execution_count": 1, "id": "cc88ddd8", "metadata": {}, "outputs": [], "source": ["#finding the learning rate of this class_learner\nclass_learner.lr_find()"]}, {"cell_type": "code", "execution_count": 1, "id": "cac33549", "metadata": {}, "outputs": [], "source": ["#Plotting the Recorder Plot for the class learner\nclass_learner.recorder.plot()"]}, {"cell_type": "markdown", "id": "f0e5ec9b", "metadata": {}, "source": ["The learning rate from the above plot is 1e-03"]}, {"cell_type": "code", "execution_count": 1, "id": "9d0d2c90", "metadata": {}, "outputs": [], "source": ["#Training the Class Learner Model\nclass_learner.fit_one_cycle(1, 1e-3, moms=(0.8,0.7))"]}, {"cell_type": "markdown", "id": "5387b9e7", "metadata": {}, "source": ["As we can observe the accuracy has increased drastically. The current accuracy is at ~ 64% which is strictly OK for the purpose of this tutorial."]}, {"cell_type": "code", "execution_count": 1, "id": "788f7cc7", "metadata": {}, "outputs": [], "source": ["#saving the Class Learner Model\nclass_learner.save_encoder('fai_classlrn_enc_tuned')"]}, {"cell_type": "code", "execution_count": 1, "id": "18560cfa", "metadata": {}, "outputs": [], "source": ["#free memory\ngc.collect()\n"]}, {"cell_type": "markdown", "id": "59c4d6f2", "metadata": {}, "source": ["## Predictions\n\nWe will now try to get the predictions for the validation set  (test data) from our learner object"]}, {"cell_type": "code", "execution_count": 1, "id": "82b6c385", "metadata": {}, "outputs": [], "source": ["class_learner.show_results()"]}, {"cell_type": "code", "execution_count": 1, "id": "2c15bf7c", "metadata": {}, "outputs": [], "source": ["# predictions\npred, trgt = class_learner.get_preds()"]}, {"cell_type": "code", "execution_count": 1, "id": "e2a71397", "metadata": {}, "outputs": [], "source": ["#Confusion matrix\nprediction = np.argmax(pred, axis = 1)\npd.crosstab (prediction, trgt)"]}, {"cell_type": "code", "execution_count": 1, "id": "4f18366b", "metadata": {}, "outputs": [], "source": ["#Prediction on Test Dataset\ntest_dataset = pd.DataFrame({'blurb': test_data['blurb_norm'], 'actual_state' : test_data['state'] })\ntest_dataset = pd.concat([test_dataset, pd.DataFrame(prediction, columns = ['predicted_state'])], axis=1)\n\ntest_dataset.head()"]}, {"cell_type": "markdown", "id": "1123895b", "metadata": {}, "source": ["**Conclusion:** Here we conclude the tutorial for Text Classification using FastAI. As we can clearly observe that the model has an average accuracy due to which not all of the predicted_state is correct. \n\nThe next step from here is to fine-tune the model to increase the accuracy but that I shall keep it for some other day. Thank you for reading."]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}