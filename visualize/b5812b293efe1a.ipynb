{"cells": [{"cell_type": "markdown", "id": "16bb09a6", "metadata": {}, "source": ["## Bike Sharing Assignment (BoomBikes) Linear Regression Model"]}, {"cell_type": "markdown", "id": "4bd46efb", "metadata": {}, "source": ["### Business Goal:\nYou are required to model the demand for shared bikes with the available independent variables. It will be used by the management to understand how exactly the demands vary with different features. They can accordingly manipulate the business strategy to meet the demand levels and meet the customer's expectations. Further, the model will be a good way for management to understand the demand dynamics of a new market. \n\n- Which variables are significant in predicting the demand for shared bikes.\n- How well those variables describe the bike demands"]}, {"cell_type": "code", "execution_count": 1, "id": "f1ec7505", "metadata": {}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]}, {"cell_type": "markdown", "id": "6eb1b64b", "metadata": {}, "source": ["### Importing and Understanding Data"]}, {"cell_type": "code", "execution_count": 1, "id": "3a73c4b4", "metadata": {}, "outputs": [], "source": ["# Supress Warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')"]}, {"cell_type": "code", "execution_count": 1, "id": "ad61bf4a", "metadata": {}, "outputs": [], "source": ["bikeSharing = pd.read_csv('/kaggle/input/boombikedata/day.csv')\nbikeSharing.head()"]}, {"cell_type": "markdown", "id": "bf4df4a5", "metadata": {}, "source": ["#### Inspect the various aspects of the bikeSharing dataframe"]}, {"cell_type": "code", "execution_count": 1, "id": "c90ae1af", "metadata": {}, "outputs": [], "source": ["bikeSharing.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "1fe376c1", "metadata": {}, "outputs": [], "source": ["bikeSharing.info()"]}, {"cell_type": "code", "execution_count": 1, "id": "225e93b6", "metadata": {}, "outputs": [], "source": ["# percentage of missing values in each column\nround(100*(bikeSharing.isnull().sum()/len(bikeSharing)), 2).sort_values(ascending=False)"]}, {"cell_type": "markdown", "id": "3ed3bc93", "metadata": {}, "source": ["#### No null/NA values identified "]}, {"cell_type": "code", "execution_count": 1, "id": "50160990", "metadata": {}, "outputs": [], "source": ["bikeSharing.describe()"]}, {"cell_type": "markdown", "id": "c73b554e", "metadata": {}, "source": ["## Data Preparation and Visualising\n\nLet's now spend some time doing what is arguably the most important step - **understanding the data**.\n- If there is some obvious multicollinearity going on, this is the first place to catch it\n- Here's where you'll also identify if some predictors directly have a strong association with the outcome variable\n\nWe'll visualise our data using `matplotlib` and `seaborn`."]}, {"cell_type": "code", "execution_count": 1, "id": "5105f7bb", "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\n# setting the style for seaborn plots\n%matplotlib inline"]}, {"cell_type": "markdown", "id": "8dfdc9c7", "metadata": {}, "source": ["### Drop  unnessasary variables \n   - instant - index variale, so it doest make any siginficance for model, so we will consider to drop\n   - dteday - year ,month and weekday as separate columns, we can consider to drop\n   - casual,registered = cnt, we will consider to drop casual,registered and treate \"cnt\" as target variable "]}, {"cell_type": "code", "execution_count": 1, "id": "485d0c27", "metadata": {}, "outputs": [], "source": ["#Dropping instant and dteday since they dont have significance with data\nbikeSharing.drop(['instant','dteday','casual','registered'],inplace=True,axis=1)\nbikeSharing.head()"]}, {"cell_type": "markdown", "id": "20fed758", "metadata": {}, "source": ["### Converting categarical variables "]}, {"cell_type": "code", "execution_count": 1, "id": "1b6d5d8a", "metadata": {}, "outputs": [], "source": ["bikeSharing['season'] = bikeSharing['season'].map({1:'spring',2:'summer', 3:'fall', 4:'winter'})\nbikeSharing['mnth'] = bikeSharing['mnth'].map({1:'Jan',2:'Feb', 3:'Mar', 4:'Apr',5:'May',6:'Jun',7:'Jul',8:'Aug',9:'Sept',10:'Oct',11:'Nov',12:'Dec'})\nbikeSharing['weekday'] = bikeSharing['weekday'].map({0:'Sunday',1:'Monday',2:'Tuesday',3:'Wednesday',4:'Thursday',5:'Friday',6:'Saturday'})\nbikeSharing['weathersit'] = bikeSharing['weathersit'].map({1:'Clear-Partlycloudy',2:'Mist-Cloudy',3:'LightSnow-lightRain-Thunderstorm',4:'HeavyRain-IcePallets-Thunderstorm'})\n\nbikeSharing.head()\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "37f41473", "metadata": {}, "outputs": [], "source": ["bikeSharing.info()"]}, {"cell_type": "markdown", "id": "41d4fe28", "metadata": {}, "source": ["#### Visualising Categorical Variables\n\nAs we can notice, there are a few categorical variables as well. Let's make a boxplot for some of these variables with target variables to find best target variable."]}, {"cell_type": "code", "execution_count": 1, "id": "81c0fd32", "metadata": {}, "outputs": [], "source": ["def boxplot_cat_var(cat_var,target):\n    plt.figure(figsize=(20, 12))\n    for i in range(0,len(cat_var)):\n        plt.subplot(2,3,i+1)\n        sns.boxplot(x = cat_var[i], y = target, data = bikeSharing)\n    plt.show()\n\ncat_var =['season','yr','holiday','weekday','workingday','weathersit']\nboxplot_cat_var(cat_var,'cnt')"]}, {"cell_type": "code", "execution_count": 1, "id": "773d1ffb", "metadata": {}, "outputs": [], "source": ["sns.boxplot(x = 'mnth', y = 'cnt', data = bikeSharing)"]}, {"cell_type": "markdown", "id": "6c0f5438", "metadata": {}, "source": ["### Observations:\n#### season: \n- Almost 32% of the bike booking were happening in fall with a median of over 5000 booking (for the period of 2 years). This was followed by summer & winter with 27% & 25% of total booking. This indicates, season can be a good predictor for the dependent variable.\n####  yr: \n- Almost 99% of the bike booking were increased in year with median of close to previus year booking (for the period of 2 years). This indicates, yr can be a good predictor for the dependent variable\n####  weathersit:\n- Almost 67% of the bike booking were happening during \u2018Clear-Partlycloudy with a median of close to 5000 booking (for the period of 2 years). This was followed by Mist-Cloudy with 30% of total booking. This indicates, weathersit does show some trend towards the bike bookings can be a good predictor for the dependent variable.\n####  holiday: \n- Almost 97.6% of the bike booking were happening when it is not a holiday which means this data is clearly biased. This indicates, holiday CANNOT be a good predictor for the dependent variable.\n#### weekday: \n- weekday variable shows very close trend (between 13.5%-14.8% of total booking on all days of the week) having their independent medians between 4000 to 5000 bookings. This variable can have some or no influence towards the predictor. I will let the model decide if this needs to be added or not.\n####  workingday: \n- Almost 69% of the bike booking were happening in \u2018workingday\u2019 with a median of close to 5000 booking (for the period of 2 years). This indicates, workingday can be a good predictor for the dependent variable\n####  mnth: \n- Almost 10% of the bike booking were happening in the months may,jun,jul,aug & sept with a median of over 4000 booking per month. This indicates, mnth has some trend for bookings and can be a good predictor for the dependent variable."]}, {"cell_type": "markdown", "id": "bfed3a90", "metadata": {}, "source": ["### Dummy Variables"]}, {"cell_type": "code", "execution_count": 1, "id": "d1e740d6", "metadata": {}, "outputs": [], "source": ["# Defining the map function\ndef dummies(x,df):\n    temp = pd.get_dummies(df[x], drop_first = True)\n    df = pd.concat([df, temp], axis = 1)\n    df.drop([x], axis = 1, inplace = True)\n    return df\n# Applying the function to the bikeSharing\n\nbikeSharing = dummies('season',bikeSharing)\nbikeSharing = dummies('mnth',bikeSharing)\nbikeSharing = dummies('weekday',bikeSharing)\nbikeSharing = dummies('weathersit',bikeSharing)\nbikeSharing.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "ddb9436b", "metadata": {}, "outputs": [], "source": ["bikeSharing.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "072e0089", "metadata": {}, "outputs": [], "source": ["bikeSharing.describe()"]}, {"cell_type": "markdown", "id": "cbf83f55", "metadata": {}, "source": ["- The heatmap clearly shows which all variable are multicollinear in nature, and which variable have high collinearity with the target variable.\n- We will refer this map back-and-forth while building the linear model so as to validate different correlated values along with VIF & p-value, for identifying the correct variable to select/eliminate from the model."]}, {"cell_type": "markdown", "id": "c342358f", "metadata": {}, "source": ["### Model Building\n#### Assumptions\n- Linear relationship\n- Multivariate normality\n- No or little multicollinearity\n- No auto-correlation\n- Homoscedasticity"]}, {"cell_type": "markdown", "id": "28cc3b6b", "metadata": {}, "source": ["## Splitting the Data into Training and Testing Sets"]}, {"cell_type": "code", "execution_count": 1, "id": "9a5e8e47", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n\n# We specify this so that the train and test data set always have the same rows, respectively\nnp.random.seed(0)\n\ndf_train, df_test = train_test_split(bikeSharing, train_size = 0.7, test_size = 0.3, random_state = 100)"]}, {"cell_type": "code", "execution_count": 1, "id": "6676b3a0", "metadata": {}, "outputs": [], "source": ["df_train.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "99fedc9b", "metadata": {}, "outputs": [], "source": ["df_test.shape"]}, {"cell_type": "markdown", "id": "a1ff7c1f", "metadata": {}, "source": ["#### Visualising Numeric Variables\n\nLet's make a pairplot of all the numeric variables"]}, {"cell_type": "code", "execution_count": 1, "id": "0885ccc2", "metadata": {}, "outputs": [], "source": ["# we can see patterns between variables \nsns.pairplot(df_train[[ 'temp','atemp', 'hum', 'windspeed','cnt']],diag_kind='kde')\nplt.show()"]}, {"cell_type": "markdown", "id": "bb568524", "metadata": {}, "source": ["#### The above Pair-Plot tells us that there is a LINEAR RELATION between 'temp','atemp' and 'cnt' , we can see both variables has close values, we will predict with model to remove one variable "]}, {"cell_type": "code", "execution_count": 1, "id": "04e4a07d", "metadata": {}, "outputs": [], "source": ["#Correlation using heatmap\nplt.figure(figsize = (30, 25))\nsns.heatmap(df_train.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()"]}, {"cell_type": "markdown", "id": "e399b258", "metadata": {}, "source": ["### Observations:\n- The heatmap clearly shows which all variable are multicollinear in nature, and which variable have high collinearity with the target variable.\n- We will refer this map back-and-forth while building the linear model so as to validate different correlated values along with VIF & p-value, for identifying the correct variable to select/eliminate from the model."]}, {"cell_type": "markdown", "id": "0ecfe5ec", "metadata": {}, "source": ["### Rescaling the Features \n\nWe will use MinMax scaling."]}, {"cell_type": "code", "execution_count": 1, "id": "e2120c1d", "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()"]}, {"cell_type": "code", "execution_count": 1, "id": "f3e40c49", "metadata": {}, "outputs": [], "source": ["# Apply scaler() to all the columns except 'dummy' variables\nnum_vars = ['temp','atemp', 'hum', 'windspeed', 'cnt']\n\ndf_train[num_vars] = scaler.fit_transform(df_train[num_vars])\n\ndf_train.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "890a3a90", "metadata": {}, "outputs": [], "source": ["df_train.describe()"]}, {"cell_type": "markdown", "id": "35376320", "metadata": {}, "source": ["### Dividing into X and Y sets for the model building"]}, {"cell_type": "code", "execution_count": 1, "id": "3765bbbb", "metadata": {}, "outputs": [], "source": ["y_train = df_train.pop('cnt')\nX_train = df_train"]}, {"cell_type": "markdown", "id": "e87fea02", "metadata": {}, "source": ["### RFE\nRecursive feature elimination"]}, {"cell_type": "code", "execution_count": 1, "id": "9b83df26", "metadata": {}, "outputs": [], "source": ["#importing libs for RFE\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.api as sm \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor"]}, {"cell_type": "code", "execution_count": 1, "id": "a205acb7", "metadata": {}, "outputs": [], "source": ["# Running RFE with the output number of the variable equal to 15\nlm = LinearRegression()\nlm.fit(X_train,y_train)\nrfe = RFE(lm, 15)\nrfe = rfe.fit(X_train, y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "508c8498", "metadata": {}, "outputs": [], "source": ["list(zip(X_train.columns,rfe.support_,rfe.ranking_))"]}, {"cell_type": "code", "execution_count": 1, "id": "fe38982a", "metadata": {}, "outputs": [], "source": ["X_train.columns[rfe.support_]"]}, {"cell_type": "code", "execution_count": 1, "id": "e4bef10f", "metadata": {}, "outputs": [], "source": ["X_train.columns[~rfe.support_]"]}, {"cell_type": "markdown", "id": "031fa587", "metadata": {}, "source": ["#### Building model using statsmodel, for the detailed statistics"]}, {"cell_type": "code", "execution_count": 1, "id": "43d28e30", "metadata": {}, "outputs": [], "source": ["X_train_rfe = X_train[X_train.columns[rfe.support_]]\nX_train_rfe.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "b637a036", "metadata": {}, "outputs": [], "source": ["def build_model(X,y):\n    X = sm.add_constant(X) #Adding the constant\n    lm = sm.OLS(y,X).fit() # fitting the model\n    print(lm.summary()) # model summary\n    return X\n    \ndef checkVIF(X):\n    vif = pd.DataFrame()\n    vif['Features'] = X.columns\n    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    vif['VIF'] = round(vif['VIF'], 2)\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    return(vif)\n\n"]}, {"cell_type": "markdown", "id": "86718cdd", "metadata": {}, "source": ["#### MODEL 1"]}, {"cell_type": "code", "execution_count": 1, "id": "6694c31f", "metadata": {}, "outputs": [], "source": ["X_train_new = build_model(X_train_rfe,y_train)"]}, {"cell_type": "markdown", "id": "940a20b9", "metadata": {}, "source": ["p-vale of `Jan` seems to be higher than the significance value of 0.05, hence dropping it as it is insignificant in presence of other variables."]}, {"cell_type": "code", "execution_count": 1, "id": "a057588d", "metadata": {}, "outputs": [], "source": ["#Calculating the Variance Inflation Factor\ncheckVIF(X_train_new)"]}, {"cell_type": "code", "execution_count": 1, "id": "f729c203", "metadata": {}, "outputs": [], "source": ["X_train_new=X_train_new.drop([\"Jan\"], axis = 1)"]}, {"cell_type": "markdown", "id": "43881b1e", "metadata": {}, "source": ["#### MODEL 2"]}, {"cell_type": "code", "execution_count": 1, "id": "3b092cfe", "metadata": {}, "outputs": [], "source": ["X_train_new = build_model(X_train_new,y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "3adba2bd", "metadata": {}, "outputs": [], "source": ["#Calculating the Variance Inflation Factor\ncheckVIF(X_train_new)"]}, {"cell_type": "markdown", "id": "1e047645", "metadata": {}, "source": ["- we can see holiday has high P value , will consider to drop"]}, {"cell_type": "code", "execution_count": 1, "id": "2e043171", "metadata": {}, "outputs": [], "source": ["X_train_new=X_train_new.drop([\"holiday\"], axis = 1)"]}, {"cell_type": "markdown", "id": "38b5eb77", "metadata": {}, "source": ["#### MODEL 3"]}, {"cell_type": "code", "execution_count": 1, "id": "33381960", "metadata": {}, "outputs": [], "source": ["X_train_new = build_model(X_train_new,y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "629d3acf", "metadata": {}, "outputs": [], "source": ["#Calculating the Variance Inflation Factor\ncheckVIF(X_train_new)"]}, {"cell_type": "markdown", "id": "f94fe7dd", "metadata": {}, "source": ["- spring has high VIF and p value, so we will consider to drop"]}, {"cell_type": "code", "execution_count": 1, "id": "5411d5c7", "metadata": {}, "outputs": [], "source": ["X_train_new=X_train_new.drop([\"spring\"], axis = 1)"]}, {"cell_type": "markdown", "id": "c346e102", "metadata": {}, "source": ["#### MODEL 4"]}, {"cell_type": "code", "execution_count": 1, "id": "7fbe4f01", "metadata": {}, "outputs": [], "source": ["X_train_new = build_model(X_train_new,y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "3d994d76", "metadata": {}, "outputs": [], "source": ["#Calculating the Variance Inflation Factor\ncheckVIF(X_train_new)"]}, {"cell_type": "markdown", "id": "a9dbdfa5", "metadata": {}, "source": ["- Jul has high P value comparativly, we will consider to drop"]}, {"cell_type": "code", "execution_count": 1, "id": "856b9451", "metadata": {}, "outputs": [], "source": ["X_train_new=X_train_new.drop([\"Jul\"], axis = 1)"]}, {"cell_type": "markdown", "id": "e2272077", "metadata": {}, "source": ["#### MODEL 5"]}, {"cell_type": "code", "execution_count": 1, "id": "84562b15", "metadata": {}, "outputs": [], "source": ["X_train_new = build_model(X_train_new,y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "736fcbfd", "metadata": {}, "outputs": [], "source": ["#Calculating the Variance Inflation Factor\ncheckVIF(X_train_new)"]}, {"cell_type": "markdown", "id": "88fd5551", "metadata": {}, "source": ["#### We can see P values 0 and VIF less than 2, now we will check for correlation between remaining varibales"]}, {"cell_type": "code", "execution_count": 1, "id": "9690824f", "metadata": {}, "outputs": [], "source": ["#Correlation using heatmap\nplt.figure(figsize = (30, 25))\nsns.heatmap(X_train_new.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()"]}, {"cell_type": "markdown", "id": "91c8c70b", "metadata": {}, "source": ["#### We can see workingday and Saturday high negative correlation value"]}, {"cell_type": "code", "execution_count": 1, "id": "4a5e7384", "metadata": {}, "outputs": [], "source": ["X_train_new=X_train_new.drop([\"workingday\"], axis = 1)"]}, {"cell_type": "markdown", "id": "37aad673", "metadata": {}, "source": ["#### MODEL 6"]}, {"cell_type": "code", "execution_count": 1, "id": "e2c04dd7", "metadata": {}, "outputs": [], "source": ["X_train_new = build_model(X_train_new,y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "c050871c", "metadata": {}, "outputs": [], "source": ["X_train_new=X_train_new.drop([\"Saturday\"], axis = 1)"]}, {"cell_type": "markdown", "id": "d45da273", "metadata": {}, "source": ["#### MODEL 7"]}, {"cell_type": "code", "execution_count": 1, "id": "18c8debd", "metadata": {}, "outputs": [], "source": ["X_train_new = build_model(X_train_new,y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "3492b000", "metadata": {}, "outputs": [], "source": ["#Calculating the Variance Inflation Factor\ncheckVIF(X_train_new)"]}, {"cell_type": "markdown", "id": "df431440", "metadata": {}, "source": ["### Observations \n- Model looks perfect with 9 variables and R-squared- 83.3 , Adj. R-squared- 83\n- VIF less than - 2\n- P values - 0\n- Prob (F-statistic) - almost equal to - 0"]}, {"cell_type": "markdown", "id": "6b3ffe98", "metadata": {}, "source": ["### Residual Analysis of Model"]}, {"cell_type": "code", "execution_count": 1, "id": "4af88189", "metadata": {}, "outputs": [], "source": ["lm = sm.OLS(y_train,X_train_new).fit()\ny_train_cnt= lm.predict(X_train_new)"]}, {"cell_type": "code", "execution_count": 1, "id": "f96b3258", "metadata": {}, "outputs": [], "source": ["# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((y_train - y_train_cnt), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)   "]}, {"cell_type": "markdown", "id": "e593e2ca", "metadata": {}, "source": ["Error terms seem to be approximately normally distributed, so the assumption on the linear modeling seems to be fulfilled."]}, {"cell_type": "markdown", "id": "82322eef", "metadata": {}, "source": ["### Prediction and Evaluation"]}, {"cell_type": "code", "execution_count": 1, "id": "b0a0d6a8", "metadata": {}, "outputs": [], "source": ["#Scaling the test set\nnum_vars = ['temp','atemp', 'hum', 'windspeed', 'cnt']\ndf_test[num_vars] = scaler.fit_transform(df_test[num_vars])"]}, {"cell_type": "markdown", "id": "1fc56078", "metadata": {}, "source": ["### Dividing the testset into X and Y sets for the model building"]}, {"cell_type": "code", "execution_count": 1, "id": "390bd314", "metadata": {}, "outputs": [], "source": ["#Dividing into X and y\ny_test = df_test.pop('cnt')\nX_test = df_test"]}, {"cell_type": "code", "execution_count": 1, "id": "1b131dd4", "metadata": {}, "outputs": [], "source": ["# Now let's use our model to make predictions.\nX_train_new = X_train_new.drop('const',axis=1)\n# Creating X_test_new dataframe by dropping variables from X_test\nX_test_new = X_test[X_train_new.columns]\n\n# Adding a constant variable \nX_test_new = sm.add_constant(X_test_new)"]}, {"cell_type": "code", "execution_count": 1, "id": "f20f5657", "metadata": {}, "outputs": [], "source": ["# Making predictions\ny_pred = lm.predict(X_test_new)"]}, {"cell_type": "markdown", "id": "6663c8b9", "metadata": {}, "source": ["#### Evaluation of test via comparison of y_pred and y_test"]}, {"cell_type": "code", "execution_count": 1, "id": "eba3f439", "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import r2_score \nr2=r2_score(y_test, y_pred)\nprint(r2)"]}, {"cell_type": "markdown", "id": "f3e5c391", "metadata": {}, "source": ["### Adjusted R^2 Value for TEST"]}, {"cell_type": "code", "execution_count": 1, "id": "cc5a7715", "metadata": {}, "outputs": [], "source": ["X_test_new.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "f579bae8", "metadata": {}, "outputs": [], "source": ["# We already have the value of R^2 (calculated in above step)\n# n is number of rows in X\nn = X_test_new.shape[0]\n\n# Number of features (predictors, p) is the shape along axis 1\np = X_test_new.shape[1]\n\n# We find the Adjusted R-squared using the formula\n\nadjusted_r2 = 1-(1-r2)*(n-1)/(n-p-1)\nadjusted_r2"]}, {"cell_type": "code", "execution_count": 1, "id": "3a982aac", "metadata": {}, "outputs": [], "source": ["#EVALUATION OF THE MODEL\n# Plotting y_test and y_pred to understand the spread.\nfig = plt.figure()\nplt.scatter(y_test,y_pred)\nfig.suptitle('y_test vs y_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_pred', fontsize=16)   "]}, {"cell_type": "markdown", "id": "39ebdcfe", "metadata": {}, "source": ["### Final Results \n- Train R^2 :0.833\n- Train Adjusted R^2 :0.830\n- Test R^2 :0.800\n- Test Adjusted R^2 :0.791\n#### This seems to be a really good model that can very well 'Generalize' various datasets."]}, {"cell_type": "code", "execution_count": 1, "id": "cfcdd4e0", "metadata": {}, "outputs": [], "source": ["print(lm.summary())"]}, {"cell_type": "markdown", "id": "b9a951af", "metadata": {}, "source": ["### We can see that the equation of our best fitted line is:\n- cnt= 0.2215+0.2292 *yr+0.5754 * temp-0.1755 *hum-0.1890 *windspeed+0.0909 *summer+0.1391 *winter+0.1034 *sept-0.2320 *LightSnow-lightRain-Thunderstorm-0.0499 *Mist-Cloudy"]}, {"cell_type": "markdown", "id": "79a045bd", "metadata": {}, "source": ["### Hypothesis Testing\n##### Hypothesis testing states that:\n\n- H0:B1=B2=...=Bn=0\n- H1: at least one Bi!=0\n\n- From the final model summary, it is evident that all our coefficients are not equal to zerowhich means We REJECT the NULL HYPOTHESIS\n\n"]}, {"cell_type": "markdown", "id": "30a97034", "metadata": {}, "source": ["### Analysing the above model, the comapny should focus on the following features:\n\n- year: The company should encounter an increase in the number of users when the situation comes back to normal as compared to 2019.\n- season: The company should focus on expanding it's business in the Summer and the Fall season.\n- weather: The users prefer to rent a bike when the weather is pleasant i.e. either clear or cloudy.\n- temp: The users prefer to ride or rent a bike in a moderate temperature.\n\n### Hence when the situation comes back to normal, the company should face an increase in the business as compared to 2019 and should expand it's business with new availing offers or schemes in the season of summer and fall when the weather is pleasant with clear sky and moderate temperature."]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}