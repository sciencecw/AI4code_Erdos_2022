{"cells": [{"cell_type": "code", "execution_count": 1, "id": "b99d60d4", "metadata": {}, "outputs": [], "source": ["from IPython import display\ndisplay.Image('https://raw.githubusercontent.com/Dutta-SD/Images_Unsplash/master/Kaggle/dorian-mongel-5Rgr_zI7pBw-unsplash.jpg', width = 3000, height = 500)"]}, {"cell_type": "markdown", "id": "4db5f579", "metadata": {}, "source": ["# Titanic - An Introduction to Neural Networks\n\nThe titanic disaster is one of the major disasters that the world has faced. It led to tragic loss of lives and destruction of the beautiful Titanic ship.\n\n# Objective-To predict survival with simple explanation\n\nLet us use Machine Learning to try to predict which passengers survived and which passengers did not. I will try to explain as simply as possible so that beginners can unserstand it easily"]}, {"cell_type": "markdown", "id": "68662caa", "metadata": {}, "source": ["# Importing the data\n* In Machine Learning, our objective is to train a model which will 'train' from some data and then predict on 'test' data.\n* So, first we would read data using pandas.\n* Here, the train data and the test data are named _train.csv_ and _test.csv_"]}, {"cell_type": "code", "execution_count": 1, "id": "6af9ef87", "metadata": {}, "outputs": [], "source": ["# Import Necessary libraries\nimport pandas as pd\nimport numpy as np"]}, {"cell_type": "code", "execution_count": 1, "id": "976da99e", "metadata": {}, "outputs": [], "source": ["# import dataset\ntrain_data = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest_data = pd.read_csv('/kaggle/input/titanic/test.csv')"]}, {"cell_type": "code", "execution_count": 1, "id": "ba5f856e", "metadata": {}, "outputs": [], "source": ["# Head of training data\ntrain_data.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "020ae1c8", "metadata": {}, "outputs": [], "source": ["# Head of submission file\ntest_data.head()"]}, {"cell_type": "markdown", "id": "ef64eb79", "metadata": {}, "source": ["* The Name, Ticket, Cabin, PassengerId columns do not seem to be meaningful. Let us drop those columns.\n\n* The Cabin column is full of _NaN_ values, that is null values, so we should better drop it.\n\n* Dropping Data might lead to loss of accuracy"]}, {"cell_type": "code", "execution_count": 1, "id": "ed67d1f7", "metadata": {}, "outputs": [], "source": ["PassengerID = test_data.PassengerId\n## code for dropping data\ntrain_data.drop(['Name', 'Ticket', 'Cabin', 'PassengerId'], inplace=True, axis=1)\ntest_data.drop(['Name', 'Ticket', 'Cabin', 'PassengerId'], inplace=True, axis=1)\n\ntest_data.head()"]}, {"cell_type": "markdown", "id": "60ae20df", "metadata": {}, "source": ["# Null Value Management\nNow, let us try and see if there are null values or not. Null or NAN values represent data that is missing.\nDropping them might be useful sometimes, but it is generally better to replace them with some suitable value."]}, {"cell_type": "code", "execution_count": 1, "id": "51cfa121", "metadata": {}, "outputs": [], "source": ["# Check for NaN values\nprint(train_data.isnull().any())"]}, {"cell_type": "code", "execution_count": 1, "id": "8ecf139b", "metadata": {}, "outputs": [], "source": ["test_data.isnull().any()"]}, {"cell_type": "markdown", "id": "e387489e", "metadata": {}, "source": ["So we see that there are indeed null values.We are going to fix them real soon."]}, {"cell_type": "markdown", "id": "9981ad66", "metadata": {}, "source": ["# Split into independent and dependent features\n\n* We are trying to predict whether the passenger Survived or not. \n* So let us take the feature we want to predict to be 'y' and the training data to be X.\n\nWe will use X to predict y. So X is called Independent features and y is called dependent feautures. "]}, {"cell_type": "code", "execution_count": 1, "id": "74b2335d", "metadata": {}, "outputs": [], "source": ["# Split into dependent and independent dataframes\ny = train_data.Survived\n\n# drop the Survived columns from the independent features\n## Retaining only dependendt features\nX = train_data.drop(['Survived'], axis = 1)\n\nprint(y.head())\nprint(X.head())"]}, {"cell_type": "markdown", "id": "3130af79", "metadata": {}, "source": ["# Exploratory Data Analysis\n* Visualisation will help us explore the data. This can give us a lot of important information about the nature of data.\n* This step should never be skipped."]}, {"cell_type": "code", "execution_count": 1, "id": "7eaaeecb", "metadata": {}, "outputs": [], "source": ["## Useful Plotting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns"]}, {"cell_type": "markdown", "id": "eee15244", "metadata": {}, "source": ["### 1. BarPlot \nThis plot is used to visualise the number of passengers that survived v/s passengers that died"]}, {"cell_type": "code", "execution_count": 1, "id": "00e3cf25", "metadata": {}, "outputs": [], "source": ["# survived\nsns.barplot(x = y.unique(), y = y.value_counts());"]}, {"cell_type": "markdown", "id": "8dff073f", "metadata": {}, "source": ["### 2. PairPlot\nLet us visualise the pairplot between different varibles to see their relation to each other"]}, {"cell_type": "code", "execution_count": 1, "id": "6e4bfb2a", "metadata": {}, "outputs": [], "source": ["# Pairplot\nsns.pairplot(data = train_data, corner = True, palette = 'summer');"]}, {"cell_type": "markdown", "id": "40262df3", "metadata": {}, "source": ["# Training Phase\n1. Now we start to train the model\n2. We first split the data to train and validation set.\n3. Validation set is necessary so that you have some estimate on how the model performs on unseen data.\n4. Later you train the model on the whole data so that its performance increases"]}, {"cell_type": "code", "execution_count": 1, "id": "ee52957e", "metadata": {}, "outputs": [], "source": ["X_train, X_test, y_train, y_test = X, test_data, y, None"]}, {"cell_type": "code", "execution_count": 1, "id": "642c12b1", "metadata": {}, "outputs": [], "source": ["# The indexes are random order, we need to reset them\nX_train.reset_index(drop=True, inplace=True)\nX_test.reset_index(drop=True, inplace=True)\n\ny_train.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)\n\nX_train.info()"]}, {"cell_type": "code", "execution_count": 1, "id": "0566036e", "metadata": {}, "outputs": [], "source": ["# Lets separate the object data from numerical data\ns = (X_train.dtypes=='object')\ncategorical_cols = list(s[s].index)\n\n# Get numerical data column names\nnumerical_cols = [ i for i in X_train.columns if not i in categorical_cols ]\nnumerical_cols"]}, {"cell_type": "markdown", "id": "3f7dde0a", "metadata": {}, "source": ["# Filling NULL Values with KNN\n\n1. KNN Imputer imputes(fills null values) by using KNN. \n2. It takes k nearest data points of the point with missing values and fill the missing value in."]}, {"cell_type": "code", "execution_count": 1, "id": "f89d228b", "metadata": {}, "outputs": [], "source": ["from sklearn.impute import KNNImputer\n##from sklearn.preprocessing import StandardScaler   ## We turned off scaling here, you can try if you want\n\n# Imputer Object\nnm_imputer = KNNImputer()\n## ss is the scaler, you can try it if you want\n### We will not scale here\n# ss = StandardScaler()\n\n# Transform the necessary columns\nX_train_numerical = pd.DataFrame(nm_imputer.fit_transform(X_train[numerical_cols]),\n                                 columns = numerical_cols)\n###X_train_numerical = pd.DataFrame(ss.fit_transform(X_train_numerical[numerical_cols]), columns = numerical_cols)\n\nX_test_numerical = pd.DataFrame(nm_imputer.transform(X_test[numerical_cols]),\n                                 columns = numerical_cols)\n#X_test_numerical = pd.DataFrame(ss.transform(X_test_numerical[numerical_cols]), columns = numerical_cols)"]}, {"cell_type": "code", "execution_count": 1, "id": "3e2a9d67", "metadata": {}, "outputs": [], "source": ["# Drop the non required columns(with missing values)\nX_train = X_train.drop(numerical_cols, axis = 1)\nX_test = X_test.drop(numerical_cols, axis = 1)\n\n# put new colums in dataframe by joining\nX_train = X_train.join(X_train_numerical)\nX_test = X_test.join(X_test_numerical)\n\nX_train.isnull().any()"]}, {"cell_type": "markdown", "id": "891e135d", "metadata": {}, "source": ["# Simple Imputer\nSimple imputer imputes values with the values it is provided"]}, {"cell_type": "code", "execution_count": 1, "id": "3132826e", "metadata": {}, "outputs": [], "source": ["# Impute categorical columns\nfrom sklearn.impute import SimpleImputer\n\n# Imputer Object\nnm_imputer = SimpleImputer(strategy='most_frequent')\n\n# Transform the necessary columns\nX_train_numerical = pd.DataFrame(nm_imputer.fit_transform(X_train[categorical_cols]),\n                                 columns = categorical_cols)\n\nX_test_numerical = pd.DataFrame(nm_imputer.transform(X_test[categorical_cols]),\n                                 columns = categorical_cols)"]}, {"cell_type": "code", "execution_count": 1, "id": "c3bd40c7", "metadata": {}, "outputs": [], "source": ["# Drop the non required columns(with missing values)\nX_train = X_train.drop(categorical_cols, axis = 1)\nX_test = X_test.drop(categorical_cols, axis = 1)\n\n# put new colums in dataframe\nX_train = X_train.join(X_train_numerical)\nX_test = X_test.join(X_test_numerical)\n\nX_train.isnull().any()"]}, {"cell_type": "markdown", "id": "080f590f", "metadata": {}, "source": ["# One Hot Encoder\n\n1. Let us say we have some data like male or female.\n2. We can then create a column like isFemale where if it is 0, it would denote female else male\n3. This is the idea behind One hot encoding."]}, {"cell_type": "code", "execution_count": 1, "id": "bebe5b75", "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import OneHotEncoder\n\nOH_encoder = OneHotEncoder(handle_unknown = 'ignore', sparse=False)\n\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[categorical_cols]))\nOH_cols_test = pd.DataFrame(OH_encoder.transform(X_test[categorical_cols]) )\n\n#Reset the index\nOH_cols_train.index = X_train.index\nOH_cols_test.index = X_test.index\n\n# Remove Categorical Columns\nnum_X_train = X_train.drop(categorical_cols, axis = 1)\nnum_X_test = X_test.drop(categorical_cols, axis = 1)\n\n# Join\nX_train = num_X_train.join(OH_cols_train, how='left')\nX_test = num_X_test.join(OH_cols_test, how='left')\n\nX_train.head()\n\n                             "]}, {"cell_type": "code", "execution_count": 1, "id": "80309bc8", "metadata": {}, "outputs": [], "source": ["X_test.info()"]}, {"cell_type": "code", "execution_count": 1, "id": "e74d8d82", "metadata": {}, "outputs": [], "source": ["X_train.info()"]}, {"cell_type": "markdown", "id": "4a5ea264", "metadata": {}, "source": ["# Neural Network\n\nFinally, we will create our neural network model.\n\nA neural network looks like this:\n![Neural Network image](https://raw.githubusercontent.com/Dutta-SD/Images_Unsplash/master/Kaggle/Screenshot%20from%202020-08-24%2012-26-52.png)\n**Image taken from dair.ai github repository**\n* The inputs are given in the input layer, it passes through the hidden layers which transforms it. \n* It add's weights to the inputs and adds a term called bias. This helps to figure out which figure are important\n* It also uses some special functions called 'activations' which helps in giving non linearity to the network"]}, {"cell_type": "code", "execution_count": 1, "id": "2e7e84e2", "metadata": {}, "outputs": [], "source": ["# Create a validation set \nfrom sklearn.model_selection import train_test_split\n\nX_train_2, X_val, y_train_2,  y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 10)"]}, {"cell_type": "markdown", "id": "0167a0e8", "metadata": {}, "source": ["# And Finally, KERAS!\nKeras is the library we will be using for"]}, {"cell_type": "code", "execution_count": 1, "id": "4dbcfbfd", "metadata": {}, "outputs": [], "source": ["from tensorflow import keras"]}, {"cell_type": "code", "execution_count": 1, "id": "e7b665b3", "metadata": {}, "outputs": [], "source": ["from keras import Sequential\nfrom keras.layers import BatchNormalization, Dense\n## Dropout is a form of regularisation for neural networks"]}, {"cell_type": "markdown", "id": "a51048bc", "metadata": {}, "source": ["# 1. Create Model"]}, {"cell_type": "code", "execution_count": 1, "id": "d010a5ea", "metadata": {}, "outputs": [], "source": ["model = Sequential()\n\nmodel.add(Dense(128, activation = 'relu', input_shape = (10,) ))\nmodel.add(BatchNormalization())\nmodel.add(Dense(64, activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(8, activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(1, activation = 'sigmoid'))\n\nmodel.summary()"]}, {"cell_type": "markdown", "id": "51911bcd", "metadata": {}, "source": ["# 2. Compile It"]}, {"cell_type": "code", "execution_count": 1, "id": "28f220fb", "metadata": {}, "outputs": [], "source": ["model.compile(optimizer='adam',\n              loss=keras.losses.BinaryCrossentropy(),\n              metrics = ['accuracy'])"]}, {"cell_type": "markdown", "id": "8d7b3fbf", "metadata": {}, "source": ["# Initial Test to see how well the model is performing.\nAfter we are done, we will train using the entire dataset"]}, {"cell_type": "code", "execution_count": 1, "id": "e8b79856", "metadata": {}, "outputs": [], "source": ["history = model.fit(\n    X_train_2,\n    y_train_2,\n    batch_size=32,\n    epochs=20,\n    validation_data=(X_val, y_val)\n)"]}, {"cell_type": "markdown", "id": "77ed5f23", "metadata": {}, "source": ["# train with all"]}, {"cell_type": "code", "execution_count": 1, "id": "e6896508", "metadata": {}, "outputs": [], "source": ["model.compile(optimizer='adam',\n              loss=keras.losses.BinaryCrossentropy(),\n              metrics = ['accuracy'])\n\n\nhistory = model.fit(\n    X_train,\n    y_train,\n    batch_size=32,\n    epochs=20\n)"]}, {"cell_type": "markdown", "id": "b45606a3", "metadata": {}, "source": ["## Generate Predicitons"]}, {"cell_type": "code", "execution_count": 1, "id": "e9728606", "metadata": {}, "outputs": [], "source": ["y_preds = model.predict_classes(X_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "f2e5f601", "metadata": {}, "outputs": [], "source": ["y_preds[:2]"]}, {"cell_type": "markdown", "id": "271f050f", "metadata": {}, "source": ["## Generating submission File"]}, {"cell_type": "code", "execution_count": 1, "id": "054d1c83", "metadata": {}, "outputs": [], "source": ["file_name = \"MyTitanicSubmission.csv\"\n\ny_pred_series = pd.Series(y_preds.flatten(), name = 'Survived')\n\nfile = pd.concat([PassengerID, y_pred_series], axis = 1)\n\nfile.to_csv(file_name, index = False)"]}, {"cell_type": "markdown", "id": "0575b574", "metadata": {}, "source": ["# Final Step\n* Now save and commit the model\n* Go to Viewer Option\n* Get the output file in OUTPUT section and press Submit"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}