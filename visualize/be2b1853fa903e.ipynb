{"cells": [{"cell_type": "code", "execution_count": 1, "id": "8371af89", "metadata": {}, "outputs": [], "source": ["DATA_DIR = '../input/animefacedataset'"]}, {"cell_type": "markdown", "id": "473e3427", "metadata": {}, "source": ["#### I am using Kaggle notebook so that I don't have to download the data and GPUs here are much faster"]}, {"cell_type": "code", "execution_count": 1, "id": "46f4dd0e", "metadata": {}, "outputs": [], "source": ["image_size = 64\nbatch_size = 128\nstats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)"]}, {"cell_type": "code", "execution_count": 1, "id": "6d7a093a", "metadata": {}, "outputs": [], "source": ["import os\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms as tt\nimport torch\nimport torch.nn as nn\nimport cv2\nfrom tqdm.notebook import tqdm\nimport torch.nn.functional as F\nfrom torchvision.utils import save_image\nfrom torchvision.utils import make_grid\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline"]}, {"cell_type": "code", "execution_count": 1, "id": "e3399d84", "metadata": {}, "outputs": [], "source": ["train_ds = ImageFolder(DATA_DIR, transform=tt.Compose([\n    tt.Resize(image_size),\n    tt.CenterCrop(image_size),\n    tt.ToTensor(),\n    tt.Normalize(*stats)]))"]}, {"cell_type": "code", "execution_count": 1, "id": "50b53052", "metadata": {}, "outputs": [], "source": ["train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2, pin_memory=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "903492cb", "metadata": {}, "outputs": [], "source": ["def denorm(img_tensors):\n    return img_tensors * stats[1][0] + stats[0][0]\n\ndef show_images(images, nmax=64):\n    fig, ax = plt.subplots(figsize=(8, 8))\n    ax.set_xticks([]); ax.set_yticks([])\n    ax.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0))\n\ndef show_batch(dl, nmax=64):\n    for images, _ in dl:\n        show_images(images, nmax)\n        break"]}, {"cell_type": "code", "execution_count": 1, "id": "73749aaf", "metadata": {}, "outputs": [], "source": ["show_batch(train_dl)"]}, {"cell_type": "code", "execution_count": 1, "id": "28176034", "metadata": {}, "outputs": [], "source": ["def to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)"]}, {"cell_type": "code", "execution_count": 1, "id": "a20b7d79", "metadata": {}, "outputs": [], "source": ["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ndevice"]}, {"cell_type": "code", "execution_count": 1, "id": "2f876cd6", "metadata": {}, "outputs": [], "source": ["train_dl = DeviceDataLoader(train_dl, device)"]}, {"cell_type": "code", "execution_count": 1, "id": "041efe1b", "metadata": {}, "outputs": [], "source": ["discriminator = nn.Sequential(\n    # in: 3 x 64 x 64\n\n    nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(64),\n    nn.LeakyReLU(0.2, inplace=True),\n    # out: 64 x 32 x 32\n\n    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(128),\n    nn.LeakyReLU(0.2, inplace=True),\n    # out: 128 x 16 x 16\n\n    nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(256),\n    nn.LeakyReLU(0.2, inplace=True),\n    # out: 256 x 8 x 8\n\n    nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(512),\n    nn.LeakyReLU(0.2, inplace=True),\n    # out: 512 x 4 x 4\n\n    nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),\n    # out: 1 x 1 x 1\n\n    nn.Flatten(),\n    nn.Sigmoid())"]}, {"cell_type": "code", "execution_count": 1, "id": "7972541a", "metadata": {}, "outputs": [], "source": ["discriminator = to_device(discriminator, device)"]}, {"cell_type": "code", "execution_count": 1, "id": "58c79ec1", "metadata": {}, "outputs": [], "source": ["latent_size = 128"]}, {"cell_type": "code", "execution_count": 1, "id": "a8bcedc2", "metadata": {}, "outputs": [], "source": ["generator = nn.Sequential(\n    # in: latent_size x 1 x 1\n\n    nn.ConvTranspose2d(latent_size, 512, kernel_size=4, stride=1, padding=0, bias=False),\n    nn.BatchNorm2d(512),\n    nn.ReLU(True),\n    # out: 512 x 4 x 4\n\n    nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(256),\n    nn.ReLU(True),\n    # out: 256 x 8 x 8\n\n    nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(128),\n    nn.ReLU(True),\n    # out: 128 x 16 x 16\n\n    nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(64),\n    nn.ReLU(True),\n    # out: 64 x 32 x 32\n\n    nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.Tanh()\n    # out: 3 x 64 x 64\n)"]}, {"cell_type": "code", "execution_count": 1, "id": "dc181188", "metadata": {}, "outputs": [], "source": ["xb = torch.randn(batch_size, latent_size, 1, 1) \nfake_images = generator(xb)\nprint(fake_images.shape)\nshow_images(fake_images)"]}, {"cell_type": "code", "execution_count": 1, "id": "d7e711fc", "metadata": {}, "outputs": [], "source": ["generator = to_device(generator, device)"]}, {"cell_type": "code", "execution_count": 1, "id": "b4866bb3", "metadata": {}, "outputs": [], "source": ["sample_dir = 'generated'\nos.makedirs(sample_dir, exist_ok=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "ecdeda36", "metadata": {}, "outputs": [], "source": ["fixed_latent = torch.randn(64, latent_size, 1, 1, device=device)"]}, {"cell_type": "code", "execution_count": 1, "id": "9115c643", "metadata": {}, "outputs": [], "source": ["fixed_latent.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "fc0346b2", "metadata": {}, "outputs": [], "source": ["def save_samples(index, latent_tensors, show=True):\n    fake_images = generator(latent_tensors)\n    fake_fname = 'generated-images.png'.format(index)\n    save_image(denorm(fake_images), os.path.join(sample_dir, fake_fname), nrow=8)\n    print('Saving', fake_fname)\n    if show:\n        fig, ax = plt.subplots(figsize=(8, 8))\n        ax.set_xticks([]); ax.set_yticks([])\n        ax.imshow(make_grid(fake_images.cpu().detach(), nrow=8).permute(1, 2, 0))\n    return fake_images.cpu().detach()"]}, {"cell_type": "code", "execution_count": 1, "id": "3f86c562", "metadata": {}, "outputs": [], "source": ["model = {\n    \"discriminator\": discriminator.to(device),\n    \"generator\": generator.to(device)\n}\n\ncriterion = {\n    \"discriminator\": nn.BCELoss(),\n    \"generator\": nn.BCELoss()\n}\nlr = 0.0002\nepochs = 40\n"]}, {"cell_type": "code", "execution_count": 1, "id": "c2c0ed44", "metadata": {}, "outputs": [], "source": ["!nvidia-smi"]}, {"cell_type": "code", "execution_count": 1, "id": "2450379d", "metadata": {}, "outputs": [], "source": ["model[\"discriminator\"].train()\nmodel[\"generator\"].train()\ntorch.cuda.empty_cache()\n\n# Losses & scores\nlosses_g = []\nlosses_d = []\nreal_scores = []\nfake_scores = []\nsaved_samples = []\n\n# Create optimizers\noptimizer = {\n    \"discriminator\": torch.optim.Adam(model[\"discriminator\"].parameters(), \n                                      lr=lr, betas=(0.5, 0.999)),\n    \"generator\": torch.optim.Adam(model[\"generator\"].parameters(),\n                                  lr=lr, betas=(0.5, 0.999))\n}\n\nfor epoch in range(epochs):\n    loss_d_per_epoch = []\n    loss_g_per_epoch = []\n    real_score_per_epoch = []\n    fake_score_per_epoch = []\n    for real_images, _ in tqdm(train_dl):\n        # Train discriminator\n        # Clear discriminator gradients\n        optimizer[\"discriminator\"].zero_grad()\n\n        # Pass real images through discriminator\n        real_preds = model[\"discriminator\"](real_images)\n        real_targets = torch.ones(real_images.size(0), 1, device=device)\n        real_loss = criterion[\"discriminator\"](real_preds, real_targets)\n        cur_real_score = torch.mean(real_preds).item()\n\n        # Generate fake images\n        latent = torch.randn(batch_size, latent_size, 1, 1, device=device)\n        fake_images = model[\"generator\"](latent)\n\n        # Pass fake images through discriminator\n        fake_targets = torch.zeros(fake_images.size(0), 1, device=device)\n        fake_preds = model[\"discriminator\"](fake_images)\n        fake_loss = criterion[\"discriminator\"](fake_preds, fake_targets)\n        cur_fake_score = torch.mean(fake_preds).item()\n\n        real_score_per_epoch.append(cur_real_score)\n        fake_score_per_epoch.append(cur_fake_score)\n\n        # Update discriminator weights\n        loss_d = real_loss + fake_loss\n        loss_d.backward()\n        optimizer[\"discriminator\"].step()\n        loss_d_per_epoch.append(loss_d.item())\n\n\n        # Train generator\n        # Clear generator gradients\n        optimizer[\"generator\"].zero_grad()\n\n        # Generate fake images\n        latent = torch.randn(batch_size, latent_size, 1, 1, device=device)\n        fake_images = model[\"generator\"](latent)\n\n        # Try to fool the discriminator\n        preds = model[\"discriminator\"](fake_images)\n        targets = torch.ones(batch_size, 1, device=device)\n        loss_g = criterion[\"generator\"](preds, targets)\n\n        # Update generator weights\n        loss_g.backward()\n        optimizer[\"generator\"].step()\n        loss_g_per_epoch.append(loss_g.item())\n\n    # Record losses & scores\n    losses_g.append(np.mean(loss_g_per_epoch))\n    losses_d.append(np.mean(loss_d_per_epoch))\n    real_scores.append(np.mean(real_score_per_epoch))\n    fake_scores.append(np.mean(fake_score_per_epoch))\n\n    # Log losses & scores (last batch)\n    print(\"Epoch [{}/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}\".format(\n        epoch+1, epochs, \n        losses_g[-1], losses_d[-1], real_scores[-1], fake_scores[-1]))\n\n    # Save generated images\n    if epoch+1 in [5, 10, 20, 40]:\n        saved_samples.append(generator(fixed_latent).cpu().detach().numpy())\n        print('Samples are saved')"]}, {"cell_type": "markdown", "id": "ea5f15af", "metadata": {}, "source": ["#### Actually samples are not saved they are just recorded into python list"]}, {"cell_type": "code", "execution_count": 1, "id": "acf0ee56", "metadata": {}, "outputs": [], "source": ["saved_samples = np.array(saved_samples)\nsaved_samples.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "ad7fe513", "metadata": {}, "outputs": [], "source": ["real_imgs = []\nfor images, _ in train_dl:\n    real_imgs = images[:64].cpu().detach().numpy()\n    break\nreal_imgs.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "b60a24fd", "metadata": {}, "outputs": [], "source": ["real_imgs.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "70e4303c", "metadata": {}, "outputs": [], "source": ["show_images(torch.from_numpy(real_imgs)) # real images"]}, {"cell_type": "code", "execution_count": 1, "id": "c2c7aac2", "metadata": {}, "outputs": [], "source": ["show_images(torch.from_numpy(saved_samples[0])) # generated images when batch = 5"]}, {"cell_type": "code", "execution_count": 1, "id": "bbed4d19", "metadata": {}, "outputs": [], "source": ["show_images(torch.from_numpy(saved_samples[1])) # generated images when batch = 10"]}, {"cell_type": "code", "execution_count": 1, "id": "1d5c6f97", "metadata": {}, "outputs": [], "source": ["show_images(torch.from_numpy(saved_samples[2])) # generated images when batch = 20"]}, {"cell_type": "code", "execution_count": 1, "id": "b1d852a1", "metadata": {}, "outputs": [], "source": ["show_images(torch.from_numpy(saved_samples[3])) # generated images when batch = 40"]}, {"cell_type": "markdown", "id": "e183b42a", "metadata": {}, "source": ["#### Now let's cluster these images"]}, {"cell_type": "code", "execution_count": 1, "id": "6a46af46", "metadata": {}, "outputs": [], "source": ["import numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport pandas as pd\n# for loading/processing the images  \nfrom keras.preprocessing.image import load_img \nfrom keras.preprocessing.image import img_to_array \nfrom keras.applications.vgg16 import preprocess_input \n\n# models \nfrom keras.applications.vgg16 import VGG16 \nfrom keras.models import Model\n\n# clustering and dimension reduction\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\n# for everything else\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\nimport pandas as pd\nimport pickle"]}, {"cell_type": "code", "execution_count": 1, "id": "91d04bec", "metadata": {}, "outputs": [], "source": ["import keras\nfrom keras.applications.vgg16 import VGG16\nfrom keras.models import Model\nfrom keras.layers import Dense, GlobalAveragePooling2D, Dropout, UpSampling2D, Conv2D, MaxPooling2D, Activation, Flatten\n\nbase_model = VGG16(weights='imagenet', input_shape=(64, 64, 3), include_top=False)\n\nbase_out = base_model.output\nflt = Flatten()(base_out)\n\n# dns = Dense(units=100)(flt)\n\nmodel = Model(base_model.input, flt) #Let's leave it this way so that we keep al the information about image, also stardatization will be made further\n\n\n\n# model.layers[-1:][0].get_weights()[0]\n\n# shape = model.layers[-1:][0].get_weights()[0].shape #get shape of the last dense layer\n# print(shape)\n\n# weights = np.ones(shape) #(2048, 100) shaped ones\n# bias = np.zeros((100,)) #(100,) shaped zeros\n# model.layers[-1].set_weights([weights, bias]) #setting new weights to the last dense layer"]}, {"cell_type": "code", "execution_count": 1, "id": "6ee07771", "metadata": {}, "outputs": [], "source": ["model.summary()"]}, {"cell_type": "code", "execution_count": 1, "id": "c9864a50", "metadata": {}, "outputs": [], "source": ["real_imgs = real_imgs.transpose(0, 2, 3, 1)\nreal_imgs.shape # DO NOT RUN THESE ONES SEVERAL TIMES"]}, {"cell_type": "code", "execution_count": 1, "id": "3d2ff44e", "metadata": {}, "outputs": [], "source": ["saved_samples = saved_samples.transpose(0, 1, 3, 4, 2)\nsaved_samples.shape # DO NOT RUN THESE ONES SEVERAL TIMES"]}, {"cell_type": "code", "execution_count": 1, "id": "b5a09800", "metadata": {}, "outputs": [], "source": ["all_classes = np.append(saved_samples, [real_imgs], axis=0)\nall_classes.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "620b0d75", "metadata": {}, "outputs": [], "source": [" def extract_features(img, model):\n    # reshape the data for the model reshape(num_of_samples, dim 1, dim 2, channels)\n    reshaped_img = img.reshape(1,64,64,3) \n    # prepare image for model\n    imgx = preprocess_input(reshaped_img)\n    # get the feature vector\n    features = model.predict(imgx, use_multiprocessing=True)\n    return features"]}, {"cell_type": "code", "execution_count": 1, "id": "34fec187", "metadata": {}, "outputs": [], "source": ["dataset = []\nfor i in range(len(all_classes)):\n    print(i)\n    for j in range(len(all_classes[0])):\n        dataset.append(extract_features(all_classes[i,j], model)[0])"]}, {"cell_type": "code", "execution_count": 1, "id": "aae91a45", "metadata": {}, "outputs": [], "source": ["dataset = np.array(dataset)\ndataset.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "8b344acd", "metadata": {}, "outputs": [], "source": ["dataset.mean()"]}, {"cell_type": "code", "execution_count": 1, "id": "8d692c43", "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndataset = scaler.fit_transform(dataset)\ndataset.mean()"]}, {"cell_type": "code", "execution_count": 1, "id": "e2d9fd95", "metadata": {}, "outputs": [], "source": ["kmeans = KMeans(n_clusters=5, random_state=22)"]}, {"cell_type": "code", "execution_count": 1, "id": "b79abe98", "metadata": {}, "outputs": [], "source": ["%%time\nkmeans.fit(dataset)"]}, {"cell_type": "code", "execution_count": 1, "id": "1accf2c1", "metadata": {}, "outputs": [], "source": ["clusters = kmeans.predict(dataset)\nclasses = np.array([0]*64+[1]*64+[2]*64+[3]*64+[4]*64)"]}, {"cell_type": "code", "execution_count": 1, "id": "1dbc7fd3", "metadata": {}, "outputs": [], "source": ["np.unique(clusters, return_counts=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "148639b7", "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\nconfusion_matrix(classes, clusters)"]}, {"cell_type": "markdown", "id": "4f02d58e", "metadata": {}, "source": ["##### Judging by the confusion matrix cluster values should be adjusted (calibrated)\n##### Let's do this as follows: \n##### cluster=0 gets value 2 (because 21 observations of 64 of 2nd class are concentrated at cluster 0)\n#####                           cluster=1 gets value 4\n#####                           cluster=2 gets value 1\n#####                           cluster=3 gets value 3\n#####                           cluster=4 gets value 0"]}, {"cell_type": "code", "execution_count": 1, "id": "5d486fad", "metadata": {}, "outputs": [], "source": ["clusters"]}, {"cell_type": "code", "execution_count": 1, "id": "530d89a5", "metadata": {}, "outputs": [], "source": ["new_clusters = np.empty(5, dtype=int)\nnew_clusters[[0,1,2,3,4]]=[2,4,1,3,0]\nclusters = new_clusters[clusters] # The values have been replaced"]}, {"cell_type": "code", "execution_count": 1, "id": "b46a3d54", "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\nprint(confusion_matrix(classes, clusters))\nprint(classification_report(classes, clusters))"]}, {"cell_type": "markdown", "id": "105670d3", "metadata": {}, "source": ["### Results are not that bad, now let's see if it can distinguish between real images and generated by 40 epochs images"]}, {"cell_type": "code", "execution_count": 1, "id": "c80eae80", "metadata": {}, "outputs": [], "source": ["kmeans = KMeans(n_clusters=2, random_state=22)\nkmeans.fit(dataset[-128:])\npred = kmeans.predict(dataset[-128:])"]}, {"cell_type": "code", "execution_count": 1, "id": "5a0cbc9e", "metadata": {}, "outputs": [], "source": ["pred"]}, {"cell_type": "code", "execution_count": 1, "id": "7b090058", "metadata": {}, "outputs": [], "source": ["classes = np.array([1]*64+[0]*64)\nclasses"]}, {"cell_type": "code", "execution_count": 1, "id": "93305662", "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\nprint(confusion_matrix(pred, classes), '\\n\\n')\nprint(classification_report(pred, classes))"]}, {"cell_type": "markdown", "id": "be07bd2f", "metadata": {}, "source": ["### Great results! It does not struggle identifying fake images at all, although there are some missclassifications of real images"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}