{"cells": [{"cell_type": "code", "execution_count": 1, "id": "2e507be9", "metadata": {}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\nfrom math import sqrt\nfrom xgboost import XGBRegressor\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]}, {"cell_type": "code", "execution_count": 1, "id": "0e3bdc69", "metadata": {}, "outputs": [], "source": ["train_data = pd.read_csv('../input/30-days-of-ml/train.csv', index_col='id')\ntest_data = pd.read_csv('../input/30-days-of-ml/test.csv')\n\nfeature_columns = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8',\n     'cat9', 'cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6',\n       'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\nlen_features = len(feature_columns)\n\ntarget_cloumn = 'target'\nX_data = train_data[feature_columns]\ny_data = train_data[target_cloumn]\nX_test = test_data[feature_columns]\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_data, y_data, train_size=0.8, test_size=0.2,\n                                                      random_state=0)\nnumerical_cols = [col for col in X_train.columns if X_train[col].dtype != 'object']\ncategorical_cols = [col for col in X_train.columns if X_train[col].dtype == 'object']\nhigh_cardinality_cols = [col for col in categorical_cols  if X_train[col].nunique() >= 10]\nlow_cardinality_cols = [col for col in categorical_cols  if X_train[col].nunique() < 10]\n"]}, {"cell_type": "code", "execution_count": 1, "id": "6718f974", "metadata": {}, "outputs": [], "source": ["len(X_valid)"]}, {"cell_type": "code", "execution_count": 1, "id": "646ff5d9", "metadata": {}, "outputs": [], "source": ["print(high_cardinality_cols)\nlow_cardinality_cols"]}, {"cell_type": "code", "execution_count": 1, "id": "6c0f171c", "metadata": {}, "outputs": [], "source": ["i=0\na = [col for col in X_train.columns if X_train[col].isnull().any()]\nprint(a)"]}, {"cell_type": "markdown", "id": "ebe5a060", "metadata": {}, "source": ["**** dropping high cardinality cols ****"]}, {"cell_type": "code", "execution_count": 1, "id": "f8f65019", "metadata": {}, "outputs": [], "source": ["y=[]\nx=[]\nz=[]\nnumerical_transformer = SimpleImputer(strategy='median')\n\ncategorical_transformer1 = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\ncategorical_transformer2 = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('ordi', OrdinalEncoder())\n])\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer1, low_cardinality_cols),\n        ('ordi', categorical_transformer2, high_cardinality_cols),\n    ])\n\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "8e66a8ab", "metadata": {}, "outputs": [], "source": ["#reduced_X_train = X_train.drop(high_cardinality_cols, axis=1)\n#reduced_X_valid = X_valid.drop(high_cardinality_cols, axis=1)\nlabel_X_train = X_train.copy()\nlabel_X_valid = X_valid.copy()\n\nordinal_encoder = OrdinalEncoder()\nlabel_X_train[high_cardinality_cols] = ordinal_encoder.fit_transform(X_train[high_cardinality_cols])\nlabel_X_valid[high_cardinality_cols] = ordinal_encoder.transform(X_valid[high_cardinality_cols])\n\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(label_X_train[low_cardinality_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(label_X_valid[low_cardinality_cols]))\n\nOH_cols_train.index = label_X_train.index\nOH_cols_valid.index = label_X_valid.index\n\nnum_X_train = label_X_train.drop(low_cardinality_cols, axis=1)\nnum_X_valid = label_X_valid.drop(low_cardinality_cols, axis=1)\n\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "48de129c", "metadata": {}, "outputs": [], "source": ["my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\nmy_model.fit(OH_X_train, y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "056a943d", "metadata": {}, "outputs": [], "source": ["\nvalid_preds = my_model.predict(OH_X_valid)\nprint(valid_preds)\n"]}, {"cell_type": "code", "execution_count": 1, "id": "481b1697", "metadata": {}, "outputs": [], "source": ["\nscore = mean_squared_error(y_valid, valid_preds)\nmae = mean_absolute_error(y_valid, valid_preds)\nprint(mae)\nprint('MSE:', score)\nrscore = sqrt(score)\nprint('RMSE:', rscore)"]}, {"cell_type": "code", "execution_count": 1, "id": "2773dda9", "metadata": {}, "outputs": [], "source": ["output = pd.DataFrame({'id': test_data.id ,'target': preds})\noutput.to_csv('submission_1.csv', index=False)"]}, {"cell_type": "code", "execution_count": 1, "id": "73602f40", "metadata": {}, "outputs": [], "source": ["XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.05, max_delta_step=0, max_depth=6,\n             min_child_weight=1, missing=nan, monotone_constraints='()',\n             n_estimators=1000, n_jobs=4, num_parallel_tree=1, random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n             tree_method='exact', validate_parameters=1, verbosity=None)\n\nMSE: 0.5219331100061471\nRMSE: 0.7224493823141848\n    \n#drop high cardinality columns"]}, {"cell_type": "code", "execution_count": 1, "id": "befe1dec", "metadata": {}, "outputs": [], "source": ["model = RandomForestRegressor(n_estimators=150,random_state=0)\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),('model', model)])\n# Preprocessing of training data, fit model \nmy_pipeline.fit(X_train, y_train)\npreds = my_pipeline.predict(X_valid)\n\n    # Evaluate the model\nscore = mean_squared_error(y_valid, preds)\nprint('MSE:', score)\nrscore = sqrt(score)\nprint('RMSE:', rscore)\ny.append(rscore)\nz.append(score)\n\n#7.36\n#drop cat9 col..with high cardinality\npreds = my_pipeline.predict(X_test)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}