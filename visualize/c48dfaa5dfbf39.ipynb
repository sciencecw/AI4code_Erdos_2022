{"cells": [{"cell_type": "markdown", "id": "dd07a9ae", "metadata": {}, "source": ["# California Housing Price Prediction"]}, {"cell_type": "markdown", "id": "f72c7cd2", "metadata": {}, "source": ["## Step 1: Frame the problem"]}, {"cell_type": "markdown", "id": "204f77fd", "metadata": {}, "source": ["- Objective: Predicting the median housing price for given block.\n- Problem Type: Supervise & Regression\n\nThe dataset contains information about houses in California district, obtained from 1990 California census.\nThere are around 20000 records along with 10 features in the dataset. Feature names are self explanatory: longitude,\tlatitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, median_house_value, ocean_proximity\n\nImportant to remember that the each record in dataset is not about the house but it is about the block.\n\n1. longitude: A measure of how far west a house is; a higher value is farther west\n\n2. latitude: A measure of how far north a house is; a higher value is farther north\n\n3. housingMedianAge: Median age of a house within a block; a lower number is a newer building\n\n4. totalRooms: Total number of rooms within a block\n\n5. totalBedrooms: Total number of bedrooms within a block\n\n6. population: Total number of people residing within a block\n\n7. households: Total number of households, a group of people residing within a home unit, for a block\n\n8. medianIncome: Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n\n9. medianHouseValue: Median house value for households within a block (measured in US Dollars)\n\n10. oceanProximity: Location of the house w.r.t ocean/sea\n\n\n- Few thing to remeber about dataset:\n    - The median income, housing median age and the median house value were capped. \n    - Median income not expressed in US dollars. The data has been scaled and capped at 15 for higher median incomes, and at 0.5 for lower median incomes. \n    - Capping median house value may be a serious problem since it is the target attribute (your labels). Machine Learning algorithms may learn that prices never go beyond that limit."]}, {"cell_type": "code", "execution_count": 1, "id": "19f2e00e", "metadata": {}, "outputs": [], "source": ["%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")"]}, {"cell_type": "code", "execution_count": 1, "id": "b8154755", "metadata": {}, "outputs": [], "source": ["data = pd.read_csv(\"../input/california-housing-prices/housing.csv\")\nprint(data.shape)\ndata.head()"]}, {"cell_type": "markdown", "id": "670544e2", "metadata": {}, "source": ["## Step 2: Data Exploratoration"]}, {"cell_type": "code", "execution_count": 1, "id": "b57a3b35", "metadata": {}, "outputs": [], "source": ["data_explore = data.copy()"]}, {"cell_type": "code", "execution_count": 1, "id": "63abc9fc", "metadata": {}, "outputs": [], "source": ["data_explore.info()"]}, {"cell_type": "markdown", "id": "5d43788c", "metadata": {}, "source": ["Only total_bedrooms column contain null values, total 207."]}, {"cell_type": "code", "execution_count": 1, "id": "292a6927", "metadata": {}, "outputs": [], "source": ["data_explore.describe()"]}, {"cell_type": "markdown", "id": "44c9f27c", "metadata": {}, "source": ["Comparing mean, std or min, 25% or max 75%, we can see that there are some outliers in some columns."]}, {"cell_type": "markdown", "id": "2f6165dc", "metadata": {}, "source": ["### Histograms"]}, {"cell_type": "code", "execution_count": 1, "id": "a66dee34", "metadata": {}, "outputs": [], "source": ["data_explore.hist(figsize=(15, 8))"]}, {"cell_type": "markdown", "id": "fd3d0ed7", "metadata": {}, "source": ["- Observations:\n    - There are many blocks for which median housing prize lies in between 2.5 to 5.5\n    - Many histograms are tail heavy: they extend much farther to the right of the median than to the left. This may make it a bit harder for some Machine Learning algorithms to detect patterns."]}, {"cell_type": "markdown", "id": "bb41abb9", "metadata": {}, "source": ["### Outliers"]}, {"cell_type": "code", "execution_count": 1, "id": "54cb6e37", "metadata": {}, "outputs": [], "source": ["columns = ['households', 'population', 'total_bedrooms', 'total_rooms']\nplt.figure(figsize=(15, 8))\nsns.boxplot(data=data_explore[columns])\nplt.ylim((-100, 7000))"]}, {"cell_type": "code", "execution_count": 1, "id": "81b77a2f", "metadata": {}, "outputs": [], "source": ["Q1 = data_explore.quantile(0.25)\nQ3 = data_explore.quantile(0.75)\nIQR = Q3 - Q1\n((data_explore < (Q1 - 1.5 * IQR)) | (data_explore > (Q3 + 1.5 * IQR))).sum()"]}, {"cell_type": "code", "execution_count": 1, "id": "8e0105b7", "metadata": {}, "outputs": [], "source": ["data_explore['total_bedrooms'].mean(), data_explore['total_bedrooms'].median()"]}, {"cell_type": "markdown", "id": "fc867ce5", "metadata": {}, "source": ["There are more than 1000 outliers, I will replace them by median instead of mean."]}, {"cell_type": "code", "execution_count": 1, "id": "3cf6947f", "metadata": {}, "outputs": [], "source": ["median = data_explore['total_bedrooms'].median()\ndata_explore['total_bedrooms'].fillna(value=median, inplace=True)\ndata_explore['total_bedrooms'].isna().sum()"]}, {"cell_type": "markdown", "id": "8df3a4a3", "metadata": {}, "source": ["### Median Housing Value Accross Different Geo Locations"]}, {"cell_type": "code", "execution_count": 1, "id": "07b93f3d", "metadata": {}, "outputs": [], "source": ["import matplotlib.image as mpimg\ncalifornia_img=mpimg.imread('../input/images/calfornia_img.jpg')\ncalifornia_state=mpimg.imread('../input/images/calfornia_state.jpg')"]}, {"cell_type": "code", "execution_count": 1, "id": "0fb579bc", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(15, 8))\nplt.subplot(1, 2, 1)\nplt.imshow(california_state)\nplt.axis('off')\nplt.subplot(1, 2, 2)\nax = plt.gca()\ndata_explore.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4, s=data_explore[\"population\"]/100,\n             label=\"population\", figsize=(15,6), c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), ax=ax)\nax.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05])"]}, {"cell_type": "markdown", "id": "a70d7c66", "metadata": {}, "source": ["- Observations: \n    - We can see that the density is larger in Bay Area, Los Angeles, San Diego.\n    - It is observed that the housing prices are higher near ocean region and in high population area. This observed not hold true for north california region.\n    - As we go away from ocean there is decrease in housing prizes. Most housing prizes are below $20k."]}, {"cell_type": "markdown", "id": "b753e1cb", "metadata": {}, "source": ["### Correlation Plot"]}, {"cell_type": "code", "execution_count": 1, "id": "4b4f1716", "metadata": {}, "outputs": [], "source": ["data_explore[\"rooms_per_household\"] = data_explore[\"total_rooms\"]/data_explore[\"households\"]\ndata_explore[\"bedrooms_per_room\"] = data_explore[\"total_bedrooms\"]/data_explore[\"total_rooms\"]"]}, {"cell_type": "code", "execution_count": 1, "id": "5ed269d0", "metadata": {}, "outputs": [], "source": ["data_explore_dummies = pd.get_dummies(data_explore) \nplt.figure(figsize=(18, 10))\ncorr_matrix = data_explore_dummies.corr(method='pearson')\nsns.heatmap(corr_matrix, mask=np.zeros_like(corr_matrix, dtype=np.bool), square=True, annot=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "73825908", "metadata": {}, "outputs": [], "source": ["corr_matrix[\"median_house_value\"].sort_values(ascending=False)"]}, {"cell_type": "markdown", "id": "36877968", "metadata": {}, "source": ["- Observations:\n    - Median house value is highly correlated with median income. Other than median income no other feature is highly correlated with target variable.\n    - We can see that newly created features are somewhat correlated to target variable. This correlation is more than the correlation of those indivisual features with target variable.\n    - There is strong correlation among some of feature variables such as latitude & longitude, population & households etc.\n\n    \nLets explore more about the relationship between correlated features with median house value."]}, {"cell_type": "code", "execution_count": 1, "id": "7f7fd276", "metadata": {}, "outputs": [], "source": ["from pandas.plotting import scatter_matrix\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\nscatter_matrix(data_explore[attributes], figsize=(15, 10))\nplt.show()"]}, {"cell_type": "markdown", "id": "aa343056", "metadata": {}, "source": ["- Observe the plot between median houseing value and median income, there is strong correlation between them and also the points are not too dispersed. Interesting observation is that we can see the horizontal line at top of chart(at 500,000). This is because of capping.\n- Having capped data is not good for training the model because there is possibility that model will learn that the maximum price of house will not go above 500k USD."]}, {"cell_type": "markdown", "id": "241f1927", "metadata": {}, "source": ["## Step 3: Data Preprocessing"]}, {"cell_type": "markdown", "id": "4fd7e64c", "metadata": {}, "source": ["First lets get rid of those records for which median house value is capped to $500k."]}, {"cell_type": "code", "execution_count": 1, "id": "b05086aa", "metadata": {}, "outputs": [], "source": ["data_capped = data[data['median_house_value']>=500000]\ndata = data[data['median_house_value']<500000]\ndata_capped.shape, data.shape"]}, {"cell_type": "markdown", "id": "16b5f02f", "metadata": {}, "source": ["- Since dataset is not large enough and (As per experts) \"median income\" is a very important attribute to predict median housing prices, We have to ensure that the test set is representative of the various categories of incomes in the whole dataset.\n- The median income is a continuous numerical attribute, created an new income \"category\" attribute to use for stratisfied sampling. \n- I will generated train and test data using StratifiedShuffleSplit method."]}, {"cell_type": "code", "execution_count": 1, "id": "69327207", "metadata": {}, "outputs": [], "source": ["data[\"income_cat\"] = pd.cut(data[\"median_income\"], bins=[0., 1.5, 3.0, 4.5, 6., np.inf], labels=[1, 2, 3, 4, 5])"]}, {"cell_type": "code", "execution_count": 1, "id": "06e3f3f2", "metadata": {}, "outputs": [], "source": ["data[\"income_cat\"].hist()"]}, {"cell_type": "code", "execution_count": 1, "id": "5d2496dd", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)"]}, {"cell_type": "code", "execution_count": 1, "id": "c1f4552a", "metadata": {}, "outputs": [], "source": ["for train_index, test_index in split.split(data, data[\"income_cat\"]):\n    strat_train_set = data.iloc[train_index]\n    strat_test_set = data.iloc[test_index]\n\nstrat_train_set.drop(\"income_cat\", axis = 1, inplace=True)\nstrat_test_set.drop(\"income_cat\", axis = 1, inplace=True)\nstrat_train_set.shape, strat_test_set.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "33aa4c2f", "metadata": {}, "outputs": [], "source": ["X_train = strat_train_set.drop('median_house_value', axis=1)\ny_train = strat_train_set['median_house_value'].copy()\nX_test = strat_test_set.drop('median_house_value', axis=1)\ny_test = strat_test_set['median_house_value'].copy()"]}, {"cell_type": "markdown", "id": "48c19332", "metadata": {}, "source": ["During exploration, I performed two operations on data:\n1. Replace null values with median\n2. Added two new columns: bedrooms_per_room, rooms_per_household"]}, {"cell_type": "markdown", "id": "0cd4b8e8", "metadata": {}, "source": ["I will define custom transformer, which will create the two extra columns."]}, {"cell_type": "code", "execution_count": 1, "id": "b0c5f4b7", "metadata": {}, "outputs": [], "source": ["from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import PowerTransformer, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin"]}, {"cell_type": "code", "execution_count": 1, "id": "1094077d", "metadata": {}, "outputs": [], "source": ["rooms_ix, bedrooms_ix, households_ix = 3, 4, 6    # column ids\n\nclass CombineAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n        bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n        return np.c_[X, rooms_per_household, bedrooms_per_room]"]}, {"cell_type": "code", "execution_count": 1, "id": "8fa16d89", "metadata": {}, "outputs": [], "source": ["num_attrs = list(X_train.columns)\nnum_attrs.remove('ocean_proximity')\ncat_attrs = ['ocean_proximity',]"]}, {"cell_type": "code", "execution_count": 1, "id": "6c8722cf", "metadata": {}, "outputs": [], "source": ["num_pipeline = Pipeline([('imputer', SimpleImputer(strategy=\"median\")),\n                         ('attribs_adder', CombineAttributesAdder()),\n                         ('scaler', PowerTransformer(method='yeo-johnson', standardize=True))])\n\npre_process = ColumnTransformer([(\"nums\", num_pipeline, num_attrs),\n                                   (\"cat\", OneHotEncoder(handle_unknown='ignore'), cat_attrs)], remainder='passthrough')"]}, {"cell_type": "code", "execution_count": 1, "id": "02710800", "metadata": {}, "outputs": [], "source": ["X_train_transformed = pre_process.fit_transform(X_train)\nX_test_transformed = pre_process.transform(X_test)\nX_train_transformed.shape, X_test_transformed.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "c80128d1", "metadata": {}, "outputs": [], "source": ["feature_columns = list(X_train.columns)\nfeature_columns.extend(['rooms_per_household','bedrooms_per_room'])\nnew_cols = list(X_train['ocean_proximity'].unique())\nfeature_columns.extend(new_cols)\nfeature_columns.remove('ocean_proximity')"]}, {"cell_type": "markdown", "id": "fbf1ed1a", "metadata": {}, "source": ["## Step 4: Select and Train a Model"]}, {"cell_type": "markdown", "id": "4874cde8", "metadata": {}, "source": ["- I will be trying out Linear as well as Ensemble learning techniques.\n- Following are the 5 models I will be using:\n    1. Stochastic Gradient Descent with L-2 Regularization.\n    2. Decision Tree\n    3. Random Forest\n    4. XGBoost Regression\n\n\n- I will be using RMSE as evaluation metric."]}, {"cell_type": "code", "execution_count": 1, "id": "76d66e07", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import cross_val_score\n\nresults=[]\n\ndef cv_results(model, X, y):\n    scores = cross_val_score(model, X, y, cv = 7, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)\n    rmse_scores = -scores\n    rmse_scores = np.round(rmse_scores, 3)\n    print('CV Scores: ', rmse_scores)\n    print('rmse: {},  S.D.:{} '.format(np.mean(rmse_scores), np.std(rmse_scores)))\n    results.append([model.__class__.__name__, np.mean(rmse_scores), np.std(rmse_scores)])"]}, {"cell_type": "markdown", "id": "bd83f680", "metadata": {}, "source": ["### Stochastic Gradient Descent"]}, {"cell_type": "code", "execution_count": 1, "id": "692f3427", "metadata": {}, "outputs": [], "source": ["from sklearn.linear_model import SGDRegressor"]}, {"cell_type": "code", "execution_count": 1, "id": "c20d9cd8", "metadata": {}, "outputs": [], "source": ["sgd_reg = SGDRegressor(alpha=1, penalty='l1', random_state=42)\nsgd_reg.fit(X_train_transformed, y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "32ed09d8", "metadata": {}, "outputs": [], "source": ["feature_imp = [ col for col in zip(feature_columns,sgd_reg.coef_)]\nfeature_imp.sort(key=lambda x:x[1], reverse=True)\nfeature_imp"]}, {"cell_type": "markdown", "id": "7dceea2d", "metadata": {}, "source": ["- Looking at the coefficient values, model has given much importance to the median income attribute. Another most important attributes according this model are proximity of location from ocean and median age of house."]}, {"cell_type": "code", "execution_count": 1, "id": "4d61fcb6", "metadata": {}, "outputs": [], "source": ["cv_results(sgd_reg, X_train_transformed, y_train)"]}, {"cell_type": "markdown", "id": "ee28420e", "metadata": {}, "source": ["- RMSE is around than $60000. Which is huge. This indicates that model is poorely fitted with given data.\n- Either features included not providing the enough information or model is not powerfull.\n- We have observed the underfitting with Linear Regression."]}, {"cell_type": "markdown", "id": "4ec41088", "metadata": {}, "source": ["### Decision Tree"]}, {"cell_type": "code", "execution_count": 1, "id": "d7a0aafe", "metadata": {}, "outputs": [], "source": ["from sklearn.tree import DecisionTreeRegressor"]}, {"cell_type": "code", "execution_count": 1, "id": "aedc0b87", "metadata": {}, "outputs": [], "source": ["tree_reg = DecisionTreeRegressor(criterion=\"mse\", random_state=42)\ntree_reg.fit(X_train_transformed, y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "e805079d", "metadata": {}, "outputs": [], "source": ["cv_results(tree_reg, X_train_transformed, y_train)"]}, {"cell_type": "markdown", "id": "dbed11d8", "metadata": {}, "source": ["### Random Forest"]}, {"cell_type": "code", "execution_count": 1, "id": "3da3eab6", "metadata": {}, "outputs": [], "source": ["from sklearn.ensemble import RandomForestRegressor"]}, {"cell_type": "code", "execution_count": 1, "id": "5044fd20", "metadata": {}, "outputs": [], "source": ["forest_reg = RandomForestRegressor(criterion='mse', n_estimators=100, n_jobs=-1, random_state=42)\nforest_reg.fit(X_train_transformed, y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "5438b2cb", "metadata": {}, "outputs": [], "source": ["feature_imp = [ col for col in zip(feature_columns,forest_reg.feature_importances_)]\nfeature_imp.sort(key=lambda x:x[1], reverse=True)\nfeature_imp"]}, {"cell_type": "code", "execution_count": 1, "id": "18f4e9e0", "metadata": {}, "outputs": [], "source": ["cv_results(forest_reg, X_train_transformed, y_train)"]}, {"cell_type": "markdown", "id": "36a00f17", "metadata": {}, "source": ["Though RMSE is still high but we got better result compared to previous two models. "]}, {"cell_type": "markdown", "id": "394d5e5c", "metadata": {}, "source": ["### XGBoost Regression"]}, {"cell_type": "code", "execution_count": 1, "id": "eb28a3a6", "metadata": {}, "outputs": [], "source": ["from xgboost import XGBRegressor"]}, {"cell_type": "code", "execution_count": 1, "id": "1a10f162", "metadata": {}, "outputs": [], "source": ["xgb_reg = XGBRegressor(n_estimators=100, max_depth=8, learning_rate=0.1, objective='reg:squarederror', random_state=42)\nxgb_reg.fit(X_train_transformed, y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "ebddc982", "metadata": {}, "outputs": [], "source": ["feature_imp = [ col for col in zip(feature_columns,xgb_reg.feature_importances_)]\nfeature_imp.sort(key=lambda x:x[1], reverse=True)\nfeature_imp"]}, {"cell_type": "code", "execution_count": 1, "id": "3cbf165e", "metadata": {}, "outputs": [], "source": ["cv_results(xgb_reg, X_train_transformed, y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "dccbe648", "metadata": {}, "outputs": [], "source": ["result_df = pd.DataFrame(data=results, columns=['Model', 'RMSE', 'S.D'])\nresult_df"]}, {"cell_type": "markdown", "id": "aae60be7", "metadata": {}, "source": ["Got better cross validation result for XGBoost than all previously implemented algorithms. Remember in Random Forest we have use full grown decision trees where as in XGBoost we have use Decision trees of height=8.\n\nNow, I will tune parameters of Random Forest and XGBoost. Once best models are obtained, I will evaluate each best model and then the model which give best results on test dataset will be the final model. "]}, {"cell_type": "markdown", "id": "5fd37914", "metadata": {}, "source": ["## Step 5: Fine Tune a Model"]}, {"cell_type": "code", "execution_count": 1, "id": "f0c45b81", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import GridSearchCV"]}, {"cell_type": "markdown", "id": "f575c58f", "metadata": {}, "source": ["First we will find best Random Forest regression model."]}, {"cell_type": "code", "execution_count": 1, "id": "8e7fb293", "metadata": {}, "outputs": [], "source": ["rf_grid_parm=[{'n_estimators':[50, 100, 300], 'max_depth':[8, 16, 24]}]\nrf_grid_search = GridSearchCV(RandomForestRegressor(random_state=42, n_jobs=-1), rf_grid_parm, cv=5, scoring=\"neg_root_mean_squared_error\", return_train_score=True, n_jobs=-1)\nrf_grid_search.fit(X_train_transformed, y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "bcde6a82", "metadata": {}, "outputs": [], "source": ["rf_grid_search.best_params_, -rf_grid_search.best_score_"]}, {"cell_type": "code", "execution_count": 1, "id": "91476a27", "metadata": {}, "outputs": [], "source": ["cvres = rf_grid_search.cv_results_\nprint(\"Results for each run of Random Forest Regression...\")\nfor train_mean_score, test_mean_score, params in zip(cvres[\"mean_train_score\"], cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(-train_mean_score, -test_mean_score, params)"]}, {"cell_type": "code", "execution_count": 1, "id": "7c484c01", "metadata": {}, "outputs": [], "source": ["best_forest_reg = rf_grid_search.best_estimator_\nbest_forest_reg"]}, {"cell_type": "markdown", "id": "f9c6a57e", "metadata": {}, "source": ["Now lets find best XGBoost regression model."]}, {"cell_type": "code", "execution_count": 1, "id": "0b6f8708", "metadata": {}, "outputs": [], "source": ["xgb_grid_parm=[{'n_estimators':[50, 100, 300], 'max_depth':[6, 8, 12]}]\nxgb_grid_search = GridSearchCV(XGBRegressor(objective='reg:squarederror', learning_rate=0.1, n_jobs=-1, random_state=42), xgb_grid_parm, cv=5, scoring=\"neg_root_mean_squared_error\", return_train_score=True, n_jobs=-1)\nxgb_grid_search.fit(X_train_transformed, y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "33bf5a52", "metadata": {}, "outputs": [], "source": ["xgb_grid_search.best_params_, -xgb_grid_search.best_score_"]}, {"cell_type": "code", "execution_count": 1, "id": "ddc4e543", "metadata": {}, "outputs": [], "source": ["cvres = xgb_grid_search.cv_results_\nprint(\"Results for each run of XGBoost Regression...\")\nfor train_mean_score, test_mean_score, params in zip(cvres[\"mean_train_score\"], cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(-train_mean_score, -test_mean_score, params)"]}, {"cell_type": "code", "execution_count": 1, "id": "e63e10ff", "metadata": {}, "outputs": [], "source": ["best_xgb_reg = xgb_grid_search.best_estimator_\nbest_xgb_reg"]}, {"cell_type": "markdown", "id": "748287ae", "metadata": {}, "source": ["## Step 6: Model Evaluation"]}, {"cell_type": "markdown", "id": "48442b4b", "metadata": {}, "source": ["Evaluate Random Forest regression model."]}, {"cell_type": "code", "execution_count": 1, "id": "d218d056", "metadata": {}, "outputs": [], "source": ["cv_results(best_forest_reg, X_test_transformed, y_test)"]}, {"cell_type": "markdown", "id": "9103daba", "metadata": {}, "source": ["Evaluate XGBoost regression model."]}, {"cell_type": "code", "execution_count": 1, "id": "f0edac91", "metadata": {}, "outputs": [], "source": ["cv_results(best_xgb_reg, X_test_transformed, y_test)"]}, {"cell_type": "markdown", "id": "09c1d424", "metadata": {}, "source": ["- Error is still large, but among both models XGBoost performs slightly better on both train and test dataset. So I will select XGBoost as final model."]}, {"cell_type": "markdown", "id": "289063de", "metadata": {}, "source": ["Lets analyse model's prediciton on overall dataset. This will help to find out where the model is making many mistakes."]}, {"cell_type": "code", "execution_count": 1, "id": "26da3720", "metadata": {}, "outputs": [], "source": ["combine_data = pd.concat([strat_train_set, strat_test_set], axis=0)\ncombine_data.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "dbb19046", "metadata": {}, "outputs": [], "source": ["y_train_pred = best_xgb_reg.predict(X_train_transformed)\ny_test_pred = best_xgb_reg.predict(X_test_transformed)"]}, {"cell_type": "code", "execution_count": 1, "id": "ba1cc6e3", "metadata": {}, "outputs": [], "source": ["y_pred = np.concatenate([y_train_pred, y_test_pred], axis=0)\ny_pred.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "1961885e", "metadata": {}, "outputs": [], "source": ["combine_data['predicted_value'] = y_pred"]}, {"cell_type": "code", "execution_count": 1, "id": "a846dcff", "metadata": {}, "outputs": [], "source": ["combine_data.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "ff3c840c", "metadata": {}, "outputs": [], "source": ["combine_data.describe()"]}, {"cell_type": "code", "execution_count": 1, "id": "b8f75fb4", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(15, 6))\nplt.subplot(1, 2, 1)\ncombine_data['median_house_value'].hist()\nplt.title('Observed Median House Value')\nplt.subplot(1, 2, 2)\ncombine_data['predicted_value'].hist()\nplt.title('Predicted Median House Value')\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "8cad4349", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(12, 8))\nplt.scatter(combine_data['median_income'], combine_data['median_house_value'], c='green', alpha=0.7, label=\"Observed\")\nplt.scatter(combine_data['median_income'], combine_data['predicted_value'], c='red', alpha=0.7, label=\"Predicted\")\nplt.xlabel('Median Income')\nplt.ylabel('Median House Value')\nplt.legend()\nplt.show()"]}, {"cell_type": "markdown", "id": "93fdaa33", "metadata": {}, "source": ["- Observations:\n    - We can see that there were many blocks for which median house value above 450000. But model has predicted the value lesser than the orignal median house value.\n    - For some block having median value less than 100000, model has predicted the value higher than the orignal value.\n    - Look at top left corner, For blocks where median house value is less than 3 and median observed value above 300000, model has predicted lesser value than orignal value."]}, {"cell_type": "code", "execution_count": 1, "id": "40ba2665", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(18, 10))\nfig, ax = plt.subplots(nrows=1, ncols=2)\ncombine_data.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4, figsize=(15,8), c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), ax=ax[0], colorbar=False)\nax[0].imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05])\nax[0].set_title('Observed Median House Values')\ncombine_data.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,figsize=(15,8), c=\"predicted_value\", cmap=plt.get_cmap(\"jet\"), ax=ax[1], colorbar=False)\nax[1].imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05])\nax[1].set_title('Predicted Median House Values')\nplt.show()"]}, {"cell_type": "markdown", "id": "c8895e02", "metadata": {}, "source": ["- Observe the redness in both graph. Near the ocean region we can see that there is more redness in observed median house value than the predicted. \n\nNote: Redness indicates the high house value where as blue indicates low house values. "]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}