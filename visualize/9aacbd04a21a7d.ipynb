{"cells": [{"cell_type": "markdown", "id": "09713ed1", "metadata": {}, "source": ["# Libraries"]}, {"cell_type": "code", "execution_count": 1, "id": "0beeaf70", "metadata": {}, "outputs": [], "source": ["#--DATA CLEANING --#\n#refer to Indexed report for Commentary\n# conda create --name workshop\n# conda activate workshop\n# conda install folium \n# conda install seaborn\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\n"]}, {"cell_type": "markdown", "id": "d88ac9cf", "metadata": {}, "source": ["# Loading In and Reading Data"]}, {"cell_type": "code", "execution_count": 1, "id": "3ac33120", "metadata": {}, "outputs": [], "source": ["\ndf_0 = pd.read_csv('../input/barcelonaairbnbgeojson/listings.csv')\nnum_rows = len(df_0['id'])\ndf_0.head(5)\n\n# trouble with only displaying some columns or rows?\n# pd.set_option('display.max_columns', None)\n# pd.set_option('display.max_rows', None)\n\n\n# scraped on sept19, 2019 from insideairbnb.com"]}, {"cell_type": "markdown", "id": "06924c51", "metadata": {}, "source": ["`df.describe()` displays a quick summary of the data with some statistics per column"]}, {"cell_type": "code", "execution_count": 1, "id": "904ac59a", "metadata": {}, "outputs": [], "source": ["display(df_0.describe())"]}, {"cell_type": "markdown", "id": "07a9f373", "metadata": {}, "source": ["# Checking for Missing / Null Values\n`df.isnull()` returns a table of booleans. Use `sum()` to get the total number of `null` values."]}, {"cell_type": "code", "execution_count": 1, "id": "3c3af5a5", "metadata": {}, "outputs": [], "source": ["display(df_0.isnull().sum())"]}, {"cell_type": "markdown", "id": "01182d05", "metadata": {}, "source": ["Here we want to remove the columns that have greater than 60% of values missing."]}, {"cell_type": "code", "execution_count": 1, "id": "7cb19cb5", "metadata": {}, "outputs": [], "source": ["#Remove Majority Null Columns\n\ncolsToDrop = []\nfor col in df_0.columns:\n    if df_0[col].isnull().sum() > (.6 * num_rows):\n        colsToDrop.append(col)  \nprint(f'Number of columns to be dropped: {len(colsToDrop)}')\nfor col in colsToDrop:\n    print(col)\ndf_0.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "3afbb0e4", "metadata": {}, "outputs": [], "source": ["df_1 = df_0.drop(colsToDrop, axis=1)\n\ndf_1.shape"]}, {"cell_type": "markdown", "id": "48debb06", "metadata": {}, "source": ["And remove the unnecessary columns"]}, {"cell_type": "code", "execution_count": 1, "id": "9fa36be7", "metadata": {}, "outputs": [], "source": ["# Remove Columns with One Unique Value\n# Why? For example, country. If all data is in the same country, we don't need 20404 rows that say Spain\ncolsToDrop = []\n\nfor col in df_1.columns:\n    if df_1[col].nunique() == 1:\n        colsToDrop.append(col)\n        \nfor col in colsToDrop:\n    print(col)\ndf_1.shape\n"]}, {"cell_type": "code", "execution_count": 1, "id": "62b5a233", "metadata": {}, "outputs": [], "source": ["# Reassign to df_2 variable\n\ndf_2 = df_1.drop(colsToDrop, axis = 1)\ndf_2.shape"]}, {"cell_type": "markdown", "id": "33a4e5ca", "metadata": {}, "source": ["# Checking Data Types\nOur data includes many Strings, which are very useful for any categorical analysis. We will be removing some columns as we don't need."]}, {"cell_type": "code", "execution_count": 1, "id": "e6bcedcf", "metadata": {}, "outputs": [], "source": ["# what data types make up our dataframe?\n# object == string\n\ndf_2.dtypes"]}, {"cell_type": "code", "execution_count": 1, "id": "299c275b", "metadata": {}, "outputs": [], "source": ["# As previously seen, pandas' nunique function counts the number of unique values in a column\ndisplay(df_2.nunique())"]}, {"cell_type": "code", "execution_count": 1, "id": "a77b5230", "metadata": {}, "outputs": [], "source": ["# Bulk Removal of Redundant Columns and String Columns\n\n# Some of these string columns may be helpful for categorical analysis and language processing, \n# but for the purpose of this workshop we will leave them out.\n# \n\n\ndf_3 = df_2.drop(['listing_url', 'last_scraped', 'name', 'summary', 'space', 'description',\n                 'neighborhood_overview', 'notes', 'transit', 'access', 'interaction', 'house_rules', 'picture_url',\n                 'host_url', 'host_name', 'host_since', 'host_location', 'host_about', 'host_thumbnail_url', \n                  'host_neighbourhood', 'host_listings_count', 'host_total_listings_count', 'host_response_time', \n                  'host_response_rate', 'street', 'host_verifications', 'host_picture_url', 'amenities', 'calendar_updated', \n                  'calendar_last_scraped', 'availability_30', 'availability_90', 'availability_60','neighbourhood', \n                  'smart_location', 'is_location_exact', 'first_review', 'last_review', 'license','minimum_minimum_nights',\n                  'maximum_minimum_nights','minimum_maximum_nights','maximum_maximum_nights','minimum_nights_avg_ntm',\n                  'maximum_nights_avg_ntm','calculated_host_listings_count_entire_homes',\n                  'calculated_host_listings_count_shared_rooms','calculated_host_listings_count_private_rooms'], axis =1)\nprint(f'Before removing redundant values: {df_2.shape}')\nprint(f'After removing redundant values:  {df_3.shape}')"]}, {"cell_type": "code", "execution_count": 1, "id": "5a2b0ab6", "metadata": {}, "outputs": [], "source": ["# Look at what data remains, along with how many unique values there are\ndf_3.nunique()"]}, {"cell_type": "markdown", "id": "4aa40e63", "metadata": {}, "source": ["As you may have noticed, everytime we display the data the ID has been its own column. Set that as the index."]}, {"cell_type": "code", "execution_count": 1, "id": "fca8501d", "metadata": {}, "outputs": [], "source": ["df_3 = df_3.set_index('id')"]}, {"cell_type": "code", "execution_count": 1, "id": "f3ed42d0", "metadata": {}, "outputs": [], "source": ["df_3.head(20)"]}, {"cell_type": "markdown", "id": "c597591f", "metadata": {}, "source": ["# Visualizing Proportions of Data\nLooking at how the data is distributed can help when working through a dataset."]}, {"cell_type": "code", "execution_count": 1, "id": "a3788140", "metadata": {}, "outputs": [], "source": ["# Why? To visualize the proportions of data relative to each other\ndf_3.room_type.value_counts(normalize=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "a07653e8", "metadata": {}, "outputs": [], "source": ["df_3.cancellation_policy.value_counts(normalize=True)\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "166976eb", "metadata": {}, "outputs": [], "source": ["df_3.neighbourhood_group_cleansed.value_counts(normalize=True)\n        "]}, {"cell_type": "code", "execution_count": 1, "id": "6e7e9da9", "metadata": {}, "outputs": [], "source": ["df_3.bed_type.value_counts(normalize=True)"]}, {"cell_type": "markdown", "id": "bc1e640a", "metadata": {}, "source": ["Removing reviews"]}, {"cell_type": "code", "execution_count": 1, "id": "2e72e86c", "metadata": {}, "outputs": [], "source": ["# Almost everybody has a real bed, let's remove this variable\ndf_4 = df_3.drop(['bed_type', 'review_scores_accuracy',\n'review_scores_cleanliness',\n'review_scores_checkin',\n'review_scores_communication',\n'review_scores_location',\n'review_scores_value',\n'state',\n'zipcode',\n'market',\n], axis = 1)"]}, {"cell_type": "code", "execution_count": 1, "id": "17a02e4f", "metadata": {}, "outputs": [], "source": ["df_4.isnull().sum()\n#must parse strings and change variable type later\n\n"]}, {"cell_type": "markdown", "id": "b5cd638b", "metadata": {}, "source": ["When `availability_365` is 0, the listing is not active. We take those out too."]}, {"cell_type": "code", "execution_count": 1, "id": "68c9615f", "metadata": {}, "outputs": [], "source": ["# Returns every row where the 'availability_365' column equals 0.\ndf_4.loc[df_4['availability_365'] == 0].head()"]}, {"cell_type": "code", "execution_count": 1, "id": "d0282dda", "metadata": {}, "outputs": [], "source": ["# Removal of Inactive Listings\n\ndf_4 = df_4[df_4['availability_365'] != 0] \n"]}, {"cell_type": "markdown", "id": "0f88592e", "metadata": {}, "source": ["Converting strings for price into floats"]}, {"cell_type": "code", "execution_count": 1, "id": "81872a0f", "metadata": {}, "outputs": [], "source": ["#Parsing Floats from Price Columns\n\ndf_5 = df_4\ndf_6 = df_5\ndf_6['price'] = df_6['price'].str.replace('$', '').str.replace(',', '').astype(float)\ndf_6['security_deposit'] = df_6['security_deposit'].str.replace('$', '').str.replace(',', '').astype(float)\ndf_6['cleaning_fee'] = df_6['cleaning_fee'].str.replace('$', '').str.replace(',', '').astype(float)\ndf_6['extra_people'] = df_6['extra_people'].str.replace('$', '').str.replace(',', '').astype(float)"]}, {"cell_type": "code", "execution_count": 1, "id": "8b31c4c6", "metadata": {}, "outputs": [], "source": ["df_6['security_deposit'].fillna(0, inplace = True)\ndf_6['cleaning_fee'].fillna(0, inplace = True)\ndf_6['extra_people'].fillna(0, inplace = True)\ndf_6['review_scores_rating'].fillna(0, inplace = True)\ndf_6['reviews_per_month'].fillna(0, inplace = True)"]}, {"cell_type": "code", "execution_count": 1, "id": "9b985c4a", "metadata": {}, "outputs": [], "source": ["# Review the remaining data\ndf_6.head()"]}, {"cell_type": "markdown", "id": "9c105ef9", "metadata": {}, "source": ["Dropping more columns"]}, {"cell_type": "code", "execution_count": 1, "id": "dcd73beb", "metadata": {}, "outputs": [], "source": ["df_7 = df_6.drop(['review_scores_rating', 'number_of_reviews', 'number_of_reviews_ltm'], axis=1)\n"]}, {"cell_type": "code", "execution_count": 1, "id": "b7200c10", "metadata": {}, "outputs": [], "source": ["df_7.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "b24d490b", "metadata": {}, "outputs": [], "source": ["#PRICE LIMIT SETTING AND PRICE DISTRIBUTION\ndf_7.price.describe()"]}, {"cell_type": "code", "execution_count": 1, "id": "13b64076", "metadata": {}, "outputs": [], "source": ["\n\n\ndf_7[df_7.price > 1000].head()"]}, {"cell_type": "code", "execution_count": 1, "id": "f5925dc6", "metadata": {}, "outputs": [], "source": ["df_8 = df_7.drop(df_7[df_7.price > 650].index, axis=0)\ndf_8 = df_8.drop('maximum_nights', axis=1)\ndf_8.price.describe()\n\n#df_7 has prices above 500, df_8 does not. there are about 500 listings priced above 500"]}, {"cell_type": "code", "execution_count": 1, "id": "189fd648", "metadata": {}, "outputs": [], "source": ["import seaborn as sns\nplt.figure(figsize=(10,10))\nsns.violinplot(x=df_8.price, palette = \"Set3\").set_title(\"Price Distribution (removed listings $650 and above)\", size=16)\nsns.despine"]}, {"cell_type": "code", "execution_count": 1, "id": "b48cf5ab", "metadata": {}, "outputs": [], "source": ["df_8.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "c54aabee", "metadata": {}, "outputs": [], "source": ["df_8.isnull().sum()\n#because there are so few, we will investigate the cases where there is null"]}, {"cell_type": "code", "execution_count": 1, "id": "7a6c680a", "metadata": {}, "outputs": [], "source": ["null_cases = df_8[df_8.isnull().any(axis=1)]\nnull_cases.head(25)"]}, {"cell_type": "code", "execution_count": 1, "id": "4c2ce0ac", "metadata": {}, "outputs": [], "source": ["null_cases.info()"]}, {"cell_type": "markdown", "id": "ee426f49", "metadata": {}, "source": ["Dropping all nulls for `df_9`"]}, {"cell_type": "code", "execution_count": 1, "id": "b61fe6f1", "metadata": {}, "outputs": [], "source": ["df_9 = df_8.dropna()\ndf_9.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "94936498", "metadata": {}, "outputs": [], "source": ["df_9.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "44afc30e", "metadata": {}, "outputs": [], "source": ["df_9.isnull().sum()"]}, {"cell_type": "code", "execution_count": 1, "id": "5687da06", "metadata": {}, "outputs": [], "source": ["df_9.nunique()"]}, {"cell_type": "markdown", "id": "f6e8a0ff", "metadata": {}, "source": ["# Seaborn - Violin Plot\nFirst of the plots, we have the violin plot. It displays the density of data. \nThe distribution is display, along with a boxplot found in the middle."]}, {"cell_type": "code", "execution_count": 1, "id": "957b9469", "metadata": {}, "outputs": [], "source": ["# Explain the parts of the violin chart:\n# Curve = Normal Disribution\n# Bottom of veritcal line = Smallest value\n# Top of vertical line = Highest value\n# Bottom of the middle box = First quartile\n# Top of the middle box = Third quartile\n# White dot = mean\nplt.figure(figsize=(20,10))\nsorted_room = df_9.groupby(['room_type'])['price'].median().sort_values()\nsns.violinplot(x=df_9.room_type, y=df_9.price, order=list(sorted_room.index)).set_title(\"Price by Room Type\", size=16)\n#sns.despine"]}, {"cell_type": "code", "execution_count": 1, "id": "fc26c4fd", "metadata": {}, "outputs": [], "source": ["\nplt.figure(figsize=(20,10))\n\nsorted_room = df_9.groupby(['room_type'])['reviews_per_month'].median().sort_values()\nsns.violinplot(x=df_9.room_type, y=df_9.reviews_per_month, order=list(sorted_room.index)).set_title(\"Reviews Per Month by Room Type\", size=16)\nsns.despine"]}, {"cell_type": "markdown", "id": "e20751e3", "metadata": {}, "source": ["Distributions will (most likely) not be perfectly symmetric, so we want to look at the skew of each column of data."]}, {"cell_type": "code", "execution_count": 1, "id": "dc6a4367", "metadata": {}, "outputs": [], "source": ["# Explain skew\ndf_9.skew()"]}, {"cell_type": "code", "execution_count": 1, "id": "3fe4a45f", "metadata": {}, "outputs": [], "source": ["df_9['minimum_nights'][df_9['minimum_nights'] > 365] = 366"]}, {"cell_type": "markdown", "id": "8b4a64d8", "metadata": {}, "source": ["With `distplot` we can view our distributions to visualize skew."]}, {"cell_type": "code", "execution_count": 1, "id": "b8f67b68", "metadata": {}, "outputs": [], "source": ["#Predictor variables and Skewed Distribution\n\n\nf, axes = plt.subplots(2, 4, figsize=(15, 15))\nsns.despine(left=True)\n\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_9.price, ax = axes[0,0])\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_9.minimum_nights, ax = axes[1,0])\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_9.calculated_host_listings_count, ax = axes[0,1])\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_9.accommodates, ax = axes[1,1])\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_9.reviews_per_month, ax = axes[0,2])\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_9.bedrooms, ax = axes[0,3])\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_9.bathrooms, ax = axes[1,3])\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_9.extra_people, ax = axes[1,2])\n\n\ndf_9.skew(axis=0)"]}, {"cell_type": "code", "execution_count": 1, "id": "126676e4", "metadata": {}, "outputs": [], "source": ["df_9.nunique()\n"]}, {"cell_type": "code", "execution_count": 1, "id": "b4909869", "metadata": {}, "outputs": [], "source": ["df_9.describe()"]}, {"cell_type": "code", "execution_count": 1, "id": "79f8d255", "metadata": {}, "outputs": [], "source": ["df_9.minimum_nights.describe()"]}, {"cell_type": "code", "execution_count": 1, "id": "340ab753", "metadata": {}, "outputs": [], "source": ["df_9.calculated_host_listings_count.describe()"]}, {"cell_type": "markdown", "id": "d39a51db", "metadata": {}, "source": ["`bedrooms`, `beds`, `accommodates` are very similar, better to limit to one column.\nAlso limiting the number of unique values on `minimum_nights` by grouping together."]}, {"cell_type": "code", "execution_count": 1, "id": "8dbaacb2", "metadata": {}, "outputs": [], "source": ["# BINNING VARIABLES (drop beds because we have bedrooms)\n\ndf_10 = df_9.drop(['beds', 'accommodates'], axis=1)\ndf_10['binned_min_nights'] = pd.cut(df_9['minimum_nights'], bins=[0, 1, 2, 10, 1900],\n                                                labels = ['oneNight', '2to3','4to10', '11+']\n                                                )\ndf_10.binned_min_nights.value_counts()\n"]}, {"cell_type": "code", "execution_count": 1, "id": "684c992a", "metadata": {}, "outputs": [], "source": ["df_10.skew()"]}, {"cell_type": "code", "execution_count": 1, "id": "be28925c", "metadata": {}, "outputs": [], "source": ["\n\ndf_10.calculated_host_listings_count.describe()"]}, {"cell_type": "markdown", "id": "1e156c76", "metadata": {}, "source": ["In order to have an approximately normal distribution, we can try to apply a log transformation to our skewed data."]}, {"cell_type": "code", "execution_count": 1, "id": "9836883b", "metadata": {}, "outputs": [], "source": ["#log transforming price, sec deposit, cleaning fee, extra people, reviews_per_month variables\n\ndf_10['log_price'] = np.log(df_10['price']*10 + 1)\ndf_10['log_security'] = np.log(df_10['security_deposit'] + 1)\ndf_10['log_cleaning'] = np.log(df_10['cleaning_fee']*10 + 1)\n\ndf_10['log_extra_people'] = np.log(df_10['extra_people']*10 + 1)\ndf_10['log_reviews_pm'] = np.log((df_10['reviews_per_month']+ 1) * 10)\ndf_10['log_bedrooms'] = np.log(df_10['bedrooms'] + 1)\ndf_10['log_bathrooms'] = np.log(df_10['bathrooms']*10 + 1)\ndf_10['log_guests_included'] = np.log(df_10['guests_included']*100 + 1)\ndf_10['log_listings_count'] = np.log(df_10['calculated_host_listings_count']*10 + 1)\n\ndf_10.skew()\n"]}, {"cell_type": "markdown", "id": "e16b61de", "metadata": {}, "source": ["Through these two distplots, we can see how the log function effected our distribution and skew."]}, {"cell_type": "code", "execution_count": 1, "id": "42666312", "metadata": {}, "outputs": [], "source": ["# Predictor variables and Skewed Distribution\n\nsns.set({'xtick.labelsize': 10, 'ytick.labelsize': 10})\n\nf, axes = plt.subplots(2,1, figsize=(5, 10))\nsns.despine(left=True)\n\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_10.reviews_per_month, ax = axes[0]).set_title(\"Log transformed Reviews Distribution\")\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_10.log_reviews_pm, ax = axes[1])\n\n\n\ndf_10.skew(axis=0)"]}, {"cell_type": "code", "execution_count": 1, "id": "48dd33c3", "metadata": {}, "outputs": [], "source": ["# Predictor variables and Skewed Distribution\n\n\nf, axes = plt.subplots(2,1, figsize=(18, 9))\nsns.despine(left=True)\n\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_10.price, ax = axes[0]).set_title(\"Log transformed Price Distribution\")\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_10.log_price, ax = axes[1])\n\n\n\ndf_10.skew(axis=0)"]}, {"cell_type": "code", "execution_count": 1, "id": "1249e5cc", "metadata": {}, "outputs": [], "source": ["df_10.room_type.value_counts()"]}, {"cell_type": "markdown", "id": "29b4ff7b", "metadata": {}, "source": ["# Boxplots\nA part of the violin plot. Although we cannot see our distribution in this graph, it is helpful in visualizing outliers and where the majority of data lies."]}, {"cell_type": "code", "execution_count": 1, "id": "6aa9c74e", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(15,20))\nsns.set({'xtick.labelsize': 20, 'ytick.labelsize': 20})\nsorted_hood = df_10.groupby(['neighbourhood_group_cleansed'])['log_price'].median().sort_values()\nsns.boxplot(palette = 'Set3', y=df_10.neighbourhood_group_cleansed, x=df_10.log_price, order=list(sorted_hood.index)).set_title(\"Log(Price) by Neighbourhood\", size=20)\nsns.despine"]}, {"cell_type": "markdown", "id": "a91a0771", "metadata": {}, "source": ["# Heatmaps\nThe denser the color, the higher the correlation. A red hue means a positive correlation, a blue hue means a negative correlation."]}, {"cell_type": "code", "execution_count": 1, "id": "1fc76afb", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(20,20))\nsns.heatmap(df_10.corr(), annot=True, cmap='coolwarm', linewidth=.2)"]}, {"cell_type": "markdown", "id": "c3e56eaf", "metadata": {}, "source": ["Viewing more distributions after log"]}, {"cell_type": "code", "execution_count": 1, "id": "8cfe0fa7", "metadata": {}, "outputs": [], "source": ["f, axes = plt.subplots(2, 2, figsize=(10, 10))\nsns.set()\n\nsns.distplot(df_10.bedrooms, ax = axes[0,0])\nsns.distplot(df_10.log_bedrooms, ax = axes[1,0])\nsns.distplot(df_10.log_reviews_pm, ax = axes[1,1])\nsns.distplot(df_10.reviews_per_month, ax = axes[0,1])\n"]}, {"cell_type": "markdown", "id": "48a5108d", "metadata": {}, "source": ["# descartes and geopandas\n"]}, {"cell_type": "code", "execution_count": 1, "id": "5d717bb0", "metadata": {}, "outputs": [], "source": ["import descartes\nimport geopandas as gpd\n\ngjsonFile = \"../input/barcelonaairbnbgeojson/neighbourhoods.geojson\"\nbarc_hoods = gpd.read_file(gjsonFile)\n\nbarc_hoods.plot(figsize=(10,10), column=\"neighbourhood_group\", cmap = \"tab10\")"]}, {"cell_type": "code", "execution_count": 1, "id": "86fc2d58", "metadata": {}, "outputs": [], "source": ["barc_hoods['neighbourhood_group'].value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "4bb97125", "metadata": {}, "outputs": [], "source": ["barc_hoods.plot(figsize=(20,20), column=\"neighbourhood_group\", cmap='tab10', alpha = .5)\n#plt.figure(figsize=(20,20))\n\nsns.scatterplot(x='longitude', \n                y = 'latitude', \n                hue='price', \n                size = 'price', \n                sizes= (20, 600),\n                alpha = .8,\n                marker=\".\",\n                data = df_10,\n                )"]}, {"cell_type": "code", "execution_count": 1, "id": "da600184", "metadata": {}, "outputs": [], "source": ["barc_hoods.plot(figsize=(10,10), alpha = .5)\n#plt.figure(figsize=(20,20))\n#here we add lat and longitude lines to plot for context\n\n\nsns.set({'font.size' : 10})\nsns.set_style(\"whitegrid\")\n#plot by reviews per month\nsns.scatterplot(x='longitude', \n                y = 'latitude', \n                hue='neighbourhood_group_cleansed', \n                alpha = .5,\n                marker=\"o\",\n                data = df_10,\n                cmap='Set3')\n                #size = 15)"]}, {"cell_type": "code", "execution_count": 1, "id": "9e17965e", "metadata": {}, "outputs": [], "source": ["import folium\nfrom folium.plugins import HeatMap, MarkerCluster\n\nmapp = folium.Map(location=[41.40,2.15], zoom_start=12, figsize=(20,20))\ncluster = MarkerCluster().add_to(mapp)\n# add a marker for every record in the filtered data, use a clustered view\nfor each in df_10.iterrows():\n    folium.Marker(\n        location = [each[1]['latitude'],each[1]['longitude']], \n        clustered_marker = True).add_to(cluster)\n  \nmapp.save(outfile='map.html')\ndisplay(mapp)"]}, {"cell_type": "code", "execution_count": 1, "id": "93acfe00", "metadata": {}, "outputs": [], "source": ["\n\nmax_price_map = df_10['price'].max() #this should be 650\nbarc_map = folium.Map(location=[41.40, 2.15], zoom_start=12, )\n\nheatmap = HeatMap( list(zip(df_10.latitude, df_10.longitude, df_10.price)),\n                 min_opacity = .3,\n                 max_val = max_price_map, \n                 radius = 3,\n                 blur = 2,\n                 max_zoom=1)\n\nfolium.GeoJson(barc_hoods).add_to(barc_map)\nbarc_map.add_child(heatmap)\n\nbarc_map.save(outfile=\"mapp.html\")"]}, {"cell_type": "code", "execution_count": 1, "id": "c51c0148", "metadata": {}, "outputs": [], "source": ["\nimport folium\nfrom folium.plugins import HeatMap\n\n\nmax_reviews_pm = df_10['reviews_per_month'].max() \nbarc_map = folium.Map(location=[41.40, 2.15], zoom_start=12, )\n\nheatmap = HeatMap( list(zip(df_10.latitude, df_10.longitude, df_10.reviews_per_month)),\n                 min_opacity = .3,\n                 max_val = max_reviews_pm, \n                 radius = 3,\n                 blur = 2,\n                 max_zoom=1)\n\nfolium.GeoJson(barc_hoods).add_to(barc_map)\nbarc_map.add_child(heatmap)\n\n"]}, {"cell_type": "markdown", "id": "8002118d", "metadata": {}, "source": ["Using boxplot to view relationships between price and other columns."]}, {"cell_type": "code", "execution_count": 1, "id": "4fa039e7", "metadata": {}, "outputs": [], "source": ["sns.set()\nplt.figure(figsize=(20,10))\nsorted_hood_by_price = df_10.groupby(['neighbourhood_group_cleansed'])['log_price'].median().sort_values()\n\nsns.boxplot(x=\"neighbourhood_group_cleansed\",y=\"log_price\",data=df_10, palette=\"tab10\", order = list(sorted_hood_by_price.index)).set_title(\"Log(Price) by neighbourhood\", size=14)\n"]}, {"cell_type": "code", "execution_count": 1, "id": "94e01008", "metadata": {}, "outputs": [], "source": ["\nplt.figure(figsize=(20,10))\nsorted_hood_by_price = df_10.groupby(['neighbourhood_group_cleansed'])['reviews_per_month'].median().sort_values()\n\nsns.boxplot(x=\"neighbourhood_group_cleansed\",y=\"log_reviews_pm\",\n            data=df_10, palette=\"tab10\", order = list(sorted_hood_by_price.index)\n           ).set_title(\"Log(ReviewsPerMonth) by neighbourhood\", size=14)\n"]}, {"cell_type": "code", "execution_count": 1, "id": "fb830351", "metadata": {}, "outputs": [], "source": ["\nplt.figure(figsize=(20,10))\nsorted_hood_by_price = df_10.groupby(['room_type'])['log_price'].median().sort_values()\n\nsns.boxplot(x=\"room_type\",\n            y=\"log_price\",\n            data=df_10, \n            palette=\"tab10\", \n            order = list(sorted_hood_by_price.index)\n           ).set_title(\"Log(Price) by Room Type\", size=15)\n"]}, {"cell_type": "code", "execution_count": 1, "id": "be1636db", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(20,10))\n\nsorted_hood_by_activity = df_10.groupby(['neighbourhood_group_cleansed'])['log_price'].median().sort_values()\nsns.violinplot(x=df_10.neighbourhood_group_cleansed, y=df_10.log_price, order=list(sorted_hood_by_activity.index))\nsns.despine"]}, {"cell_type": "code", "execution_count": 1, "id": "62e8ca08", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(20,10))\n\nsorted_hood_by_activity = df_10.groupby(['room_type'])['log_reviews_pm'].median().sort_values()\nsns.violinplot(x=df_10.room_type, y=df_10.log_reviews_pm, order=list(sorted_hood_by_activity.index))\nsns.despine\n#private rooms getting reviewed the most"]}, {"cell_type": "markdown", "id": "d31935e8", "metadata": {}, "source": ["# Scatterplot\nScatterplots are great for categorical data, allowing you to see a relationship."]}, {"cell_type": "code", "execution_count": 1, "id": "205ed86a", "metadata": {}, "outputs": [], "source": ["# UGLY, but potentially helpful plot\n\n# why may we want to do a scatterplot for categorical data??\n\n\nf, axes = plt.subplots(1, 1, figsize=(10, 15))\nsns.despine\n\nsns.scatterplot(x='log_guests_included', \n                y = 'log_price', \n                hue='room_type',\n                size = 'log_reviews_pm', \n                sizes= (5, 400),\n                alpha = .8,\n                palette = \"tab10\",\n                data = df_10).set_title(\"Price by Bedrooms (weighted by Reviews)\")"]}, {"cell_type": "code", "execution_count": 1, "id": "fc3cef5f", "metadata": {}, "outputs": [], "source": ["#appears that reviews_per_month is an important activity heuristic, \n#we can see there is a potential relationship between price and bedrooms, especially in active airbnbs"]}, {"cell_type": "code", "execution_count": 1, "id": "10f8fea4", "metadata": {}, "outputs": [], "source": ["df_10.info()"]}, {"cell_type": "code", "execution_count": 1, "id": "35e15fd0", "metadata": {}, "outputs": [], "source": ["df_10.nunique()"]}, {"cell_type": "markdown", "id": "5be06a45", "metadata": {}, "source": ["Removing columns that were effected by the log transformations."]}, {"cell_type": "code", "execution_count": 1, "id": "3803ccc6", "metadata": {}, "outputs": [], "source": ["#Removing Log Transformed Variables\n\ndf_lg = df_10.drop(['price', \n                    'security_deposit', \n                    'cleaning_fee', \n                    'extra_people', \n                    'reviews_per_month', \n                    'bedrooms', \n                    'bathrooms', \n                    'guests_included', \n                    'calculated_host_listings_count'], axis = 1)"]}, {"cell_type": "code", "execution_count": 1, "id": "925ce67f", "metadata": {}, "outputs": [], "source": ["df_lg.nunique()"]}, {"cell_type": "code", "execution_count": 1, "id": "fcb3808b", "metadata": {}, "outputs": [], "source": ["df_lg.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "7454eb61", "metadata": {}, "outputs": [], "source": ["df_lg2 = df_lg.drop(['host_id', 'neighbourhood_cleansed', 'city', 'property_type'], axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "c50462e4", "metadata": {}, "outputs": [], "source": ["df_lg2.nunique()"]}, {"cell_type": "code", "execution_count": 1, "id": "80863a66", "metadata": {}, "outputs": [], "source": ["df_lg2.skew()"]}, {"cell_type": "code", "execution_count": 1, "id": "3ba75935", "metadata": {}, "outputs": [], "source": ["df_lg2.dtypes"]}, {"cell_type": "markdown", "id": "26a0b1b4", "metadata": {}, "source": ["# Modeling"]}, {"cell_type": "code", "execution_count": 1, "id": "fefc31db", "metadata": {}, "outputs": [], "source": ["#MODELING STARTS\nX = df_lg2\nX = pd.get_dummies(data=X, drop_first = True)\n\nY = X['log_price']\nX2 = X.drop('log_price', axis = 1)\nX = X2\n\nX.nunique()"]}, {"cell_type": "code", "execution_count": 1, "id": "ec1e3e30", "metadata": {}, "outputs": [], "source": ["#save cleaned X and Y for further analysis in R or elsewhere\n\nX.to_csv(\"./Xcleaned.csv\")\nY.to_csv(\"./Ycleaned.csv\")"]}, {"cell_type": "code", "execution_count": 1, "id": "7ee1dad4", "metadata": {}, "outputs": [], "source": ["#BASELINE MULTIVARIATE REGRESSION MODEL\n\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = .2, random_state = 40)\n\nest = sm.OLS(Y_train, X_train).fit()\ndisplay(est.summary())\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "80ff1058", "metadata": {}, "outputs": [], "source": ["# explore Variance inflation factor of each variable in regression model\n\n# big VIF == bad\n\n# For each X, calculate VIF and save in dataframe\nvif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\n\nvif[\"features\"] = X_train.columns\n\nvif.round(1)"]}, {"cell_type": "code", "execution_count": 1, "id": "f55f13f1", "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import mean_squared_error, r2_score\n\n#EVALUATION OF MODEL\n\npredicted = est.predict(X_test)\n\nprint(\"MSE of model when comparing Y_test and predicted is %lf\" %mean_squared_error(Y_test, predicted))\n\n    \nfig, ax = plt.subplots(figsize=(7,7))\nax.scatter(Y_test, predicted)\nax.plot([Y_test.min(), Y_test.max()], [Y_test.min(), Y_test.max()], 'k--', lw=4, alpha=.5)\nax.set_xlabel('measured')\nax.set_ylabel('predicted')\nax.set_title(\"Baseline Train Predictions\")\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "7b7be082", "metadata": {}, "outputs": [], "source": ["plot_lm_1 = plt.figure(1)\nplot_lm_1.set_figheight(5)\nplot_lm_1.set_figwidth(14)\n\nplot_lm_1.axes[0] = sns.residplot(est.fittedvalues, y=Y_train, data=X_train, \n                          lowess=True, \n                          scatter_kws={'alpha': 0.5}, \n                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n\nplot_lm_1.axes[0].set_title('Residuals vs Fitted')\nplot_lm_1.axes[0].set_xlabel('Fitted values')\nplot_lm_1.axes[0].set_ylabel('Residuals')"]}, {"cell_type": "code", "execution_count": 1, "id": "070d1fa6", "metadata": {}, "outputs": [], "source": ["\nmodel_fitted_y = est.fittedvalues\n# model residuals\nmodel_residuals = est.resid\n# normalized residuals\nmodel_norm_residuals = est.get_influence().resid_studentized_internal\n# absolute squared normalized residuals\nmodel_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals))\n# absolute residuals\nmodel_abs_resid = np.abs(model_residuals)\n# leverage, from statsmodels internals\nmodel_leverage = est.get_influence().hat_matrix_diag\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "eb9b506a", "metadata": {}, "outputs": [], "source": ["QQ = sm.ProbPlot(model_norm_residuals)\nplot_lm_2 = QQ.qqplot(line='45', alpha=0.5, color='#4C72B0', lw=1)\n\nplot_lm_2.set_figheight(6)\nplot_lm_2.set_figwidth(6)\n\nplot_lm_2.axes[0].set_title('Normal Q-Q')\nplot_lm_2.axes[0].set_xlabel('Theoretical Quantiles')\nplot_lm_2.axes[0].set_ylabel('Standardized Residuals');\n\n# annotations\nabs_norm_resid = np.flip(np.argsort(np.abs(model_norm_residuals)), 0)\nabs_norm_resid_top_3 = abs_norm_resid[:3]\n\nfor r, i in enumerate(abs_norm_resid_top_3):\n    plot_lm_2.axes[0].annotate(i, \n                               xy=(np.flip(QQ.theoretical_quantiles, 0)[r],\n                                   model_norm_residuals[i]));"]}, {"cell_type": "code", "execution_count": 1, "id": "b9a51853", "metadata": {}, "outputs": [], "source": ["coef = pd.Series(est.params, index = X_train.columns)\n\n\nimp_coef = pd.concat([coef.sort_values().head(18),\n                     coef.sort_values().tail(18)])\nplt.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Baseline Model\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}