{"cells": [{"cell_type": "code", "execution_count": 1, "id": "e6921d99", "metadata": {}, "outputs": [], "source": ["import os\nprint(os.listdir(\"../input\"))"]}, {"cell_type": "markdown", "id": "f9d75ad9", "metadata": {}, "source": ["# MBTI classification notebook\n## Team 14 Kaggle competition (connoisseurs_foodies)\n\n\"I tweet, therefore I am...\" Not really, but what we write can provide insight into our type of personality. In this competition, we aim to build a classifier that will be able to predict a person's personality based on what they write in twitter posts. The 16 MBTI personality types will be used, which is given by four binary variables as follows:\n\n- Mind: Introverted (I) or Extroverted (E) \n- Energy: Sensing (S) or Intuitive (N) \n- Nature: Feeling (F) or Thinking (T) \n- Tactics: Perceiving (P) or Judging (J)\n\nA training set with rows of tweets and associated personality types have been provided, as well as a test set containing only tweet posts.\n\nOur task is to predict each one of the categories, mind, energy, nature, and tactics, separately. The combination of the four predictions will then provide the overall personality type.\n\nThe categories will be encoded as follows: \n- Mind: I = 0, E = 1 \n- Energy: S = 0, N = 1 \n- Nature: F = 0, T = 1 \n- Tactics: P = 0, J = 1"]}, {"cell_type": "markdown", "id": "e19b0892", "metadata": {}, "source": ["# Data exploration and cleaning"]}, {"cell_type": "markdown", "id": "0e7715af", "metadata": {}, "source": ["## Loading data and constructing separate categories"]}, {"cell_type": "code", "execution_count": 1, "id": "e1d6b53a", "metadata": {}, "outputs": [], "source": ["# Import libraries for data manipulation and viewing\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline"]}, {"cell_type": "code", "execution_count": 1, "id": "d70e478d", "metadata": {}, "outputs": [], "source": ["# Load the provided training and test data into dataframes\ntrain = pd.read_csv('../input/train.csv' )\ntest = pd.read_csv('../input/test.csv' )\n\n# Check the dataframe sizes, type assignments, and missing values\nprint('Train info:')\ntrain.info()\nprint('\\nTest info:')\ntest.info()"]}, {"cell_type": "code", "execution_count": 1, "id": "6ceb5ad5", "metadata": {}, "outputs": [], "source": ["# View the frequencies of the personality types\nplt.subplots(figsize=(22, 5))\nsns.countplot(train['type'].sort_values(), palette='Accent')"]}, {"cell_type": "markdown", "id": "b6c02405", "metadata": {}, "source": ["From the dataframe information, we can see that the training set contains around three times more entries than the test set. Neither set has any missing values or faulty data type assignments. Looking at the counts of each personality type in the training set, it is clear that the majority of posts are from  IN__ type personalities. \n\nAs we need to predict each personality type category individually, we will separate the categories into columns and encode them as noted above:\n- Mind: I = 0, E = 1 \n- Energy: S = 0, N = 1 \n- Nature: F = 0, T = 1 \n- Tactics: P = 0, J = 1"]}, {"cell_type": "code", "execution_count": 1, "id": "1728c434", "metadata": {}, "outputs": [], "source": ["# Separate type categories\ntrain['mind'] = train['type'].str[0]\ntrain['energy'] = train['type'].str[1]\ntrain['nature'] = train['type'].str[2]\ntrain['tactics'] = train['type'].str[3]"]}, {"cell_type": "code", "execution_count": 1, "id": "6a035779", "metadata": {}, "outputs": [], "source": ["# Encode category letters as either 0 or 1 \ntrain['mind'] = train['mind'].apply(lambda x: 0 if x == 'I' else 1)\ntrain['energy'] = train['energy'].apply(lambda x: 0 if x == 'S' else 1)\ntrain['nature'] = train['nature'].apply(lambda x: 0 if x == 'F' else 1)\ntrain['tactics'] = train['tactics'].apply(lambda x: 0 if x == 'P' else 1)\n\ntrain.head()"]}, {"cell_type": "markdown", "id": "9dbf9c5a", "metadata": {}, "source": ["The twitter posts contain a variety of words, punctuation characters that form part of text, part of links to webpages, or indicate new tweets, various uses of capitalization, and  emoticons (emoji's) to list a few. Different personality types might have different preferences in the way they use words and text characters to express themselves. Thus, in order to use the twitter posts constructively for personality type classification, we need to preform some general text cleaning and processing."]}, {"cell_type": "markdown", "id": "a73666a0", "metadata": {}, "source": ["## Text cleaning and processing"]}, {"cell_type": "markdown", "id": "f3cd65b0", "metadata": {}, "source": ["To perform the same cleaning and processing on both the training set posts and the test set posts, we will define functions for specific tasks. We can then apply the same function to both sets of posts. \n\nThe first function uses regular expressions to identify character sequences and \"clean-up\" some of text in the following way:\n- Webpage links would be replaced by the word 'url'.\n- Twitter handles will be replaced by the word 'twithandle'\n- Apostrophe suffixes would be replaced by the corresponding words. This would enable better matching and vectorization of pronouns.\n- A selection of punctuation would be removed. We will not remove all punctuation as we hypothesize that the use of exclamation and question marks might differ between personalty types.\n- All words would be converted to lower case.\n"]}, {"cell_type": "code", "execution_count": 1, "id": "065bf969", "metadata": {}, "outputs": [], "source": ["import re\n\ndef word_replace(df):\n    \n    \"\"\"Converts apostrophe suffixes to words, replace webpage links with url, annotate hashtags and mentions, remove a selection of punctuation, and convert all words to lower case.\n    Args:\n        df (DataFrame): dataframe containing 'posts' column to convert\n    Returns:\n        df (DataFrame): dataframe with converted 'posts' column \n    \"\"\"\n\n    # Change all webpage links to 'url'\n    df['posts'] = df['posts'].str.replace(r'http.?://[^\\s]+[\\s]?', 'url ')\n    \n    # Replace apostrophe's with words\n    df['posts'] = df['posts'].str.replace(r'n\\'t', ' not')\n    df['posts'] = df['posts'].str.replace(r'\\'s', ' is')\n    df['posts'] = df['posts'].str.replace(r'\\'m', ' am')\n    df['posts'] = df['posts'].str.replace(r'\\'re', ' are')\n    df['posts'] = df['posts'].str.replace(r'\\'ve', ' have')\n    df['posts'] = df['posts'].str.replace(r'\\'ll', ' will')\n    df['posts'] = df['posts'].str.replace(r'\\'d', ' would')\n\n    # Replace # and @ with word\n    df['posts'] = df['posts'].str.replace(r'#|@', 'twithandle ')\n    \n    # Remove selected punctuation\n    df['posts'] = df['posts'].str.replace(r\"[',.():|-]\", \" \")\n\n    # Convert all words to lower case\n    df['posts'] = df['posts'].str.lower()\n    \n    return df"]}, {"cell_type": "code", "execution_count": 1, "id": "c1950321", "metadata": {}, "outputs": [], "source": ["# Replace webpages, apostrophe's, selected punctuation etc.\ntrain_clean = word_replace(train.copy())\ntest_clean = word_replace(test.copy())"]}, {"cell_type": "markdown", "id": "9185bfe7", "metadata": {}, "source": ["In order to turn a sequence of words of type string into a vector, we need to convert each sequence into a list of word units or tokens. There are various ways to go about it, such as to split the string on white spaces of example. As we are dealing with twitter posts, we decided to use the TweetTokenizer within the natural language toolkit (nltk) library to perform word tokenization. The TweetTokenizer will preserve emoticons when splitting string sequences into tokens.\n\nWe will also employ the nltk library, WordNetLemmatizer, to transform the word tokens to their corresponding root words (i.e. their lemma or dictionary form). "]}, {"cell_type": "markdown", "id": "2bc8fcff", "metadata": {}, "source": ["*Reveal and run the code below to import and download nltk libraries*"]}, {"cell_type": "code", "execution_count": 1, "id": "986ce70c", "metadata": {}, "outputs": [], "source": ["# # Download NLTK libraries\n# import nltk\n# nltk.download()"]}, {"cell_type": "code", "execution_count": 1, "id": "ceec3918", "metadata": {}, "outputs": [], "source": ["# Import required nltk functions \nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem import WordNetLemmatizer\n\nget_tokens = TweetTokenizer()\nget_lemmas = WordNetLemmatizer()\n\n# Define tokenization and lemmetization function\ndef tokenize_lemmatize(df):\n    \n    \"\"\"Tokenize and lemmatize posts.\n    Args:\n        df (DataFrame): dataframe containing 'posts' column to convert\n    Returns:\n        df (DataFrame): dataframe with converted 'posts' column   \n    \"\"\"\n    \n    df['posts'] = df.apply(lambda row: [get_lemmas.lemmatize(w) for w in get_tokens.tokenize(row['posts'])], axis=1)  #Tokenize and lemmatize\n    \n    return df\n  "]}, {"cell_type": "code", "execution_count": 1, "id": "a62a0fdd", "metadata": {}, "outputs": [], "source": ["# Tokenize and lemmatize posts\ntrain_clean = tokenize_lemmatize(train_clean.copy())\ntest_clean = tokenize_lemmatize(test_clean.copy())"]}, {"cell_type": "markdown", "id": "8eb23041", "metadata": {}, "source": ["As we are dealing with twitter posts that contain emoticons. As we hypothesize that different personality types might have different preferences around the use of emoticons, we will denote all emoticons with the word 'emoji'. We will again write a function in order to equally perform the transformation on both the training and test sets.\nInitially, we more or less manually found the sequences and replaced the \ud83e\udd84 in the posts. However, we resorted to use the emoji library to facilitate the transformations with more ease.\n\nSome of the emoticons found in the posts are as follows:\n\n\ud83c\udf39 \ud83c\udf44 \ud83d\udc0d \ud83d\udc48 \ud83d\udc4b \ud83d\udc4c \ud83d\udc4d \ud83d\udc4f \ud83d\udc8e \ud83d\udc93 \ud83d\udc96 \ud83d\udc9b \ud83d\udd25 \ud83d\udd5a \ud83d\ude00 \ud83d\ude01 \ud83d\ude02 \ud83d\ude03 \ud83d\ude04 \ud83d\ude05 \ud83d\ude06 \ud83d\ude07 \ud83d\ude09 \ud83d\ude0a \ud83d\ude0b \ud83d\ude0c \ud83d\ude0d \ud83d\ude0e \ud83d\ude0f \ud83d\ude10 \ud83d\ude13 \ud83d\ude15 \ud83d\ude18 \ud83d\ude1c \ud83d\ude1d \ud83d\ude1e \ud83d\ude21 \ud83d\ude22 \ud83d\ude23 \ud83d\ude27 \ud83d\ude2b \ud83d\ude2c \ud83d\ude2e \ud83d\ude31 \ud83d\ude33 \ud83d\ude34 \ud83d\ude3a \ud83d\ude4b \ud83e\udd13 \ud83e\udd14 \ud83e\udd17 \ud83e\udd84 \u270c \u270d \u2728\n\n"]}, {"cell_type": "markdown", "id": "39731cd7", "metadata": {}, "source": ["*Reveal and run the code below to import the emoji library*"]}, {"cell_type": "code", "execution_count": 1, "id": "4ac506e6", "metadata": {}, "outputs": [], "source": ["# # Install emoji library\n# !pip install emoji"]}, {"cell_type": "code", "execution_count": 1, "id": "d21722a8", "metadata": {}, "outputs": [], "source": ["import emoji\n\n# Create list of emoticon codes\nemojies = set(emoji.UNICODE_EMOJI.keys())\n\n# Define emoticon replacement functions\ndef swap_emoji(word):\n    \n    \"\"\"Replace emoticon with 'emoji'.\n    Args:\n        word (str): word to replace\n    Returns:\n        word (str): replacement word   \n    \"\"\"\n    \n    if word in emojies:      #  replace emoticon\n        return 'emoji'\n    \n    return word \n\n\ndef emoji_convert(df):\n    \n    \"\"\"Iterate through post and replace emoticons with 'emoji'.\n    Args:\n        df (DataFrame): dataframe containing 'posts' column to convert\n    Returns:\n        df (DataFrame): dataframe with converted 'posts' column   \n    \"\"\" \n    \n    df['posts'] = df['posts'].apply(lambda row: [swap_emoji(word) for word in row])   #  apply swap_emoji function to words in posts\n    \n    return df\n"]}, {"cell_type": "code", "execution_count": 1, "id": "16f031c4", "metadata": {}, "outputs": [], "source": ["# Substitute emoticon codes with 'emoji'\ntrain_clean = emoji_convert(train_clean.copy())\ntest_clean = emoji_convert(test_clean.copy())"]}, {"cell_type": "markdown", "id": "5f95f7d1", "metadata": {}, "source": ["The English language contains multiple conjunctions and words that support sentence structure, but do not add much meaning. These words usually have a high frequency in text documents, and can be seen as noise in the text data. To decrease the noise and also decrease the number of words in the posts, we will remove these \"words of little meaning\" or stopwords. The nltk library have a list of such stopwords, which also include pronouns. Since we hypothesize that the use of pronouns might differ between personality types, we will remove the pronouns from the stopword list before using the list to remove stopwords from the posts."]}, {"cell_type": "markdown", "id": "8c8d83fa", "metadata": {}, "source": ["The list of nltk English stopwords is as follows:\n\n['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]"]}, {"cell_type": "code", "execution_count": 1, "id": "2a924885", "metadata": {}, "outputs": [], "source": ["from nltk.corpus import stopwords\n\n# List nltk English stopwords and add 'could' and 'would'\nstop_words = stopwords.words('english')\nstop_words = stop_words + ['could', 'would']\n\n# Remove selected pronouns from stopword list\nstop_words = [w for w in stop_words if w not in ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves']]\n\n# Define function to remove stopwords\ndef remove_words(df):\n    \n    \"\"\"Remove stopwords from posts.\n    Args:\n        df (DataFrame): dataframe containing 'posts' column to convert\n    Returns:\n        df (DataFrame): dataframe with converted 'posts' column   \n    \"\"\" \n    \n    df['posts'] = df['posts'].apply(lambda row: [word for word in row if word not in stop_words])   #iterate through words in posts and eliminate stopwords\n    \n    return df"]}, {"cell_type": "code", "execution_count": 1, "id": "3a3d84d6", "metadata": {}, "outputs": [], "source": ["# Remove stop words from tokens\ntrain_clean = remove_words(train_clean.copy())\ntest_clean = remove_words(test_clean.copy())"]}, {"cell_type": "markdown", "id": "bbc029ce", "metadata": {}, "source": ["## Visualize character use by personality category"]}, {"cell_type": "markdown", "id": "385d7c44", "metadata": {}, "source": ["With the processed and cleaned posts, we can explore and visualize some of the word tokens."]}, {"cell_type": "code", "execution_count": 1, "id": "674f92b1", "metadata": {}, "outputs": [], "source": ["# Create personality catagory and annotation lists\ncolumns = ['mind', 'energy', 'nature', 'tactics']\nticks = [('I', 'E'), ('S', 'N'), ('F', 'T'), ('P', 'J')]\n\n# Plot the number of posts per personality category\nprint('Number of posts per personality catagory:')\nplt.subplots(figsize=(22, 100))\nfor i in range(len(columns)):\n    j = i+1\n    plt.subplot(22,4,j)\n    ax = sns.countplot(train_clean[columns[i]], palette='Accent')\n    ax.set_xticklabels(ticks[i])"]}, {"cell_type": "code", "execution_count": 1, "id": "0c0c5ddf", "metadata": {}, "outputs": [], "source": ["# Calculate personality category class distributions\nE = train_clean['mind'][train_clean['mind'] == 1].count() / train_clean['mind'].count()\nprint('E:I = {} : {}'.format(round(E, 2), 1-round(E, 2)))\n\nN = train_clean['energy'][train_clean['energy'] == 1].count() / train_clean['energy'].count()\nprint('N:S = {} : {}'.format(round(N, 2), 1-round(N, 2)))\n\nT = train_clean['nature'][train_clean['nature'] == 1].count() / train_clean['nature'].count()\nprint('T:F = {} : {}'.format(round(T, 2), 1-round(T, 2)))\n\nJ = train_clean['tactics'][train_clean['tactics'] == 1].count() / train_clean['tactics'].count()\nprint('J:P = {} : {}'.format(round(J, 2), 1-round(J, 2)))"]}, {"cell_type": "code", "execution_count": 1, "id": "be3327aa", "metadata": {}, "outputs": [], "source": ["# Count the number of emoticons used\ntrain_clean['emoji_count'] = train_clean['posts'].apply(lambda row: len(['emoji' for word in row if word == 'emoji']))\n\n# Plot the average number of emoticons used per personality category\nprint('Average number of emoticons used per personality category:')\nplt.subplots(figsize=(22, 100))\nfor i in range(len(columns)):\n    j = i+1\n    plt.subplot(22,4,j)\n    df = train_clean.groupby(columns[i])[['emoji_count']].mean()\n    df.index = ticks[i]\n    sns.barplot(x=df.index, y=df['emoji_count'], palette='Greens')\n    plt.xlabel(columns[i])"]}, {"cell_type": "code", "execution_count": 1, "id": "e595c178", "metadata": {}, "outputs": [], "source": ["# Count the number of webpage links\ntrain_clean['url_count'] = train_clean['posts'].apply(lambda row: len(['url' for word in row if word == 'url']))\n\n# Plot the average number of webpage links per personality category\nprint('Average number of webpage links per personality category:')\nplt.subplots(figsize=(22, 100))\nfor i in range(len(columns)):\n    j = i+1\n    plt.subplot(22,4,j)\n    df = train_clean.groupby(columns[i])[['url_count']].mean()\n    df.index = ticks[i]\n    sns.barplot(x=df.index, y=df['url_count'], palette='Blues')\n    plt.xlabel(columns[i])"]}, {"cell_type": "code", "execution_count": 1, "id": "0d91e1ce", "metadata": {}, "outputs": [], "source": ["# Count the number of exclamation marks used\ntrain_clean['exclamation'] = train_clean['posts'].apply(lambda row: len(['!' for word in row if word == '!']))\n\n# Plot the average number of exclamation marks used per personality category\nprint('Average number of exclamation marks used per personality category:')\nplt.subplots(figsize=(22, 100))\nfor i in range(len(columns)):\n    j = i+1\n    plt.subplot(22,4,j)\n    df = train_clean.groupby(columns[i])[['exclamation']].mean()\n    df.index = ticks[i]\n    sns.barplot(x=df.index, y=df['exclamation'], palette='Purples')\n    plt.xlabel(columns[i])"]}, {"cell_type": "code", "execution_count": 1, "id": "aff894ba", "metadata": {}, "outputs": [], "source": ["# Count the use of pronoun 'I'\ntrain_clean['I'] = train_clean['posts'].apply(lambda row: len(['i' for word in row if word == 'i']))\n\n# Plot the average number of times 'I' is used per personality category\nprint(\"Average use of 'I' per personality category:\")\nplt.subplots(figsize=(22, 100))\nfor i in range(len(columns)):\n    j = i+1\n    plt.subplot(22,4,j)\n    df = train_clean.groupby(columns[i])[['I']].mean()\n    df.index = ticks[i]\n    sns.barplot(x=df.index, y=df['I'], palette='Oranges')\n    plt.xlabel(columns[i])"]}, {"cell_type": "code", "execution_count": 1, "id": "d2e98e6b", "metadata": {}, "outputs": [], "source": ["# Count the use of twitter handles\ntrain_clean['handle'] = train_clean['posts'].apply(lambda row: len(['handle' for word in row if word == 'twithandle']))\n\n# Plot the average number of twitter handles used per personality category\nprint(\"Average use of twitter handles per personality category:\")\nplt.subplots(figsize=(22, 100))\nfor i in range(len(columns)):\n    j = i+1\n    plt.subplot(22,4,j)\n    df = train_clean.groupby(columns[i])[['handle']].mean()\n    df.index = ticks[i]\n    sns.barplot(x=df.index, y=df['handle'], palette='Reds')\n    plt.xlabel(columns[i])"]}, {"cell_type": "markdown", "id": "75d2aed2", "metadata": {}, "source": ["Looking at the breakdown of posts between the categories, we can again see that imbalanced classes. The biggest difference is between I and E, and S and N. As an exercise, not shown in this notebook, we have investigated both resampling with replacement, as well as sampling subsets, in order to equal out the number of posts across categories. This can be done using the sklearn.utils.resample function. However, we have found that resampling does not enhance model performance for any of the categories.\n\nWe can see that the average number of use of selected characters and tags vary between categories. From observation, not statistically speaking, we note the following:\n- Emoticons: Intuitive, feeling, and judging personalities respectively tend to use more emoticons that their sensing, thinking, and perceiving counterparts.\n- Webpage links: The biggest difference in the average number of webpage links posted is between introverts and extroverts.\n- Exclamation marks: Introverts vs. extroverts and feeling vs. thinking personalities show the biggest difference in average use of exclamation marks.\n- 'I': The average use of 'I' appears fairly similar across categories, with feeling personalities using 'I' slightly more.\n- Twitter handles: The biggest difference in the average use of twitter handles appears to be between extroverts and introverts, with introverts using less handles.\n\nAll in all, there do seem to be differences in the posts from different personality types. We can thus proceed use the posts to create vectors and explore classification models. "]}, {"cell_type": "markdown", "id": "2345931a", "metadata": {}, "source": ["# Vectorization and model exploration"]}, {"cell_type": "markdown", "id": "80c9506e", "metadata": {}, "source": ["## TfidfVectorizer"]}, {"cell_type": "markdown", "id": "5ef9644e", "metadata": {}, "source": ["As we would like to increase emphasis on word tokens that appear less frequent among the posts, relative to word tokens that appear in almost all posts, we decided to employ the nltk TfidfVectorizer. The TfidfVectorizer calculates the term (token) frequencies (Tf), and multiplies it by the logarithm of the ratio between the number of records and the number of records that contain the specific term (the inverse document frequency or idf). As such, it will \"penalize\" the term frequency according to the inverse document frequency and give more weight to less frequent, but important terms (tokens). \n\nIn addition to the word tokens, we would also like to capture some phrase context. We therefore also investigated the inclusion of bigrams and trigrams in the vector (not shown). The inclusion of bigrams was found to provided the better predictive capability. Only tokens and bigrams that occur more than once among the training records will be included in the vector. We will again define a function to fit the vectorizer to the specified training set parameter, and subsequently vectorize both the specified training and test set parameters similarly. "]}, {"cell_type": "code", "execution_count": 1, "id": "117c61a2", "metadata": {}, "outputs": [], "source": ["# Import vectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Initialise vectorizer\ntt = TfidfVectorizer(preprocessor=list, tokenizer=list, ngram_range=(1,2), min_df=2, smooth_idf=False)\n\n# Define vectorization function\ndef vectorise(train_set, test_set):\n    \n    \"\"\"Fit a vector to train_set and transform train_set and test_set accordingly. \n    Args:\n        train_set (array or DataFrame): train features to vectorize\n        test_set (array or DataFrame): test features to vectorize\n    Returns:\n        train_vect (sparse array): train_set vector\n        test_vect (sparse array): test_set vector\n    \"\"\"\n    \n    tt.fit(train_set)\n    train_vect = tt.transform(train_set)\n    test_vect = tt.transform(test_set)\n    \n    return train_vect, test_vect\n"]}, {"cell_type": "markdown", "id": "09b83d67", "metadata": {}, "source": ["## Hyperparameter search with GridSearchCV\n"]}, {"cell_type": "markdown", "id": "2d00f09a", "metadata": {}, "source": ["The scoring metric used for model evaluation in the Kaggle competition is the average log loss across the four personality categories. Log loss for a binary prediction is calculated as follows:\n\nlog loss = -(y\\log(p) + (1 - y)\\log(1 - p))\n\nwhere:\n- y: true class (i.e. 0 or 1)\n- p: predicted probability\n- log: natural logarithm\n\nLog loss thus takes the uncertainty of the prediction into account (http://wiki.fast.ai/index.php/Log_Loss).\n\nSince the scoring metric used requires the probability of predicting a class, we opted to investigate various classification models that are based on probability predictions to make class assignments. These include Multinomial Naive Bayes, Logistic Regression, and Random Forests. We found that Logistic Regression performed the best on the task at hand. Therefore, we will henceforth only discuss our implementation of Logistic Regression. However, with the standardized way of implementing models using sklearn libraries, it should be noted that the code below can easily be tailored to investigate all the aforementioned models. \n"]}, {"cell_type": "markdown", "id": "6161126e", "metadata": {}, "source": ["Logistic regression in sklearn facilitate L1 regularization when the liblinear solver in used. Since we have a large feature vector, L1 regularization will aid in reducing the number of features, as it is able to shrink coefficients to zero. Using Logistic Regression with the liblinear solver and L1 regularization then leaves the hyperparameter 'C' to be tuned. 'C' controls the amount of regularization, such that a smaller value for 'C' enforce greater regularization. \n\nWe will use the sklearn GridSearchCV function to automatically search through a specified list of values for 'C'. As GridSearchCV maximizes the scoring metric, the negative log loss will be used. Log loss will thus in effect be minimized. The average negative log loss across four-fold cross-validation will be used for comparing the different values of 'C'.\n\nAs the different personality type categories have different distributions, and are in essence different binary classification problems, the hyperparameter 'C' will be tuned for each category separately."]}, {"cell_type": "code", "execution_count": 1, "id": "3965f262", "metadata": {}, "outputs": [], "source": ["# Import models, metrics, and model selection utilities\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import log_loss, confusion_matrix, accuracy_score\nfrom sklearn.linear_model import LogisticRegression"]}, {"cell_type": "markdown", "id": "7e9e7fce", "metadata": {}, "source": ["Cross-validation uses the full complement of the training set. Therefore, the vectorizer will be fitted to the entire training set."]}, {"cell_type": "code", "execution_count": 1, "id": "57fcf6f5", "metadata": {}, "outputs": [], "source": ["# Vectorize training posts\nX_train_vect, X_test_vect = vectorise(train_clean['posts'], test_clean['posts'])\nX_train_vect.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "eb2b8ba0", "metadata": {}, "outputs": [], "source": ["# Initialize the model to be used\ncm = LogisticRegression(penalty='l1', solver='liblinear', random_state=11)\n\n# Specify the range of 'C' parameters\nparams = {'C': [0.1, 0.5, 1, 5, 10, 50, 100]}\n\n# Initialize the grid search\ngrid = GridSearchCV(cm, param_grid=params, scoring='neg_log_loss', n_jobs=-1, cv=4)\n\n# Execute a grid search for each personality category\ncategories = ['mind', 'energy', 'nature', 'tactics']\nfor cat in categories:\n    grid.fit(X_train_vect, train_clean[cat])\n    print(cat + ':')\n    print('Best score: ', -grid.best_score_)\n    print('Best paramaters: ', grid.best_params_)\n    print('\\n')"]}, {"cell_type": "markdown", "id": "90bf008f", "metadata": {}, "source": ["## Finer hyperparameter search with an isolated test set"]}, {"cell_type": "markdown", "id": "c11bec20", "metadata": {}, "source": ["Since the aim is to construct a model that would be able to classify unseen data, we would further \"fine-tune\" the hyperparameter 'C' using a hold-out test set from the training data. The \"fine-tuning\" was manually done with 'C' values in the range of five to seven (not all results shown). "]}, {"cell_type": "code", "execution_count": 1, "id": "4646df19", "metadata": {}, "outputs": [], "source": ["# Partition a rondom test set from the training data\nX_train, X_test, y_train, y_test = train_test_split(train_clean['posts'], train_clean[['mind', 'energy', 'nature', 'tactics']], random_state=11)\n\n# Vectorize the training and test sets\nX_train_vect, X_test_vect = vectorise(X_train, X_test)\nX_train_vect.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "fb47b521", "metadata": {}, "outputs": [], "source": ["# Define model building function\ndef build_model(X_train_vect_f, X_test_vect_f, y_train_f, y_test_f, model):\n    \"\"\"Fit the specified model, make predictions, and calculate the log loss and confusion matrix.\n    Args:\n        X (array or DataFrame): independent variables\n        y (array or DataFrame): dependent variable\n        model (model): sklearn model and parameters\n    Returns:\n        train_pred (array): predictions from training data \n        test_pred (array): predictions from test data\n        train_loss (float): log loss on training data\n        test_loss (float): log loss on test data\n        test_accuracy (float): accuracy score on test data\n        test_matrix (array): confusion matrix on test data \n    \"\"\"\n    \n    model.fit(X_train_vect_f, y_train_f)                         # fit model using training data\n\n    train_pred = model.predict(X_train_vect_f)                   # predict training set labels\n    test_pred = model.predict(X_test_vect_f)                     # predict test set labels\n    \n    train_proba = model.predict_proba(X_train_vect_f)            # predict label probabilities from training set\n    test_proba = model.predict_proba(X_test_vect_f)              # predict label probabilities from test set\n    \n    train_loss = log_loss(y_train_f, train_proba, eps=1e-15)     # log loss on training data\n    test_loss = log_loss(y_test_f, test_proba, eps=1e-15)        # log loss on test data   \n    \n    test_accuracy = accuracy_score(y_test_f, test_pred)          # accuracy score on test data\n    test_matrix = confusion_matrix(y_test_f, test_pred)          # confusion matrix on test data\n        \n    return train_pred, test_pred, train_loss, test_loss, test_accuracy, test_matrix"]}, {"cell_type": "code", "execution_count": 1, "id": "14bfadac", "metadata": {}, "outputs": [], "source": ["# Initialize a list to store log loss and accuracy on test sets\nll_scores = []\nac_scores = []"]}, {"cell_type": "code", "execution_count": 1, "id": "7a936a56", "metadata": {}, "outputs": [], "source": ["# Initialize model for the mind category\ncm = LogisticRegression(penalty='l1', solver='liblinear', C=4, random_state=11)\n\n# Train and test the model for the mind category\npred_train, pred_test, loss_train, loss_test, acc_test, matrix_test = build_model(X_train_vect, X_test_vect, y_train['mind'], y_test['mind'], cm)\nll_scores.append(loss_test)\nac_scores.append(acc_test)\nprint('Mind:')\nprint('Log loss on training set: ',loss_train)\nprint('Log loss on test set: ', loss_test)\nprint('Accuracy on test set:', acc_test)\nprint('Test set confusion matrix: \\n', matrix_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "a9422ef5", "metadata": {}, "outputs": [], "source": ["# Initialize model for the energy category\ncm = LogisticRegression(penalty='l1', solver='liblinear', C=5, random_state=11)\n\n# Train and test the model for the energy category\npred_train, pred_test, loss_train, loss_test, acc_test, matrix_test = build_model(X_train_vect, X_test_vect, y_train['energy'], y_test['energy'], cm)\nll_scores.append(loss_test)\nac_scores.append(acc_test)\nprint('Energy:')\nprint('Log loss on training set: ',loss_train)\nprint('Log loss on test set: ', loss_test)\nprint('Accuracy on test set:', acc_test)\nprint('Test set confusion matrix: \\n', matrix_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "59b596d2", "metadata": {}, "outputs": [], "source": ["# Initialize model for the nature category\ncm = LogisticRegression(penalty='l1', solver='liblinear', C=5, random_state=11)\n\n# Train and test the model for the nature category\npred_train, pred_test, loss_train, loss_test, acc_test, matrix_test = build_model(X_train_vect, X_test_vect, y_train['nature'], y_test['nature'], cm)\nll_scores.append(loss_test)\nac_scores.append(acc_test)\nprint('Nature:')\nprint('Log loss on training set: ',loss_train)\nprint('Log loss on test set: ', loss_test)\nprint('Accuracy on test set:', acc_test)\nprint('Test set confusion matrix: \\n', matrix_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "e9e2df82", "metadata": {}, "outputs": [], "source": ["# Initialize model for the tactics category\ncm = LogisticRegression(penalty='l1', solver='liblinear', C=3, random_state=11)\n\n# Train and test the model for the tactics category\npred_train, pred_test, loss_train, loss_test, acc_test, matrix_test = build_model(X_train_vect, X_test_vect, y_train['tactics'], y_test['tactics'], cm)\nll_scores.append(loss_test)\nac_scores.append(acc_test)\nprint('Tactics:')\nprint('Log loss on training set: ',loss_train)\nprint('Log loss on test set: ', loss_test)\nprint('Accuracy on test set:', acc_test)\nprint('Test set confusion matrix: \\n', matrix_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "4c19b676", "metadata": {}, "outputs": [], "source": ["print('Average log loss across personality categories: ', round(np.mean(ll_scores), 3))\nprint('Average accuracy across personality categories: ', round(np.mean(ac_scores), 3))"]}, {"cell_type": "markdown", "id": "33fa8467", "metadata": {}, "source": ["Optimal 'C' values for each personality category was found to be as follows:\n- Mind: 4\n- Energy: 5\n- Nature: 5\n- Tactics: 3\n\nThese hyperparameter settings resulted in an average log loss of around 0.34 across the different personality type categories. In addition, for the respective categories, the log loss on the training and test sets were within 1.5-fold of each other. Thus, the models are not overfitting the training data and we can expect them to be able to generalize to unseen data.\nAn average accuracy score of around 86% were achieved across the models. Individually, the models also perform better than predicting the majority class of each personality category."]}, {"cell_type": "markdown", "id": "46f64701", "metadata": {}, "source": ["## Kaggle test output"]}, {"cell_type": "markdown", "id": "c7ac74cf", "metadata": {}, "source": ["The Kaggle competition supplies a \"type-blinded\" test set. Therefore, we can use all the given training data to construct the final classification model for each personality type category. \n\nLog loss, thus a score based on the probabilities, is stipulated as the scoring metric for the competition. However, the example submission seem to contain the predicted 0 and 1 labels for each personality type category. We will thus investigate both the predicted labels, as well as the probability predictions as outputs."]}, {"cell_type": "code", "execution_count": 1, "id": "5f3aa1fe", "metadata": {}, "outputs": [], "source": ["# Initialize output dataframe\nresults = test[['id']]\n\n# Vectorize the training and test sets\nX_train_vect, X_test_vect = vectorise(train_clean['posts'], test_clean['posts'])\nX_train_vect.shape"]}, {"cell_type": "markdown", "id": "17861548", "metadata": {}, "source": ["### Label output"]}, {"cell_type": "code", "execution_count": 1, "id": "bfbf5e5f", "metadata": {}, "outputs": [], "source": ["c_vals = [4, 5, 5, 3]      # C parameter list for final models\nclass1_proba = []          # List to store model probability predictions\n\n# Train final classification models for each category\nfor i in range(4):\n    cm = LogisticRegression(penalty='l1', solver='liblinear', C=c_vals[i], random_state=11)\n    cm.fit(X_train_vect, train[categories[i]].values)\n    pred_test = cm.predict(X_test_vect)                 # Predict labels\n    results[categories[i]] = pred_test                  # Compile label results\n    probability = cm.predict_proba(X_test_vect)         # Predict probabilities\n    class1_proba.append(probability[:,1])               # Save probabilities for predicting class label 1"]}, {"cell_type": "code", "execution_count": 1, "id": "c3cfb2e3", "metadata": {}, "outputs": [], "source": ["results.to_csv('MBTI_l1_liblinear_C4553_split_tt_all_pp_emoji_lib_handles.csv', index=False)\n# Kaggle public score: 4.64397"]}, {"cell_type": "markdown", "id": "52a487bf", "metadata": {}, "source": ["The public Kaggle score on the label predictions are 4.6397, which is far off from what we have seen for the cross-validation and hold-out model assessments."]}, {"cell_type": "markdown", "id": "035e6ca1", "metadata": {}, "source": ["### Probability output"]}, {"cell_type": "code", "execution_count": 1, "id": "dbed1822", "metadata": {}, "outputs": [], "source": ["# Compile probability results\nfor i in range(4):\n    results[categories[i]] = class1_proba[i]"]}, {"cell_type": "code", "execution_count": 1, "id": "acb7b805", "metadata": {}, "outputs": [], "source": ["results.to_csv('MBTI_l1_liblinear_C4553_split_tt_all_pp_emoji_lib_handles_proba.csv', index=False)\n# Kaggle public score: 0.34227"]}, {"cell_type": "markdown", "id": "340c0436", "metadata": {}, "source": ["The public Kaggle log loss score given the probabilities comes to 0.34227, which is in line with what we have seen during the model assessments."]}, {"cell_type": "markdown", "id": "1298f348", "metadata": {}, "source": ["# Conclusions\n\nPredicting personality types using twitter posts relies on they hypothesis that there is a distinction between the words, phrases, topics, and style used by people of different personalities. Finding the distinctions were facilitated by the implementation of a selection of natural language processing tasks. The tasks included punctuation adjustments, labeling of similar entities such as twitter handles, webpage links, and emoticons, and the removal of \"common words with little meaning\" (stopwords).Taking the document frequency of words into account during vectorization helped to increase the importance of words that might be of more relevance.\n\nAs the word feature space were large, the employment of Logistic Regression models, that is capable of feature selection through regularization, proved successful. The Logistic Regression models for each personality type category generalized to unseen test data. It provided predictive capability beyond that of predicting the majority class, and afforded an accuracy across predictions of 86%.\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}