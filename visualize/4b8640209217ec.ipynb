{"cells": [{"cell_type": "markdown", "id": "082adf30", "metadata": {}, "source": ["# Step 1: Import helpful libraries"]}, {"cell_type": "code", "execution_count": 1, "id": "ecfe26c7", "metadata": {}, "outputs": [], "source": ["# Importing Libraries\r\nimport pandas as pd # Intermediate DS\r\nimport numpy as np # Scientific Operations\r\n\r\nimport optuna # For Hyper Parameter Tuning\r\nfrom functools import partial\r\nimport multiprocessing\r\n\r\nfrom sklearn import preprocessing # Preprocesing library for Encoding, etc.\r\nfrom sklearn.model_selection import train_test_split # For splitting train and test data\r\nfrom sklearn.metrics import mean_squared_error # For Calculating Mean Squared Error\r\nimport lightgbm as lgb # Light GBM Regressor\r\n\r\nimport warnings\r\nwarnings.filterwarnings('ignore')"]}, {"cell_type": "markdown", "id": "5f1269c3", "metadata": {}, "source": ["# Step 2: Load the data\n\nNext, we'll load the training and test data.  "]}, {"cell_type": "code", "execution_count": 1, "id": "fa5c4a65", "metadata": {}, "outputs": [], "source": ["# Load the training data\r\ntrain = pd.read_csv(\"input/train.csv\", index_col=0)\r\ntest = pd.read_csv(\"input/test.csv\", index_col=0)\r\n\r\n# Preview the data\r\ntrain.head()\r\ntest.head()"]}, {"cell_type": "markdown", "id": "19d1d025", "metadata": {}, "source": ["The next section removes target from train data and creates the target variable"]}, {"cell_type": "code", "execution_count": 1, "id": "aeaa61a3", "metadata": {}, "outputs": [], "source": ["y_train = train.target\r\nX_train = train.drop(['target'], axis=1)\r\nX_test = test.copy()\r\n\r\n# Preview features\r\nX_train.head()"]}, {"cell_type": "markdown", "id": "8ed98b1d", "metadata": {}, "source": ["The next section helps to identify the categorical columns and treat those specific columns by using Ordinal Encoder"]}, {"cell_type": "code", "execution_count": 1, "id": "1a548506", "metadata": {}, "outputs": [], "source": ["# Extract the Categorical Columns\r\ncat_cols = [feature for feature in train.columns if 'cat' in feature]\r\nprint(cat_cols)\r\n\r\n# Copy of original data to prevent overwwritting them\r\nlabel_X_train = X_train.copy()\r\nlabel_X_test = X_test.copy()\r\n\r\n# Apply ordinal encoder to each column with categorical data\r\nordinal_encoder = preprocessing.OrdinalEncoder()\r\nlabel_X_train[cat_cols] = ordinal_encoder.fit_transform(label_X_train[cat_cols])\r\nlabel_X_test[cat_cols] = ordinal_encoder.transform(label_X_test[cat_cols])"]}, {"cell_type": "code", "execution_count": 1, "id": "64fb48bc", "metadata": {}, "outputs": [], "source": ["label_X_train"]}, {"cell_type": "code", "execution_count": 1, "id": "9843c4b9", "metadata": {}, "outputs": [], "source": ["label_X_test"]}, {"cell_type": "markdown", "id": "1440bd66", "metadata": {}, "source": ["Use Optuna to perform hyper parameter tuning"]}, {"cell_type": "code", "execution_count": 1, "id": "22f59756", "metadata": {}, "outputs": [], "source": ["def objective(trial, X, y):\r\n    train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.2)\r\n    dtrain = lgb.Dataset(train_x, label=train_y, free_raw_data=False)\r\n    dvalid = lgb.Dataset(valid_x, label=valid_y, free_raw_data=False)\r\n\r\n    param = {\r\n        \"metric\": \"rmse\",\r\n        \"verbosity\": -1,\r\n        'max_depth':trial.suggest_int('max_depth', 5, 50),\r\n        'n_estimators':trial.suggest_int(\"n_estimators\", 1000, 50000, step=100),\r\n        'subsample': trial.suggest_uniform('subsample', 0.2, 1.0),\r\n        'colsample_bytree':trial.suggest_uniform('colsample_bytree', 0.2, 1.0),\r\n        'learning_rate':trial.suggest_uniform('learning_rate', 0.001, 0.01),\r\n        'reg_lambda':trial.suggest_uniform('reg_lambda', 0.01, 50),\r\n        'reg_alpha':trial.suggest_uniform('reg_alpha', 0.01, 50),\r\n        'min_child_samples':trial.suggest_int('min_child_samples', 5, 100),\r\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 10, 200),\r\n        'max_bin':trial.suggest_int('max_bin', 30, 1000),\r\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\r\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\r\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\r\n        'learning_rate':trial.suggest_uniform('learning_rate', 0.001, 0.01),\r\n        'cat_smooth':trial.suggest_int('cat_smooth', 5, 100),\r\n        'cat_l2':trial.suggest_loguniform('cat_l2', 1e-3, 100),\r\n        'num_threads': multiprocessing.cpu_count()-2,\r\n        #'device': 'gpu',\r\n        #'gpu_platform_id': 0,\r\n        #'gpu_device_id': 0\r\n    }\r\n\r\n    lightgbm_model = lgb.train(param, dtrain, valid_sets=(dtrain, dvalid), early_stopping_rounds=250)\r\n    \r\n    train_score = np.round(mean_squared_error(train_y, lightgbm_model.predict(train_x), squared=False), 5)\r\n    test_score = np.round(mean_squared_error(valid_y, lightgbm_model.predict(valid_x), squared=False), 5)\r\n    \r\n    print(f'TRAIN RMSE : {train_score} || TEST RMSE : {test_score}')\r\n    \r\n    return test_score"]}, {"cell_type": "code", "execution_count": 1, "id": "735d447f", "metadata": {}, "outputs": [], "source": ["print(multiprocessing.cpu_count())"]}, {"cell_type": "code", "execution_count": 1, "id": "4e0f3b19", "metadata": {}, "outputs": [], "source": ["optimize = partial(objective, X=label_X_train, y=y_train)\r\nstudy_lgbm = optuna.create_study(direction='minimize')\r\nstudy_lgbm.optimize(optimize, n_trials=100)\r\n\r\n\r\nprint(\"Number of finished trials: {}\".format(len(study_lgbm.trials)))\r\n\r\nprint(\"Best trial:\")\r\ntrial = study_lgbm.best_trial\r\n\r\nprint(\"  Value: {}\".format(trial.value))\r\n\r\nprint(\"  Params: \")\r\nfor key, value in trial.params.items():\r\n    print(\"    {}: {}\".format(key, value))"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}