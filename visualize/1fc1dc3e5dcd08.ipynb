{"cells": [{"cell_type": "markdown", "id": "e16ac2e1", "metadata": {}, "source": ["source used"]}, {"cell_type": "markdown", "id": "78fae4d8", "metadata": {}, "source": [" https://www.pyimagesearch.com/2019/07/08/keras-imagedatagenerator-and-data-augmentation/\n https://keras.io/api/preprocessing/image/\n https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/"]}, {"cell_type": "code", "execution_count": 1, "id": "26814789", "metadata": {}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]}, {"cell_type": "code", "execution_count": 1, "id": "6287af7b", "metadata": {}, "outputs": [], "source": ["import pandas as pd\nimport shutil\nfrom PIL import Image as PImage\nfrom matplotlib import pyplot as plt\nimport keras\nimport tensorflow as tf"]}, {"cell_type": "code", "execution_count": 1, "id": "e9cbd671", "metadata": {}, "outputs": [], "source": ["from keras.datasets import cifar10"]}, {"cell_type": "code", "execution_count": 1, "id": "62159ae2", "metadata": {}, "outputs": [], "source": ["data = tf.keras.datasets.cifar10"]}, {"cell_type": "code", "execution_count": 1, "id": "0a04ffbc", "metadata": {}, "outputs": [], "source": ["(x_train, y_train), (x_test, y_test) = data.load_data()"]}, {"cell_type": "code", "execution_count": 1, "id": "c02bcd62", "metadata": {}, "outputs": [], "source": ["size = x_train.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "d8c7b45f", "metadata": {}, "outputs": [], "source": ["size"]}, {"cell_type": "code", "execution_count": 1, "id": "66d3e19a", "metadata": {}, "outputs": [], "source": ["x_train[0]"]}, {"cell_type": "code", "execution_count": 1, "id": "85b31327", "metadata": {}, "outputs": [], "source": ["plt.imshow(x_train[1])\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "90e86424", "metadata": {}, "outputs": [], "source": ["x_train[0]"]}, {"cell_type": "markdown", "id": "5810cf5c", "metadata": {}, "source": ["using 1st image to check how images are generated using data augmentation"]}, {"cell_type": "code", "execution_count": 1, "id": "f05c8f59", "metadata": {}, "outputs": [], "source": ["samples  = x_train[0]"]}, {"cell_type": "code", "execution_count": 1, "id": "5e72c770", "metadata": {}, "outputs": [], "source": ["from keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator(width_shift_range=[-0.2,0.2],height_shift_range= [-0.2,0.2])"]}, {"cell_type": "code", "execution_count": 1, "id": "0e74a5cf", "metadata": {}, "outputs": [], "source": ["# prepare iterator\nit = datagen.flow(x_train[0:1,:] ,batch_size=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "dedccade", "metadata": {}, "outputs": [], "source": ["# generate samples and plot\nfor i in range(9):\n    # define subplot\n    plt.subplot(330 + 1 + i)\n    # generate batch of images\n    batch = it.next()\n    # convert to unsigned integers for viewing\n    image = batch[0].astype('uint8')\n    # plot raw pixel data\n    plt.imshow(image)"]}, {"cell_type": "markdown", "id": "42148b77", "metadata": {}, "source": ["ow using complete x_train and producing 1000 new images"]}, {"cell_type": "code", "execution_count": 1, "id": "9bdf82a9", "metadata": {}, "outputs": [], "source": ["data_lst =  []\nlabel_lst = []"]}, {"cell_type": "code", "execution_count": 1, "id": "96ed3772", "metadata": {}, "outputs": [], "source": ["# prepare iterator\nit = datagen.flow(x_train,y_train ,batch_size=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "cf85d39d", "metadata": {}, "outputs": [], "source": ["i = 0\nfor x_batch,y_batch in it:\n    #plt.subplot(330 + 1 + i)\n    image = x_batch[0].astype('uint8')\n    data_lst.append(image)\n    label_lst.append(y_batch[0])\n    #print(image)\n    #print(y_batch)\n    i= i+1\n    #plt.imshow(image)\n    if(i>999):\n        break"]}, {"cell_type": "code", "execution_count": 1, "id": "017eae03", "metadata": {}, "outputs": [], "source": ["label_lst"]}, {"cell_type": "code", "execution_count": 1, "id": "506c5faa", "metadata": {}, "outputs": [], "source": ["final = []\nfor i in range(1000):\n    final.extend(label_lst[i])"]}, {"cell_type": "code", "execution_count": 1, "id": "a443fd8f", "metadata": {}, "outputs": [], "source": ["final"]}, {"cell_type": "code", "execution_count": 1, "id": "3e9d8cc5", "metadata": {}, "outputs": [], "source": ["from collections import Counter\nfinal = Counter(final)"]}, {"cell_type": "code", "execution_count": 1, "id": "2525f12a", "metadata": {}, "outputs": [], "source": ["final"]}, {"cell_type": "code", "execution_count": 1, "id": "3636b102", "metadata": {}, "outputs": [], "source": ["data_augmented  = np.stack(data_lst,axis=0)"]}, {"cell_type": "code", "execution_count": 1, "id": "680712a6", "metadata": {}, "outputs": [], "source": ["data_augmented.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "ced09e4f", "metadata": {}, "outputs": [], "source": ["final_data  = np.vstack((x_train,data_augmented))"]}, {"cell_type": "code", "execution_count": 1, "id": "20d608d2", "metadata": {}, "outputs": [], "source": ["final_data.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "ebc6cf11", "metadata": {}, "outputs": [], "source": ["label_augmented = np.array(label_lst)"]}, {"cell_type": "code", "execution_count": 1, "id": "efc1257a", "metadata": {}, "outputs": [], "source": ["label_augmented.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "cce0f675", "metadata": {}, "outputs": [], "source": ["y_final = np.vstack((y_train,label_lst))"]}, {"cell_type": "code", "execution_count": 1, "id": "103000ce", "metadata": {}, "outputs": [], "source": ["y_final.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "cec4948d", "metadata": {}, "outputs": [], "source": ["x_train = final_data.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')"]}, {"cell_type": "code", "execution_count": 1, "id": "313fbe6c", "metadata": {}, "outputs": [], "source": ["num_classes = 10\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_final, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)"]}, {"cell_type": "markdown", "id": "89175c26", "metadata": {}, "source": ["using lenet"]}, {"cell_type": "code", "execution_count": 1, "id": "01b4e612", "metadata": {}, "outputs": [], "source": ["input_shape = (32,32,3)\nbatch_size =100\nepochs =10"]}, {"cell_type": "code", "execution_count": 1, "id": "a73e5def", "metadata": {}, "outputs": [], "source": ["from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D"]}, {"cell_type": "code", "execution_count": 1, "id": "b2b50960", "metadata": {}, "outputs": [], "source": ["model  = Sequential()\n\nmodel.add(Conv2D(6,(5,5),activation = 'relu',input_shape = input_shape))\nmodel.add((MaxPooling2D(pool_size = (2,2))))\n\nmodel.add(Conv2D(16,(5,5),activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size= (2,2)))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(120,activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(84,activation = 'relu'))\n\nmodel.add(Dense(10,activation = 'softmax'))"]}, {"cell_type": "code", "execution_count": 1, "id": "130639c5", "metadata": {}, "outputs": [], "source": ["model.summary()"]}, {"cell_type": "code", "execution_count": 1, "id": "5bee2e2c", "metadata": {}, "outputs": [], "source": ["model.compile(loss= 'categorical_crossentropy',optimizer = 'adam' , metrics = ['accuracy'])\nhistory=model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_test, y_test))"]}, {"cell_type": "code", "execution_count": 1, "id": "5d0ed344", "metadata": {}, "outputs": [], "source": ["%matplotlib notebook\n%matplotlib inline\n\nimport time\n# https://gist.github.com/greydanus/f6eee59eaf1d90fcb3b534a25362cea4\n# https://stackoverflow.com/a/14434334\n# this function is used to update the plots for each epoch and error\ndef plt_dynamic(x, vy, ty, ax, colors=['b']):\n    ax.plot(x, vy, 'b', label=\"Validation Loss\")\n    ax.plot(x, ty, 'r', label=\"Train Loss\")\n    #plt.legend()\n    plt.grid()\n    plt.show()\n    fig.canvas.draw()"]}, {"cell_type": "code", "execution_count": 1, "id": "eec5bf1a", "metadata": {}, "outputs": [], "source": ["fig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,epochs+1))\n\n# print(history.history.keys())\n# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n\n# we will get val_loss and val_acc only when you pass the paramter validation_data\n# val_loss : validation loss\n# val_acc : validation accuracy\n\n# loss : training loss\n# acc : train accuracy\n# for each key in histrory.histrory we will have a list of length equal to number of epochs\n\nvy = history.history['val_accuracy']\nty = history.history['accuracy']\nplt_dynamic(x, vy, ty, ax)"]}, {"cell_type": "markdown", "id": "1e92e60d", "metadata": {}, "source": ["# As we can see after using data augmentation there is less overfitting as the accuracy of train and test data are almost similar"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}