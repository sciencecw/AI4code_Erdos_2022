{"cells": [{"cell_type": "markdown", "id": "8f84d674", "metadata": {}, "source": ["### Preps & Introduction\n\nSo I've checked the public notebooks before I start and saw everbody is using **XGB**, **LGBM**, **CatBoost** and other shenanigans, decided to try CNN so here we go. Let's import the libraries."]}, {"cell_type": "code", "execution_count": 1, "id": "a88dd2b1", "metadata": {}, "outputs": [], "source": ["\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\n\nimport tensorflow as tf\nimport os\nimport numpy as np\nimport pandas as pd\n%matplotlib inline\n"]}, {"cell_type": "code", "execution_count": 1, "id": "5586dcea", "metadata": {}, "outputs": [], "source": ["os.listdir('../input/tabular-playground-series-sep-2021/')\n"]}, {"cell_type": "markdown", "id": "7b7cea4b", "metadata": {}, "source": ["Loading the dataset"]}, {"cell_type": "code", "execution_count": 1, "id": "5ee181fe", "metadata": {}, "outputs": [], "source": ["train_data = pd.read_csv(\n    '../input/tabular-playground-series-sep-2021/train.csv')\ntest_data = pd.read_csv('../input/tabular-playground-series-sep-2021/test.csv')\nsubmission_data = pd.read_csv(\n    '../input/tabular-playground-series-sep-2021/sample_solution.csv')\n"]}, {"cell_type": "markdown", "id": "449e437c", "metadata": {}, "source": ["### Pre-processing\n\nStandard stuff."]}, {"cell_type": "code", "execution_count": 1, "id": "c8624702", "metadata": {}, "outputs": [], "source": ["y = train_data['claim']\ntmp = train_data.copy()\ntmp['n_missing'] = train_data.isnull().sum(axis=1)\nX = tmp.drop(['claim', 'id'], axis=1)\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=0)"]}, {"cell_type": "code", "execution_count": 1, "id": "45b25b37", "metadata": {}, "outputs": [], "source": ["imputer = SimpleImputer(strategy='mean')\nimp_X_train = pd.DataFrame(imputer.fit_transform(X_train))\nimp_X_val = pd.DataFrame(imputer.transform(X_val))\nimp_X_train.columns = X_train.columns\nimp_X_val.columns = X_val.columns"]}, {"cell_type": "code", "execution_count": 1, "id": "1ed53166", "metadata": {}, "outputs": [], "source": ["test_tmp = test_data.copy().drop('id', axis=1)\ntest_tmp['n_missing'] = test_data.isnull().sum(axis=1)\n\nimp_X_test = pd.DataFrame(imputer.fit_transform(test_tmp))\nimp_X_test.columns = test_tmp.columns"]}, {"cell_type": "code", "execution_count": 1, "id": "a935e277", "metadata": {}, "outputs": [], "source": ["imp_X_train"]}, {"cell_type": "code", "execution_count": 1, "id": "61bb68c2", "metadata": {}, "outputs": [], "source": ["y_train"]}, {"cell_type": "code", "execution_count": 1, "id": "e4dbf253", "metadata": {}, "outputs": [], "source": ["imp_X_test"]}, {"cell_type": "code", "execution_count": 1, "id": "b8d0841e", "metadata": {}, "outputs": [], "source": ["print(imp_X_train.shape)\nprint(y_train.shape)\nprint(imp_X_test.shape)\n"]}, {"cell_type": "code", "execution_count": 1, "id": "4758fe26", "metadata": {}, "outputs": [], "source": ["X_train_expanded = tf.expand_dims(imp_X_train, axis=-1)\nX_val_expanded = tf.expand_dims(X_val, axis=-1)"]}, {"cell_type": "markdown", "id": "31025469", "metadata": {}, "source": ["### Creating CNN model.\n\nThere's likely a better approach. \ud83e\udd37\u200d\u2642\ufe0f"]}, {"cell_type": "code", "execution_count": 1, "id": "468c5f2d", "metadata": {}, "outputs": [], "source": ["model = tf.keras.models.Sequential()\n\nmodel.add(tf.keras.layers.Input(shape=(imp_X_train.shape[1], 1,)))\n\nmodel.add(tf.keras.layers.Conv1D(128, kernel_size=1,\n          activation='relu', padding='same'))\nmodel.add(tf.keras.layers.Conv1D(\n    128, kernel_size=1, activation='relu', padding='same'))\nmodel.add(tf.keras.layers.MaxPooling1D(pool_size=1, strides=1))\nmodel.add(tf.keras.layers.Dropout(0.25))\nmodel.add(tf.keras.layers.BatchNormalization())\n\nmodel.add(tf.keras.layers.Conv1D(256, kernel_size=1,\n          activation='relu', padding='same'))\nmodel.add(tf.keras.layers.Conv1D(\n    128, kernel_size=1, activation='relu', padding='same'))\nmodel.add(tf.keras.layers.MaxPooling1D(pool_size=1, strides=1))\nmodel.add(tf.keras.layers.Dropout(0.25))\nmodel.add(tf.keras.layers.BatchNormalization())\n\nmodel.add(tf.keras.layers.Conv1D(512, kernel_size=1,\n          activation='relu', padding='same'))\nmodel.add(tf.keras.layers.Conv1D(\n    128, kernel_size=1, activation='relu', padding='same'))\nmodel.add(tf.keras.layers.MaxPooling1D(pool_size=1, strides=1))\nmodel.add(tf.keras.layers.Dropout(0.5))\nmodel.add(tf.keras.layers.BatchNormalization())\n\nmodel.add(tf.keras.layers.Flatten())\n\nmodel.add(tf.keras.layers.Dense(512, activation='relu', activity_regularizer=tf.keras.regularizers.l2(\n    0.00001), bias_regularizer=tf.keras.regularizers.l2(0.0001)))\nmodel.add(tf.keras.layers.Dropout(0.5))\nmodel.add(tf.keras.layers.Dense(512, activation='relu', activity_regularizer=tf.keras.regularizers.l2(\n    0.00001), bias_regularizer=tf.keras.regularizers.l2(0.0001)))\nmodel.add(tf.keras.layers.Dropout(0.5))\nmodel.add(tf.keras.layers.Dense(512, activation='relu', activity_regularizer=tf.keras.regularizers.l2(\n    0.00001), bias_regularizer=tf.keras.regularizers.l2(0.0001)))\nmodel.add(tf.keras.layers.Dropout(0.5))\n\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n\nmodel.summary()"]}, {"cell_type": "markdown", "id": "0bb1ba87", "metadata": {}, "source": ["Examine our model."]}, {"cell_type": "code", "execution_count": 1, "id": "b0fc49ee", "metadata": {}, "outputs": [], "source": ["tf.keras.utils.plot_model(model, show_shapes=True)\n"]}, {"cell_type": "markdown", "id": "f6e7027c", "metadata": {}, "source": ["Let's build our model and train it! I am using a high epoch but should stop if things go down."]}, {"cell_type": "code", "execution_count": 1, "id": "3ceaf7e7", "metadata": {}, "outputs": [], "source": ["auc = tf.keras.metrics.AUC(name='aucroc')\noptimizer = tf.keras.optimizers.RMSprop(learning_rate=0.00001, rho=0.9, epsilon=1e-08, decay=0.0)\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy', auc])\n"]}, {"cell_type": "code", "execution_count": 1, "id": "5f40f7fd", "metadata": {}, "outputs": [], "source": ["earlystopping = tf.keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\", min_delta=0, patience=5, verbose=1, restore_best_weights=True\n)\n\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\", factor=0.3, patience=3, verbose=1, min_delta=1e-4\n)\n\ncallbacks = [earlystopping, reduce_lr]\n\nhistory = model.fit(x=X_train_expanded, y=y_train, batch_size=512, shuffle=True,\n                    epochs=30, validation_data=(X_val_expanded, y_val), callbacks=callbacks)\n"]}, {"cell_type": "code", "execution_count": 1, "id": "27d6e613", "metadata": {}, "outputs": [], "source": ["def vis_training(history_list, start=1):\n    loss = np.concatenate([h.history[\"loss\"] for h in history_list])\n    val_loss = np.concatenate([h.history[\"val_loss\"] for h in history_list])\n    acc = np.concatenate([h.history[\"accuracy\"] for h in history_list])\n    val_acc = np.concatenate([h.history[\"val_accuracy\"] for h in history_list])\n\n    epoch_range = range(1, len(loss) + 1)\n\n    plt.figure(figsize=[12, 6])\n    plt.subplot(1, 2, 1)\n    plt.plot(epoch_range[start - 1:], loss[start - 1:], label=\"Training Loss\")\n    plt.plot(epoch_range[start - 1:],\n             val_loss[start - 1:], label=\"Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(epoch_range[start - 1:], acc[start - 1:],\n             label=\"Training Accuracy\")\n    plt.plot(\n        epoch_range[start - 1:], val_acc[start -\n                                         1:], label=\"Validation Accuracy\"\n    )\n    plt.xlabel(\"Epoch\")\n    plt.legend()\n\n    plt.show()\n"]}, {"cell_type": "markdown", "id": "3739dcc4", "metadata": {}, "source": ["I like plots. \ud83e\uddd1\u200d\ud83d\udcbb"]}, {"cell_type": "code", "execution_count": 1, "id": "b3e7e98b", "metadata": {}, "outputs": [], "source": ["vis_training([history])\n"]}, {"cell_type": "markdown", "id": "64104469", "metadata": {}, "source": ["Creating the submission thingy."]}, {"cell_type": "code", "execution_count": 1, "id": "5baacb37", "metadata": {}, "outputs": [], "source": ["X_test_extended = tf.expand_dims(imp_X_test, axis=-1);"]}, {"cell_type": "code", "execution_count": 1, "id": "634d81ce", "metadata": {}, "outputs": [], "source": ["sub = pd.DataFrame()\nsub['id'] = test_data['id']\nsub['claim'] = model.predict(X_test_extended)\nsub = sub.set_index('id')"]}, {"cell_type": "code", "execution_count": 1, "id": "7435223f", "metadata": {}, "outputs": [], "source": ["sub.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "f39749f4", "metadata": {}, "outputs": [], "source": ["sub.shape\n"]}, {"cell_type": "code", "execution_count": 1, "id": "8a2fc70c", "metadata": {}, "outputs": [], "source": ["sub.to_csv('submission.csv')\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}