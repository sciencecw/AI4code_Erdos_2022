{"cells": [{"cell_type": "code", "execution_count": 1, "id": "c179cb06", "metadata": {}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]}, {"cell_type": "code", "execution_count": 1, "id": "c1a857ac", "metadata": {}, "outputs": [], "source": ["# Load Data\ndf = pd.read_csv('../input/nlp-tweet-sentiment-analysis/bitcointweets.csv', header=None)\ndf = df[[1,7]]\ndf.columns = ['tweet','label']\ndf.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "9a653e4b", "metadata": {}, "outputs": [], "source": ["# inspect sentiment\nsns.countplot(df['label'])"]}, {"cell_type": "markdown", "id": "58eaf7e2", "metadata": {}, "source": ["Majority of tweets are neutral and positive. Looks like there are not much negative tweets on Bitcoin! No wonder the price is skyrocketing!"]}, {"cell_type": "code", "execution_count": 1, "id": "bc0002bf", "metadata": {}, "outputs": [], "source": ["# text length\ndf['text_length'] = df['tweet'].apply(len)\ndf[['label','text_length','tweet']].head()"]}, {"cell_type": "code", "execution_count": 1, "id": "32ba4a81", "metadata": {}, "outputs": [], "source": ["g = sns.FacetGrid(df,col='label')\ng.map(plt.hist,'text_length')"]}, {"cell_type": "markdown", "id": "8c8f12bf", "metadata": {}, "source": ["As expected, most tweets are very short in length."]}, {"cell_type": "markdown", "id": "cd8e2772", "metadata": {}, "source": ["We are going to clean up the tweets, remove special chars, stop words, URL links, etc.."]}, {"cell_type": "code", "execution_count": 1, "id": "13468d2c", "metadata": {}, "outputs": [], "source": ["\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nimport re\n\ndef clean_text(s):\n    s = re.sub(r'http\\S+', '', s)\n    s = re.sub('(RT|via)((?:\\\\b\\\\W*@\\\\w+)+)', ' ', s)\n    s = re.sub(r'@\\S+', '', s)\n    s = re.sub('&amp', ' ', s)\n    return s\ndf['clean_tweet'] = df['tweet'].apply(clean_text)\n\ntext = df['clean_tweet'].to_string().lower()    \nwordcloud = WordCloud(\n    collocations=False,\n    relative_scaling=0.5,\n    stopwords=set(stopwords.words('english'))).generate(text)\n\nplt.figure(figsize=(12,12))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "ddb4c675", "metadata": {}, "outputs": [], "source": ["# Encode Categorical Variable\nX = df['clean_tweet']\ny = pd.get_dummies(df['label']).values\nnum_classes = df['label'].nunique()"]}, {"cell_type": "code", "execution_count": 1, "id": "624dc467", "metadata": {}, "outputs": [], "source": ["seed = 101 # fix random seed for reproducibility\nnp.random.seed(seed)"]}, {"cell_type": "code", "execution_count": 1, "id": "8732ec6f", "metadata": {}, "outputs": [], "source": ["# Split Train Test sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.2,\n                                                    stratify=y,\n                                                    random_state=seed)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "b2b1788d", "metadata": {}, "outputs": [], "source": ["# Tokenize Text\nfrom keras.preprocessing.text import Tokenizer\nmax_features = 20000\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(X_train))\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "ad8b01ab", "metadata": {}, "outputs": [], "source": ["totalNumWords = [len(one_comment) for one_comment in X_train]\nplt.hist(totalNumWords,bins = 30)\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "d349eb64", "metadata": {}, "outputs": [], "source": ["from keras.preprocessing import sequence\nmax_words = 30\nX_train = sequence.pad_sequences(X_train, maxlen=max_words)\nX_test = sequence.pad_sequences(X_test, maxlen=max_words)\nprint(X_train.shape,X_test.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "6c8e8fcb", "metadata": {}, "outputs": [], "source": ["import keras.backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,Conv1D,MaxPooling1D,LSTM\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n\nbatch_size = 128\nepochs = 3"]}, {"cell_type": "markdown", "id": "08c7b58c", "metadata": {}, "source": ["This deep learning model will have 2 CNN layers, 1 LSTM layer, and final dense layer for classification"]}, {"cell_type": "code", "execution_count": 1, "id": "17c8572d", "metadata": {}, "outputs": [], "source": ["def get_model(max_features, embed_dim):\n    np.random.seed(seed)\n    K.clear_session()\n    model = Sequential()\n    model.add(Embedding(max_features, embed_dim, input_length=X_train.shape[1]))\n    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n    model.add(MaxPooling1D(pool_size=2))\n    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n    model.add(MaxPooling1D(pool_size=2))    \n    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    print(model.summary())\n    return model"]}, {"cell_type": "code", "execution_count": 1, "id": "9fd9ebd5", "metadata": {}, "outputs": [], "source": ["def model_train(model):\n    # train the model\n    model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), \n                          epochs=epochs, batch_size=batch_size, verbose=2)"]}, {"cell_type": "code", "execution_count": 1, "id": "4b05ef23", "metadata": {}, "outputs": [], "source": ["def model_evaluate(): \n    # predict class with test set\n    y_pred_test =  np.argmax(model.predict(X_test), axis=1)\n    print('Accuracy:\\t{:0.1f}%'.format(accuracy_score(np.argmax(y_test,axis=1),y_pred_test)*100))\n    \n    #classification report\n    print('\\n')\n    print(classification_report(np.argmax(y_test,axis=1), y_pred_test))\n\n    #confusion matrix\n    confmat = confusion_matrix(np.argmax(y_test,axis=1), y_pred_test)\n\n    fig, ax = plt.subplots(figsize=(4, 4))\n    ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n    for i in range(confmat.shape[0]):\n        for j in range(confmat.shape[1]):\n            ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n    plt.xlabel('Predicted label')\n    plt.ylabel('True label')\n    plt.tight_layout()"]}, {"cell_type": "code", "execution_count": 1, "id": "5d057718", "metadata": {}, "outputs": [], "source": ["# train the model\nmax_features = 20000\nembed_dim = 100\nmodel = get_model(max_features, embed_dim)\nmodel_train(model)"]}, {"cell_type": "code", "execution_count": 1, "id": "39903f7e", "metadata": {}, "outputs": [], "source": ["# evaluate model with test set\nmodel_evaluate()"]}, {"cell_type": "markdown", "id": "870b3277", "metadata": {}, "source": ["Looks like we achieved very respectable accuracy (>97%), classifying most of the tweets in the test set correctly!"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}