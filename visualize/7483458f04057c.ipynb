{"cells": [{"cell_type": "code", "execution_count": 1, "id": "5fc9b294", "metadata": {}, "outputs": [], "source": ["!pip install torchsummary"]}, {"cell_type": "code", "execution_count": 1, "id": "8d6558b8", "metadata": {}, "outputs": [], "source": ["import os\nimport pickle\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.utils.data as data_utils\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torch import nn\n\nfrom torchsummary import summary"]}, {"cell_type": "code", "execution_count": 1, "id": "2726db6e", "metadata": {}, "outputs": [], "source": ["# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\nfor dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))"]}, {"cell_type": "code", "execution_count": 1, "id": "7b6421ae", "metadata": {}, "outputs": [], "source": ["!tar -xvf /kaggle/working/cifar-10-python.tar.gz"]}, {"cell_type": "code", "execution_count": 1, "id": "03d357ab", "metadata": {}, "outputs": [], "source": ["!rm -rf /kaggle/working/cifar-10-python.tar.gz\n!ls /kaggle/working/cifar-10-batches-py"]}, {"cell_type": "code", "execution_count": 1, "id": "bfa6b7fd", "metadata": {}, "outputs": [], "source": ["DATA_DIR = '/kaggle/working/cifar-10-batches-py'"]}, {"cell_type": "markdown", "id": "b7792d8f", "metadata": {}, "source": ["## Let's explore the data format\n\nThe complete details of format of the data is given [here](https://www.cs.toronto.edu/~kriz/cifar.html). "]}, {"cell_type": "code", "execution_count": 1, "id": "d5db534b", "metadata": {}, "outputs": [], "source": ["def unpickle(file):\n    with open(file, 'rb') as fo:\n        dict = pickle.load(fo, encoding='bytes')\n    return dict"]}, {"cell_type": "code", "execution_count": 1, "id": "41526675", "metadata": {}, "outputs": [], "source": ["metadata = unpickle(os.path.join(DATA_DIR, 'batches.meta'))[b'label_names']\nmetadata = [m.decode('utf-8') for m in metadata]\nmetadata"]}, {"cell_type": "code", "execution_count": 1, "id": "4bd0f956", "metadata": {}, "outputs": [], "source": ["data_batch_1 = unpickle(os.path.join(DATA_DIR, 'data_batch_1'))\nlist(data_batch_1.keys())"]}, {"cell_type": "code", "execution_count": 1, "id": "0352434d", "metadata": {}, "outputs": [], "source": ["print(f\"Batch label: {data_batch_1[b'batch_label']}\")\nprint(f\"Shape of Labels: {len(data_batch_1[b'labels'])}\")\nprint(f\"Actual image data shape: {data_batch_1[b'data'].shape}\")\nprint(f\"Filenames: {len(data_batch_1[b'filenames'])}\")"]}, {"cell_type": "markdown", "id": "22cc5fb1", "metadata": {}, "source": ["The return dict contains following keys:\n\n* **batch_label:** The label of batch\n* **labels:** Labels of given images in `data` key for training\n* **data:** Flattened colored images for training\n* **filename:** Names of file from the image is read (Useless in our case)"]}, {"cell_type": "code", "execution_count": 1, "id": "ad02268d", "metadata": {}, "outputs": [], "source": ["def load_data(data_type='TRAIN'):\n    X, Y = [], []\n    if data_type == 'TRAIN':\n        for i in range(5):\n            batch = unpickle(os.path.join(DATA_DIR, f'data_batch_{i+1}'))\n            X.append(batch[b'data'])\n            Y.append(batch[b'labels'])\n    else:\n        test_batch = unpickle(os.path.join(DATA_DIR, f'test_batch'))\n        X.append(test_batch[b'data'])\n        Y.append(test_batch[b'labels'])\n\n    return torch.from_numpy(np.concatenate(np.array(X), axis=0)), torch.from_numpy(np.concatenate(np.array(Y), axis=0))"]}, {"cell_type": "code", "execution_count": 1, "id": "f8352acf", "metadata": {}, "outputs": [], "source": ["X_train, Y_train = load_data()\nX_test, Y_test = load_data('TEST')"]}, {"cell_type": "markdown", "id": "e8a8349c", "metadata": {}, "source": ["## Exploratory Data Analysis"]}, {"cell_type": "code", "execution_count": 1, "id": "67efd428", "metadata": {}, "outputs": [], "source": ["print(f'Shape of X_train: {X_train.shape}')\nprint(f'Shape of Y_train: {Y_train.shape}')\nprint(f'Shape of X_test: {X_test.shape}')\nprint(f'Shape of Y_test: {Y_test.shape}')"]}, {"cell_type": "markdown", "id": "00f51304", "metadata": {}, "source": ["From the above results, we've 50k training images and 10k testing images. Training images will be further splitted into training and validation images."]}, {"cell_type": "code", "execution_count": 1, "id": "6f63d47e", "metadata": {}, "outputs": [], "source": ["X_train, X_val, Y_train, Y_val = train_test_split(X_train.cpu().detach().numpy(), Y_train.cpu().detach().numpy(), test_size=0.1, random_state=666)"]}, {"cell_type": "code", "execution_count": 1, "id": "6001d94e", "metadata": {}, "outputs": [], "source": ["# Convert to PyTorch tensor\nX_train = torch.from_numpy(X_train)\nX_val = torch.from_numpy(X_val)\nY_train = torch.from_numpy(Y_train)\nY_val = torch.from_numpy(Y_val)"]}, {"cell_type": "code", "execution_count": 1, "id": "9c9cd3bb", "metadata": {}, "outputs": [], "source": ["print(f'Shape of X_train: {X_train.shape}')\nprint(f'Shape of Y_train: {Y_train.shape}')\nprint(f'Shape of X_test: {X_val.shape}')\nprint(f'Shape of Y_test: {Y_val.shape}')"]}, {"cell_type": "code", "execution_count": 1, "id": "38aa4c56", "metadata": {}, "outputs": [], "source": ["IMG_SIZE = 32\nCHANNELS = 3\nW, H = IMG_SIZE, IMG_SIZE"]}, {"cell_type": "code", "execution_count": 1, "id": "1cff4ffe", "metadata": {}, "outputs": [], "source": ["# Visualize few samples of training dataset\nfig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16, 8))\ncount=0\nfor row in ax:\n    for col in row:\n        col.imshow(torch.stack([X_train[count, :][:1024].reshape(IMG_SIZE, IMG_SIZE), X_train[count, :][1024: 2048].reshape(IMG_SIZE, IMG_SIZE), X_train[count, :][2048:].reshape(IMG_SIZE, IMG_SIZE)], axis=2))\n        col.set_title(metadata[Y_train[count]])\n        count += 1\nplt.show()"]}, {"cell_type": "markdown", "id": "8d8d2fd3", "metadata": {}, "source": ["### Distribution of class"]}, {"cell_type": "code", "execution_count": 1, "id": "d153b092", "metadata": {}, "outputs": [], "source": ["sns.set(rc={'figure.figsize':(13,8)})\nax = sns.distplot(Y_train, kde=False)\nax.set(xlabel='Labels', ylabel='# of records', title='Distribution of targets')\nplt.show()"]}, {"cell_type": "markdown", "id": "3106e68e", "metadata": {}, "source": ["There are 5000 records for all 10 classes of images."]}, {"cell_type": "markdown", "id": "dab919fb", "metadata": {}, "source": ["## Creating custom PyTorch data generator and Data Loader\n\nReferences: \n\n* https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n* https://github.com/utkuozbulak/pytorch-custom-dataset-examples\n* https://stackoverflow.com/questions/41924453/pytorch-how-to-use-dataloaders-for-custom-datasets\n"]}, {"cell_type": "code", "execution_count": 1, "id": "aac77ba7", "metadata": {}, "outputs": [], "source": ["class CFAR10Dataset(Dataset):\n    \"\"\"\n    Custom CIFAR-10 dataset\n    \"\"\"\n    def __init__(self, X, Y, transforms=None):\n        self.X = X\n        self.Y = Y\n        self.transforms = transforms\n    \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, index):\n        image = torch.stack([self.X[index, :][:1024].reshape(IMG_SIZE, IMG_SIZE), self.X[index, :][1024: 2048].reshape(IMG_SIZE, IMG_SIZE), self.X[index, :][2048:].reshape(IMG_SIZE, IMG_SIZE)], axis=2).permute(2, 1, 0).float()/255\n        if self.transforms:\n            image = self.transforms(image)\n        return image, self.Y[index]"]}, {"cell_type": "code", "execution_count": 1, "id": "95cf0bf0", "metadata": {}, "outputs": [], "source": ["batch_size=3000"]}, {"cell_type": "markdown", "id": "a24a6127", "metadata": {}, "source": ["### Define data augmentations using pytorch transforms"]}, {"cell_type": "code", "execution_count": 1, "id": "2b76d11f", "metadata": {}, "outputs": [], "source": ["transformations = transforms.Compose([transforms.CenterCrop(100),\n                                      transforms.ToTensor()])"]}, {"cell_type": "code", "execution_count": 1, "id": "9ea06f2a", "metadata": {}, "outputs": [], "source": ["train_dataset = CFAR10Dataset(X_train, Y_train)\ntrain_loader = data_utils.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"]}, {"cell_type": "markdown", "id": "e97f42ae", "metadata": {}, "source": ["## Basic CNN Model using PyTorch\n\nLet's create a CNN model. Architecture reference: https://www.kaggle.com/kaushal2896/bengali-graphemes-starter-eda-multi-output-cnn"]}, {"cell_type": "code", "execution_count": 1, "id": "86fdf857", "metadata": {}, "outputs": [], "source": ["class Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(3, 3), padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 3), padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 3), padding=1),\n            nn.ReLU(True),                           \n            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 3), padding=1),\n            nn.ReLU(True),\n            nn.BatchNorm2d(num_features=32),\n            nn.MaxPool2d(kernel_size=(2, 2)),\n            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(5, 5), padding=2),\n            nn.ReLU(True),\n            nn.Dropout2d(p=0.3)\n        )\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), padding=1),\n            nn.BatchNorm2d(num_features=64),\n            nn.MaxPool2d(kernel_size=(2, 2)),\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(5, 5), padding=2),\n            nn.ReLU(True),\n            nn.Dropout2d(p=0.3)\n        )\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), padding=1),\n            nn.ReLU(True),\n            nn.BatchNorm2d(num_features=128),\n            nn.MaxPool2d(kernel_size=(2, 2)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(5, 5), padding=2),\n            nn.ReLU(True),\n            nn.Dropout2d(p=0.3)\n        )\n        self.layer4 = nn.Sequential(\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3, 3), padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3, 3), padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3, 3), padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3, 3), padding=1),\n            nn.ReLU(True),\n            nn.BatchNorm2d(num_features=256),\n            nn.MaxPool2d(kernel_size=(2, 2)),\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(5, 5), padding=2),\n            nn.ReLU(True),\n            nn.Dropout2d(p=0.3)\n        )\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(in_features=1024, out_features=1024)\n        self.fc1_dropout = nn.Dropout2d(p=0.3)\n        self.fc2 = nn.Linear(in_features=1024, out_features=512)\n        self.fc3 = nn.Linear(in_features=512, out_features=10)\n        \n        \n    def forward(self, X):\n        output = self.layer1(X)\n        output = self.layer2(output)\n        output = self.layer3(output)\n        output = self.layer4(output)\n        output = self.flatten(output)\n        output = self.fc1(output)\n        output = self.fc1_dropout(output)\n        output = self.fc2(output)\n        output = self.fc3(output)\n        return output"]}, {"cell_type": "code", "execution_count": 1, "id": "a607cd92", "metadata": {}, "outputs": [], "source": ["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]}, {"cell_type": "code", "execution_count": 1, "id": "447e390c", "metadata": {}, "outputs": [], "source": ["model = Model().to(device)\n\n# Print summary of our model\nsummary(model, input_size=(CHANNELS, IMG_SIZE, IMG_SIZE))"]}, {"cell_type": "code", "execution_count": 1, "id": "c8f8bbc7", "metadata": {}, "outputs": [], "source": ["LEARNING_RATE = 0.1\nEPOCHS = 100\nCLASSES = 10\nCUTMIX_ALPHA = 1"]}, {"cell_type": "code", "execution_count": 1, "id": "7902ca62", "metadata": {}, "outputs": [], "source": ["model = nn.DataParallel(model)\noptimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\ncriterion = nn.CrossEntropyLoss()"]}, {"cell_type": "markdown", "id": "419de637", "metadata": {}, "source": ["https://discuss.pytorch.org/t/multi-class-classifier/29901/2"]}, {"cell_type": "code", "execution_count": 1, "id": "e518248b", "metadata": {}, "outputs": [], "source": ["X_val.shape\nX_val_list = []\nfor x in X_val:\n    X_val_list.append((torch.stack([x[:1024].reshape(IMG_SIZE, IMG_SIZE), x[1024: 2048].reshape(IMG_SIZE, IMG_SIZE), x[2048:].reshape(IMG_SIZE, IMG_SIZE)], axis=2).permute(2, 1, 0).float()/255))\nX_val = torch.stack(X_val_list)"]}, {"cell_type": "code", "execution_count": 1, "id": "cb024cec", "metadata": {}, "outputs": [], "source": ["def get_accuracy(preds, actual):\n    assert len(preds) == len(actual)\n    \n    total = len(actual)\n    _, predicted = torch.max(preds.data, axis=1)\n    correct = (predicted == actual).sum().item()\n    return correct / total"]}, {"cell_type": "code", "execution_count": 1, "id": "8f555cbc", "metadata": {}, "outputs": [], "source": ["def shuffle_minibatch(x, y):\n    assert x.size(0)== y.size(0)\n    indices = torch.randperm(x.size(0))\n    return x[indices], y[indices]"]}, {"cell_type": "code", "execution_count": 1, "id": "e50bcd68", "metadata": {}, "outputs": [], "source": ["total_steps = len(train_loader)\nloss_list, train_acc_list, val_acc_list = [], [], []\nplot_flag = False\nfor epoch in range(EPOCHS):\n    for i, (x_train, y_train) in enumerate(train_loader):\n        x_train = x_train.to(device)\n        y_train = y_train.to(device)\n        \n        cutmix_decision = np.random.rand()\n        if cutmix_decision > 0.50:\n            # Cutmix: https://arxiv.org/pdf/1905.04899.pdf\n            x_train_shuffled, y_train_shuffled = shuffle_minibatch(x_train, y_train)\n            lam = np.random.beta(CUTMIX_ALPHA, CUTMIX_ALPHA)\n            cut_rat = np.sqrt(1. - lam)\n            cut_w = np.int(W * cut_rat)\n            cut_h = np.int(H * cut_rat)\n\n            # uniform\n            cx = np.random.randint(W)\n            cy = np.random.randint(H)\n\n            bbx1 = np.clip(cx - cut_w // 2, 0, W)\n            bby1 = np.clip(cy - cut_h // 2, 0, H)\n            bbx2 = np.clip(cx + cut_w // 2, 0, W)\n            bby2 = np.clip(cy + cut_h // 2, 0, H)\n\n            x_train[:, :, bbx1:bbx2, bby1:bby2] = x_train_shuffled[:, :, bbx1:bbx2, bby1:bby2]\n            lam = 1 - (bbx2 - bbx1) * (bby2 - bby1) / (W * H)\n        \n        # Forward pass\n        y_preds = model(x_train)\n        \n        # Calculate loss\n        if cutmix_decision > 0.50:\n            loss = criterion(y_preds, y_train) * lam + criterion(y_preds, y_train_shuffled) * (1. - lam)\n        else:\n            loss = criterion(y_preds, y_train)\n        if i+1 == total_steps:\n            loss_list.append(loss)\n        \n        # Backpropagate\n        optimizer.zero_grad()  # Reason: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n        loss.backward()\n        optimizer.step()\n        \n        # Calculate the accuracy\n        train_acc = get_accuracy(y_preds, y_train)\n        if i+1 == total_steps:\n            train_acc_list.append(train_acc)\n            \n        # Calculate validation accuracy\n        X_val = X_val.to(device)\n        Y_val = Y_val.to(device)\n        \n        # Predict on validation set\n        val_preds = model(X_val)\n        \n        val_acc = get_accuracy(val_preds, Y_val)\n        if i+1 == total_steps:\n            val_acc_list.append(val_acc)\n\n        if (i + 1) % 10 == 0:\n            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Train accuracy: {:.2f}%, Validation accuracy: {:.2f}%'\n                  .format(epoch + 1, EPOCHS, i + 1, total_steps, loss.item(),\n                          train_acc * 100, val_acc * 100))\n    print()"]}, {"cell_type": "code", "execution_count": 1, "id": "ebc2946a", "metadata": {}, "outputs": [], "source": ["plt.style.use('ggplot')\nplt.figure()\nplt.plot(np.arange(0, EPOCHS), loss_list, label='train_loss')\n\nplt.title('Loss')\nplt.xlabel('# of epochs')\nplt.ylabel('Loss')\nplt.legend(loc='upper right')\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "210980e5", "metadata": {}, "outputs": [], "source": ["plt.style.use('ggplot')\nplt.figure()\nplt.plot(np.arange(0, EPOCHS), train_acc_list, label='train_accuracy')\nplt.plot(np.arange(0, EPOCHS), val_acc_list, label='val_accuracy')\n\nplt.title('Accuracy')\nplt.xlabel('# of epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='upper right')\nplt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}