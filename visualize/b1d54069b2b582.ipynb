{"cells": [{"cell_type": "markdown", "id": "0108273f", "metadata": {}, "source": ["<body style=\"color:white\">\n    <h1 style='color:cyan' align='center'><font size=\"+3\">                   Welcome to the GYM:</font></h1>\n\n<h3 align='center'><font>Take a Monthly Membership <a href=https://www.kaggle.com/competitions\n                                                                              class=https://www.kaggle.com/competitions>HERE</a> and grind hard .</font></h3>\n   \n</body>\n"]}, {"cell_type": "markdown", "id": "1b0c3ce4", "metadata": {}, "source": ["<body style=\"color:white\">\n    <h1 style='color:red' align='center'><font size=\"+3\">AND THE RESULT IS BELOW !!!</font></h1>\n   \n</body>\n"]}, {"cell_type": "code", "execution_count": 1, "id": "559514e1", "metadata": {}, "outputs": [], "source": ["from IPython.display import Image\nImage(filename=\"../input/images/kagglememe.png\", width=1200, height=300)\n"]}, {"cell_type": "markdown", "id": "f465acac", "metadata": {}, "source": ["<body style=\"color:white\">\n    <h1 style='color:#F12626' align='center'><font size=\"+3\">Introduction</font></h1>\n\n<h3 align='center'><font>In this notebook, there are <font color=#000ECA>Two</font> parts : Understanding the data , Using Regressor models . \n    We have been provided with Tabular data , with <font color=#CA00BA>14</font> different independent dataset. In this notebook , 6 different regression techniques have been used , namely , <font color=#85C1E9>Linear Regression</font> , <font color=#27AE60>Decision Tree Regressor</font> ,<font color=#800080>eXtreme Gradient Boosting Regressor</font> , <font color=#EC7063>Light Gradient Boosting Regressor</font> , <font color=#2E86C1 >Category Boosting Regressor</font> and <font color=#FA520E>Regression Artificial Neural Network</font>. \n    I hope you will like this notebook and if you have , then please <font color=#2E86C1 >UPVOTE</font> and support the effort !!! I am a beginner , so any suggestion or constructive criticism is appreciated. Please do comment !!</font></h3>\n   \n</body>"]}, {"cell_type": "code", "execution_count": 1, "id": "b92a331a", "metadata": {}, "outputs": [], "source": ["import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score, RandomizedSearchCV\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import train_test_split\n        \ninput_path = Path('/kaggle/input/tabular-playground-series-jan-2021/')"]}, {"cell_type": "code", "execution_count": 1, "id": "d9d57987", "metadata": {}, "outputs": [], "source": ["import seaborn as sns"]}, {"cell_type": "code", "execution_count": 1, "id": "74880e07", "metadata": {}, "outputs": [], "source": ["train = pd.read_csv(input_path / 'train.csv', index_col='id')\ndisplay(train.head())"]}, {"cell_type": "code", "execution_count": 1, "id": "d4523c97", "metadata": {}, "outputs": [], "source": ["test = pd.read_csv(input_path / 'test.csv', index_col='id')\ndisplay(test.head())"]}, {"cell_type": "code", "execution_count": 1, "id": "a446a234", "metadata": {}, "outputs": [], "source": ["target = train.pop('target')"]}, {"cell_type": "markdown", "id": "599ad7c5", "metadata": {}, "source": ["<body style=\"color:white\">\n    <h1 style='color:#F12626' align='center'><font size=\"+3\"><font color=#000ECA>Exploratory Data Analysis</font> : Understanding the Data</font></h1>\n\n<h3 align='center'><font>Here we will see the properties and variations in the data. We know that the data consists of 14 different features named as cont[N] where N->1,2,....13,14. The values of these range from : 0.0 - ~1.2. Where as the <font color=#000ECA>Target</font> has a range of approx 0->10.3. In the following cells we will se the Distribution of the values of the data , and also compare the values of the two provided Train and Test data and also compare the values with the Target.\n    Maybe because this is a <font color=#000ECA>Playground Competition</font> , the data provided is clean with no missing values.</font></h3>\n   \n</body>"]}, {"cell_type": "code", "execution_count": 1, "id": "cf4bc90a", "metadata": {}, "outputs": [], "source": ["fig = plt.figure(figsize=(14, 8))\ngs = fig.add_gridspec(1, 1)\nax = fig.add_subplot(gs[0, 0])\nsns.distplot(target ,ax=ax, color='#1BCBE5')\n\nbackground_color = \"#FFFFFF\"\n\nax.text(2.9, 0.74, 'Distribution of Target', fontsize=24, fontweight='bold', fontfamily='serif')\nax.get_yaxis().set_visible(False)\nax.set_facecolor(background_color)"]}, {"cell_type": "markdown", "id": "924804c7", "metadata": {}, "source": ["* We see here that the Target has two different peaks of distribution which are separated at the value 8."]}, {"cell_type": "code", "execution_count": 1, "id": "f1040b5c", "metadata": {}, "outputs": [], "source": ["fig, p = plt.subplots(7, 2, figsize=(20,22))\n#ax = fig.subplots()\n#ax.text(2.9, 0.74, 'Distribution of Target', fontsize=24, fontweight='bold', fontfamily='serif')\nn_bins = 50\n\nr=0\nc=0\ni=\"cont1\"\nfor i in train.columns:\n    \n    tr = train[i]\n    te = test[i]\n    \n    p[r, c].hist(tr, \n                 n_bins, \n                 density=True, \n                 histtype='bar', \n                 color=\"#ff0032\", \n                 label='train', \n                 linestyle='dashed',\n                 edgeColor = 'white')\n    \n    p[r, c].hist(te, \n                 n_bins, \n                 density=True, \n                 histtype='bar', \n                 color=\"#00a8f9\", \n                 label='test', \n                 alpha=0.6)\n    \n    p[r, c].legend(loc='upper right')\n    p[r, c].set_xlabel(i)\n    \n    if c == 0:\n        p[r, c].set_ylabel('frequency')\n    \n    if r == 0:\n        p[r, c].set_title('Histogram')\n    \n    if c < 1:\n        c+=1\n    else:\n        c=0\n        r+=1\n        \nplt.show()  \n    "]}, {"cell_type": "markdown", "id": "b3038eaa", "metadata": {}, "source": ["* The above plots shows that more or less the Test and Train data Cont Values are same with little difference and in ***cont5*** the values are **overlapping**. "]}, {"cell_type": "code", "execution_count": 1, "id": "8dcbd468", "metadata": {}, "outputs": [], "source": ["fig, p = plt.subplots(7, 2, figsize=(20,22))\n#ax = fig.subplots()\n#ax.text(2.9, 0.74, 'Distribution of Target', fontsize=24, fontweight='bold', fontfamily='serif')\nn_bins = 50\n\nr=0\nc=0\ni=\"cont1\"\nfor i in train.columns:\n    \n    tr = train[i]\n    te = target\n    \n    p[r, c].hist((tr*10), \n                 n_bins, \n                 density=True, \n                 histtype='bar', \n                 color=\"#FF25C0\", \n                 label='train', \n                 linestyle='dashed',\n                 edgeColor = 'white')\n    \n    p[r, c].hist(te, \n                 n_bins, \n                 density=True, \n                 histtype='bar', \n                 color=\"#FFCC00\", \n                 label='target', \n                 alpha=0.6)\n    \n    p[r, c].legend(loc='upper right')\n    p[r, c].set_xlabel(i)\n    \n    if c == 0:\n        p[r, c].set_ylabel('frequency')\n    \n    if r == 0:\n        p[r, c].set_title('Histogram')\n    \n    if c < 1:\n        c+=1\n    else:\n        c=0\n        r+=1\n        \nplt.show()  "]}, {"cell_type": "markdown", "id": "63708ffe", "metadata": {}, "source": ["* Here in the above plot , we can see that there is no similarity between the distributions of Train and Target data. Here the Train data values have been multiplied by 10 , so that the values can be compared , because the ranges of the values of the two data are not the same. "]}, {"cell_type": "code", "execution_count": 1, "id": "995aac62", "metadata": {}, "outputs": [], "source": ["corr_mat = train.corr().stack().reset_index(name=\"correlation\")"]}, {"cell_type": "code", "execution_count": 1, "id": "584db73b", "metadata": {}, "outputs": [], "source": ["g = sns.relplot(\n    data=corr_mat,\n    x=\"level_0\", y=\"level_1\", hue=\"correlation\", size=\"correlation\",\n    palette=\"Blues\", hue_norm=(-1, 1), edgecolor=\".7\",\n    height=10, sizes=(50, 250), size_norm=(-.2, .8),\n)\n\ng.set(xlabel=\"Train\", ylabel=\"Train\", aspect=\"equal\")\ng.despine(left=True, bottom=True)\ng.ax.margins(.2)\nfor label in g.ax.get_xticklabels():\n    label.set_rotation(90)"]}, {"cell_type": "markdown", "id": "9e47b13e", "metadata": {}, "source": ["* In the above Correlation Heatmap , we can see that there is correlation between cont1, cont6, cont9, cont10, cont11, cont12 and cont13."]}, {"cell_type": "markdown", "id": "ccd26f99", "metadata": {}, "source": ["<body style=\"color:white\">\n    <h1 style='color:#F12626' align='center'><font size=\"+3\"><font color=#000ECA>Using Regressor Models</font></font></h1>\n\n<h3 align='center'><font>In the follwing cells we are using the regressor models which are as stated earlier , <font color=#85C1E9>Linear Regression</font> , <font color=#27AE60>Decision Tree Regressor</font>, <font color=#800080>eXtreme Gradient Boosting Regressor</font> , <font color=#EC7063>Light Gradient Boosting Regressor</font> , <font color=#2E86C1 >Category Boosting Regressor</font> and <font color=#FA520E>Regression Artificial Neural Network</font>. \nEach model works through 100 epochs , and the minimum loss function has been displayed for each regressor.To make my work easy , I have used the best parameters according to <a href=https://www.kaggle.com/dwin183287 class=Sharlto Cope><font color=#EC7063>Sharlto Cope's</font></a> notebook. It's a great Notebook , please check it out (<a href=https://www.kaggle.com/dwin183287/tps-jan-2021-eda-models class=Notebook >click here</a>) and support it by upvoting !!</font></h3>\n   \n</body>"]}, {"cell_type": "markdown", "id": "5541b298", "metadata": {}, "source": ["# Training Data"]}, {"cell_type": "code", "execution_count": 1, "id": "5d9ad3fd", "metadata": {}, "outputs": [], "source": ["X_train, X_test, y_train, y_test = train_test_split(train, target, train_size=0.80)"]}, {"cell_type": "code", "execution_count": 1, "id": "9b6d52cd", "metadata": {}, "outputs": [], "source": ["input_d=X_train.shape[1]"]}, {"cell_type": "markdown", "id": "5cfc12a1", "metadata": {}, "source": ["### Linear Regression"]}, {"cell_type": "code", "execution_count": 1, "id": "749a5faf", "metadata": {}, "outputs": [], "source": ["linear = LinearRegression()\nscores = cross_val_score(linear, X_train, y_train, scoring='neg_root_mean_squared_error', cv=100)\nlin_rmse_scrs = -scores\n#print('Linear Regression performance:', lin_rmse_scrs)\nprint(min(lin_rmse_scrs))"]}, {"cell_type": "markdown", "id": "eef6acd1", "metadata": {}, "source": ["### Decision Tree "]}, {"cell_type": "code", "execution_count": 1, "id": "982acc5c", "metadata": {}, "outputs": [], "source": ["tree = DecisionTreeRegressor(max_depth= 10, max_features=9)\nscores = cross_val_score(tree, X_train, y_train, scoring='neg_root_mean_squared_error', cv=100)\ntree_rmse_scrs = -scores\n#print('Decision Tree Regressor performance:', tree_rmse_scrs)\nprint(min(tree_rmse_scrs))"]}, {"cell_type": "markdown", "id": "ef354410", "metadata": {}, "source": ["### Light Gradient Boosting Machine "]}, {"cell_type": "code", "execution_count": 1, "id": "58842d6c", "metadata": {}, "outputs": [], "source": ["lgbm = LGBMRegressor(max_depth=11,learning_rate=0.3071070857621833)\nscores = cross_val_score(lgbm, X_train, y_train, scoring='neg_root_mean_squared_error', cv=100)\nlgbm_rmse_scrs = -scores\n#print('LGBM performance:', lgbm_rmse_scrs)\nprint(min(lgbm_rmse_scrs))"]}, {"cell_type": "markdown", "id": "a78faafe", "metadata": {}, "source": ["### Categorical Boosting"]}, {"cell_type": "code", "execution_count": 1, "id": "0b35596b", "metadata": {}, "outputs": [], "source": ["catb = CatBoostRegressor(verbose=False)\nscores = cross_val_score(catb, X_train, y_train, scoring='neg_root_mean_squared_error', cv=10)\ncatb_rmse_scrs = -scores\n#print('CatBoost performance:', catb_rmse_scrs)\nprint(min(catb_rmse_scrs))"]}, {"cell_type": "markdown", "id": "54c37c6f", "metadata": {}, "source": ["### eXtreme Gradient Boosting"]}, {"cell_type": "code", "execution_count": 1, "id": "8106c678", "metadata": {}, "outputs": [], "source": ["xgb_reg = XGBRegressor(random_state=42)\nscores = cross_val_score(xgb_reg, X_train, y_train, scoring='neg_mean_squared_error', cv=10)\nxgb_rmse_scores = np.sqrt(-scores)\n#print('XGBoost performance:', xgb_rmse_scores)\nprint(min(xgb_rmse_scores))"]}, {"cell_type": "markdown", "id": "762dd6b7", "metadata": {}, "source": ["### Artificial Neural Network"]}, {"cell_type": "code", "execution_count": 1, "id": "fd28b0b3", "metadata": {}, "outputs": [], "source": ["import keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom sklearn.metrics import classification_report\nfrom keras.callbacks import History \nhistory = History()\n\n\nnn  = Sequential()\nnn.add(Dense(units= 28, activation = 'linear', input_dim=14, kernel_initializer=\"uniform\"))\nnn.add(Dense(units= 28, activation = 'linear',kernel_initializer=\"uniform\"))\nnn.add(Dense(units= 1, activation = 'linear',kernel_initializer=\"uniform\"))\nnn.compile(optimizer='adam',\n              loss='mean_squared_error')\nresult=nn.fit(X_train, y_train, validation_split=0.2,\n                       verbose=0, epochs=100,callbacks=[history])\n\n#print(result.history['val_loss'])\nmin(result.history['val_loss'])"]}, {"cell_type": "markdown", "id": "3ac646c4", "metadata": {}, "source": ["## Making predictions using the Neural Network\n\n* Because the Neural Network generates lesser loss , we are using it to make predictions."]}, {"cell_type": "code", "execution_count": 1, "id": "400d7768", "metadata": {}, "outputs": [], "source": ["target_ans=nn.predict(test)"]}, {"cell_type": "code", "execution_count": 1, "id": "88181320", "metadata": {}, "outputs": [], "source": ["from pandas import DataFrame\nans = DataFrame(target_ans,columns=['target'])\nans.head()"]}, {"cell_type": "markdown", "id": "32d0e426", "metadata": {}, "source": ["* Converting the list to a dataframe"]}, {"cell_type": "code", "execution_count": 1, "id": "49f4460a", "metadata": {}, "outputs": [], "source": ["test1=pd.read_csv(input_path / 'test.csv', index_col='id')\ntest1.reset_index(drop=False, inplace=True)\n\ntest1=test1['id']\ntest1.to_frame()\nsub=pd.concat([test1, ans], axis=1, join='inner')"]}, {"cell_type": "markdown", "id": "1ba80516", "metadata": {}, "source": ["* Generating the dataframe with only two columns - \"id\" and \"target\"."]}, {"cell_type": "code", "execution_count": 1, "id": "34c4d8d1", "metadata": {}, "outputs": [], "source": ["sub.to_csv('submit.csv',index=False)"]}, {"cell_type": "markdown", "id": "ec5c2b84", "metadata": {}, "source": ["* Submitting the csv file created.\n"]}, {"cell_type": "markdown", "id": "4fc2c81c", "metadata": {}, "source": ["<h1><center> <font size=\"+3\"><font color=#440BE3>\u269c\u269c\u269c\u269c</font><font color=#0B1AE3>\u269c\u269c\u269c\u269c</font><font color=#0B6CE3>\u269c\u269c\u269c\u269c</font><font color=#0BE317>\u269c\u269c\u269c\u269c</font><font color=#F7DF21>\u269c\u269c\u269c\u269c</font><font color=#F73821>\u269c\u269c\u269c\u269c</font><font color=#DA0A0A>\u269c\u269c\u269c\u269c</font><font color=#29EAEE>\u269c\u269c\u269c\u269c</font></font></center></h1>"]}, {"cell_type": "markdown", "id": "b19caadd", "metadata": {}, "source": [" <h1 style='color:#FA0048' align='center'><font size=\"+3\">                  The END</font></h1>"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}