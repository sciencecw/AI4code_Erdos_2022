{"cells": [{"cell_type": "code", "execution_count": 1, "id": "541ac7b0", "metadata": {}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nimport plotly.offline as py\nimport plotly.express as px\nimport seaborn\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]}, {"cell_type": "code", "execution_count": 1, "id": "924816aa", "metadata": {}, "outputs": [], "source": ["#Configure necessary imports\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,accuracy_score,roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.gridspec as gridspec\nfrom scipy.stats import skew\nfrom sklearn.preprocessing import RobustScaler,MinMaxScaler\nimport matplotlib.gridspec as gridspec\nfrom scipy import stats\nimport matplotlib.style as style\nstyle.use('seaborn-colorblind')"]}, {"cell_type": "code", "execution_count": 1, "id": "1723e303", "metadata": {}, "outputs": [], "source": ["df = pd.read_csv('/kaggle/input/ai4all-project/results/classifier/lasso_1se/lasso_nonzero_coefs.csv')\ndf.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "7078fd66", "metadata": {}, "outputs": [], "source": ["n_r=0.6                # Remove Null value ratio more than n_r. For example 0.6 means if column null ratio more than %60 then remove column\ns_r=0.50               # If skewness more than %75 transform column to get normal distribution\nc_r=1                  # Remove correlated columns\nn_f= df.shape[1]  # n_f number of features. dataset.shape[1] means all columns. If you change it to 10, it will select 10 most correlated feature\nr_s=42                  # random seed"]}, {"cell_type": "code", "execution_count": 1, "id": "fc5fc634", "metadata": {}, "outputs": [], "source": ["print(f\"data shape: {df.shape}\")"]}, {"cell_type": "code", "execution_count": 1, "id": "31a08592", "metadata": {}, "outputs": [], "source": ["sns.heatmap(df.isnull(),cmap = 'magma',cbar = False)"]}, {"cell_type": "code", "execution_count": 1, "id": "eca0ddf9", "metadata": {}, "outputs": [], "source": ["# categorical features\ncategorical_feat = [feature for feature in df.columns if df[feature].dtypes=='O']\nprint('Total categorical features: ', len(categorical_feat))\nprint('\\n',categorical_feat)"]}, {"cell_type": "code", "execution_count": 1, "id": "715d268a", "metadata": {}, "outputs": [], "source": ["# categorical features with missing values\ncategorical_nan = [feature for feature in df.columns if df[feature].isna().sum()>0 and df[feature].dtypes=='O']\nprint(categorical_nan)"]}, {"cell_type": "markdown", "id": "95eadf40", "metadata": {}, "source": ["#Fill null values with Mode/Median (for categorical features -Mode and for numbers-Median)"]}, {"cell_type": "code", "execution_count": 1, "id": "6b13ea05", "metadata": {}, "outputs": [], "source": ["cat=df.select_dtypes(\"object\")\nfor column in cat:\n    df[column].fillna(df[column].mode()[0], inplace=True)\n    #dataset[column].fillna(\"NA\", inplace=True)\n\n\nfl=df.select_dtypes([\"float64\",\"int64\"]).drop(\"coef\",axis=1)\nfor column in fl:\n    df[column].fillna(df[column].median(), inplace=True)\n    #dataset[column].fillna(0, inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "dbd49d92", "metadata": {}, "outputs": [], "source": ["sns.heatmap(df.isnull(),cmap = 'magma',cbar = False)"]}, {"cell_type": "markdown", "id": "444f3461", "metadata": {}, "source": ["#There is NO MORE MISSING VALUES"]}, {"cell_type": "markdown", "id": "cd7b59d9", "metadata": {}, "source": ["#Label Encoding"]}, {"cell_type": "code", "execution_count": 1, "id": "30c34815", "metadata": {}, "outputs": [], "source": ["from sklearn import preprocessing\nencoder = preprocessing.LabelEncoder()\ndf[\"variable\"] = encoder.fit_transform(df[\"variable\"].fillna('Nan'))\ndf[\"gene_name\"] = encoder.fit_transform(df[\"gene_name\"].fillna('Nan'))\ndf.head()"]}, {"cell_type": "markdown", "id": "e486903e", "metadata": {}, "source": ["#Indicator"]}, {"cell_type": "code", "execution_count": 1, "id": "3ea5f58c", "metadata": {}, "outputs": [], "source": ["degree=round(df['coef'].mean(),2)\nfig = go.Figure(go.Indicator(\n    mode = \"gauge+number\",\n    gauge = {\n       'axis': {'range': [None, 100]}},\n    value = degree,\n    title = {'text': \"Average coef %\"},\n    domain = {'x': [0, 1], 'y': [0, 1]}\n))\nfig.show()"]}, {"cell_type": "markdown", "id": "54a020f9", "metadata": {}, "source": ["#A good idea to look at the spread of the outliers via a categorical strip plot."]}, {"cell_type": "code", "execution_count": 1, "id": "6dfb58be", "metadata": {}, "outputs": [], "source": ["sns.catplot('gene_name','coef',data = df)"]}, {"cell_type": "markdown", "id": "ab557cc5", "metadata": {}, "source": ["#Prior to the regression, it'd be useful taking a look at some of the other categorical variables.\nJust to capture any chronological trends."]}, {"cell_type": "code", "execution_count": 1, "id": "3b844996", "metadata": {}, "outputs": [], "source": ["t = df[['variable','gene_name']].groupby('gene_name').agg([np.sum])\n\nt"]}, {"cell_type": "code", "execution_count": 1, "id": "9140b09d", "metadata": {}, "outputs": [], "source": ["t.plot()"]}, {"cell_type": "markdown", "id": "0a5e4b4f", "metadata": {}, "source": ["#Codes from Mehmet Sungur https://www.kaggle.com/medyasun/house-price-all-regressor-algorithms"]}, {"cell_type": "code", "execution_count": 1, "id": "e66cad7f", "metadata": {}, "outputs": [], "source": ["def plotting_3_chart(df, feature): \n    ## Creating a customized chart. and giving in figsize and everything. \n    fig = plt.figure(constrained_layout=True, figsize=(12,8))\n    ## crea,ting a grid of 3 cols and 3 rows. \n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n    #gs = fig3.add_gridspec(3, 3)\n\n    ## Customizing the histogram grid. \n    ax1 = fig.add_subplot(grid[0, :2])\n    ## Set the title. \n    ax1.set_title('Histogram')\n    ## plot the histogram. \n    sns.distplot(df.loc[:,feature], norm_hist=True, ax = ax1)\n\n    # customizing the QQ_plot. \n    ax2 = fig.add_subplot(grid[1, :2])\n    ## Set the title. \n    ax2.set_title('QQ_plot')\n    ## Plotting the QQ_Plot. \n    stats.probplot(df.loc[:,feature], plot = ax2)\n\n    ## Customizing the Box Plot. \n    ax3 = fig.add_subplot(grid[:, 2])\n    ## Set title. \n    ax3.set_title('Box Plot')\n    ## Plotting the box plot. \n    sns.boxplot(df.loc[:,feature], orient='v', ax = ax3 );\n \n\nprint('Skewness: '+ str(df['coef'].skew())) \nprint(\"Kurtosis: \" + str(df['coef'].kurt()))\nplotting_3_chart(df, 'coef')"]}, {"cell_type": "markdown", "id": "01f0f069", "metadata": {}, "source": ["#Target was skewed so we need to transformation. Mehmet used log but you try other transformation."]}, {"cell_type": "code", "execution_count": 1, "id": "5afacf15", "metadata": {}, "outputs": [], "source": ["#log transform the target:\ndf[\"coef\"] = np.log1p(df[\"coef\"])"]}, {"cell_type": "code", "execution_count": 1, "id": "e6f2e63f", "metadata": {}, "outputs": [], "source": ["print('Skewness: '+ str(df['coef'].skew()))   \nprint(\"Kurtosis: \" + str(df['coef'].kurt()))\nplotting_3_chart(df, 'coef')"]}, {"cell_type": "markdown", "id": "ba65299e", "metadata": {}, "source": ["#Now the Target is normalized. When making the submission, this transformation need to be undone.\nI have no clue why. Better ask Mehmet the author of the original (House Price)script."]}, {"cell_type": "markdown", "id": "6256b508", "metadata": {}, "source": ["#Auto Detect Outliers"]}, {"cell_type": "code", "execution_count": 1, "id": "a7f90c74", "metadata": {}, "outputs": [], "source": ["train_o=df[df[\"coef\"].notnull()]\nfrom sklearn.neighbors import LocalOutlierFactor\ndef detect_outliers(x, y, top=5, plot=True):\n    lof = LocalOutlierFactor(n_neighbors=40, contamination=0.1)\n    x_ =np.array(x).reshape(-1,1)\n    preds = lof.fit_predict(x_)\n    lof_scr = lof.negative_outlier_factor_\n    out_idx = pd.Series(lof_scr).sort_values()[:top].index\n    if plot:\n        f, ax = plt.subplots(figsize=(9, 6))\n        plt.scatter(x=x, y=y, c=np.exp(lof_scr), cmap='RdBu')\n    return out_idx\n\nouts = detect_outliers(train_o['gene_name'], train_o['coef'],top=5)\nouts\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "86d600f3", "metadata": {}, "outputs": [], "source": ["outs"]}, {"cell_type": "markdown", "id": "c431aec0", "metadata": {}, "source": ["#Detect and Remove outliers"]}, {"cell_type": "code", "execution_count": 1, "id": "18b6eb04", "metadata": {}, "outputs": [], "source": ["from collections import Counter\noutliers=outs\nall_outliers=[]\nnumeric_features = train_o.dtypes[train_o.dtypes != 'object'].index\nfor feature in numeric_features:\n    try:\n        outs = detect_outliers(train_o[feature], train_o['coef'],top=5, plot=False)\n    except:\n        continue\n    all_outliers.extend(outs)\n\nprint(Counter(all_outliers).most_common())\nfor i in outliers:\n    if i in all_outliers:\n        print(i)\ntrain_o = train_o.drop(train_o.index[outliers])\ntest_o=df[df[\"coef\"].isna()]\ndf =  pd.concat(objs=[train_o, test_o], axis=0,sort=False).reset_index(drop=True)"]}, {"cell_type": "markdown", "id": "fdc7f837", "metadata": {}, "source": ["#I don't know how to create an array to fix that index error."]}, {"cell_type": "markdown", "id": "3055083e", "metadata": {}, "source": ["#Check Skewness and fit transformations if needed."]}, {"cell_type": "code", "execution_count": 1, "id": "a2bb74be", "metadata": {}, "outputs": [], "source": ["from scipy.special import boxcox1p\nfrom scipy.stats import boxcox\nlam = 0.15\n\n#log transform skewed numeric features:\nnumeric_feats = df.dtypes[df.dtypes != \"object\"].index\n\nskewed_feats = df[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > s_r]\nskewed_feats = skewed_feats.index\n\ndf[skewed_feats] = boxcox1p(df[skewed_feats],lam)"]}, {"cell_type": "markdown", "id": "3c58fe26", "metadata": {}, "source": ["#Now we don't have any missing value"]}, {"cell_type": "code", "execution_count": 1, "id": "0ec030a6", "metadata": {}, "outputs": [], "source": ["df.columns[df.isnull().any()]"]}, {"cell_type": "markdown", "id": "cef2237a", "metadata": {}, "source": ["#Check Correlation between features and remove features with high correlations."]}, {"cell_type": "code", "execution_count": 1, "id": "e1dcdedc", "metadata": {}, "outputs": [], "source": ["train_heat=df[df[\"coef\"].notnull()]\ntrain_heat=train_heat.drop([\"gene_name\"],axis=1)\nstyle.use('ggplot')\nsns.set_style('whitegrid')\nplt.subplots(figsize = (10,6))\n## Plotting heatmap. \n\n# Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(train_heat.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\nsns.heatmap(train_heat.corr(), \n            cmap=sns.diverging_palette(255, 133, l=60, n=7), \n            mask = mask, \n            annot=True, \n            center = 0, \n           );\n## Give title. \nplt.title(\"Heatmap of all the Features\", fontsize = 30);"]}, {"cell_type": "markdown", "id": "1497f043", "metadata": {}, "source": ["#Remove correlated features"]}, {"cell_type": "code", "execution_count": 1, "id": "9cf24974", "metadata": {}, "outputs": [], "source": ["feature_corr = train_heat.corr().abs()\ntarget_corr=df.corr()[\"coef\"].abs()\ntarget_corr=pd.DataFrame(target_corr)\ntarget_corr=target_corr.reset_index()\nfeature_corr_unstack= feature_corr.unstack()\ndf_fc=pd.DataFrame(feature_corr_unstack,columns=[\"corr\"])\ndf_fc=df_fc[(df_fc[\"corr\"]>=.80)&(df_fc[\"corr\"]<1)].sort_values(by=\"corr\",ascending=False)\ndf_dc=df_fc.reset_index()\n\n#df_dc=pd.melt(df_dc, id_vars=['corr'], var_name='Name')\ntarget_corr=df_dc.merge(target_corr, left_on='level_1', right_on='index',\n          suffixes=('_left', '_right'))\n\ncols=target_corr[\"level_0\"].values\n\ntarget_corr"]}, {"cell_type": "markdown", "id": "7bebb677", "metadata": {}, "source": ["#Remove low features with low variances"]}, {"cell_type": "code", "execution_count": 1, "id": "79d48f16", "metadata": {}, "outputs": [], "source": ["all_features = df.keys()\n# Removing features.\ndf = df.drop(df.loc[:,(df==0).sum()>=(df.shape[0]*0.9994)],axis=1)\ndf = df.drop(df.loc[:,(df==1).sum()>=(df.shape[0]*0.9994)],axis=1) \n# Getting and printing the remaining features.\nremain_features = df.keys()\nremov_features = [st for st in all_features if st not in remain_features]\nprint(len(remov_features), 'features were removed:', remov_features)"]}, {"cell_type": "markdown", "id": "a9608a4b", "metadata": {}, "source": ["#Create regression models and compare the accuracy to our best regressor."]}, {"cell_type": "code", "execution_count": 1, "id": "9f30c2d5", "metadata": {}, "outputs": [], "source": ["train=df[df[\"coef\"].notnull()]\ntest=df[df[\"coef\"].isna()]"]}, {"cell_type": "code", "execution_count": 1, "id": "a6a16920", "metadata": {}, "outputs": [], "source": ["k = n_f # if you change it 10 model uses most 10 correlated features\ncorrmat=abs(df.corr())\ncols = corrmat.nlargest(k, 'coef')['coef'].index\ntrain_x=df[cols].drop(\"coef\",axis=1)\ntrain_y=df[\"coef\"]\nX_test=test[cols].drop(\"coef\",axis=1)"]}, {"cell_type": "markdown", "id": "ef301c4a", "metadata": {}, "source": ["#Classic Train Test Split"]}, {"cell_type": "code", "execution_count": 1, "id": "ce9dccb9", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(train_x, train_y, test_size=0.20, random_state=r_s)"]}, {"cell_type": "markdown", "id": "04151e3f", "metadata": {}, "source": ["#Find best model and make a submission\n\n#Do you know all models names in sckitlearn? I learnt Now"]}, {"cell_type": "code", "execution_count": 1, "id": "52c71eb7", "metadata": {}, "outputs": [], "source": ["from sklearn.utils.testing import all_estimators\nfrom sklearn import base\n\nestimators = all_estimators()\n\nfor name, class_ in estimators:\n    if issubclass(class_, base.RegressorMixin):\n       print(name+\"()\")"]}, {"cell_type": "code", "execution_count": 1, "id": "3d964c76", "metadata": {}, "outputs": [], "source": ["np.random.seed(seed=r_s)\n\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor,AdaBoostRegressor,ExtraTreesRegressor,HistGradientBoostingRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import Ridge,RidgeCV,BayesianRidge,LinearRegression,Lasso,LassoCV,ElasticNet,RANSACRegressor,HuberRegressor,PassiveAggressiveRegressor,ElasticNetCV\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.cross_decomposition import CCA\nfrom sklearn.neural_network import MLPRegressor\n\n\n\nmy_regressors=[ \n               ElasticNet(alpha=0.001,l1_ratio=0.70,max_iter=100,tol=0.01, random_state=r_s),\n               ElasticNetCV(l1_ratio=0.9,max_iter=100,tol=0.01,random_state=r_s),\n               CatBoostRegressor(logging_level='Silent',random_state=r_s),\n               GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber',random_state =r_s),\n               LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       random_state=r_s\n                                       ),\n               RandomForestRegressor(random_state=r_s),\n               AdaBoostRegressor(random_state=r_s),\n               ExtraTreesRegressor(random_state=r_s),\n               SVR(C= 20, epsilon= 0.008, gamma=0.0003),\n               Ridge(alpha=6),\n               RidgeCV(),\n               BayesianRidge(),\n               DecisionTreeRegressor(),\n               LinearRegression(),\n               KNeighborsRegressor(),\n               Lasso(alpha=0.00047,random_state=r_s),\n               LassoCV(),\n               KernelRidge(),\n               CCA(),\n               MLPRegressor(random_state=r_s),\n               HistGradientBoostingRegressor(random_state=r_s),\n               HuberRegressor(),\n               RANSACRegressor(random_state=r_s),\n               PassiveAggressiveRegressor(random_state=r_s)\n               #XGBRegressor(random_state=r_s)\n              ]\n\nregressors=[]\n\nfor my_regressor in my_regressors:\n    regressors.append(my_regressor)\n\n\nscores_val=[]\nscores_train=[]\nMAE=[]\nMSE=[]\nRMSE=[]\n\n\nfor regressor in regressors:\n    scores_val.append(regressor.fit(X_train,y_train).score(X_val,y_val))\n    scores_train.append(regressor.fit(X_train,y_train).score(X_train,y_train))\n    y_pred=regressor.predict(X_val)\n    MAE.append(mean_absolute_error(y_val,y_pred))\n    MSE.append(mean_squared_error(y_val,y_pred))\n    RMSE.append(np.sqrt(mean_squared_error(y_val,y_pred)))\n\n    \nresults=zip(scores_val,scores_train,MAE,MSE,RMSE)\nresults=list(results)\nresults_score_val=[item[0] for item in results]\nresults_score_train=[item[1] for item in results]\nresults_MAE=[item[2] for item in results]\nresults_MSE=[item[3] for item in results]\nresults_RMSE=[item[4] for item in results]\n\n\ndf_results=pd.DataFrame({\"Algorithms\":my_regressors,\"Training Score\":results_score_train,\"Validation Score\":results_score_val,\"MAE\":results_MAE,\"MSE\":results_MSE,\"RMSE\":results_RMSE})\ndf_results"]}, {"cell_type": "markdown", "id": "61a774ed", "metadata": {}, "source": ["#There is NO MORE MISSING VALUES. Though the program says that I need to see the tutorial."]}, {"cell_type": "code", "execution_count": 1, "id": "f3056019", "metadata": {}, "outputs": [], "source": ["best_models=df_results.sort_values(by=\"RMSE\")\nbest_model=best_models.iloc[0][0]\nbest_stack=best_models[\"Algorithms\"].values\nbest_models"]}, {"cell_type": "code", "execution_count": 1, "id": "80c2bdba", "metadata": {}, "outputs": [], "source": ["best_model.fit(X_train,y_train)\ny_test=best_model.predict(X_test)\ntest_variable=test['variable']\nmy_submission = pd.DataFrame({'variable': test_variable, 'coef': np.expm1(y_test)})\nmy_submission.to_csv('submission_bm.csv', index=False)\nprint(\"Model Name: \"+str(best_model))\nprint(best_model.score(X_val,y_val))\ny_pred=best_model.predict(X_val)\nprint(\"RMSE: \"+str(np.sqrt(mean_squared_error(y_val,y_pred))))"]}, {"cell_type": "code", "execution_count": 1, "id": "69436059", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(10,7))\ny_pred=best_model.predict(X_val)\nsns.regplot(x=y_val,y=y_pred,truncate=False)\nplt.show()"]}, {"cell_type": "markdown", "id": "a75ecb0c", "metadata": {}, "source": ["Das War's Kaggle Notebook Runner: Mar\u00edlia Prata  @mpwolke"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}