{"cells": [{"cell_type": "code", "execution_count": 1, "id": "508ae78c", "metadata": {}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]}, {"cell_type": "markdown", "id": "aff0659c", "metadata": {}, "source": ["In this notebook, we'll be analyzing the National Football League (NFL) data. This dataset contains all regular season games from 2009-2016. It contains 356,768 rows and 100 columns.\nWell, let's get started!"]}, {"cell_type": "code", "execution_count": 1, "id": "ec1a7b14", "metadata": {}, "outputs": [], "source": ["# Libraries we will need:\nimport pandas as pd\nimport numpy as np"]}, {"cell_type": "code", "execution_count": 1, "id": "a049f969", "metadata": {}, "outputs": [], "source": ["nfl_play = pd.read_csv('../input/nflplaybyplay2009to2016/NFL Play by Play 2009-2017 (v4).csv')"]}, {"cell_type": "code", "execution_count": 1, "id": "d1a95e9e", "metadata": {}, "outputs": [], "source": ["np.random.seed(0)\n#Now, let's take a lok at sample data \nnfl_play.sample(10)"]}, {"cell_type": "markdown", "id": "c688bcf9", "metadata": {}, "source": ["Above, we can clearly see, that the dataset has some missing values. Les's find them. "]}, {"cell_type": "code", "execution_count": 1, "id": "2a6e7615", "metadata": {}, "outputs": [], "source": ["mis_values = nfl_play.isnull().sum()\nmis_values "]}, {"cell_type": "markdown", "id": "c2a7a125", "metadata": {}, "source": ["We have a fairly large percent of missing value, let's clarify how large..."]}, {"cell_type": "code", "execution_count": 1, "id": "423af422", "metadata": {}, "outputs": [], "source": ["tot_cells = np.product(nfl_play.shape)\ntot_mis = mis_values.sum()\n\n# missing percent\n(tot_mis/tot_cells) * 100"]}, {"cell_type": "markdown", "id": "59a5ed4a", "metadata": {}, "source": ["As predicted, we have a whole 24.9% of missing value. "]}, {"cell_type": "markdown", "id": "d84c0a11", "metadata": {}, "source": ["# Dropping Missing Value \n\nTheoretically, 30% is the maximum missing values are allowed, beyond which we might want to drop the variable from analysis. But practically this varies. At times we get variables with ~50% of missing values but still, the customer insists to have it for analysis.h When the data goes missing on 60\u201370 percent of the variable, dropping the variable should be considered.\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "ff7d9791", "metadata": {}, "outputs": [], "source": ["#Let's drop rows with at least one missing value.\nnfl_play.dropna()"]}, {"cell_type": "markdown", "id": "27d64f1a", "metadata": {}, "source": ["Now, let's do the same with columns.We'll remove those that have at least one missing value. "]}, {"cell_type": "code", "execution_count": 1, "id": "49d03618", "metadata": {}, "outputs": [], "source": ["col_with_na_dropped = nfl_play.dropna(axis=1)\ncol_with_na_dropped.head()\n"]}, {"cell_type": "code", "execution_count": 1, "id": "98a91206", "metadata": {}, "outputs": [], "source": ["#let's see hom much data did we lose at this point.\nprint(\"Columns original dataset: %d \\n\" % nfl_play.shape[1])\nprint(\"Collumns with na's removed: %d\" % col_with_na_dropped.shape[1])"]}, {"cell_type": "markdown", "id": "f76d2485", "metadata": {}, "source": ["All Nan's have been excluded from data. "]}, {"cell_type": "markdown", "id": "7f8af5c9", "metadata": {}, "source": ["# Filling in Missing Values \nNow,instead of removing, we'll try filling in the missing values.\n"]}, {"cell_type": "code", "execution_count": 1, "id": "9de25b86", "metadata": {}, "outputs": [], "source": ["subset_nfl = nfl_play.loc[:, 'EPA':'Season'].head()\nsubset_nfl"]}, {"cell_type": "markdown", "id": "8f6343ab", "metadata": {}, "source": ["Time to replace Nan's with some value, in our case we'll replace them with 0 "]}, {"cell_type": "code", "execution_count": 1, "id": "0c94fdb7", "metadata": {}, "outputs": [], "source": ["#all Nan's replaced with 0\nsubset_nfl.fillna(0)"]}, {"cell_type": "code", "execution_count": 1, "id": "111f6b79", "metadata": {}, "outputs": [], "source": ["# replace all NA's the value that comes directly after it in the same column,then replace all the remaining na's with 0\nsubset_nfl.fillna(method='bfill', axis=0).fillna(0)"]}, {"cell_type": "markdown", "id": "3adc7777", "metadata": {}, "source": ["# Normalization and Scaling\n\nScaling is a method used to normalize the range of independent variables or features of data. In data processing, it is also known as data normalization. We use it during the data preprocessing step."]}, {"cell_type": "code", "execution_count": 1, "id": "f0d4c05b", "metadata": {}, "outputs": [], "source": ["#Libraries we\"ll use:\n\nimport pandas as pd\nimport numpy as np\n\n# for Box-Cox Transf\nfrom scipy import stats\n\n# for min_max scal\nfrom mlxtend.preprocessing import minmax_scaling\n\n# plotting\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# set seed for reproducibility\nnp.random.seed(0)"]}, {"cell_type": "code", "execution_count": 1, "id": "e7661ee3", "metadata": {}, "outputs": [], "source": ["# we'll generate 1000 data points randomly drawn from an exponential distribution\norig_data = np.random.exponential(size=1000)\n\n# mix-max scale the data between 0 and 1\nscaled_data = minmax_scaling(orig_data, columns=[0])\n\n# plot both together to compare\nfig, ax = plt.subplots(1,2)\nsns.distplot(orig_data, ax=ax[0])\nax[0].set_title(\"Original Nfl Data\")\nsns.distplot(scaled_data, ax=ax[1])\nax[1].set_title(\"Scaled data\")"]}, {"cell_type": "markdown", "id": "987baa95", "metadata": {}, "source": ["The goal of normalization is to change the values of numeric columns in the dataset to a common scale, without distorting differences in the ranges of values. For machine learning, every dataset does not require normalization. It is required only when features have different ranges."]}, {"cell_type": "code", "execution_count": 1, "id": "b8bc7ed2", "metadata": {}, "outputs": [], "source": ["#We'll normalize the exponerial data with boxcox\nnormal_data = stats.boxcox(orig_data)\n\n# plot both together to compare\nfig, ax=plt.subplots(1,2)\nsns.distplot(orig_data, ax=ax[0])\nax[0].set_title(\"Original Nfl Data\")\nsns.distplot(normal_data[0], ax=ax[1])\nax[1].set_title(\"Normalized Nfl data\")"]}, {"cell_type": "markdown", "id": "43431041", "metadata": {}, "source": ["We can clearly see that the shape of our data has changed. It was almost L-shaped. After normalizing it looks more like the outline of a bell curve."]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}