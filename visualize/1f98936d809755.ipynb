{"cells": [{"cell_type": "code", "execution_count": 1, "id": "ab020da2", "metadata": {}, "outputs": [], "source": ["# Import required libraries\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn')\nimport seaborn as sns\nimport time\nfrom geopy.distance import great_circle\n\nfrom collections import Counter\nimport re\nimport xgboost as xgb\n\nfrom sklearn.linear_model import LinearRegression as lg\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.tree import DecisionTreeRegressor\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")"]}, {"cell_type": "code", "execution_count": 1, "id": "27e3a693", "metadata": {}, "outputs": [], "source": ["# Load the dataset\ndata_initial = pd.read_csv('../input/listings_summary.csv')\n# Print the columns of Initial Dataset loaded\ndata_initial.columns"]}, {"cell_type": "code", "execution_count": 1, "id": "4a8d9fc3", "metadata": {}, "outputs": [], "source": ["# Move the selected Features for analysis into a variable\nfeatures_to_keep = ['id', 'space', 'description', 'host_has_profile_pic',\n                    'latitude', 'longitude', 'property_type', 'room_type', 'accommodates', 'bathrooms',  \n                   'bedrooms', 'bed_type', 'amenities', 'square_feet', 'price', 'cleaning_fee', \n                   'security_deposit', 'extra_people', 'guests_included', 'minimum_nights',  \n                   'instant_bookable', 'cancellation_policy', 'experiences_offered', \n                    'neighborhood_overview','access', 'house_rules']\n\n# Load the Features into a dataset variable\ndata_raw = data_initial[features_to_keep].set_index('id')\n# Check the Shape of the Dataset\nprint(\"The dataset with selected features has {} rows and {} columns.\".format(*data_raw.shape))"]}, {"cell_type": "markdown", "id": "affd634e", "metadata": {}, "source": ["Clean, Normalize and Standardize the Features"]}, {"cell_type": "code", "execution_count": 1, "id": "1d86dcd3", "metadata": {}, "outputs": [], "source": ["# Normalizing the 'room_type' feature\ndata_raw.room_type.value_counts(normalize=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "0dc9b634", "metadata": {}, "outputs": [], "source": ["# Normalizing the 'property_type' feature\ndata_raw.property_type.value_counts(normalize=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "bbdd9465", "metadata": {}, "outputs": [], "source": ["#Print First 3 rows of the selected features\ndata_raw[['price', 'cleaning_fee', 'extra_people', 'security_deposit']].head(3)"]}, {"cell_type": "code", "execution_count": 1, "id": "6d0d5622", "metadata": {}, "outputs": [], "source": ["# Checking for Nan's in 'price' column\ndata_raw.price.isna().sum()"]}, {"cell_type": "code", "execution_count": 1, "id": "35865598", "metadata": {}, "outputs": [], "source": ["# Checking for Nan's in 'cleaning_fee' column\ndata_raw.cleaning_fee.isna().sum()"]}, {"cell_type": "code", "execution_count": 1, "id": "19324491", "metadata": {}, "outputs": [], "source": ["#Replace Nan's with $0.00 for 'cleaning_fee'\ndata_raw.cleaning_fee.fillna('$0.00', inplace=True)\ndata_raw.cleaning_fee.isna().sum()"]}, {"cell_type": "code", "execution_count": 1, "id": "f77a25cd", "metadata": {}, "outputs": [], "source": ["# Checking for Nan's in 'security_deposit' column\ndata_raw.security_deposit.isna().sum()"]}, {"cell_type": "code", "execution_count": 1, "id": "fb8eed91", "metadata": {}, "outputs": [], "source": ["#Replace Nan's with $0.00 for 'security_deposit'\ndata_raw.security_deposit.fillna('$0.00', inplace=True)\ndata_raw.security_deposit.isna().sum()"]}, {"cell_type": "code", "execution_count": 1, "id": "cac908a0", "metadata": {}, "outputs": [], "source": ["# Checking for Nan's in 'extra_people' column\ndata_raw.extra_people.isna().sum()"]}, {"cell_type": "code", "execution_count": 1, "id": "64f32d83", "metadata": {}, "outputs": [], "source": ["# Cleaning up the features using method chaining\ndata_raw.price = data_raw.price.str.replace('$', '').str.replace(',', '').astype(float)\ndata_raw.cleaning_fee = data_raw.cleaning_fee.str.replace('$', '').str.replace(',', '').astype(float)\ndata_raw.security_deposit = data_raw.security_deposit.str.replace('$', '').str.replace(',', '').astype(float)\ndata_raw.extra_people = data_raw.extra_people.str.replace('$', '').str.replace(',', '').astype(float)"]}, {"cell_type": "code", "execution_count": 1, "id": "86cac53a", "metadata": {}, "outputs": [], "source": ["# Analyzing the 'price' feature\ndata_raw['price'].describe()"]}, {"cell_type": "code", "execution_count": 1, "id": "63af2f73", "metadata": {}, "outputs": [], "source": ["green_square = dict(markerfacecolor='g', markeredgecolor='g', marker='.')\ndata_raw['price'].plot(kind='box', xlim=(0, 1000), vert=False, flierprops=green_square, figsize=(16,3));"]}, {"cell_type": "code", "execution_count": 1, "id": "a93fb1e5", "metadata": {}, "outputs": [], "source": ["# Based on the plot, to improve the dataset quality removed listings with prices above 400 and 0.00 \ndata_raw.drop(data_raw[ (data_raw.price > 400) | (data_raw.price == 0) ].index, axis=0, inplace=True)\ndata_raw['price'].describe()\nprint(\"The dataset after price-wise preprocessed has {} rows and {} columns.\".format(*data_raw.shape))"]}, {"cell_type": "code", "execution_count": 1, "id": "af572c78", "metadata": {}, "outputs": [], "source": ["# Viewing all the dataset features for Nan's and Missing Values\ndata_raw.isna().sum()"]}, {"cell_type": "code", "execution_count": 1, "id": "0ce0a203", "metadata": {}, "outputs": [], "source": ["# Droping features with too many Nan's\ndata_raw.drop(columns=['square_feet', 'space','neighborhood_overview','access', 'house_rules'], inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "c31db6b3", "metadata": {}, "outputs": [], "source": ["# Droping rows with Nan's in features 'bathrooms', 'bedrooms'\ndata_raw.dropna(subset=['bathrooms', 'bedrooms', ], inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "c0410094", "metadata": {}, "outputs": [], "source": ["# Replacing Nan's with no for 'host_has_profile_pic' \ndata_raw.host_has_profile_pic.fillna(value='f', inplace=True)\ndata_raw.host_has_profile_pic.unique()"]}, {"cell_type": "code", "execution_count": 1, "id": "b2fb5f4b", "metadata": {}, "outputs": [], "source": ["# Checking the dataset features after dropping features with higher Nan's\ndata_raw.isna().sum()"]}, {"cell_type": "code", "execution_count": 1, "id": "b3c3ba6e", "metadata": {}, "outputs": [], "source": ["# 'description' has lot of Nan's yet there might be useful information regading size\n# Trying to extract size by identyfying numbers followed by text like 'sm' or 'm' and \n# transform it into a new feature 'size'\n# Extract numbers from 'description' feature\ndata_raw['size'] = data_raw['description'].str.extract('(\\d{2,3}\\s?[smSM])', expand=True)\ndata_raw['size'] = data_raw['size'].str.replace(\"\\D\", \"\")\n\n# Now change datatype of size into float\ndata_raw['size'] = data_raw['size'].astype(float)\n"]}, {"cell_type": "code", "execution_count": 1, "id": "a9cc4b40", "metadata": {}, "outputs": [], "source": ["# Dropping 'description' feature\ndata_raw.drop(['description'], axis=1, inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "36a5b39d", "metadata": {}, "outputs": [], "source": ["#Adding a new feature 'distance' to the dataset as location is most important in determining the price\ndef distance_from_midberlin(lat, lon):\n    berlin_centre = (52.5027778, 13.404166666666667)\n    record = (lat, lon)\n    return great_circle(berlin_centre, record).km\n\ndata_raw['distance'] = data_raw.apply(lambda x: distance_from_midberlin(x.latitude, x.longitude), axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "3316f58c", "metadata": {}, "outputs": [], "source": ["print(\"After preprocessing for missing values and adding new features the dataset has {} rows and {} columns.\".format(*data_raw.shape))"]}, {"cell_type": "markdown", "id": "ef7496ad", "metadata": {}, "source": ["Using Linear Regressor Model for Training the data to predict missing values for 'size'"]}, {"cell_type": "code", "execution_count": 1, "id": "fb75f2a4", "metadata": {}, "outputs": [], "source": ["# Filtering out sub_data to detrmine missing values in 'size' based on related independent features\nsub_data = data_raw[['accommodates', 'bathrooms', 'bedrooms',  'price', 'cleaning_fee', \n                 'security_deposit', 'extra_people', 'guests_included', 'distance', 'size']]"]}, {"cell_type": "code", "execution_count": 1, "id": "b4c26dbf", "metadata": {}, "outputs": [], "source": ["# Split datasets into train and test\ntrain_data = sub_data[sub_data['size'].notnull()]\ntest_data  = sub_data[sub_data['size'].isnull()]\n\n# Define X\nX_train = train_data.drop('size', axis=1)\nX_test  = test_data.drop('size', axis=1)\n\n# Define y\ny_train = train_data['size']"]}, {"cell_type": "code", "execution_count": 1, "id": "d8626b03", "metadata": {}, "outputs": [], "source": ["# Describe train_data, test_data, X_train, x_test and y_train data sets \nprint(\"Shape of Train Data:    \",train_data.shape)\nprint(\"Shape of Test Data:    \",test_data.shape)\nprint(\"\\nShape of X_train:\", X_train.shape)\nprint(\"Shape of X_test:\", X_test.shape)\nprint(\"\\nShape of y_train:\", y_train.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "59fa6118", "metadata": {}, "outputs": [], "source": ["\n# instantiate the Linear Regression Model\nlinreg = lg()\n\n# Fit Linear regression model to training data\nlinreg.fit(X_train, y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "6da04984", "metadata": {}, "outputs": [], "source": ["# Make predictions using the model\ny_test = linreg.predict(X_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "b2250dbb", "metadata": {}, "outputs": [], "source": ["# Add the new predicted values to the dataframe 'size' feature\ny_test = pd.DataFrame(y_test)\ny_test.columns = ['size']\nprint(y_test.shape)\ny_test.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "b40481cf", "metadata": {}, "outputs": [], "source": ["print(X_test.shape)\nX_test.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "c107329a", "metadata": {}, "outputs": [], "source": ["# Add the index of X_test to an own dataframe\nprelim_index = pd.DataFrame(X_test.index)\nprelim_index.columns = ['prelim']\n\n# Concat this dataframe with y_test to form our new test dataset\ny_test = pd.concat([y_test, prelim_index], axis=1)\ny_test.set_index(['prelim'], inplace=True)\ny_test.head()\nnew_test_data = pd.concat([X_test, y_test], axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "8acdc215", "metadata": {}, "outputs": [], "source": ["# check the new test dataset for Nan's in 'size' feature\nprint(new_test_data.shape)\nnew_test_data['size'].isna().sum()"]}, {"cell_type": "code", "execution_count": 1, "id": "a664f09c", "metadata": {}, "outputs": [], "source": ["# concant train and test data to a new sub dataset\nsub_data_new = pd.concat([new_test_data, train_data], axis=0)\n\nprint(sub_data_new.shape)\nsub_data_new.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "eb953a6e", "metadata": {}, "outputs": [], "source": ["# check if the new sub dataset had Nan's in 'size' column\nsub_data_new['size'].isna().sum()"]}, {"cell_type": "code", "execution_count": 1, "id": "28fc0bb4", "metadata": {}, "outputs": [], "source": ["# prepare the features before concatening\ndata_raw.drop(['accommodates', 'bathrooms', 'bedrooms', 'price', 'cleaning_fee', \n             'security_deposit', 'extra_people', 'guests_included', 'distance', 'size'], \n            axis=1, inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "4db7739b", "metadata": {}, "outputs": [], "source": ["# concate the dataset output from linear regression model to complete original dataframe\ndf = pd.concat([sub_data_new, data_raw], axis=1)\n\nprint(df.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "9c947ed9", "metadata": {}, "outputs": [], "source": ["# analyze 'size' feature to improve quality of the data\ngreen_square = dict(markerfacecolor='g', markeredgecolor='g', marker='.')\ndf['size'].plot(kind='box', xlim=(0, 1000), vert=False, flierprops=green_square, figsize=(16,4));"]}, {"cell_type": "code", "execution_count": 1, "id": "1e608352", "metadata": {}, "outputs": [], "source": ["# drop the rows with 'size' column values 0 and greater than 400 as they are only few \ndf.drop(df[ (df['size'] == 0.) | (df['size'] > 400.) ].index, axis=0, inplace=True)\nprint(\"The dataset after preprocessing 'size' feature has {} rows and {} columns.\".format(*df.shape))"]}, {"cell_type": "code", "execution_count": 1, "id": "4485d995", "metadata": {}, "outputs": [], "source": ["# Analyzing another important feature 'ameneties' \nresults = Counter()\ndf['amenities'].str.strip('{}')\\\n               .str.replace('\"', '')\\\n               .str.lstrip('\\\"')\\\n               .str.rstrip('\\\"')\\\n               .str.split(',')\\\n               .apply(results.update)\n\nresults.most_common(30)"]}, {"cell_type": "code", "execution_count": 1, "id": "272f6ff3", "metadata": {}, "outputs": [], "source": ["# create a new sub dataframe with 'amenity' and 'count'\nsub_df = pd.DataFrame(results.most_common(30), columns=['amenity', 'count'])"]}, {"cell_type": "code", "execution_count": 1, "id": "ab3cd292", "metadata": {}, "outputs": [], "source": ["# ploting the top 20 amenities \nsub_df.sort_values(by=['count'], ascending=True).plot(kind='barh', x='amenity', y='count',  \n                                                      figsize=(10,10), legend=False, color='green',\n                                                      title='Feature_Amenities')\nplt.xlabel('Count');"]}, {"cell_type": "code", "execution_count": 1, "id": "5c4fd893", "metadata": {}, "outputs": [], "source": ["# adding new features using 'amenities'\ndf['Laptop_friendly_workspace'] = df['amenities'].str.contains('Laptop friendly workspace')\ndf['TV'] = df['amenities'].str.contains('TV')\ndf['Family_kid_friendly'] = df['amenities'].str.contains('Family/kid friendly')\ndf['Host_greets_you'] = df['amenities'].str.contains('Host greets you')\ndf['Smoking_allowed'] = df['amenities'].str.contains('Smoking allowed')"]}, {"cell_type": "code", "execution_count": 1, "id": "d96994b3", "metadata": {}, "outputs": [], "source": ["# after adding new features drop redundant 'amenities' column \ndf.drop(['amenities'], axis=1, inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "98cb6d3f", "metadata": {}, "outputs": [], "source": ["# Check the exisiting columns on the dataset\ndf.columns"]}, {"cell_type": "code", "execution_count": 1, "id": "721d2eb6", "metadata": {}, "outputs": [], "source": ["# print information of the dataset \ndf.info()"]}, {"cell_type": "code", "execution_count": 1, "id": "7f5d63c7", "metadata": {}, "outputs": [], "source": ["# drop the unhelpful columns\ndf.drop(['latitude', 'longitude', 'property_type'], axis=1, inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "526826f2", "metadata": {}, "outputs": [], "source": ["# convert all string columns into categorical features\nfor col in ['host_has_profile_pic', 'room_type', 'bed_type', 'instant_bookable', \n            'cancellation_policy']:\n    df[col] = df[col].astype('category')"]}, {"cell_type": "code", "execution_count": 1, "id": "b2e83e5a", "metadata": {}, "outputs": [], "source": ["# define target\ntarget = df[[\"price\"]]\n\n# define features \nfeatures = df.drop([\"price\"], axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "69b2b99c", "metadata": {}, "outputs": [], "source": ["# identify neumerical features\nnum_fea = features.select_dtypes(include=['float64', 'int64', 'bool']).copy()\n\n# one-hot encoding of categorical features\ncat_fea = features.select_dtypes(include=['category']).copy()\ncat_fea = pd.get_dummies(cat_fea)"]}, {"cell_type": "code", "execution_count": 1, "id": "5c220744", "metadata": {}, "outputs": [], "source": ["# concat numerical and categorical features\nfeatures_recoded = pd.concat([num_fea, cat_fea], axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "6efd3406", "metadata": {}, "outputs": [], "source": ["print(features_recoded.shape)\nfeatures_recoded.head(2)"]}, {"cell_type": "markdown", "id": "3bdd2e39", "metadata": {}, "source": ["**XGBoost Regressor**"]}, {"cell_type": "code", "execution_count": 1, "id": "db76d357", "metadata": {}, "outputs": [], "source": ["# split dataset into training and test datasets\nX_train, X_test, y_train, y_test = train_test_split(features_recoded, target, test_size=0.2)\n"]}, {"cell_type": "code", "execution_count": 1, "id": "db19c656", "metadata": {}, "outputs": [], "source": ["# Instantiating XGB Regressor\n\nbooster_start = time.time()\n\nbooster = xgb.XGBRegressor()\n\n# create Grid\n                \nparam_grid = {'n_estimators': [200, 300, 400],\n              'learning_rate': [.03, 0.05, .07], \n              'max_depth': [3, 4, 5],\n              'min_child_weight': [4],\n              'colsample_bytree': [0.7, 0.8, 1],\n              'gamma': [0.0, 0.1, 0.2]}\n\n# instantiate the tuned random forest\nbooster_grid_search = GridSearchCV(booster, param_grid, cv=3, n_jobs=-1)\n\n# train the tuned random forest\nbooster_grid_search.fit(X_train, y_train)\n\n# print best estimator parameters found during the grid search\nprint(booster_grid_search.best_params_)\n\nbooster = xgb.XGBRegressor(colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, \n                           max_depth=6, n_estimators=200)\nbooster.fit(X_train, y_train)\ntraining_preds_booster = booster.predict(X_train)\nval_preds_booster = booster.predict(X_test)\n\nbooster_end = time.time()\n\n# Printing the results\n\nprint(\"\\nTraining RMSE:\", round(np.sqrt(mean_squared_error(y_train, training_preds_booster)),4))\nprint(\"Validation RMSE:\", round(np.sqrt(mean_squared_error(y_test, val_preds_booster)),4))\nprint(\"\\nTraining r2:\", round(r2_score(y_train, training_preds_booster),4))\nprint(\"Validation r2:\", round(r2_score(y_test, val_preds_booster),4))\nprint(f\"Time taken to run: {round((booster_end - booster_start)/60,1)} minutes\")\n\n# Producing a dataframe of feature importances\nft_weights_booster = pd.DataFrame(booster.feature_importances_, columns=['weight'], index=features_recoded.columns)\nft_weights_booster.sort_values('weight', inplace=True)\nft_weights_booster\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "ed30de37", "metadata": {}, "outputs": [], "source": ["# Plotting feature importances\nplt.figure(figsize=(8,20))\nplt.barh(ft_weights_booster.index, ft_weights_booster.weight, align='center') \nplt.title(\"Feature importances in the XGBoost model\", fontsize=14)\nplt.xlabel(\"Feature importance\")\nplt.margins(y=0.01)\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "ac4c8603", "metadata": {}, "outputs": [], "source": ["# Using Cross Validation with 10 fold \nxg_train = xgb.DMatrix(data=X_train, label=y_train)\n\nparams = {'colsample_bytree': 0.3,'learning_rate': 0.2,\n                'max_depth': 5, 'alpha': 10}\n\ncv_results = xgb.cv(dtrain=xg_train, params=params, nfold=10,\n                    num_boost_round=200,early_stopping_rounds=5,metrics=\"rmse\", as_pandas=True, seed=123)\n\nprint((cv_results[\"test-rmse-mean\"]).tail(1))"]}, {"cell_type": "code", "execution_count": 1, "id": "ba6f075d", "metadata": {}, "outputs": [], "source": ["# plot the important features needed for Simple Model\nfeat_importances = pd.Series(booster.feature_importances_, index=features_recoded.columns)\nfeat_importances.nlargest(10).sort_values().plot(kind='barh', color='green', figsize=(10,5))\nplt.xlabel('Relative Feature Importance with XGBoost');"]}, {"cell_type": "code", "execution_count": 1, "id": "965ab9f4", "metadata": {}, "outputs": [], "source": ["# Print the columns of original dataset\nfeatures_recoded.columns"]}, {"cell_type": "code", "execution_count": 1, "id": "f0d87f08", "metadata": {}, "outputs": [], "source": ["# Define Simple Model Dataset with 10 features\nsimplefeatures_to_keep = ['accommodates', 'bathrooms', 'bedrooms', 'extra_people', 'guests_included', 'size',\n                          'room_type_Private room','room_type_Entire home/apt','room_type_Shared room', \n                          'cancellation_policy_super_strict_60']\nsimplefeatures = features_recoded[simplefeatures_to_keep]\n\n# Check the Shape of the Dataset with important features for Simple Model\nprint(\"The dataset with selected features has {} rows and {} columns.\".format(*simplefeatures.shape))\n\n# split dataset into training and test datasets\nSimpleX_train, SimpleX_test, Simpley_train, Simpley_test = train_test_split(simplefeatures, target, test_size=0.2)\n\n# Instantiating XGB Regressor for training Simple model\n\nsimplebooster_start = time.time()\n\nsimplebooster = xgb.XGBRegressor()\n\nsimplebooster = xgb.XGBRegressor(colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, \n                           max_depth=6, n_estimators=200, random_state=4)\nsimplebooster.fit(SimpleX_train, Simpley_train)\ntraining_preds_simplebooster = simplebooster.predict(SimpleX_train)\nval_preds_simplebooster = simplebooster.predict(SimpleX_test)\n\nsimplebooster_end = time.time()\n\n\n# Printing the results for Simple Model\n\nprint(\"\\nTraining RMSE:\", round(np.sqrt(mean_squared_error(Simpley_train, training_preds_simplebooster)),4))\nprint(\"Validation RMSE:\", round(np.sqrt(mean_squared_error(Simpley_test, val_preds_simplebooster)),4))\nprint(\"\\nTraining r2:\", round(r2_score(Simpley_train, training_preds_simplebooster),4))\nprint(\"Validation r2:\", round(r2_score(Simpley_test, val_preds_simplebooster),4))\nprint(f\"Time taken to run: {round((simplebooster_end - simplebooster_start)/60,1)} minutes\")\n"]}, {"cell_type": "markdown", "id": "d1fd4668", "metadata": {}, "source": ["**Decision Tree Regressor Model**"]}, {"cell_type": "code", "execution_count": 1, "id": "f9d930a0", "metadata": {}, "outputs": [], "source": ["# split test and train datasets \nX = features_recoded\ny = df[\"price\"]\n\nX_treetrain, X_treetest, y_treetrain, y_treetest = train_test_split(X, y, test_size=0.2,random_state=42)"]}, {"cell_type": "code", "execution_count": 1, "id": "e1bceafc", "metadata": {}, "outputs": [], "source": ["# instantiate Decision Tree Regression Model\n\ntree_start = time.time()\n\nregr_tree = DecisionTreeRegressor()\n# fit and train model\nregr_tree.fit(X_treetrain, y_treetrain)\n\nval_pred_decisiontree = regr_tree.predict(X_treetest)\n\ntree_end = time.time()\n\n# Printing the results for Decision Tree Regression Model\n\nprint(\"Validation RMSE:\", round(np.sqrt(mean_squared_error(y_treetest, val_pred_decisiontree)),4))\nprint(f\"Time taken to run: {round((tree_end - tree_start),1)} seconds\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}