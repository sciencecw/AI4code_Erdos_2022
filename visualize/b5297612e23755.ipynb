{"cells": [{"cell_type": "markdown", "id": "85af4502", "metadata": {}, "source": ["# The Titanic and its survivors"]}, {"cell_type": "markdown", "id": "fa9d1119", "metadata": {}, "source": ["### The dataset that I will be exploring today is the Titanic dataset, which contains information on passengers who were aboard the RMS Titanic during its fatal voyage"]}, {"cell_type": "markdown", "id": "ad299500", "metadata": {}, "source": ["The task is to build a machine learning model that can predict as accurately as possible the likelihood of some given passengers to survive. We are supplied some training data with which to train the model on, and some testing data to test the models accuracy.\nBefore I jump in its probably a good idea to do a little research the historical event in question."]}, {"cell_type": "markdown", "id": "dd7285bb", "metadata": {}, "source": ["* The RMS Titantic sank in the early hours of the morning on the 15th of April 1912.\n* This resulted in an estimated 1500 deaths (between 1490 and 1635 people) out the the 2224 people on board.\n* The Titanic only had enough lifeboats to carry about half of those on board. \n* Even if they had all lifeboats available, if they full capacity of the ship was met (3339) there would only be enough lifeboats to save 1/3 of those onboard."]}, {"cell_type": "markdown", "id": "4d2dc94f", "metadata": {}, "source": ["It's obvious that they were not prepared any potential disaster,Thomas E. Bonsall, a historian of the disaster, has commented that the evacuation was so badly organised that \"even if they had the number of lifeboats they needed, it is impossible to see how they could have launched them\". The lack of lifeboats meant that priority would have to be given to certain people. Our intuitions would tell us women and children would most likely be first and therefore most likely to survive. Lets investigate whether that's true and what other factors played a role in peoples survival."]}, {"cell_type": "markdown", "id": "e718ea8b", "metadata": {}, "source": ["# Read in the data"]}, {"cell_type": "markdown", "id": "e80431e7", "metadata": {}, "source": ["I am going to load in the data using Pandas a data manipulation library"]}, {"cell_type": "code", "execution_count": 1, "id": "728aa4ea", "metadata": {}, "outputs": [], "source": ["import pandas as pd\nimport numpy as np\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_columns', 30)\n\ntrain = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest  = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\nboth = [train,test]"]}, {"cell_type": "markdown", "id": "f5cb6a57", "metadata": {}, "source": ["# Preliminary investigation of the data"]}, {"cell_type": "code", "execution_count": 1, "id": "4f182061", "metadata": {}, "outputs": [], "source": ["train.head()"]}, {"cell_type": "markdown", "id": "fb8f4600", "metadata": {}, "source": ["### The contents\n* PassengerId - This isn't going to be helpful\n* Survived - What we are trying to predict. 0 = No, 1 = Yes\n* Pclass - The passenger class. 1 = 1st, 2 = 2nd, 3 = 3rd\n* Name - Full name inclding title\n* Sex - Male,Female\n* Age - In years\n* SibSp - The number of siblings or spouses related to this passenger on board\n* Parch - The number of parents or children related to this passenger on board\n* Ticket - Ticket number. Doesn't appear to be that useful in first glance\n* Fare - How much was paid for the ticket\n* Cabin - Letter and number indicating the posistion of the passenegr on the ship\n* Embarked - C = Cherbourg, Q = Queenstown, S = Southampton"]}, {"cell_type": "code", "execution_count": 1, "id": "575e83c0", "metadata": {}, "outputs": [], "source": ["print(\"-\"*15 +\"Train\"+\"-\"*15)\nprint(train.info())\nprint(\"-\"*15 +\"Test\"+\"-\"*15)\nprint(test.info())"]}, {"cell_type": "markdown", "id": "cd43e30c", "metadata": {}, "source": ["Exlcuding \"Survived\" (as thats what I am trying to predict in test) we have the same number of columns with the same datatypes. It seems like however there quite a few nulls throughout, especially in cabin.\nLets take a better look at what null values ware present."]}, {"cell_type": "markdown", "id": "c6814071", "metadata": {}, "source": ["# Missing values"]}, {"cell_type": "code", "execution_count": 1, "id": "7dc21650", "metadata": {}, "outputs": [], "source": ["percent_missing = train.isnull().mean() *100\nmissing_value_df = pd.DataFrame({\"missing\":train.isnull().sum(),\n                                 'percent_missing': percent_missing})\nprint(missing_value_df)"]}, {"cell_type": "code", "execution_count": 1, "id": "f874feb7", "metadata": {}, "outputs": [], "source": ["percent_missing = test.isnull().mean() *100\nmissing_value_df = pd.DataFrame({\"missing\":test.isnull().sum(),\n                                 'percent_missing': percent_missing})\nprint(missing_value_df)"]}, {"cell_type": "markdown", "id": "be311ab9", "metadata": {}, "source": ["There is a huge amount of missing data for \"Cabin\", it might be a good idea to drop this column when it comes to feature engineering but we'll see with further analysis.\nWe might be able to gleen further insights on the number of missing values using the library msno."]}, {"cell_type": "code", "execution_count": 1, "id": "b64c7392", "metadata": {}, "outputs": [], "source": ["import missingno as msno\nmsno.matrix(train)"]}, {"cell_type": "code", "execution_count": 1, "id": "57da3c3a", "metadata": {}, "outputs": [], "source": ["train.describe()"]}, {"cell_type": "markdown", "id": "9f4f9ea8", "metadata": {}, "source": ["* Out of the 891 people in this dataset only 38% survived, compared to the roughly 32% of the full actual data.\n* The ages ranged from as young as 5 months to as old as 80. with the average age being close to 30.\n* The minimum fare is 0. This could be due to people sneaking aboard, taking very poor accommodation, or most likely that the crew are part of the dataset."]}, {"cell_type": "markdown", "id": "9f060f76", "metadata": {}, "source": ["# Visualisations"]}, {"cell_type": "markdown", "id": "d734502c", "metadata": {}, "source": ["Now, we have some intial assumptions on what features might predict survival, such as age, sex etc. Lets explore those, and see what else we can find out."]}, {"cell_type": "code", "execution_count": 1, "id": "57965df2", "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline"]}, {"cell_type": "code", "execution_count": 1, "id": "fe188144", "metadata": {}, "outputs": [], "source": ["sns.set_style(\"darkgrid\") # Style that Seaborn will use fo the figures"]}, {"cell_type": "markdown", "id": "8ef9de97", "metadata": {}, "source": ["## Sex"]}, {"cell_type": "code", "execution_count": 1, "id": "4fcf32a3", "metadata": {}, "outputs": [], "source": ["sex_surv = train.groupby(\"Sex\").mean()[\"Survived\"]\nsex_surv"]}, {"cell_type": "code", "execution_count": 1, "id": "aeb0df37", "metadata": {}, "outputs": [], "source": ["sex_surv =train.groupby(\"Sex\").mean()[\"Survived\"] \n\nf_1 = round(sex_surv[0] * 100,1)\nf_0 = round((1-sex_surv[0]) * 100,1)\nm_1 = round(sex_surv[1] * 100,1)\nm_0 = round((1-sex_surv[1]) * 100,1)\n\nm_surv = [m_0,m_1]\nf_surv = [f_0,f_1]\n\nmale_df = train[train.Sex==\"male\"]\nfemale_df = train[train.Sex==\"female\"]\n\nfig,ax = plt.subplots(1,2,sharey=True)\n\naxis_1 = sns.countplot(data=male_df, x=\"Sex\",hue=\"Survived\",ax=ax[0])\naxis_2 = sns.countplot(data=female_df, x=\"Sex\",hue=\"Survived\",ax=ax[1])\n\ndef percent_label(ax,surv):\n    \n    for c,p in enumerate(ax.patches[:]):\n        h = p.get_height()\n        x = p.get_x()+p.get_width()/2.\n        ax.annotate(str(surv[c])+\"%\", xy=(x,h), xytext=(0,4), \n                    textcoords=\"offset points\", ha=\"center\", va=\"bottom\")\n\npercent_label(axis_1,m_surv)\npercent_label(axis_2,f_surv)\n\nfig.suptitle(\"Sex vs survival\",y=1.07, fontsize=15)\nfig.tight_layout(pad=0)\naxis_1.set_ylim(0,500)\naxis_1.legend_.remove()\naxis_2.set_ylabel(\"\")\n\nfor ax in [axis_1,axis_2]:\n    ax.set_xlabel(\"\");"]}, {"cell_type": "markdown", "id": "9f3d1eeb", "metadata": {}, "source": ["* Sex seems to be highly correlated with survival\n* 74% of women survived compared to 19% of men\n* Women were 4x as likely to survive than men"]}, {"cell_type": "markdown", "id": "858b1360", "metadata": {}, "source": ["## Passenger Class"]}, {"cell_type": "code", "execution_count": 1, "id": "4f8241ca", "metadata": {}, "outputs": [], "source": ["fig,ax = plt.subplots(1,2,figsize=(9,5))\n\nax[0].set_title(\"Count of survival\",{'fontsize': 13},y=1.01)\nax[1].set_title(\"Survival likelihood\",{\"fontsize\": 13},y=1.01)\n\nsns.countplot(data=train,x=\"Pclass\",hue=\"Survived\",ax=ax[0])\nsns.barplot(data=train,x=\"Pclass\",y=\"Survived\",ax=ax[1]);"]}, {"cell_type": "markdown", "id": "5ed30bcb", "metadata": {}, "source": ["* Class correlated with survival\n* Greater than 60% chance of survival if the passenger was in 1st class\n* Only around 25% of 3rd class passengers lived"]}, {"cell_type": "markdown", "id": "46250a50", "metadata": {}, "source": ["## Age"]}, {"cell_type": "code", "execution_count": 1, "id": "cd209bd8", "metadata": {}, "outputs": [], "source": ["#fig,ax = plt.subplots()\n\nax = sns.FacetGrid(data=train,col=\"Survived\",height=4)\nax.map(sns.distplot,\"Age\",bins=16)\nax.fig.suptitle(\"Age of survival\",y=1.1,fontsize=15)\nax.set(xlim=(0,80));"]}, {"cell_type": "code", "execution_count": 1, "id": "e25be5d9", "metadata": {}, "outputs": [], "source": ["ax = sns.FacetGrid( train,hue =\"Survived\",height=4,aspect=1.5 )\nax.map(sns.kdeplot, \"Age\", shade= True )\nax.set(xlim=(0 , train[\"Age\"].max()))\nax.fig.suptitle(\"Age of survival\",y=1.05,fontsize=15)\nax.add_legend();"]}, {"cell_type": "markdown", "id": "9979e70a", "metadata": {}, "source": ["* Confirms intuition around the idea that children take precendence over adults\n* Children under 10 more likely to surive than other ages\n* The early twenties seems to be the worst age for surivival"]}, {"cell_type": "markdown", "id": "0e653677", "metadata": {}, "source": ["We've seen the features age,sex and class and their correlation with surival individually, now lets see what we notice when we combine these features in a single figure. "]}, {"cell_type": "code", "execution_count": 1, "id": "4ba77892", "metadata": {}, "outputs": [], "source": ["ax = sns.FacetGrid(train, row = \"Sex\", col = \"Pclass\", hue = \"Survived\")\nax.map(plt.hist, \"Age\",alpha=0.6,edgecolor=\"none\",histtype=\"stepfilled\")\nax.fig.suptitle(\"Age,sex and class vs survival\",y=1.07,fontsize=20)\nax.add_legend();"]}, {"cell_type": "markdown", "id": "2e5dfd62", "metadata": {}, "source": ["* 1st class females have the greatest chance of living, age doesn't appear to be that influencial for this group\n* Young 3rd class men at the greatest risk of perishing\n* Likelihood of surival decreases with passenger class for men, but not for women\n* The saying \"women and children first\" rings true here"]}, {"cell_type": "markdown", "id": "b36f5bc2", "metadata": {}, "source": ["## Fare"]}, {"cell_type": "code", "execution_count": 1, "id": "a4294a0d", "metadata": {}, "outputs": [], "source": ["for df in both:\n    df[\"Fare_bins\"] = pd.qcut(df[\"Fare\"],4)\n\nplt.title(\"Fare vs survival\",fontsize=15)\nsns.pointplot(data=train,x='Fare_bins',y='Survived');"]}, {"cell_type": "markdown", "id": "b24ceab5", "metadata": {}, "source": ["* As you might expect fare positively correlates with survival\n* But is this only to do with the money or is it ultimately to do with class"]}, {"cell_type": "markdown", "id": "e6c9bcc2", "metadata": {}, "source": ["## Embarked"]}, {"cell_type": "code", "execution_count": 1, "id": "4e00a7fb", "metadata": {}, "outputs": [], "source": ["embarked_df = train[['Embarked', 'Survived']].groupby(['Embarked'],as_index=False).mean().sort_values(\"Survived\",ascending=False)\nprint(embarked_df)\n\nplt.title(\"Embarked vs survival\",fontsize=15)\nsns.barplot(data=train, x=\"Embarked\",y=\"Survived\");"]}, {"cell_type": "markdown", "id": "e003c21b", "metadata": {}, "source": ["* Cherbourg 55% survival\n* The wealth of these ports of embarkation may be a factor in the passengers rate of survival[](http://)"]}, {"cell_type": "markdown", "id": "0dc70a4f", "metadata": {}, "source": ["## Class and embarked"]}, {"cell_type": "code", "execution_count": 1, "id": "86576ce4", "metadata": {}, "outputs": [], "source": ["ax = sns.FacetGrid(train, col = \"Embarked\",height=4)\nax.map(sns.pointplot, \"Pclass\", \"Survived\", \"Sex\", order=None,hue_order=None, palette = \"deep\")\nax.fig.suptitle(\"Class & Embarked\",y=1.05,fontsize=15)\nax.add_legend();"]}, {"cell_type": "markdown", "id": "1fdc3141", "metadata": {}, "source": ["* Regardless of where you embark from a the lower your class the less likely you were to live\n* The exception to this is in Queenstown, where the survival rate of men increases slightly from 2nd to 3rd class\n* However this may be due to the fact that there were less data points for passengers in 1st or 2nd class from Queenstown"]}, {"cell_type": "markdown", "id": "8d663f60", "metadata": {}, "source": ["## Fare class survival"]}, {"cell_type": "code", "execution_count": 1, "id": "b57c4fd4", "metadata": {}, "outputs": [], "source": ["ax = sns.FacetGrid(data=train, col=\"Pclass\",hue=\"Pclass\",height=4)\nax.map(sns.pointplot, \"Fare_bins\",\"Survived\",order=None);"]}, {"cell_type": "markdown", "id": "18e7b242", "metadata": {}, "source": ["* For 1st and 2nd class passengers paying more money may have led to a greater chance of suriving\n* However for 3rd class, it doesnt appear that the fare you paid would be likely to help your survival, perhaps the stigma or position of 3rd class rooms were too great of a detriment"]}, {"cell_type": "markdown", "id": "c938169e", "metadata": {}, "source": ["## Class age survival"]}, {"cell_type": "code", "execution_count": 1, "id": "8b12cb9b", "metadata": {}, "outputs": [], "source": ["ax = sns.FacetGrid(data=train, col=\"Pclass\",hue=\"Survived\",height=4,aspect=1.3)\nax.map(sns.kdeplot,\"Age\",shade=True)\nax.set(xlim=(0 , train['Age'].max()))\nax.add_legend();"]}, {"cell_type": "markdown", "id": "c6122d7d", "metadata": {}, "source": ["* The benefit of being a 1st class citizen seems to outweigh the negative of being an adult\n* A greater amount of children survived in classes 2 and 3, perhaps due to the fact that upper classes have less children and so affects the results?"]}, {"cell_type": "markdown", "id": "7362a619", "metadata": {}, "source": ["## Title"]}, {"cell_type": "markdown", "id": "a1ec51cf", "metadata": {}, "source": ["The name column on its own isn't any use to us, however within the entries are titles which we can extract into a new column"]}, {"cell_type": "code", "execution_count": 1, "id": "97c9b21b", "metadata": {}, "outputs": [], "source": ["import re\n\nfor df in both:\n    df[\"Title\"] = df[\"Name\"].apply(lambda x: re.split(\"(?<=, )(.*?)(?=\\.)\",x)[1])\n\n    df[\"Title\"] = df[\"Title\"].replace([\"Don\",\"Rev\",\"Dr\",\"Major\",\"Sir\",\"Col\",\"Capt\",\"Jonkheer\",\"Lady\",\"the Countess\"],\"Unique\")\n    df[\"Title\"] = df[\"Title\"].replace(\"Dona\",\"Mrs\")\n    df[\"Title\"] = df[\"Title\"].replace([\"Ms\",\"Mlle\",\"Mme\"],\"Miss\")\n\ntitle_surv = train[['Title', 'Survived']].groupby(['Title'],as_index=False).mean().sort_values(\"Survived\",ascending=False)\n\nsns.barplot(data=train,x=\"Title\",y=\"Survived\",order=[\"Mrs\",\"Miss\",\"Master\",\"Unique\",\"Mr\"]);"]}, {"cell_type": "markdown", "id": "59646963", "metadata": {}, "source": ["* This graph really fits well with our intuitions\n* Women rank higher than men, with married women (who probably have a higher social status than those umarried ) above unwedded women.\n* Master aka young boys is next,which makes sense as the young are usually preferred in live or death situations\n* Those with a unique title e.g. Dr, Rev etc (who are usually men) come second last, probably due to the benefit of their social status\n* Lastly regular men having a less than 20% chance of survival"]}, {"cell_type": "code", "execution_count": 1, "id": "32e3f11b", "metadata": {}, "outputs": [], "source": ["ax = sns.FacetGrid(data=train,col=\"Pclass\",height=4)\nax.map(sns.barplot,\"Title\",\"Survived\",palette=\"deep\",order=None);"]}, {"cell_type": "markdown", "id": "e7045742", "metadata": {}, "source": ["* Class greatly boosts mens liklihood to survive\n* Interesting to see only 10% of 2nd class \"Miss\" survived"]}, {"cell_type": "markdown", "id": "45ed9731", "metadata": {}, "source": ["## Alone"]}, {"cell_type": "markdown", "id": "c9bbd689", "metadata": {}, "source": ["Two new columns we can make are \"Alone\" and \"Family_size\" which use \"SibSp\" and \"Parch\""]}, {"cell_type": "code", "execution_count": 1, "id": "4850bbd8", "metadata": {}, "outputs": [], "source": ["for df in both:\n    df[\"Alone\"] = df.apply(lambda x: 0 if x[\"SibSp\"] + x[\"Parch\"]>=1 else 1,axis=1)\n    df[\"Family_size\"] = df.apply(lambda x: x[\"SibSp\"] + x[\"Parch\"] + 1,axis=1)\n\nalone_df = train.groupby(\"Alone\").mean()[\"Survived\"]\nprint(alone_df)"]}, {"cell_type": "code", "execution_count": 1, "id": "dbca296e", "metadata": {}, "outputs": [], "source": ["alone_surv =train.groupby(\"Alone\").mean()[\"Survived\"] \n\nnot_a_1 = round(alone_surv[0] * 100,1)\nnot_a_0 = round((1-alone_surv[0]) * 100,1)\na_1 = round(alone_surv[1] * 100,1)\na_0 = round((1-alone_surv[1]) * 100,1)\n\nalone_order = [not_a_0,not_a_1,a_0,a_1]\nalone_order\n\nax = sns.countplot(data=train,x=\"Alone\",hue=\"Survived\")\n\nfor c,p in enumerate(ax.patches[:]):\n        h = p.get_height()\n        x = p.get_x()+p.get_width()/2.\n        if h != 0:\n            ax.annotate(str(alone_order[c])+\"%\", xy=(x,h), xytext=(0,4), \n                       textcoords=\"offset points\", ha=\"center\", va=\"bottom\")\n\nax.set_ylim(0,400)\nplt.title(\"Alone vs survival\",y=1.01,fontsize=15);"]}, {"cell_type": "markdown", "id": "2614c6f5", "metadata": {}, "source": ["## Who's alone"]}, {"cell_type": "code", "execution_count": 1, "id": "8797f9ea", "metadata": {}, "outputs": [], "source": ["alone_sex = train.groupby(\"Sex\").mean()[\"Alone\"]\nalone_embarked = train.groupby(\"Embarked\").mean()[\"Alone\"]\nalone_class = train.groupby(\"Pclass\").mean()[\"Alone\"]\n\nprint(alone_sex,\"\\n\")\nprint(alone_embarked,\"\\n\")\nprint(alone_class,\"\\n\")\n\nax = sns.FacetGrid( train,hue =\"Alone\",height=4,aspect=2)\nax.map(sns.kdeplot, \"Age\", shade= True)\nax.set(xlim=(0,train.Age.max()))\nax.add_legend()\nax.fig.suptitle(\"Alone age\",y=1.05);"]}, {"cell_type": "markdown", "id": "17ee40b9", "metadata": {}, "source": ["## Alone vs class,sex,embarked"]}, {"cell_type": "code", "execution_count": 1, "id": "ce4fabe9", "metadata": {}, "outputs": [], "source": ["fig,ax = plt.subplots(1,3,figsize=(20,5))\nsns.barplot(data=train,x=\"Sex\",y=\"Survived\",hue=\"Alone\",ax=ax[0])\nsns.barplot(data=train,x=\"Pclass\",y=\"Survived\",hue=\"Alone\",ax=ax[1])\nsns.barplot(data=train,x=\"Embarked\",y=\"Survived\",hue=\"Alone\",ax=ax[2]);"]}, {"cell_type": "markdown", "id": "fb27c7af", "metadata": {}, "source": ["* You were 1.5 times more likely to die if you were alone\n* Women who were alone had greater odds than those alone\n* It doesn't appear to matter what class you are, if you are alone your chances aren't that highly affected"]}, {"cell_type": "markdown", "id": "d36f9c0e", "metadata": {}, "source": ["## Family size"]}, {"cell_type": "code", "execution_count": 1, "id": "029a34a0", "metadata": {}, "outputs": [], "source": ["ax = sns.pointplot(data=train,x=\"Family_size\",y=\"Survived\");"]}, {"cell_type": "code", "execution_count": 1, "id": "ef264a1b", "metadata": {}, "outputs": [], "source": ["cf = sns.FacetGrid(data=train,col=\"Pclass\",height=4,hue=\"Pclass\")\ncf.map(sns.pointplot,\"Family_size\",\"Survived\",order=None)\n\nef = sns.FacetGrid(data=train,col=\"Embarked\",height=4,hue=\"Embarked\")\nef.map(sns.pointplot,\"Family_size\",\"Survived\",order=None);"]}, {"cell_type": "markdown", "id": "52536895", "metadata": {}, "source": ["## Highest mortality rate"]}, {"cell_type": "markdown", "id": "c7f55e6f", "metadata": {}, "source": ["One last one for fun, let's look for the group with the highest mortality rate by creating a Treemap!"]}, {"cell_type": "code", "execution_count": 1, "id": "57a99e1f", "metadata": {}, "outputs": [], "source": ["import matplotlib\nfrom matplotlib import rcParams\nimport squarify\n\ndef label(n):\n    if n[1]==1:\n        return \"1st class {0} {1} to {2}\\n{3}% Died\".format(n[0],int(n[2].left.round()),int(n[2].right.round()),round(n[4]*100))\n    elif n[1]==2:\n        return \"2nd class {0} {1} to {2}\\n{3}% Died\".format(n[0],int(n[2].left.round()),int(n[2].right.round()),round(n[4]*100))\n    elif n[1]==3:\n        return \"3rd class {0} {1} to {2}\\n{3}% Died\".format(n[0],int(n[2].left.round()),int(n[2].right.round()),round(n[4]*100))\n    \ndef fix_zeros(n):\n    if n == 0:\n        return 0.01\n    else:\n        return n\n\nfor df in both:\n    df[\"Age_bins\"] = pd.cut(df[\"Age\"],4)\n\ncomb_df = train.groupby([\"Sex\",\"Pclass\",\"Age_bins\"]).mean()[\"Survived\"].dropna().reset_index()\n\ncomb_df[\"Died\"] = 1-comb_df[\"Survived\"]\n\ncomb_df[\"Label\"] = comb_df.apply(label,axis=1)\n\ncomb_df[\"Survived\"] = comb_df[\"Survived\"].apply(fix_zeros)\ncomb_df[\"Died\"] = comb_df[\"Died\"].apply(fix_zeros)\n\ncomb_df = comb_df.sort_values(\"Died\",ascending=False)\n\nplt.figure(figsize=(15,7))\n\nnorm = matplotlib.colors.Normalize(vmin=min(comb_df.Survived), vmax=max(comb_df.Survived))\ncolors = [matplotlib.cm.Blues(norm(value)) for value in comb_df.Died]\n\nsquarify.plot(sizes=comb_df[\"Died\"], label=comb_df[\"Label\"], color=colors, alpha=.8,pad=True)\nplt.axis('off');"]}, {"cell_type": "markdown", "id": "b8fa22ad", "metadata": {}, "source": ["* As we have come to expect, lower class old men fair very poorly in life or death situations\n\nThe lower end of the graph becomes very difficult to read, so lets create another Treemap but this time reduce the groups and show the lowest mortality rates."]}, {"cell_type": "markdown", "id": "aa5f2cce", "metadata": {}, "source": ["## Lowest mortality rate"]}, {"cell_type": "code", "execution_count": 1, "id": "9cc09ed8", "metadata": {}, "outputs": [], "source": ["def label_2(n):\n    if n[1]==1:\n        return \"1st class {0}\\n{1}% survived\".format(n[0],round(n[2]*100))\n    elif n[1]==2:\n        return \"2nd class {0}\\n{1}% survived\".format(n[0],round(n[2]*100))\n    elif n[1]==3:\n        return \"3rd class {0}\\n{1}% survived\".format(n[0],round(n[2]*100))\n    \ncomb_df2 = train.groupby([\"Sex\",\"Pclass\"]).mean()[\"Survived\"].dropna().reset_index()\\\n                                                        .sort_values(\"Survived\",ascending=False)\n\ncomb_df2[\"Label\"] = comb_df2.apply(label_2,axis=1)\ncomb_df2[\"Survived\"] = comb_df2[\"Survived\"].apply(fix_zeros)\n\nplt.figure(figsize=(10,7))\n\nnorm = matplotlib.colors.Normalize(vmin=min(comb_df2.Survived), vmax=max(comb_df2.Survived))\ncolors = [matplotlib.cm.Blues(norm(value)) for value in comb_df2.Survived]\n\n\nsquarify.plot(sizes=comb_df2[\"Survived\"], label=comb_df2[\"Label\"], color=colors, alpha=.8,pad=False)\nplt.axis('off');"]}, {"cell_type": "markdown", "id": "505d37ad", "metadata": {}, "source": ["### Conclusions\nSome of the features that seem to have the greatest affect on surival are:\n* Sex\n* Pclass\n* Age\n* Alone\n* Fare\n* Embarked"]}, {"cell_type": "markdown", "id": "cec3dd31", "metadata": {}, "source": ["# Feature engineering"]}, {"cell_type": "markdown", "id": "ac2ae339", "metadata": {}, "source": ["This is how the data is currently looking. During the visualisation process we created some new features that we could train our model on. We can create some more now."]}, {"cell_type": "code", "execution_count": 1, "id": "1e85ee87", "metadata": {}, "outputs": [], "source": ["train.head()"]}, {"cell_type": "markdown", "id": "e4a14bc8", "metadata": {}, "source": ["Looking at the cabin column we can see that the values are composed of letters and numbers, after a little Googling it appears that the letters could be the deck and the number could be the room. So we could engineer a new feature by splitting the cabin column into two new columns \"Deck\" and \"Room\"."]}, {"cell_type": "markdown", "id": "6dd64aaa", "metadata": {}, "source": ["Below are some images of the layout of the ship. These new columns could could have importance in predicting survival as deck and room indicate a persons posistion on the ship,and therefore their potential class or proximity to the impact of the iceberg."]}, {"cell_type": "markdown", "id": "5e36bdd2", "metadata": {}, "source": ["<img src=\"attachment:Titanic_cutaway_diagram.png\" width=\"400\">"]}, {"cell_type": "code", "execution_count": 1, "id": "0f76cb80", "metadata": {}, "outputs": [], "source": ["from IPython.display import Image\nImage(\"../input/class-layout/titanic class system layout.jpg\")"]}, {"cell_type": "code", "execution_count": 1, "id": "2efb69cc", "metadata": {}, "outputs": [], "source": ["def split_cabin(n):\n    try:\n        return n[0]\n    except TypeError:\n        return np.nan\n    \ndef room(n):\n    try:\n        return n[1:]\n    except TypeError:\n        return np.nan\n\nfor df in both:\n    df[\"Deck\"] = df[\"Cabin\"].apply(split_cabin)\n    df[\"Room\"] = df[\"Cabin\"].apply(room)"]}, {"cell_type": "markdown", "id": "91d5ef6d", "metadata": {}, "source": ["Now we have our two new columns. The values for deck seem to be fine, however when we look at the unique values for room we can see that we have some odd values. There are items in this list that seem to contain multiple rooms. This could potentially be a new feature, as if a passenger has bought multiple rooms,they are likely wealthier and could be of higher class, which we know from our viualisations correlate positively with survival."]}, {"cell_type": "code", "execution_count": 1, "id": "cc5d1f2c", "metadata": {}, "outputs": [], "source": ["train.Deck.unique()"]}, {"cell_type": "code", "execution_count": 1, "id": "b6cdee2b", "metadata": {}, "outputs": [], "source": ["train.Room.unique()"]}, {"cell_type": "code", "execution_count": 1, "id": "17d2cfaa", "metadata": {}, "outputs": [], "source": ["import math\nfrom statistics import mean\n\ndef multi_rooms(n):\n    try:\n        if len(n[\"Room\"])>3:\n            return 1\n        else:\n            return 0\n    except:\n        return np.nan\n    \n# Function to fix multiple rooms\n\ndef fix_rooms(n):\n    try:\n        if len(n[\"Room\"])>3:\n            r_list = [int(v) for v in re.findall(\"\\d+\",n[\"Room\"])] # create a list of the multiple rooms\n            av = math.ceil(mean(r_list))                           # Take the average room number\n            return str(min(r_list, key=lambda x:abs(x-av)))        # Find the closest real room to the average\n        elif len(n[\"Room\"]) == 0:\n            return np.nan\n        else:\n            return n[\"Room\"]\n    except TypeError:\n        return np.nan\n    \nfor df in both:    \n    df[\"Multi_room\"] = df.apply(multi_rooms,axis=1)\n    df[\"Room\"] = df.apply(fix_rooms,axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "b81716fb", "metadata": {}, "outputs": [], "source": ["train.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "21d30b48", "metadata": {}, "outputs": [], "source": ["train.Multi_room.value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "49c2623d", "metadata": {}, "outputs": [], "source": ["train.Room.unique()"]}, {"cell_type": "markdown", "id": "c2a3cd71", "metadata": {}, "source": ["# Dealing with missing values"]}, {"cell_type": "markdown", "id": "703ecb1c", "metadata": {}, "source": ["Most machine learning algorithms cannot handle missing or non numerical values, so before we can use them we need to encode the data and impute any nulls.\nThere are multiple ways of doing this, we can: \n* drop the columns which contain nulls\n* Impute missing values using mean,median or mode\n* Impute using a ML algortihm\n* Add a marker column indicating that a value is missing\n* or combinations of above\n\nLets try a few different ways and see how they play out."]}, {"cell_type": "markdown", "id": "0eb8de6d", "metadata": {}, "source": ["We'll make 4 different versions of the dataframe to test:\n* knn_marker - Uses KNNimputer which works by find \"k\" nearest neighbors and then votes for the most frequent label of those neighbors. And specify \"add_indicator=True\" which adds a marker column\n* Impute - Impute using SimpleImputer (median, mode etc)\n* Impute_marker - Impute + marker\n* Drop - Drops the columns. We'll define this dataframe later"]}, {"cell_type": "code", "execution_count": 1, "id": "f573966a", "metadata": {}, "outputs": [], "source": ["knn_marker = train.copy() \n    \nimpute = train.copy()\nimpute_marker = train.copy() \n\nimpute_test = [knn_marker,impute,impute_marker] # train will be knn without a marker"]}, {"cell_type": "code", "execution_count": 1, "id": "3305d355", "metadata": {}, "outputs": [], "source": ["train[\"Embarked\"].fillna(train[\"Embarked\"].mode()[0],inplace=True) # Fill missing values using the mode as there are very few (2) nulls in this column\ntest[\"Fare\"].fillna(test[\"Fare\"].median(),inplace=True)\n\nfor df in both:\n    df[\"Fare_cats\"] = df[\"Fare_bins\"].cat.codes # Use cat.codes to label encode \n    df[\"Age_cats\"] = df[\"Age_bins\"].cat.codes.replace(-1, np.nan) # cat.codes encodes nan as -1 so replace with np.nan\n    df.drop(columns=[\"PassengerId\",\"Name\",\"Age\",\"Ticket\",\"Fare\",\"Cabin\",\"Age_bins\",\"Fare_bins\"],inplace=True)"]}, {"cell_type": "markdown", "id": "9b492a5e", "metadata": {}, "source": ["# Encode data"]}, {"cell_type": "markdown", "id": "bfec5483", "metadata": {}, "source": ["### LabelEncoder\nThis function takes categorical data and assigns a numerical value to each unique item. For example if you had the the categories \"1st Place\", \"2nd Place\", \"3rd Place\", LabelEncoder would assign values like 0, 1, 2. I decided to use LabelEncoder for these columns as they have an implicit order to them."]}, {"cell_type": "code", "execution_count": 1, "id": "16358ab1", "metadata": {}, "outputs": [], "source": ["Image(\"../input/encoding/encoding.png\")"]}, {"cell_type": "code", "execution_count": 1, "id": "c8dc999a", "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import OrdinalEncoder\n\nle= OrdinalEncoder() # Instantiate estimator\nencode_cols = [\"SibSp\",\"Parch\",\"Deck\",\"Room\"]\n\ndef encode(data):\n    nonulls = np.array(data.dropna()) # df of non-null values\n    impute_reshape = nonulls.reshape(-1,1) # df has to be reshaped before the label encoder can be fit\n    impute_ordinal = le.fit_transform(impute_reshape)\n    data.loc[data.notnull()] = np.squeeze(impute_ordinal) # np.squeeze removes single-dimensional entries from the shape of an array\n\nfor df in both:\n    for col in encode_cols:\n        encode(df[col])"]}, {"cell_type": "markdown", "id": "ce29ec92", "metadata": {}, "source": ["### OneHotEncoder\nOneHotEncoder is simliar to LabelEncoder however it is more useful for data that has no order, such as \"Red\",\"Yellow\",\"Blue\".\nIt also differs from LabelEncoder in that instead of creating one new column containing numbers ranging from 0 to n-1 (where n is the length of unique items),it creates n new columns each with either a value of 0 or 1."]}, {"cell_type": "code", "execution_count": 1, "id": "2182894e", "metadata": {}, "outputs": [], "source": ["Image(\"../input/encoding/OHE.png\")"]}, {"cell_type": "code", "execution_count": 1, "id": "d45fd443", "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import OneHotEncoder\n\nsub = [\"Sex\",\"Embarked\",\"Title\"] # Subset of df columns to encode values\nohe = OneHotEncoder(handle_unknown = \"ignore\",sparse=False) # Instantiate estimator\n\nfor i,df in enumerate(both):\n    ohe_df = pd.DataFrame(ohe.fit_transform(both[i][sub]))  # Fit and transform OHE to the data then convert to dataframe as it returns an array\n    \n    ohe_df.columns = [\"Female\",\"Male\",\"C\",\"Q\",\"S\",\"Master\",\"Miss\",\"Mr\",\"Mrs\",\"Unique\"] # Reassign column names as OHE removes them\n    \n    both[i] = pd.concat([both[i],ohe_df],axis=1) # Concatentate  the two dataframes\n    \n    both[i].drop(columns=[\"Sex\",\"Embarked\",\"Title\"],inplace=True)"]}, {"cell_type": "markdown", "id": "a2bda260", "metadata": {}, "source": ["# Impute missing data\nTo fill in the missing values for the remaining columns I'm going to use KNNImputer which imputes using KNN."]}, {"cell_type": "code", "execution_count": 1, "id": "15e83009", "metadata": {}, "outputs": [], "source": ["from sklearn.impute import KNNImputer\n\nknn_imp = KNNImputer(n_neighbors=5,weights=\"distance\")\n\nfor i,df in enumerate(both):\n    both[i] = pd.DataFrame(knn_imp.fit_transform(both[i]),columns = both[i].columns)"]}, {"cell_type": "markdown", "id": "e6c3304e", "metadata": {}, "source": ["The final step is going to be scaling our data. Depending on which algorithm we use the inconsistent scales of our data will lead to certain features having a much greater effect than others."]}, {"cell_type": "code", "execution_count": 1, "id": "31241c09", "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import MinMaxScaler\nmm = MinMaxScaler()\n\nfor i,df in enumerate(both):\n    d = mm.fit_transform(both[i])\n    both[i] = pd.DataFrame(d,columns=both[i].columns)\n    \ntrain = both[0]\ntest = both[1]"]}, {"cell_type": "markdown", "id": "ed45870d", "metadata": {}, "source": ["Reassign our data to the standard variable nomenclature X, (features) y (target)."]}, {"cell_type": "code", "execution_count": 1, "id": "4e65c1d2", "metadata": {}, "outputs": [], "source": ["X = train.drop(columns=\"Survived\")\ny = train[\"Survived\"]"]}, {"cell_type": "markdown", "id": "17dd119a", "metadata": {}, "source": ["Our final fully wrangled data is ready."]}, {"cell_type": "code", "execution_count": 1, "id": "41e8db35", "metadata": {}, "outputs": [], "source": ["X.head()"]}, {"cell_type": "markdown", "id": "241f9e04", "metadata": {}, "source": ["We'll repeat the previous steps for the other imputation methods that we decided on earlier."]}, {"cell_type": "code", "execution_count": 1, "id": "e90ce3a6", "metadata": {}, "outputs": [], "source": ["from sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\n\nfor df in impute_test:\n    df[\"Fare_cats\"] = df[\"Fare_bins\"].cat.codes \n    df[\"Age_cats\"] = df[\"Age_bins\"].cat.codes.replace(-1, np.nan)\n    df[\"Embarked\"].fillna(df[\"Embarked\"].mode()[0],inplace=True)\n\nmode = [\"Deck\",\"Room\",\"Multi_room\"]\nmedian = [\"Age_cats\"]\n\nc = mode+median+list(impute.columns) # impute Column names\nc2 = mode+[n+\"_missing\" for n in mode]+median+[n+\"_missing\" for n in median]+list(impute_marker.columns) # Impute_marker column names\n\ntrans = [(\"mode\",SimpleImputer(strategy=\"most_frequent\"),mode),(\"median\",SimpleImputer(strategy=\"median\"),median)] # Estimators that we pass to our column transfomer\ntrans_m = [(\"mode\",SimpleImputer(strategy=\"most_frequent\",add_indicator=True),mode),(\"median\",SimpleImputer(strategy=\"median\",add_indicator=True),median)]\n\nt = ColumnTransformer(transformers=trans,remainder=\"passthrough\")\nt_m = ColumnTransformer(transformers=trans_m,remainder=\"passthrough\")\n\nimpute_test[1] = pd.DataFrame(t.fit_transform(impute),columns=list(dict.fromkeys(c)))\nimpute_test[2] = pd.DataFrame(t_m.fit_transform(impute_marker),columns=list(dict.fromkeys(c2)))\n\nfor df in impute_test:\n    for col in encode_cols:\n        encode(df[col])\n               \nfor i,df in enumerate(impute_test):\n    ohe_df = pd.DataFrame(ohe.fit_transform(impute_test[i][sub]))\n    ohe_df.columns = [\"Female\",\"Male\",\"C\",\"Q\",\"S\",\"Master\",\"Miss\",\"Mr\",\"Mrs\",\"Unique\"]\n    impute_test[i] = pd.concat([impute_test[i],ohe_df],axis=1)\n    \nfor df in impute_test:\n    df.drop(columns=[\"PassengerId\",\"Survived\",\"Name\",\"Ticket\",\"Sex\",\"Age\",\"Fare\",\"Embarked\",\"Title\",\"Age_bins\",\"Fare_bins\",\"Cabin\"],inplace=True)\n    \nknn_imp = KNNImputer(n_neighbors=5,weights=\"distance\",add_indicator=True)\nimpute_test[0] = pd.DataFrame(knn_imp.fit_transform(impute_test[0]),columns = list(impute_test[0].columns) + [\"Deck_missing\",\"Room_missing\",\"Multi_room_missing\",\"Age_cats_missing\"])\n    \nfor i in range(len(impute_test)):\n    d = mm.fit_transform(impute_test[i])\n    impute_test[i] = pd.DataFrame(d,columns=impute_test[i].columns)\n    \nknn_marker = impute_test[0]\nimpute = impute_test[1]\nimpute_marker = impute_test[2]\n\ndrop = X.drop(columns=[\"Deck\",\"Room\",\"Multi_room\"])\n\nimp_dict = {\"KNN\":X,\"KNN + marker\":knn_marker,\"SimpleImpute\":impute,\"SimpleImpute + marker\":impute_marker,\"Drop\":drop}"]}, {"cell_type": "markdown", "id": "4639e0a8", "metadata": {}, "source": ["# Baseline performance\nLet's choose some various alogrithms that we think might be relevant for this classification problem, loop through them and created at table based on their performances."]}, {"cell_type": "code", "execution_count": 1, "id": "b2115446", "metadata": {}, "outputs": [], "source": ["from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nalgs = [RandomForestClassifier(random_state=0),\n       GaussianNB(),\n       KNeighborsClassifier(),\n       SVC(probability=True,random_state=0),\n       XGBClassifier(random_state=0),\n       LogisticRegression(random_state=0),\n       DecisionTreeClassifier(random_state=0)]"]}, {"cell_type": "code", "execution_count": 1, "id": "85612ab9", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import cross_validate,ShuffleSplit,StratifiedKFold\n\nshuf = ShuffleSplit(n_splits=10,test_size=0.25,train_size=0.75,random_state=0)\nstratk = StratifiedKFold(n_splits=10)\nbaseline_df = pd.DataFrame()\n\nrow=0\nfor a in algs:\n    \n    results = cross_validate(a,X,y,cv=stratk,return_train_score=True)\n    \n    name = a.__class__.__name__\n    baseline_df.loc[row,\"Name\"] = name\n    baseline_df.loc[row,\"Feature_count\"] = len(X.columns)\n    baseline_df.loc[row,\"Train_score\"] = results[\"train_score\"].mean()\n    baseline_df.loc[row,\"Test_score\"] = results[\"test_score\"].mean()\n    baseline_df.loc[row,\"Time\"] = results[\"fit_time\"].mean()\n    row+=1\n    \nbaseline_df.sort_values(\"Test_score\",ascending=False)"]}, {"cell_type": "markdown", "id": "b3db1332", "metadata": {}, "source": ["From this inital test we can see that decision tree algorithms seem to perform better, but also overfit the data to a large degree, more so than the others."]}, {"cell_type": "markdown", "id": "b8d38f07", "metadata": {}, "source": ["There are different cross validation methods that we can use, lets see if any stand out."]}, {"cell_type": "code", "execution_count": 1, "id": "9b899198", "metadata": {}, "outputs": [], "source": ["cv_methods_df = pd.DataFrame()\ncvs = {\"ShuffleSplit\":ShuffleSplit(n_splits=10,test_size=0.25,train_size=0.75,random_state=0),\n       \"StratifiedKFold\":StratifiedKFold(n_splits=10)} \n\nrfc = RandomForestClassifier(random_state=0)\ni=0\nfor k,cv in cvs.items():\n       \n    results = cross_validate(rfc,X,y,cv=cv,return_train_score=True)\n    \n    name = list(cvs.keys())[i]\n    cv_methods_df.loc[i,\"cv_method\"] = name\n    cv_methods_df.loc[i,\"Train_score\"] = results[\"train_score\"].mean()\n    cv_methods_df.loc[i,\"Test_score\"] = results[\"test_score\"].mean()\n    cv_methods_df.loc[i,\"Time\"] = results[\"fit_time\"].mean()\n    i+=1\n        \ncv_methods_df.sort_values(\"Test_score\",ascending=False)"]}, {"cell_type": "markdown", "id": "e9988aa1", "metadata": {}, "source": ["Now let's see how our various imputations method ultimately performed"]}, {"cell_type": "code", "execution_count": 1, "id": "e7997a95", "metadata": {}, "outputs": [], "source": ["imp_methods = pd.DataFrame()\ni=0\nfor a in algs:\n    n=0\n    for k,df in imp_dict.items():\n\n        results = cross_validate(a,df,y,cv=stratk,return_train_score=True)\n\n        name = list(imp_dict.keys())[n]\n        imp_methods.loc[i,\"Method\"] = name\n        imp_methods.loc[i,\"Algorithm\"] = a.__class__.__name__\n        imp_methods.loc[i,\"Test_score\"] = results[\"test_score\"].mean()\n        imp_methods.loc[i,\"Time\"] = results[\"fit_time\"].mean()\n        i+=1\n        n+=1\n    \nimp_methods.groupby(\"Algorithm\").apply(lambda x: x.sort_values(\"Test_score\", ascending=False)).drop(columns=\"Algorithm\")"]}, {"cell_type": "markdown", "id": "23ac4813", "metadata": {}, "source": ["KNN looks to have have an edge compared to the other options."]}, {"cell_type": "markdown", "id": "186ad523", "metadata": {}, "source": ["# Feature elmination methods\nChoosing the the right features and the right amount of them is an important stage to get right in ML. As having too few features means you dont know enough about your data and too many means that your model will be worse at generalising for unseen data. It will have a high train score but a much lower test score."]}, {"cell_type": "markdown", "id": "509a877c", "metadata": {}, "source": ["### Different feature selection methods inlcude:\n\n**Variance thresholds**\n* Variance thresholds remove features whose values don't change much from observation to observation (i.e. their variance falls below a threshold). These features provide     little value.\n\n**Correlation thresholds**\n* Correlation thresholds remove features that are highly correlated with others (i.e. its values change very similarly to another's). These features provide redundant information\n\n**PCA**\n* Is a dimensionality reduction technique that aims to find the directions of maximal variance,it is unsupervised so ignores class labels\n\n**LDA**\n* Similar to to PCA, LDA is a linear transformation technique, however LDA attempts to find a feature subspace that maximizes class separability, and requires labled data.\n\n**Recursive Feature elimination**\n* Searches for the best combination of features and removes ones that dont perform well."]}, {"cell_type": "markdown", "id": "c7e0d93d", "metadata": {}, "source": ["I'm going to explore PCA,LDA and RFECV, so lets take a look at how these methods peform.\nOnly decision tree algorithms support RFECV so for ones that dont I set the columns to use as \"None\"."]}, {"cell_type": "code", "execution_count": 1, "id": "23190aa8", "metadata": {}, "outputs": [], "source": ["from sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_validate,ShuffleSplit\nfrom sklearn.feature_selection import RFECV\n\nFSM = [\"None\",\"RFECV\",\"PCA\",\"LDA\"]\nperformance_df = pd.DataFrame()\n\nalg_row=0\nfsm_row=0\n\nfor a in algs:\n    \n    for fsm in FSM:\n        \n        if fsm == \"None\":\n            x = X\n            \n        elif fsm == \"RFECV\":\n            try:\n                rfe = RFECV(a,scoring=\"roc_auc\")\n                x = rfe.fit_transform(X,y)   \n\n            except RuntimeError:\n                x = X\n                \n        elif fsm==\"PCA\":\n            pca = PCA(n_components=0.95,random_state=0)\n            x = pca.fit_transform(X)\n            \n        elif fsm==\"LDA\":\n            lda = LinearDiscriminantAnalysis()\n            x = lda.fit_transform(X,y)\n    \n        results = cross_validate(a,x,y,cv=stratk,return_train_score=True)\n\n        performance_df.loc[fsm_row,\"Name\"] = a.__class__.__name__\n        performance_df.loc[fsm_row,\"FSM\"] = fsm\n        performance_df.loc[fsm_row,\"Feature_count\"] = x.shape[1]\n        #performance_df.loc[fsm_row,\"Train_score\"] = results[\"train_score\"].mean()\n        performance_df.loc[fsm_row,\"Test_score\"] = results[\"test_score\"].mean()\n        performance_df.loc[fsm_row,\"Test_improvement\"] = results[\"test_score\"].mean() - baseline_df.loc[alg_row,\"Test_score\"]\n        performance_df.loc[fsm_row,\"Time\"] = results[\"fit_time\"].mean()\n        fsm_row+=1\n        \n    alg_row+=1\n    \nperformance_df.groupby(\"Name\").apply(lambda x: x.sort_values(\"Test_score\", ascending=False)).drop(columns=\"Name\")"]}, {"cell_type": "markdown", "id": "3f1f8c4b", "metadata": {}, "source": ["It seems to be that either None or RFECV perform the best.\n\nI think I'll go with RFCEV."]}, {"cell_type": "markdown", "id": "5ddb5c58", "metadata": {}, "source": ["# Hyper-parameter tuning\nThe estimators that we choose have lots of different parameters to change that will affect our models performance. We can search for the optimal combination by using sklearn's GridSearchCV which takes a grid of paramters and tries every combination of them until it finds the combination that peforms the best. This is a very computationally expensive process and will take a long time to do. "]}, {"cell_type": "code", "execution_count": 1, "id": "8a5090f9", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import GridSearchCV\n\nrandomforest = {\"n_estimators\":[100,300,500],\n                \"criterion\":[\"gini\",\"entropy\"],\n                \"max_depth\":[2,6,10,None],\n                \"random_state\":[0],\n                \"max_features\":[\"auto\",\"sqrt\",\"log2\"],\n               \"min_samples_split\":[2,5,10],\n               \"min_samples_leaf\":[1,5,10]}\nguassianNB = {}\n\nKneighbors = {\"n_neighbors\":[5,10,15],\n              \"leaf_size\":[30,35,40],}\n\nsvc = {\"gamma\":[\"scale\",\"auto\"],\n      \"kernel\":[\"linear\", \"poly\", \"rbf\", \"sigmoid\"]}\n\nxgb = {\"max_depth\":[2,4,6,8,10,None],\n      \"random_state\":[0]}\n\nlogisticregression = {\"penalty\":[\"l1\", \"l2\", \"elasticnet\", \"none\"],\n                     \"random_state\":[0],\n                     \"solver\":[\"newton-cg\",\"lbfgs\",\"liblinear\",\"sag\",\"saga\"]}\n\ndecisiontree = {\"criterion\":[\"gini\",\"entropy\"],\n               \"splitter\":[\"best\",\"random\"],\n               \"max_depth\":[2,6,10,None],\n               \"max_features\":[\"auto\",\"sqrt\",\"log2\"],\n               \"random_state\":[0],\n               \"min_samples_split\":[2,5,10],\n               \"min_samples_leaf\":[1,5,10]}\n\nparams =[randomforest,guassianNB,Kneighbors,svc,xgb,logisticregression,decisiontree]\n\nn=0\nalg_params={}\nwhile n<7:\n    for a in algs:\n        h_params = GridSearchCV(a,param_grid=params[n],scoring='roc_auc',cv=3,n_jobs=-1)\n        h_params.fit(X,y)\n        alg_params[a.__class__.__name__] = h_params.best_params_\n        n+=1\n        \nalg_params"]}, {"cell_type": "markdown", "id": "76f78e44", "metadata": {}, "source": ["Now that we have our new paramters lets have a look at how our model currently performs."]}, {"cell_type": "code", "execution_count": 1, "id": "f1f84ef1", "metadata": {}, "outputs": [], "source": ["rfc_best = RandomForestClassifier(**alg_params[\"RandomForestClassifier\"])\nnb_best = GaussianNB()\nknn_best = KNeighborsClassifier(**alg_params[\"KNeighborsClassifier\"])\nsvc_best = SVC(probability=True,**alg_params[\"SVC\"])\nxgb_best = XGBClassifier(**alg_params[\"XGBClassifier\"])\nlr_best = LogisticRegression(**alg_params[\"LogisticRegression\"])\ndt_best = DecisionTreeClassifier(**alg_params[\"DecisionTreeClassifier\"])\n\nbest_algs = [rfc_best,nb_best,knn_best,svc_best,xgb_best,lr_best,dt_best]\n\nrfe = RFECV(a,scoring=\"roc_auc\")\nrfe.fit(X,y)\nrfe_cols = X.columns[rfe.support_]\n\nfinal = pd.DataFrame()\nrow=0\nfor a in best_algs:\n    \n    results = cross_validate(a,X[rfe_cols],y,cv=stratk,return_train_score=True)\n    \n    name = a.__class__.__name__\n    final.loc[row,\"Name\"] = name\n    final.loc[row,\"Feature_count\"] = len(X[rfe_cols].columns)\n    final.loc[row,\"Train_score\"] = results[\"train_score\"].mean()\n    final.loc[row,\"Test_score\"] = results[\"test_score\"].mean()\n    final.loc[row,\"Time\"] = results[\"fit_time\"].mean()\n    row+=1\n    \nfinal.sort_values(\"Test_score\",ascending=False)"]}, {"cell_type": "markdown", "id": "0ff1778c", "metadata": {}, "source": ["# Voting classifier\nWe don't have try rely on just one model we can take the opinion of various ones using sklearn's VotingCLassifier. A collection of several models working together on a single set is called an ensemble, it works by combining the predictions from multiple machine learning algorithms.  The final output on a prediction is taken according to two different voting strategies, hard or soft. Choosing the voting paramter \"hard\" will take the majority vote of some predictions. Choosing \"soft\" will take into account the various probalities of each predcition."]}, {"cell_type": "code", "execution_count": 1, "id": "a6e4d724", "metadata": {}, "outputs": [], "source": ["from sklearn.ensemble import VotingClassifier\n\nrfe = RFECV(a,scoring=\"accuracy\")\nrfe.fit(X,y)\nrfe_cols = X.columns[rfe.support_]\n\nest = [('rfc', rfc_best), ('xgb', xgb_best),\n('knn', knn_best), ('svc',svc_best),('lr',lr_best),(\"nb\",nb_best),(\"dt\",DecisionTreeClassifier())]\n\nvc_hard = VotingClassifier(estimators=est,voting='hard')\nvc_hard_cv = cross_validate(vc_hard,X[rfe_cols],y,cv=stratk,return_train_score=True)\nvc_hard.fit(X[rfe_cols], y)\nprint(\"Voting classifier hard\",vc_hard_cv['test_score'].mean())\n\nvc_soft = VotingClassifier(estimators=est,voting='soft')\nvc_soft_cv = cross_validate(vc_soft,X[rfe_cols],y,cv=stratk,return_train_score=True)\nvc_soft.fit(X[rfe_cols], y)\nprint(\"Voting classifier soft\",vc_soft_cv['test_score'].mean())"]}, {"cell_type": "markdown", "id": "a0178ee8", "metadata": {}, "source": ["Our final score appears to be around 87%"]}, {"cell_type": "code", "execution_count": 1, "id": "85b31c6a", "metadata": {}, "outputs": [], "source": ["test_s = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntest_s['Survived'] = vc_soft.predict(test[rfe_cols]).astype(int)\n\nsubmission = test_s[['PassengerId','Survived']]\nsubmission.to_csv(\"../working/submission.csv\", index=False)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}