{"cells": [{"cell_type": "code", "execution_count": 1, "id": "144089ed", "metadata": {}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]}, {"cell_type": "markdown", "id": "178e1cc9", "metadata": {}, "source": ["# LIME - Local Interpretable Model-Agnostic Explanations "]}, {"cell_type": "markdown", "id": "3bf032cb", "metadata": {}, "source": ["![](https://miro.medium.com/max/2000/1*Lo4tT2xLY7cEnTzTSb27Qg.jpeg)"]}, {"cell_type": "markdown", "id": "a59abd62", "metadata": {}, "source": ["***The interpretation of machine learning models has become of prime importance nowadays. The lime stands for local interpretable model agnostic explanations takes any machine learning models as input and generates explanations about feature contributions in making a prediction. It assumes that is a black box model which means that it does not know the inner workings of models and generates explanation based on this assumption. ***"]}, {"cell_type": "markdown", "id": "0c052da1", "metadata": {}, "source": ["**Goal of this notebook is to share the basic working example of LIME on the data  and show the interpretations and visualizations.**"]}, {"cell_type": "code", "execution_count": 1, "id": "11a8d7df", "metadata": {}, "outputs": [], "source": ["import lime\nfrom lime import lime_tabular\nimport lightgbm as lgb\nfrom xgboost import XGBClassifier, XGBRFRegressor\nfrom sklearn.model_selection import KFold, train_test_split"]}, {"cell_type": "code", "execution_count": 1, "id": "726aecd6", "metadata": {}, "outputs": [], "source": ["df_train = pd.read_csv(\"../input/song-popularity-prediction/train.csv\")\ndf_test = pd.read_csv(\"../input/song-popularity-prediction/test.csv\")\nsubmission = pd.read_csv(\"../input/song-popularity-prediction/sample_submission.csv\")"]}, {"cell_type": "code", "execution_count": 1, "id": "29327b30", "metadata": {}, "outputs": [], "source": ["df_train.drop('id', axis=1,inplace=True)\ndf_test.drop('id', axis=1,inplace=True)"]}, {"cell_type": "markdown", "id": "a8b55fee", "metadata": {}, "source": ["**Lets drop na values as LIME has some issues handling NA values**"]}, {"cell_type": "code", "execution_count": 1, "id": "be7f9439", "metadata": {}, "outputs": [], "source": ["df_train.dropna(inplace=True)\ndf_train.reset_index(drop=True, inplace=True)\n\ndf_test.dropna(inplace=True)\ndf_test.reset_index(drop=True, inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "7dad73df", "metadata": {}, "outputs": [], "source": ["df_train.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "db3943c8", "metadata": {}, "outputs": [], "source": ["FEATURES = [\n    \"song_duration_ms\",\n    \"acousticness\",\n    \"danceability\",\n    \"energy\",\n    \"instrumentalness\",\n    \"key\",\n    \"liveness\",\n    \"loudness\",\n    \"audio_mode\",\n    \"speechiness\",\n    \"tempo\",\n    \"time_signature\",\n    \"audio_valence\",\n]"]}, {"cell_type": "code", "execution_count": 1, "id": "d695a0c0", "metadata": {}, "outputs": [], "source": ["len(FEATURES)"]}, {"cell_type": "code", "execution_count": 1, "id": "367aff78", "metadata": {}, "outputs": [], "source": ["dep_var = 'song_popularity'"]}, {"cell_type": "code", "execution_count": 1, "id": "ae9d7e15", "metadata": {}, "outputs": [], "source": ["X = df_train[FEATURES]\ny = df_train[dep_var]"]}, {"cell_type": "code", "execution_count": 1, "id": "f1a1a02a", "metadata": {}, "outputs": [], "source": ["X.shape,y.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "cec71465", "metadata": {}, "outputs": [], "source": ["#param_lgb are taken from https://www.kaggle.com/venkatkumar001/spp2-lgbm , with some modification\n\nparams_lgb = {\n    \"task\": \"train\",\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"binary\",\n    'subsample': 0.95312,\n    'learning_rate': 0.001635,\n    \"max_depth\": 5,\n    \"feature_fraction\": 0.2256038826485174,\n    \"bagging_fraction\": 0.7705303688019942,\n    \"min_child_samples\": 290,\n    \"reg_alpha\": 14.68267919457715,\n    \"reg_lambda\": 66.156,\n    \"max_bin\": 772,\n    \"min_data_per_group\": 177,\n    \"bagging_freq\": 1,\n    \"cat_smooth\": 96,\n    \"cat_l2\": 17,\n    \"verbosity\": -1,\n    'random_state':42,\n    'n_estimators':8000,\n    'colsample_bytree':0.1107\n    }"]}, {"cell_type": "markdown", "id": "3faa2f1e", "metadata": {}, "source": ["# Model Train"]}, {"cell_type": "code", "execution_count": 1, "id": "c35f0f07", "metadata": {}, "outputs": [], "source": ["lgb_train = lgb.Dataset(X, y)\n\nmodel = lgb.train(params=params_lgb,\n                      train_set=lgb_train,\n                      verbose_eval=False)"]}, {"cell_type": "code", "execution_count": 1, "id": "e10bd7b4", "metadata": {}, "outputs": [], "source": ["# this is required as LIME requires class probabilities in case of classification example\n# LightGBM directly returns probability for class 1 by default \ndef prob(data):\n    return np.array(list(zip(1-model.predict(data),model.predict(data))))"]}, {"cell_type": "markdown", "id": "3e65a005", "metadata": {}, "source": ["# Model interpretation"]}, {"cell_type": "markdown", "id": "35eb5800", "metadata": {}, "source": ["To start explaining the model, you first need to import the LIME library and create a tabular explainer object. It expects the following parameters:\n* **training_data** \u2013 our training data generated with train/test split. It must be in a Numpy array format.\n* **feature_names** \u2013 column names from the training set\n* **class_names** \u2013 distinct classes from the target variable\n* **mode** \u2013 type of problem you\u2019re solving (classification in this case)"]}, {"cell_type": "code", "execution_count": 1, "id": "9abeb93e", "metadata": {}, "outputs": [], "source": ["explainer = lime.lime_tabular.LimeTabularExplainer(\n    X[model.feature_name()].astype(int).values, \n    feature_names=model.feature_name(),\n    training_labels=df_train[dep_var],\n    mode='classification')"]}, {"cell_type": "markdown", "id": "75c6afc2", "metadata": {}, "source": ["Nou we call the explain_instance function of the explainer object to, well, explain the prediction. The following parameters are required:\n* **data_row** \u2013 a single observation from the dataset\n* **predict_fn** \u2013 a function used to make predictions. The predict_proba from the model is a great option because it shows probabilities"]}, {"cell_type": "markdown", "id": "9cee0a43", "metadata": {}, "source": ["The **show_in_notebook** function shows the prediction interpretation in the notebook environment\n\nThis is how the explanations look for some of the training data"]}, {"cell_type": "code", "execution_count": 1, "id": "b00f33dc", "metadata": {}, "outputs": [], "source": ["# asking for explanation for LIME model\ni = 3\nexp = explainer.explain_instance(X.loc[i,FEATURES].astype(int).values, prob)\nexp.show_in_notebook(show_table=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "c7de0869", "metadata": {}, "outputs": [], "source": ["# asking for explanation for LIME model\ni = 0\nexp = explainer.explain_instance(X.loc[i,FEATURES].astype(int).values, prob)\nexp.show_in_notebook(show_table=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "4264830d", "metadata": {}, "outputs": [], "source": ["# asking for explanation for LIME model\ni = 15000\nexp = explainer.explain_instance(X.loc[i,FEATURES].astype(int).values, prob)\nexp.show_in_notebook(show_table=True)"]}, {"cell_type": "markdown", "id": "5568f2b9", "metadata": {}, "source": ["# Continue with the more examples !!!! Have fun . Upvote will be appreciated :)"]}, {"cell_type": "markdown", "id": "7bf54a0a", "metadata": {}, "source": ["# Conclusion"]}, {"cell_type": "markdown", "id": "f0e3769a", "metadata": {}, "source": ["**Interpreting machine learning models is simple using LIME. It provides you with a great way of explaining what\u2019s going on . You don\u2019t have to worry about data visualization, as the LIME library handles that for you.**"]}, {"cell_type": "markdown", "id": "8ba1283e", "metadata": {}, "source": ["# Happy Learning !!!"]}, {"cell_type": "markdown", "id": "f572c460", "metadata": {}, "source": ["**References:** \n* https://www.youtube.com/watch?v=CY3t11vuuOM\n* https://coderzcolumn.com/tutorials/machine-learning/how-to-use-lime-to-understand-sklearn-models-predictions\n* https://towardsdatascience.com/decrypting-your-machine-learning-model-using-lime-5adc035109b5\n* https://towardsdatascience.com/lime-how-to-interpret-machine-learning-models-with-python-94b0e7e4432e\n* https://coderzcolumn.com/tutorials/machine-learning/how-to-use-lime-to-understand-sklearn-models-predictions"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}