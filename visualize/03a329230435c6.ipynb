{"cells": [{"cell_type": "markdown", "id": "f70dc0b6", "metadata": {}, "source": ["<h1><center>Pawpular : InDepth EDA + Understanding + Model + W&B</center></h1>\n                                                      \n<center><img src = \"https://www.petfinder.my/images/logo-575x100.png\" width = \"750\" height = \"500\"/></center>                                                                                               "]}, {"cell_type": "markdown", "id": "6598fc96", "metadata": {}, "source": ["<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Contents</center></h2>"]}, {"cell_type": "markdown", "id": "f30219e9", "metadata": {}, "source": ["1. [Competition Overview](#competition-overview)  \n2. [Libraries](#libraries)  \n3. [Weights and Biases](#weights-and-biases)   \n4.[Global Config](#global-config)\n5. [Load Datasets](#load-datasets)  \n6. [Tabular Exploration](#tabular-exploration)  \n7. [Distribution Plots](#distribution-plots)\n8. [Feature Wise Analysis](#feature-wise-analysis)\n9. [Pawpularity Score Wise Images](#pawpularity-score-wise-images)  \n10. [YOLO V5 Object Detection](#yolo-v5-object-detection)\n10. [Dataset and Augmentations](#dataset-and-augmentations)\n11. [Efficientnet Model and Understanding](#efficientnet-model-and-understanding)  \n12. [WandB Dashboard](#wandb-dashboard)  \n13. [References](#references)"]}, {"cell_type": "markdown", "id": "109998ab", "metadata": {}, "source": ["<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:maroon; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>If you find this notebook useful, do give me an upvote, it helps to keep up my motivation. This notebook will be updated frequently so keep checking for furthur developments.</center></h3>"]}, {"cell_type": "markdown", "id": "87687879", "metadata": {}, "source": ["<a id=\"competition-overview\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Competition Overview</center></h2>"]}, {"cell_type": "markdown", "id": "0642dd69", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Description</span>**\n\nIn this competition, you\u2019ll analyze raw images and metadata to predict the \u201cPawpularity\u201d of pet photos.   \n  \nYou'll train and test your model on PetFinder.my's thousands of pet profiles. Winning versions will offer accurate recommendations that will improve animal welfare.\n  \nIf successful, your solution will be adapted into AI tools that will guide shelters and rescuers around the world to improve the appeal of their pet profiles, automatically enhancing photo quality and recommending composition improvements.   \n  \nAs a result, stray dogs and cats can find their \"furever\" homes much faster. With a little assistance from the Kaggle community, many precious lives could be saved and more happy families created.\n\n---"]}, {"cell_type": "markdown", "id": "c1133fd3", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Evaluation Criteria</span>**\n\nSubmissions are scored on the **Root mean squared error**. \n  \n- Root Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors). \n- Residuals are a measure of how far from the regression line data points are; RMSE is a measure of how spread out these residuals are. \n- In other words, it tells you how concentrated the data is around the line of best fit. - Root mean square error is commonly used in climatology, forecasting, and regression analysis to verify experimental results.\n\n**<span style=\"color:orange;\">Resources to learn and understand RMSE:</span>**\n- [Root-mean-square deviation](https://en.wikipedia.org/wiki/Root-mean-square_deviation)\n- [Root-Mean-Squared Error](https://www.sciencedirect.com/topics/engineering/root-mean-squared-error)\n- [What does RMSE really mean?](https://towardsdatascience.com/what-does-rmse-really-mean-806b65f2e48e)\n- [\nMean Absolute Error (MAE) and Root Mean Squared Error (RMSE)](http://www.eumetrain.org/data/4/451/english/msg/ver_cont_var/uos3/uos3_ko1.htm)\n- [What is Root Mean Square Error (RMSE)?](https://www.statisticshowto.com/probability-and-statistics/regression-analysis/rmse-root-mean-square-error/)"]}, {"cell_type": "markdown", "id": "458d6c35", "metadata": {}, "source": ["<a id=\"libraries\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Libraries</center></h2>"]}, {"cell_type": "code", "execution_count": 1, "id": "677a1732", "metadata": {}, "outputs": [], "source": ["%%sh\npip install -q pytorch-lightning==1.1.8\npip install -q timm\npip install -q albumentations\npip install -q --upgrade wandb"]}, {"cell_type": "code", "execution_count": 1, "id": "5a035ffb", "metadata": {}, "outputs": [], "source": ["import gc\nimport os\nimport glob\nimport sys\nimport cv2\nimport imageio\nimport joblib\nimport math\nimport random\nimport wandb\nimport math\n\nimport numpy as np\nimport pandas as pd\n\nfrom scipy.stats import kstest\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom statsmodels.graphics.gofplots import qqplot\n\nplt.rcParams.update({'font.size': 18})\nplt.style.use('fivethirtyeight')\n\nimport seaborn as sns\nimport matplotlib\n\nfrom termcolor import colored\n\nfrom multiprocessing import cpu_count\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.stats import pearsonr\n\nimport timm\nimport torch\nimport transformers\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import r2_score, mean_squared_error\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n\nfrom albumentations import (\n    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n)\nfrom albumentations.pytorch import ToTensorV2\n\nimport warnings\nwarnings.simplefilter('ignore')\n\n# Activate pandas progress apply bar\ntqdm.pandas()"]}, {"cell_type": "code", "execution_count": 1, "id": "e1f76fbc", "metadata": {}, "outputs": [], "source": ["# Wandb Login\nimport wandb\nwandb.login()"]}, {"cell_type": "markdown", "id": "814a2c3b", "metadata": {}, "source": ["<a id=\"weights-and-biases\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Weights and Biases</center></h2>"]}, {"cell_type": "markdown", "id": "a6a80285", "metadata": {}, "source": ["<center><img src = \"https://i.imgur.com/1sm6x8P.png\" width = \"750\" height = \"500\"/></center>  "]}, {"cell_type": "markdown", "id": "3e06659c", "metadata": {}, "source": ["**Weights & Biases** is the machine learning platform for developers to build better models faster. \n\nYou can use W&B's lightweight, interoperable tools to \n- quickly track experiments, \n- version and iterate on datasets, \n- evaluate model performance, \n- reproduce models, \n- visualize results and spot regressions, \n- and share findings with colleagues. \n\nSet up W&B in 5 minutes, then quickly iterate on your machine learning pipeline with the confidence that your datasets and models are tracked and versioned in a reliable system of record.\n\nIn this notebook I will use Weights and Biases's amazing features to perform wonderful visualizations and logging seamlessly. "]}, {"cell_type": "markdown", "id": "5921d18f", "metadata": {}, "source": ["<a id=\"global-config\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Global Config</center></h2>"]}, {"cell_type": "code", "execution_count": 1, "id": "1109abaa", "metadata": {}, "outputs": [], "source": ["class config:\n    DIRECTORY_PATH = \"../input/petfinder-pawpularity-score\"\n    TRAIN_CSV_PATH = DIRECTORY_PATH + \"/train.csv\"\n    TEST_CSV_PATH = DIRECTORY_PATH + \"/test.csv\"\n    \n    SEED = 42\n    \nConfig = dict(\n    NFOLDS = 5,\n    EPOCHS = 1,\n    LR = 2e-4,\n    IMG_SIZE = (224, 224),\n    MODEL_NAME = 'tf_efficientnet_b6_ns',\n    DR_RATE = 0.35,\n    NUM_LABELS = 1,\n    TRAIN_BS = 32,\n    VALID_BS = 16,\n    min_lr = 1e-6,\n    T_max = 20,\n    T_0 = 25,\n    NUM_WORKERS = 4,\n    infra = \"Kaggle\",\n    competition = 'petfinder',\n    _wandb_kernel = 'neuracort',\n    wandb = False\n)"]}, {"cell_type": "code", "execution_count": 1, "id": "00c17bd7", "metadata": {}, "outputs": [], "source": ["# wandb config\nWANDB_CONFIG = {\n     'competition': 'PetFinder', \n              '_wandb_kernel': 'neuracort'\n    }"]}, {"cell_type": "code", "execution_count": 1, "id": "4befeeef", "metadata": {}, "outputs": [], "source": ["wandb_logger = WandbLogger(project='pytorchlightning', group='vision', job_type='train', \n                           anonymous='allow', config=Config)"]}, {"cell_type": "code", "execution_count": 1, "id": "27a2ef4c", "metadata": {}, "outputs": [], "source": ["def set_seed(seed=config.SEED):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \nset_seed()"]}, {"cell_type": "markdown", "id": "93c63651", "metadata": {}, "source": ["<a id=\"load-datasets\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Load Datasets</center></h2>"]}, {"cell_type": "markdown", "id": "6b3fc7c8", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Understanding the Structure of the Dataset</span>**"]}, {"cell_type": "markdown", "id": "07c4be42", "metadata": {}, "source": ["> ### **<span style=\"color:orange;\">Goal of Competition</span>**\n> In this competition, your task is to predict engagement with a pet's profile based on the photograph for that profile. You are also provided with hand-labelled metadata for each photo. The dataset for this competition therefore comprises both images and tabular data.\n> \n> ### **<span style=\"color:orange;\">How Pawpularity Score Is Derived</span>**\n> The Pawpularity Score is derived from each pet profile's page view statistics at the listing pages, using an algorithm that normalizes the traffic data across different pages, platforms (web & mobile) and various metrics.  \n> Duplicate clicks, crawler bot accesses and sponsored profiles are excluded from the analysis.\n>\n>---"]}, {"cell_type": "markdown", "id": "b32d6fc2", "metadata": {}, "source": ["> ### **<span style=\"color:orange;\">Training Data</span>**\n> - train/ - Folder containing training set photos of the form {id}.jpg, where {id} is a unique Pet Profile ID.\n> - train.csv - Metadata (described below) for each photo in the training set as well as the target, the photo's Pawpularity score. The Id column gives the photo's unique Pet Profile ID corresponding the photo's file name.\n> \n> ### **<span style=\"color:orange;\">Example Test Data</span>**\n> In addition to the training data, we include some randomly generated example test data to help you author submission code. When your submitted notebook is scored, this example data will be replaced by the actual test data (including the sample submission).\n> \n> - test/ - Folder containing randomly generated images in a format similar to the training set photos. The actual test data comprises about 6800 pet photos similar to the training set photos.\n> - test.csv - Randomly generated metadata similar to the training set metadata.\n> - sample_submission.csv - A sample submission file in the correct format.\n>\n>---\n\n> ### **<span style=\"color:orange;\">Photo Metadata</span>**\n> The train.csv and test.csv files contain metadata for photos in the training set and test set, respectively. Each pet photo is labeled with the value of 1 (Yes) or 0 (No) for each of the following features:\n> \n> - Focus - Pet stands out against uncluttered background, not too close / far.\n> - Eyes - Both eyes are facing front or near-front, with at least 1 eye / pupil decently clear.\n> - Face - Decently clear face, facing front or near-front.\n> Near - Single pet taking up significant portion of photo (roughly over 50% of photo width or height).\n> - Action - Pet in the middle of an action (e.g., jumping).\n> - Accessory - Accompanying physical or digital accessory / prop (i.e. toy, digital sticker), excluding collar and leash.\n> - Group - More than 1 pet in the photo.\n> - Collage - Digitally-retouched photo (i.e. with digital photo frame, combination of multiple photos).\n> - Human - Human in the photo.\n> - Occlusion - Specific undesirable objects blocking part of the pet (i.e. human, cage or fence). Note that not all blocking objects are considered occlusion.\n> - Info - Custom-added text or labels (i.e. pet name, description).\n> - Blur - Noticeably out of focus or noisy, especially for the pet\u2019s eyes and face. For Blur entries, \u201cEyes\u201d column is always set to 0.\n>\n>---"]}, {"cell_type": "code", "execution_count": 1, "id": "2c37f9db", "metadata": {}, "outputs": [], "source": ["# Efficient Data Types\ndtype = {\n    'Id': 'string',\n    'Subject Focus': np.uint8, 'Eyes': np.uint8, 'Face': np.uint8, 'Near': np.uint8,\n    'Action': np.uint8, 'Accessory': np.uint8, 'Group': np.uint8, 'Collage': np.uint8,\n    'Human': np.uint8, 'Occlusion': np.uint8, 'Info': np.uint8, 'Blur': np.uint8,\n    'Pawpularity': np.uint8,\n}\n\ntrain = pd.read_csv(config.TRAIN_CSV_PATH, dtype=dtype)\ntest = pd.read_csv(config.TEST_CSV_PATH, dtype=dtype)"]}, {"cell_type": "markdown", "id": "7bccdc75", "metadata": {}, "source": ["<a id=\"tabular-exploration\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Tabular Exploration</center></h2>"]}, {"cell_type": "markdown", "id": "b6f8b265", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Train Head</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "7ef542fd", "metadata": {}, "outputs": [], "source": ["train.head()"]}, {"cell_type": "markdown", "id": "8c9bc504", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Test Head</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "0ecd3837", "metadata": {}, "outputs": [], "source": ["test.head()"]}, {"cell_type": "markdown", "id": "42d4c016", "metadata": {}, "source": ["### **<span style=\"color:orange;\">Train Info</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "5f1f26e2", "metadata": {}, "outputs": [], "source": ["train.info()"]}, {"cell_type": "markdown", "id": "ea6f1c78", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Test Info</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "aec6fc01", "metadata": {}, "outputs": [], "source": ["test.info()"]}, {"cell_type": "markdown", "id": "70db0294", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Dataset Size</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "9760ab65", "metadata": {}, "outputs": [], "source": ["print(f\"Training Dataset Shape: {colored(train.shape, 'yellow')}\")\nprint(f\"Test Dataset Shape: {colored(test.shape, 'yellow')}\")"]}, {"cell_type": "markdown", "id": "813a5a0a", "metadata": {}, "source": ["<a id=\"distribution-plots\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Distribution Plots</center></h2>"]}, {"cell_type": "code", "execution_count": 1, "id": "549075b0", "metadata": {}, "outputs": [], "source": ["# Add File path to Train\ndef get_image_file_path(image_id):\n    return f'/kaggle/input/petfinder-pawpularity-score/train/{image_id}.jpg'\n\ntrain['file_path'] = train['Id'].apply(get_image_file_path)"]}, {"cell_type": "code", "execution_count": 1, "id": "4d0f9cae", "metadata": {}, "outputs": [], "source": ["widths = []\nheights = []\nratios = []\nfor file_path in tqdm(train['file_path']):\n    image = imageio.imread(file_path)\n    h, w, _ = image.shape\n    heights.append(h)\n    widths.append(w)\n    ratios.append(w / h)"]}, {"cell_type": "markdown", "id": "f676135c", "metadata": {}, "source": ["## **<span style=\"color:orange;\"> Images Height and Width Distribution</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "2c1d38c2", "metadata": {}, "outputs": [], "source": ["# Images Height and Width Distribution\nprint(colored('Width Statistics', 'yellow'))\ndisplay(pd.Series(widths).describe())\nprint()\n\nprint(colored('Height Statistics', 'yellow'))\ndisplay(pd.Series(heights).describe())\nprint()\n\nplt.figure(figsize=(15,8))\nplt.title(f'Images Height and Width Distribution', size=24)\nplt.hist(heights, bins=32, label='Image Heights')\nplt.hist(widths, bins=32, label='Image Widths')\nplt.legend(prop={'size': 16})\nplt.show()"]}, {"cell_type": "markdown", "id": "41d4fca9", "metadata": {}, "source": ["The image width to height ratio have a mean below zero and a peak on 0.75, pictures thus tend to be taken vertically, not horizontally."]}, {"cell_type": "markdown", "id": "915d3cb0", "metadata": {}, "source": ["## **<span style=\"color:orange;\"> Images Ratio Distribution</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "d1d2c38d", "metadata": {}, "outputs": [], "source": ["# Images Ratio Distribution\nprint(colored('Ratio Statistics', 'yellow'))\ndisplay(pd.Series(ratios).describe())\nprint()\n\nplt.figure(figsize=(15,8))\nplt.title(f'Images Ratio Distribution', size=24)\nplt.hist(ratios, bins=16, label='Image Heights')\nplt.legend(prop={'size': 16})\nplt.show()"]}, {"cell_type": "markdown", "id": "466e184a", "metadata": {}, "source": ["## **<span style=\"color:orange;\"> Pawpularity Score Distribution</span>**\nThe pawpularity score is centered around 40 and has a peak on 0 and 100."]}, {"cell_type": "code", "execution_count": 1, "id": "e0c6a6b8", "metadata": {}, "outputs": [], "source": ["# Pawpularity Score Distribution\nprint(colored('Pawpularity Statistics', 'yellow'))\ndisplay(train['Pawpularity'].describe())\nprint()\n\nplt.figure(figsize=(15,8))\nplt.title('Train Data Pawpularity Score Distribution', size=24)\nplt.hist(train['Pawpularity'], bins=32)\nplt.show()"]}, {"cell_type": "markdown", "id": "f3971709", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Quantile-Quantile plot of Pawpularity distribution</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "e09c9a94", "metadata": {}, "outputs": [], "source": ["fig = plt.figure()\nqqplot(train['Pawpularity'], line='s')\nplt.title('Quantile-Quantile plot of Pawpularity distribution', \n          fontsize=20, fontweight='bold')\nplt.show()"]}, {"cell_type": "markdown", "id": "ab9fd504", "metadata": {}, "source": ["We notice the deviation at this QQPlot which seems to indicate a non-Gaussian distribution. We will check with the Kolmogorov-Smirnov test (Shapiro-Wilks is not suitable for a dataset greater than 5000 items)."]}, {"cell_type": "code", "execution_count": 1, "id": "cf301e1d", "metadata": {}, "outputs": [], "source": ["# Kolmogorov-Smirnov test with Scipy\nstat, p = kstest(train['Pawpularity'],'norm')\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\n# interpret\nalpha = 0.05\nif p > alpha:\n    print(f'Sample looks Gaussian (fail to reject H0 at {int(alpha*100)}% test level)')\nelse:\n    print(f'Sample does not look Gaussian (reject H0 at {int(alpha*100)}% test level)')"]}, {"cell_type": "markdown", "id": "daec3163", "metadata": {}, "source": ["The test clearly indicates that the distribution does not follow a Gaussian law. It will therefore be important to normalize the data according to the modeling chosen."]}, {"cell_type": "markdown", "id": "e138ecbc", "metadata": {}, "source": ["<a id=\"feature-wise-analysis\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Feature Wise Analysis</center></h2>"]}, {"cell_type": "markdown", "id": "675c5763", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Subject Focus</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "ae3e85b8", "metadata": {}, "outputs": [], "source": ["sns.boxplot(data=train, x='Subject Focus', y='Pawpularity')\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "6b937159", "metadata": {}, "outputs": [], "source": ["sns.histplot(train, x=\"Pawpularity\", hue=\"Subject Focus\", kde=True)\nplt.show()"]}, {"cell_type": "markdown", "id": "a53cfa91", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "id": "22526145", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Eyes</span>**\n"]}, {"cell_type": "code", "execution_count": 1, "id": "ef9740dc", "metadata": {}, "outputs": [], "source": ["sns.boxplot(data=train, x='Eyes', y='Pawpularity')\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "76e4d640", "metadata": {}, "outputs": [], "source": ["sns.histplot(train, x=\"Pawpularity\", hue=\"Eyes\", kde=True)\nplt.show()"]}, {"cell_type": "markdown", "id": "cb2a234d", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "id": "ac610c90", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Face</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "db6f726a", "metadata": {}, "outputs": [], "source": ["sns.boxplot(data=train, x='Face', y='Pawpularity')\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "717c5c44", "metadata": {}, "outputs": [], "source": ["sns.histplot(train, x=\"Pawpularity\", hue=\"Face\", kde=True)\nplt.show()"]}, {"cell_type": "markdown", "id": "f8b68e8f", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "id": "cc907ad7", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Near</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "b88e60c5", "metadata": {}, "outputs": [], "source": ["sns.boxplot(data=train, x='Near', y='Pawpularity')\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "6b25f1fa", "metadata": {}, "outputs": [], "source": ["sns.histplot(train, x=\"Pawpularity\", hue=\"Near\", kde=True)\nplt.show()"]}, {"cell_type": "markdown", "id": "4cd23a68", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "id": "a64aeaca", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Action</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "0e903554", "metadata": {}, "outputs": [], "source": ["sns.boxplot(data=train, x='Action', y='Pawpularity')\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "d70fbd8c", "metadata": {}, "outputs": [], "source": ["sns.histplot(train, x=\"Pawpularity\", hue=\"Action\", kde=True)\nplt.show()"]}, {"cell_type": "markdown", "id": "7d3a40c4", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "id": "c667ee20", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Accessory</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "10390344", "metadata": {}, "outputs": [], "source": ["sns.boxplot(data=train, x='Accessory', y='Pawpularity')\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "b4d930b1", "metadata": {}, "outputs": [], "source": ["sns.histplot(train, x=\"Pawpularity\", hue=\"Accessory\", kde=True)\nplt.show()"]}, {"cell_type": "markdown", "id": "0ea13a92", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "id": "32dfc607", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Group</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "4913253b", "metadata": {}, "outputs": [], "source": ["sns.boxplot(data=train, x='Group', y='Pawpularity')\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "157fe7d7", "metadata": {}, "outputs": [], "source": ["sns.histplot(train, x=\"Pawpularity\", hue=\"Group\", kde=True)\nplt.show()"]}, {"cell_type": "markdown", "id": "98c3d92f", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "id": "a047ef4a", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Collage</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "fc6591ca", "metadata": {}, "outputs": [], "source": ["sns.boxplot(data=train, x='Collage', y='Pawpularity')\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "7716316a", "metadata": {}, "outputs": [], "source": ["sns.histplot(train, x=\"Pawpularity\", hue=\"Collage\", kde=True)\nplt.show()"]}, {"cell_type": "markdown", "id": "4fa90a96", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "id": "1de4c1f5", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Human</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "6ba7de17", "metadata": {}, "outputs": [], "source": ["sns.boxplot(data=train, x='Human', y='Pawpularity')\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "2c7c7d72", "metadata": {}, "outputs": [], "source": ["sns.histplot(train, x=\"Pawpularity\", hue=\"Human\", kde=True)\nplt.show()"]}, {"cell_type": "markdown", "id": "54443745", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "id": "3a7fea3a", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Occlusion</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "0dae99cb", "metadata": {}, "outputs": [], "source": ["sns.boxplot(data=train, x='Occlusion', y='Pawpularity')\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "b1251602", "metadata": {}, "outputs": [], "source": ["sns.histplot(train, x=\"Pawpularity\", hue=\"Occlusion\", kde=True)\nplt.show()"]}, {"cell_type": "markdown", "id": "9846b6e6", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "id": "31566743", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Info</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "8e3aa5b4", "metadata": {}, "outputs": [], "source": ["sns.boxplot(data=train, x='Info', y='Pawpularity')\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "3d733d4d", "metadata": {}, "outputs": [], "source": ["sns.histplot(train, x=\"Pawpularity\", hue=\"Info\", kde=True)\nplt.show()"]}, {"cell_type": "markdown", "id": "4cac12c8", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "id": "7bf7eab8", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Blur</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "28c2e854", "metadata": {}, "outputs": [], "source": ["sns.boxplot(data=train, x='Blur', y='Pawpularity')\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "3a7db826", "metadata": {}, "outputs": [], "source": ["sns.histplot(train, x=\"Pawpularity\", hue=\"Blur\", kde=True)\nplt.show()"]}, {"cell_type": "markdown", "id": "84e0f479", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "id": "4671abde", "metadata": {}, "source": ["<a id=\"pawpularity-score-wise-images\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Pawpularity Score Wise Images</center></h2>"]}, {"cell_type": "code", "execution_count": 1, "id": "301cdedb", "metadata": {}, "outputs": [], "source": ["def pawpularity_pics(df, num_images, desired_pawpularity, random_state):\n    \n    '''The pawpularity_pics() function accepts 4 parameters: df is a dataframe, \n    num_images is the number of images you want displayed, desired_pawpularity \n    is the pawpularity score of pics you want to see, and random state ensures reproducibility.'''\n    \n    #how many images to display\n    num_images = num_images\n    \n    #set the rample state for the sampling for reproducibility\n    random_state = random_state\n    \n    #filter the train_df on the desired_pawpularity and use .sample() to get a sample\n    random_sample = df[df[\"Pawpularity\"] == desired_pawpularity].sample(num_images, random_state=random_state).reset_index(drop=True)\n    \n    #The for loop goes as many loops as specified by the num_images\n    for x in range(num_images):\n        #start from the id in the dataframe\n        image_path_stem = random_sample.iloc[x]['Id']\n        root = '../input/petfinder-pawpularity-score/train/'\n        extension = '.jpg'\n        image_path = root + str(image_path_stem) + extension\n         \n        #get the pawpularity to confirm it worked\n        pawpularity_by_id = random_sample.iloc[x]['Pawpularity']\n    \n        #use plt.imread() to read in the image file\n        image_array = plt.imread(image_path)\n        \n        #make a subplot space that is 1 down and num_images across\n        plt.subplot(1, num_images, x+1)\n        #title is the pawpularity score from the id\n        title = pawpularity_by_id\n        plt.title(title) \n        #turn off gridlines\n        plt.axis('off')\n        \n        #then plt.imshow() can display it for you\n        plt.imshow(image_array)\n\n    plt.show()\n    plt.close()"]}, {"cell_type": "markdown", "id": "31850dcf", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Pawpularity = 10</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "4d8bc4ce", "metadata": {}, "outputs": [], "source": ["df = train\nnum_images = 5\ndesired_pawpularity = 10\nrandom_state = 1\npawpularity_pics(df, num_images, desired_pawpularity, random_state)"]}, {"cell_type": "markdown", "id": "08450e81", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Pawpularity = 20</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "bd870971", "metadata": {}, "outputs": [], "source": ["df = train\nnum_images = 5\ndesired_pawpularity = 20\nrandom_state = 1\npawpularity_pics(df, num_images, desired_pawpularity, random_state)"]}, {"cell_type": "markdown", "id": "b9faec01", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Pawpularity = 30</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "0fd3790f", "metadata": {}, "outputs": [], "source": ["df = train\nnum_images = 5\ndesired_pawpularity = 30\nrandom_state = 1\npawpularity_pics(df, num_images, desired_pawpularity, random_state)"]}, {"cell_type": "markdown", "id": "71365317", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Pawpularity = 40</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "d943d8a9", "metadata": {}, "outputs": [], "source": ["df = train\nnum_images = 5\ndesired_pawpularity = 40\nrandom_state = 1\npawpularity_pics(df, num_images, desired_pawpularity, random_state)"]}, {"cell_type": "markdown", "id": "a22f895b", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Pawpularity = 50</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "ad1960a3", "metadata": {}, "outputs": [], "source": ["df = train\nnum_images = 5\ndesired_pawpularity = 50\nrandom_state = 1\npawpularity_pics(df, num_images, desired_pawpularity, random_state)"]}, {"cell_type": "markdown", "id": "f4fdabab", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Pawpularity = 60</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "57a869d1", "metadata": {}, "outputs": [], "source": ["df = train\nnum_images = 5\ndesired_pawpularity = 60\nrandom_state = 1\npawpularity_pics(df, num_images, desired_pawpularity, random_state)"]}, {"cell_type": "markdown", "id": "1bb1e3e9", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Pawpularity = 70</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "d44bd682", "metadata": {}, "outputs": [], "source": ["df = train\nnum_images = 5\ndesired_pawpularity = 70\nrandom_state = 1\npawpularity_pics(df, num_images, desired_pawpularity, random_state)"]}, {"cell_type": "markdown", "id": "12c730b9", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Pawpularity = 80</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "863f0abd", "metadata": {}, "outputs": [], "source": ["df = train\nnum_images = 5\ndesired_pawpularity = 80\nrandom_state = 1\npawpularity_pics(df, num_images, desired_pawpularity, random_state)"]}, {"cell_type": "markdown", "id": "de4a126e", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Pawpularity = 90</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "4d5d2410", "metadata": {}, "outputs": [], "source": ["df = train\nnum_images = 5\ndesired_pawpularity = 90\nrandom_state = 1\npawpularity_pics(df, num_images, desired_pawpularity, random_state)"]}, {"cell_type": "markdown", "id": "52432945", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Pawpularity = 100</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "46fb5580", "metadata": {}, "outputs": [], "source": ["df = train\nnum_images = 5\ndesired_pawpularity = 100\nrandom_state = 1\npawpularity_pics(df, num_images, desired_pawpularity, random_state)"]}, {"cell_type": "markdown", "id": "f93afa0c", "metadata": {}, "source": ["---"]}, {"cell_type": "code", "execution_count": 1, "id": "c172205f", "metadata": {}, "outputs": [], "source": ["# Shows a batch of images\ndef show_batch_df(df, rows=8, cols=4):\n    df = df.copy().reset_index()\n    fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(cols*4, rows*4))\n    for r in range(rows):\n        for c in range(cols):\n            idx = r * cols + c\n            img = imageio.imread(df.loc[idx, 'file_path'])\n            axes[r, c].imshow(img)\n            axes[r, c].set_title(f'{idx}, label: {df.loc[idx, \"Pawpularity\"]}')"]}, {"cell_type": "markdown", "id": "c5f17732", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Least Popular Pets</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "289bcfdd", "metadata": {}, "outputs": [], "source": ["show_batch_df(train.sort_values('Pawpularity'))"]}, {"cell_type": "markdown", "id": "fd885ae5", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Most Popular Pets</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "0b34d2c0", "metadata": {}, "outputs": [], "source": ["show_batch_df(train.sort_values('Pawpularity', ascending=False))"]}, {"cell_type": "markdown", "id": "4cb0a0b1", "metadata": {}, "source": ["<a id=\"yolo-v5-object-detection\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>YOLO V5 Object Detection</center></h2>"]}, {"cell_type": "markdown", "id": "93a80a02", "metadata": {}, "source": ["YOLOv5 \ud83d\ude80 is a family of compound-scaled object detection models trained on the COCO dataset, and includes simple functionality for Test Time Augmentation (TTA), model ensembling, hyperparameter evolution, and export to ONNX, CoreML and TFLite.\n  \n[YOLOV5](https://github.com/ultralytics/yolov5) is the fifth iteration of the Yo Only Look Once object detection famility, which is quite controversial as no official paper has been published. It is however freely available, easy to use and scores fairly high in benchmarks.\n  \nUsing object detection the images can be classified as either cat or dog, the contours of the pets can be determined and the amount of pets in the images can be counted.\n  \nThis object detection can be a source of features and a fundamental tool for preprocessing.|"]}, {"cell_type": "markdown", "id": "b57cc00a", "metadata": {}, "source": ["![](https://github.com/ultralytics/yolov5/releases/download/v1.0/model_comparison.png)"]}, {"cell_type": "markdown", "id": "bf68d1ce", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "id": "cf1ab1d0", "metadata": {}, "source": ["![](https://github.com/ultralytics/yolov5/releases/download/v1.0/model_plot.png)"]}, {"cell_type": "markdown", "id": "1e0133b0", "metadata": {}, "source": ["---"]}, {"cell_type": "code", "execution_count": 1, "id": "f81e1d1d", "metadata": {}, "outputs": [], "source": ["# Download YOLOV5 GitHub Repo\n!git clone https://github.com/ultralytics/yolov5"]}, {"cell_type": "code", "execution_count": 1, "id": "149ed49b", "metadata": {}, "outputs": [], "source": ["# Load Best Performing YOLOV5X Model\nyolov5x6_model = torch.hub.load('ultralytics/yolov5', 'yolov5x6')"]}, {"cell_type": "code", "execution_count": 1, "id": "42b3e161", "metadata": {}, "outputs": [], "source": ["# Get Image Info\ndef get_image_info(file_path, plot=False):\n    # Read Image\n    image = imageio.imread(file_path)\n    h, w, c = image.shape\n    \n    if plot: # Debug Plots\n        fig, ax = plt.subplots(1, 2, figsize=(8,8))\n        ax[0].set_title('Pets detected in Image', size=16)\n        ax[0].imshow(image)\n        \n    # Get YOLOV5 results using Test Time Augmentation for better result\n    results = yolov5x6_model(image, augment=True)\n    \n    # Mask for pixels containing pets, initially all set to zero\n    pet_pixels = np.zeros(shape=[h, w], dtype=np.uint8)\n    \n    # Dictionary to Save Image Info\n    h, w, _ = image.shape\n    image_info = { \n        'n_pets': 0, # Number of pets in the image\n        'labels': [], # Label assigned to found objects\n        'thresholds': [], # confidence score\n        'coords': [], # coordinates of bounding boxes\n        'x_min': 0, # minimum x coordinate of pet bounding box\n        'x_max': w - 1, # maximum x coordinate of pet bounding box\n        'y_min': 0, # minimum y coordinate of pet bounding box\n        'y_max': h - 1, # maximum x coordinate of pet bounding box\n    }\n    \n    # Save found pets to draw bounding boxes\n    pets_found = []\n    \n    # Save info for each pet\n    for x1, y1, x2, y2, treshold, label in results.xyxy[0].cpu().detach().numpy():\n        label = results.names[int(label)]\n        if label in ['cat', 'dog']:\n            image_info['n_pets'] += 1\n            image_info['labels'].append(label)\n            image_info['thresholds'].append(treshold)\n            image_info['coords'].append(tuple([x1, y1, x2, y2]))\n            image_info['x_min'] = max(x1, image_info['x_min'])\n            image_info['x_max'] = min(x2, image_info['x_max'])\n            image_info['y_min'] = max(y1, image_info['y_min'])\n            image_info['y_max'] = min(y2, image_info['y_max'])\n            \n            # Set pixels containing pets to 1\n            pet_pixels[int(y1):int(y2), int(x1):int(x2)] = 1\n            \n            # Add found pet\n            pets_found.append([x1, x2, y1, y2, label])\n\n    if plot:\n        for x1, x2, y1, y2, label in pets_found:\n            c = 'red' if label == 'dog' else 'blue'\n            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor=c, facecolor='none')\n            # Add the patch to the Axes\n            ax[0].add_patch(rect)\n            ax[0].text(max(25, (x2+x1)/2), max(25, y1-h*0.02), label, c=c, ha='center', size=14)\n                \n    # Add Pet Ratio in Image\n    image_info['pet_ratio'] = pet_pixels.sum() / (h*w)\n\n    if plot:\n        # Show pet pixels\n        ax[1].set_title('Pixels Containing Pets', size=16)\n        ax[1].imshow(pet_pixels)\n        plt.show()\n        \n    return image_info"]}, {"cell_type": "markdown", "id": "783aff07", "metadata": {}, "source": ["<a id=\"dataset-and-augmentations\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Dataset and Augmentations</center></h2>"]}, {"cell_type": "markdown", "id": "f11869fc", "metadata": {}, "source": ["This part of the notebook has been referred from [here](https://www.kaggle.com/heyytanay/train-baseline-torch-lightning-gpu-tpu-w-b/notebook)"]}, {"cell_type": "markdown", "id": "659f0b8d", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Lightning Dataset</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "3bbede9e", "metadata": {}, "outputs": [], "source": ["class PetfinderData(Dataset):\n    def __init__(self, df, is_test=False, augments=None):\n        self.df = df\n        self.is_test = is_test\n        self.augments = augments\n        \n        self.images, self.meta_features, self.targets = self._process_df(self.df)\n    \n    def __getitem__(self, index):\n        img = self.images[index]\n        meta_feats = self.meta_features[index]\n        meta_feats = torch.tensor(meta_feats, dtype=torch.float32)\n        \n        img = cv2.imread(img)\n        img = img[:, :, ::-1]\n        img = cv2.resize(img, Config['IMG_SIZE'])\n        \n        if self.augments:\n            img = self.augments(image=img)['image']\n        \n        if not self.is_test:\n            target = torch.tensor(self.targets[index], dtype=torch.float32)\n            return img, meta_feats, target\n        else:\n            return img, meta_feats\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def _process_df(self, df):\n        TRAIN = \"../input/petfinder-pawpularity-score/train\"\n        TEST = \"../input/petfinder-pawpularity-score/test\"\n        \n        if not self.is_test:\n            df['Id'] = df['Id'].apply(lambda x: os.path.join(TRAIN, x+\".jpg\"))\n        else:\n            df['Id'] = df['Id'].apply(lambda x: os.path.join(TEST, x+\".jpg\"))\n            \n        meta_features = df.drop(['Id', 'Pawpularity'], axis=1).values\n        \n        return df['Id'].tolist(), meta_features, df['Pawpularity'].tolist()"]}, {"cell_type": "markdown", "id": "a93d2fcb", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Augmentations</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "8756d4bf", "metadata": {}, "outputs": [], "source": ["class Augments:\n    \"\"\"\n    Contains Train, Validation Augments\n    \"\"\"\n    train_augments = Compose([\n        Resize(*Config['IMG_SIZE'], p=1.0),\n        HorizontalFlip(p=0.5),\n        VerticalFlip(p=0.5),\n        Normalize(\n            mean=[0.485, 0.456, 0.406], \n            std=[0.229, 0.224, 0.225], \n            max_pixel_value=255.0, \n            p=1.0\n        ),\n        ToTensorV2(p=1.0),\n    ],p=1.)\n    \n    valid_augments = Compose([\n        Resize(*Config['IMG_SIZE'], p=1.0),\n        Normalize(\n            mean=[0.485, 0.456, 0.406], \n            std=[0.229, 0.224, 0.225], \n            max_pixel_value=255.0, \n            p=1.0\n        ),\n        ToTensorV2(p=1.0),\n    ], p=1.)"]}, {"cell_type": "markdown", "id": "5e7dc65f", "metadata": {}, "source": ["<a id=\"efficientnet-model-and-understanding\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Efficientnet Model and Understanding</center></h2>"]}, {"cell_type": "markdown", "id": "aa44c804", "metadata": {}, "source": ["## **<span style=\"color:orange;\">What is Scaling?</span>**\n\nScaling is generally done to improve the model\u2019s accuracy on a certain task, for example, ImageNet classification. Although sometimes researchers don\u2019t care much about efficient models as the competition is to beat the SOTA, scaling, if done correctly, can also help in improving the efficiency of a model."]}, {"cell_type": "markdown", "id": "1611f886", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Types of Scaling for CNNs</span>**\n\nThere are three scaling dimensions of a CNN: \n1) **Depth** - Depth simply means how deep the networks is which is equivalent to the number of layers in it.  \n2) **Width** - Width simply means how wide the network is. One measure of width, for example, is the number of channels in a Conv layer whereas Resolution is simply the image resolution that is being passed to a CNN.  \n3) **Resolution**     \n  \nThe figure below(from the paper itself) will give you a clear idea of what scaling means across different dimensions. We will discuss these in detail as well.\n\n<center><img src = \"https://miro.medium.com/max/875/1*xQCVt1tFWe7XNWVEmC6hGQ.png\" width = \"750\" height = \"500\"/></center>  \n\n---"]}, {"cell_type": "markdown", "id": "d3423d95", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Depth Scaling (d)</span>**\n\nScaling a network by depth is the most common way of scaling. Depth can be scaled up as well as scaled down by adding/removing layers respectively. For example, ResNets can be scaled up from ResNet-50 to ResNet-200 as well as they can be scaled down from ResNet-50 to ResNet-18.   \n  \nBut why depth scaling? The intuition is that a deeper network can capture richer and more complex features, and generalizes well on new tasks.  \n  \n*Fair enough. Well, let\u2019s make our network 1000 layers deep then? We don\u2019t mind adding extra layers if we have the resources and a chance to improve on this task.*  \n  \nEasier said than done! Theoretically, with more layers, the network performance should improve but practically it doesn\u2019t follow. Vanishing gradients is one of the most common problems that arises as we go deep.   \n  \nEven if you avoid the gradients to vanish, as well as use some techniques to make the training smooth, adding more layers doesn\u2019t always help. For example, ResNet-1000 has similar accuracy as ResNet-101.\n\n---"]}, {"cell_type": "markdown", "id": "215a245e", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Width Scaling (w)</span>**\n\nThis is commonly used when we want to keep our model small. Wider networks tend to be able to capture more fine-grained features. Also, smaller models are easier to train.  \n  \nWhat is the problem now?\nThe problem is that even though you can make your network extremely wide, with shallow models (less deep but wider) accuracy saturates quickly with larger width.\n\n---"]}, {"cell_type": "markdown", "id": "c86d0a8a", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Resolution (r)</span>**\n\nIntuitively, we can say that in a high-resolution image, the features are more fine-grained and hence high-res images should work better. This is also one of the reasons that in complex tasks, like Object detection, we use image resolutions like 300x300, or 512x512, or 600x600.   \n  \nBut this doesn\u2019t scale linearly. The accuracy gain diminishes very quickly. For example, increasing resolution from 500x500 to 560x560 doesn\u2019t yield significant improvements.\n  \nThe above three points lead to our first observation: Scaling up any dimension of network (width, depth or resolution) improves accuracy, but the accuracy gain diminishes for bigger models.\n\n![](https://miro.medium.com/max/875/1*yMCuuf5qzOVbYIJWmvW6Tg.png)\n\nScaling Up a Baseline Model with Different Network Width (w), Depth (d), and Resolution (r) Coefficients. Bigger networks with larger width, depth, or resolution tend to achieve higher accuracy, but the accuracy gain quickly saturate after reaching 80%, demonstrating the limitation of single dimension scaling.\n\n---"]}, {"cell_type": "markdown", "id": "1e9c1089", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Combined Scaling</span>**\n\nYes, we can combine the scalings for different dimensions but there are some points that the authors have made:  \n- Though it is possible to scale two or three dimensions arbitrarily, arbitrary scaling is a tedious task.\n- Most of the times, manual scaling results in sub-optimal accuracy and efficiency.\n  \nIntuition says that as the resolution of the images is increased, depth and width of the network should be increased as well. As the depth is increased, larger receptive fields can capture similar features that include more pixels in an image. Also, as the width is increased, more fine-grained features will be captured. To validate this intuition, the authors ran a number of experiments with different scaling values for each dimension. For example, as shown in the figure below from the paper, with deeper and higher resolution, width scaling achieves much better accuracy under the same FLOPS cost.\n\n![](https://miro.medium.com/max/875/1*99pp7-l0392l57TvpxHS9g.png)\n\nScaling Network Width for Different Baseline Networks. Each dot in a line denotes a model with different width coefficient (w). All baseline networks are from Table 1. The first baseline network (d=1.0, r=1.0) has 18 convolutional layers with resolution 224x224, while the last baseline (d=2.0, r=1.3) has 36 layers with resolution 299x299  \n  \nThese results lead to our second observation: It is critical to balance all dimensions of a network (width, depth, and resolution) during CNNs scaling for getting improved accuracy and efficiency.\n\n---"]}, {"cell_type": "markdown", "id": "fbcc7b4d", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Proposed Compound Scaling<span>**\n\nThe authors proposed a simple yet very effective scaling technique which uses a compound coefficient \u0278 to uniformly scale network width, depth, and resolution in a principled way:  \n\n![](https://miro.medium.com/max/705/1*iYn6_BvI2mFk6rls8LopVA.png)\n  \n`\u0278` is a user-specified coefficient that controls how many resources are available whereas `\u03b1`, `\u03b2`, and `\u03b3` specify how to assign these resources to network depth, width, and resolution respectively.  \n      \nIn a CNN, Conv layers are the most compute expensive part of the network. Also, FLOPS of a regular convolution op is almost proportional to `d`, `w\u00b2`, `r\u00b2`, i.e. doubling the depth will double the FLOPS while doubling width or resolution increases FLOPS almost by four times. Hence, in order to make sure that the total FLOPS don\u2019t exceed `2^\u03d5`, the constraint applied is that `(\u03b1 * \u03b2\u00b2 * \u03b3\u00b2) \u2248 2`\n    \n ---"]}, {"cell_type": "markdown", "id": "2ba2528c", "metadata": {}, "source": ["## **<span style=\"color:orange;\">EfficientNet Architecture<span>**\n\nScaling doesn\u2019t change the layer operations, hence it is better to first have a good baseline network and then scale it along different dimensions using the proposed compound scaling. The authors obtained their base network by doing a Neural Architecture Search (NAS) that optimizes for both accuracy and FLOPS. The architecture is similar to M-NASNet as it has been found using the similar search space. The network layers/blocks are as shown below:  \n\n![](https://miro.medium.com/max/1400/1*OpvSpqMP61IO_9cp4mAXnA.png)\n  \nThe MBConv block is nothing fancy but an Inverted Residual Block (used in MobileNetV2) with a Squeeze and Excite block injected sometimes.  \n      \nNow we have the base network, we can search for optimal values for our scaling parameters. If you revisit the equation, you will quickly realize that we have a total of four parameters to search for: `\u03b1`, `\u03b2`, `\u03b3`, and `\u03d5`.   \n      \nIn order to make the search space smaller and making the search operation less costly, the search for these parameters can be completed in two steps.  \n    \n1) Fix `\u03d5 =1`, assuming that twice more resources are available, and do a small grid search for `\u03b1`, `\u03b2`, and `\u03b3`. For baseline network B0, it turned out the optimal values are `\u03b1 =1.2`, `\u03b2 = 1.1`, and `\u03b3 = 1.15` such that `\u03b1 * \u03b2\u00b2 * \u03b3\u00b2 \u2248 2`  \n    \n2) Now fix `\u03b1`, `\u03b2`, and `\u03b3` as constants (with values found in above step) and experiment with different values of `\u03d5`. The different values of `\u03d5` produce EfficientNets `B1-B7`.\n\n ---"]}, {"cell_type": "markdown", "id": "ce84075d", "metadata": {}, "source": ["[](!https://miro.medium.com/max/875/1*yMCuuf5qzOVbYIJWmvW6Tg.png)"]}, {"cell_type": "code", "execution_count": 1, "id": "82a33562", "metadata": {}, "outputs": [], "source": ["class PetFinderModel(pl.LightningModule):\n    def __init__(self, pretrained=True):\n        super(PetFinderModel, self).__init__()\n        self.model = timm.create_model(Config['MODEL_NAME'], pretrained=pretrained)\n        \n        self.n_features = self.model.classifier.in_features\n        self.model.reset_classifier(0)\n        self.fc = nn.Linear(self.n_features + 12, Config['NUM_LABELS'])\n        \n        self.train_loss = nn.MSELoss()\n        self.valid_loss = nn.MSELoss()\n\n    def forward(self, images, meta):\n        features = self.model(images)\n        features = torch.cat([features, meta], dim=1)\n        output = self.fc(features)\n        return output\n    \n    def training_step(self, batch, batch_idx):\n        imgs = batch[0]\n        meta = batch[1]\n        target = batch[2]\n        \n        out = self(imgs, meta)\n        train_loss = torch.sqrt(self.train_loss(out, target))\n        \n        logs = {'train_loss': train_loss}\n        \n        return {'loss': train_loss, 'log': logs}\n    \n    def validation_step(self, batch, batch_idx):\n        imgs = batch[0]\n        meta = batch[1]\n        target = batch[2]\n        \n        out = self(imgs, meta)\n        valid_loss = torch.sqrt(self.valid_loss(out, target))\n        \n        return {'val_loss': valid_loss}\n    \n    def validation_end(self, outputs):\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n        logs = {'val_loss': avg_loss}\n        \n        print(f\"val_loss: {avg_loss}\")\n        return {'avg_val_loss': avg_loss, 'log': logs}\n    \n    def configure_optimizers(self):\n        opt = torch.optim.Adam(self.parameters(), lr=Config['LR'])\n        sch = torch.optim.lr_scheduler.CosineAnnealingLR(\n            opt, \n            T_max=Config['T_max'],\n            eta_min=Config['min_lr']\n        )\n        \n        return [opt], [sch]"]}, {"cell_type": "markdown", "id": "e6f56e52", "metadata": {}, "source": ["## **<span style=\"color:orange;\">Data Folds</span>**"]}, {"cell_type": "code", "execution_count": 1, "id": "726f801b", "metadata": {}, "outputs": [], "source": ["# Run the Kfolds training loop\nkf = StratifiedKFold(n_splits=Config['NFOLDS'])\ntrain_file = pd.read_csv(\"../input/petfinder-pawpularity-score/train.csv\")\n\nfor fold_, (train_idx, valid_idx) in enumerate(kf.split(X=train_file, y=train_file['Pawpularity'])):\n    print(f\"{'='*20} Fold: {fold_} {'='*20}\")\n    \n    train_df = train_file.loc[train_idx]\n    valid_df = train_file.loc[valid_idx]\n    \n    train_set = PetfinderData(\n        train_df,\n        augments = Augments.train_augments\n    )\n\n    valid_set = PetfinderData(\n        valid_df,\n        augments = Augments.valid_augments\n    )\n    \n    train = DataLoader(\n        train_set,\n        batch_size=Config['TRAIN_BS'],\n        shuffle=True,\n        num_workers=Config['NUM_WORKERS'],\n        pin_memory=True\n    )\n    valid = DataLoader(\n        valid_set,\n        batch_size=Config['VALID_BS'],\n        shuffle=False,\n        num_workers=Config['NUM_WORKERS']\n    )\n    \n    checkpoint_callback = ModelCheckpoint(\n        monitor=\"val_loss\",\n        dirpath=\"./\",\n        filename=f\"fold_{fold_}_{Config['MODEL_NAME']}\",\n        save_top_k=1,\n        mode=\"min\",\n    )\n    \n    model = PetFinderModel()\n    trainer = pl.Trainer(\n        max_epochs=Config['EPOCHS'], \n        gpus=1, \n        callbacks=[checkpoint_callback], \n        logger= wandb_logger\n    )\n    trainer.fit(model, train, valid)"]}, {"cell_type": "markdown", "id": "2d595bbc", "metadata": {}, "source": ["<a id=\"wandb-dashboard\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>WandB Dashboard</center></h2>"]}, {"cell_type": "markdown", "id": "a8230a40", "metadata": {}, "source": ["> ### [Link to Dashboard](https://wandb.ai/ishandutta/pytorchlightning?workspace=user-ishandutta)"]}, {"cell_type": "code", "execution_count": 1, "id": "7b238aa5", "metadata": {}, "outputs": [], "source": ["# Store all wandb image paths in a list\n\nwandb_img_paths = []\n\nfor i in range(1, 5):\n    path = \"../input/pawpularitywandb/wandb-\" + str(i) + \".png\"\n    wandb_img_paths.append(path)"]}, {"cell_type": "code", "execution_count": 1, "id": "c55ce048", "metadata": {}, "outputs": [], "source": ["def display_img(img_path):\n    \"\"\"\n    Function which takes an image path and displays it.\n    \n    params: img_path(str): Path of Image to be displayed\n    \"\"\"\n\n    fig = matplotlib.pyplot.gcf()\n    fig.set_size_inches(25.5, 17.5)\n\n    img = cv2.imread(img_path)\n\n    plt.axis('off')\n    plt.imshow(img)"]}, {"cell_type": "code", "execution_count": 1, "id": "d03afee9", "metadata": {}, "outputs": [], "source": ["display_img(wandb_img_paths[0])"]}, {"cell_type": "code", "execution_count": 1, "id": "3c9d4a1b", "metadata": {}, "outputs": [], "source": ["display_img(wandb_img_paths[1])"]}, {"cell_type": "code", "execution_count": 1, "id": "35cc70b5", "metadata": {}, "outputs": [], "source": ["display_img(wandb_img_paths[2])"]}, {"cell_type": "code", "execution_count": 1, "id": "fbc52c04", "metadata": {}, "outputs": [], "source": ["display_img(wandb_img_paths[3])"]}, {"cell_type": "markdown", "id": "39976902", "metadata": {}, "source": ["<a id=\"references\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>References</center></h2>"]}, {"cell_type": "markdown", "id": "580992f9", "metadata": {}, "source": [">- [PetFinder EDA + YOLOV5 Obj Detection + TFRecords](https://www.kaggle.com/markwijkhuizen/petfinder-eda-yolov5-obj-detection-tfrecords)\n>- [\ud83d\udd25TensorFlow Probability\ud83d\ude3a\ud83d\udc36+NGBoost+W&B](https://www.kaggle.com/usharengaraju/tensorflow-probability-ngboost-w-b)\n>- [Tutorial Part 1: EDA for Beginners](https://www.kaggle.com/alexteboul/tutorial-part-1-eda-for-beginners)\n>- [Pawpularity - EDA - Feature Engineering - Baseline](https://www.kaggle.com/michaelfumery/pawpularity-eda-feature-engineering-baseline)\n>- [[\ud83d\udc3eTrain Baseline] Torch Lightning + GPU&TPU + W&B](https://www.kaggle.com/heyytanay/train-baseline-torch-lightning-gpu-tpu-w-b/notebook)\n>- [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://medium.com/@nainaakash012/efficientnet-rethinking-model-scaling-for-convolutional-neural-networks-92941c5bfb95)\n>- [Efficientnet Paper](https://arxiv.org/abs/1905.11946)\n>- [Efficientnet Official Released Code](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet)\n>- [PetFinder EDA + YOLOV5 Obj Detection + TFRecords](https://www.kaggle.com/markwijkhuizen/petfinder-eda-yolov5-obj-detection-tfrecords)\n>\n>---"]}, {"cell_type": "markdown", "id": "67a36ac1", "metadata": {}, "source": ["<h1><center>More Plots and Models coming soon!</center></h1>\n                                                      \n<center><img src = \"https://static.wixstatic.com/media/5f8fae_7581e21a24a1483085024f88b0949a9d~mv2.jpg/v1/fill/w_934,h_379,al_c,q_90/5f8fae_7581e21a24a1483085024f88b0949a9d~mv2.jpg\" width = \"750\" height = \"500\"/></center> "]}, {"cell_type": "markdown", "id": "b6fe566c", "metadata": {}, "source": ["--- \n\n## **<span style=\"color:orange;\">Let's have a Talk!</span>**\n> ### Reach out to me on [LinkedIn](https://www.linkedin.com/in/ishandutta0098)\n\n---"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}