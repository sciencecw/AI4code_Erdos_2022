{"cells": [{"cell_type": "markdown", "id": "f9a868e4", "metadata": {}, "source": ["# Time Series data forecasting"]}, {"cell_type": "markdown", "id": "aeb6b181", "metadata": {}, "source": ["# Importing Libraries"]}, {"cell_type": "code", "execution_count": 1, "id": "e7e1e83e", "metadata": {}, "outputs": [], "source": ["\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport math\n\nimport statsmodels.api as sm\nfrom statsmodels.tsa.api import VAR #Vector AutoRegression\n\nfrom statsmodels.tsa.stattools import adfuller #for the Dicky-Fuller Test\nfrom sklearn.metrics import mean_squared_error #for calculating the performance metric\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n"]}, {"cell_type": "markdown", "id": "7548ae56", "metadata": {}, "source": ["# Data Preprocessing"]}, {"cell_type": "code", "execution_count": 1, "id": "3b1e2b2a", "metadata": {}, "outputs": [], "source": ["df_train=pd.read_csv(\"../input/into-the-future/train.csv\")\ndf_train.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "f2311cea", "metadata": {}, "outputs": [], "source": ["df1=df_train[['time','feature_1','feature_2']]\ndf1.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "a34fb7a6", "metadata": {}, "outputs": [], "source": ["df1.set_index('time', inplace=True)\ndf1.plot(figsize=(12,8))"]}, {"cell_type": "markdown", "id": "e76682a9", "metadata": {}, "source": ["Checking the info, just to be sure, in case any missing values."]}, {"cell_type": "code", "execution_count": 1, "id": "a8115b2b", "metadata": {}, "outputs": [], "source": ["df1.info()"]}, {"cell_type": "markdown", "id": "49bc75ba", "metadata": {}, "source": ["Splitting the Train data for training the model : the first 85% for Train and the remaining 15% for Cross Validation"]}, {"cell_type": "code", "execution_count": 1, "id": "ba40af5e", "metadata": {}, "outputs": [], "source": ["df1_train=df1[:int(0.85*len(df1))]\ndf1_test=df1[int(0.85*len(df1)):]"]}, {"cell_type": "markdown", "id": "09cca9c7", "metadata": {}, "source": ["Even though the data looks *stationary*, but it's still better to be sure, as the model might make wrong predictions in case of *non-stationary data*.\n\n<br></br>\nI'll be performing the ***Dicky Fuller Test*** , a Hypothesis Testing where: <br></br>\n* H0 : It's Non-stationary\n* H1 : It's Stationary\n"]}, {"cell_type": "code", "execution_count": 1, "id": "67b199f0", "metadata": {}, "outputs": [], "source": ["def adfuller_test(data):\n    result=adfuller(data,autolag='AIC')\n    labels=[\"ADF Test Statistic\",\"p-value\",\"#Lags used\",\"#Observations\"]\n    for val,lab in zip(result,labels):\n        print(lab+\":\"+str(val))\n    #taking the significance value as 0.05\n    if result[1]>0.05 :\n        print(\"Model is Not Stationary\") #Null Hypothesis\n    else:\n        print(\"Model is Stationary\") #Alternate Hypothesis\n        \n\nprint(\"FEATURE 1 \")\nadfuller_test(df1_train['feature_1'])\nprint(\"FEATURE 2 \")\nadfuller_test(df1_train['feature_2'])"]}, {"cell_type": "markdown", "id": "fc1a77b4", "metadata": {}, "source": ["As we can see, both *feature_1* and *feature_2* are stationary, so we can proceed with training the model."]}, {"cell_type": "markdown", "id": "b7bcf79c", "metadata": {}, "source": ["# Training and Testing Model"]}, {"cell_type": "code", "execution_count": 1, "id": "58680f07", "metadata": {}, "outputs": [], "source": ["model = VAR(df1_train)\nresults = model.fit(maxlags=15, ic='aic')\nresults.summary()"]}, {"cell_type": "markdown", "id": "c7e49ea4", "metadata": {}, "source": ["Forecasting the CV data and converting it to DataFrame for further use."]}, {"cell_type": "code", "execution_count": 1, "id": "4d989fd7", "metadata": {}, "outputs": [], "source": ["predicted = results.forecast(results.y, steps=len(df1_test))\n\nlabels=['feature_1','feature_2']\npredicted=pd.DataFrame(predicted, columns=labels)"]}, {"cell_type": "markdown", "id": "bd8b54ff", "metadata": {}, "source": ["Making sure if the shape of the predicted dataframe and the df1_test dataframe is same."]}, {"cell_type": "code", "execution_count": 1, "id": "28ce4071", "metadata": {}, "outputs": [], "source": ["print(predicted.shape)\ndf1_test.shape"]}, {"cell_type": "markdown", "id": "ef1a0ebf", "metadata": {}, "source": ["# Measuring the Performance"]}, {"cell_type": "markdown", "id": "14334449", "metadata": {}, "source": ["Root Mean Squared Error(RMSE) for *feature_1* and *feature_2*"]}, {"cell_type": "code", "execution_count": 1, "id": "1ed56ef1", "metadata": {}, "outputs": [], "source": ["for i in labels:\n    print('rmse for '+i+' is : '+str(math.sqrt(mean_squared_error(predicted[i],df1_test[i]))))"]}, {"cell_type": "markdown", "id": "5e7d7b3b", "metadata": {}, "source": ["Plotting the Real and Predicted data"]}, {"cell_type": "code", "execution_count": 1, "id": "1c49ee59", "metadata": {}, "outputs": [], "source": ["plt.plot(predicted['feature_2'])\nplt.plot(df1_test['feature_2'])\nplt.show()"]}, {"cell_type": "markdown", "id": "977cdf24", "metadata": {}, "source": ["So, as we can see that the RMSE for feature_1 is **8.057180950988691** and the RMSE for feature_2 is **242.59720453616242** so we can conclude that our forecast is quite accurate for the train data, so let's proceed to the final predictions of *feature_2* for Test data"]}, {"cell_type": "markdown", "id": "22e479e2", "metadata": {}, "source": ["# Final Prediction and Submission"]}, {"cell_type": "markdown", "id": "4627de73", "metadata": {}, "source": ["Loading the test.csv"]}, {"cell_type": "code", "execution_count": 1, "id": "ffabae11", "metadata": {}, "outputs": [], "source": ["df_test=pd.read_csv('../input/into-the-future/test.csv')\nprint(df_test.head())\ndf_test.shape"]}, {"cell_type": "markdown", "id": "b8327414", "metadata": {}, "source": ["Performing the same Data Processing steps as performed in the Train dataset."]}, {"cell_type": "code", "execution_count": 1, "id": "5b1f48ee", "metadata": {}, "outputs": [], "source": ["data_test=df_test[['time','feature_1']]\ndata_test.set_index('time', inplace=True)\ndata_test.head()"]}, {"cell_type": "markdown", "id": "c46ccac0", "metadata": {}, "source": ["Forecasting the final test data."]}, {"cell_type": "code", "execution_count": 1, "id": "24228405", "metadata": {}, "outputs": [], "source": ["final_prediction = results.forecast(results.y, steps=len(data_test)+len(df1_test))\nfinal_prediction.shape"]}, {"cell_type": "markdown", "id": "9a5a6005", "metadata": {}, "source": ["Converting to DataFrame for further computations."]}, {"cell_type": "code", "execution_count": 1, "id": "08e7fc97", "metadata": {}, "outputs": [], "source": ["final_prediction1 = pd.DataFrame(final_prediction,columns=['feature_1','feature_2'],index=range(len(df1_test), len(df1_test)+len(final_prediction), 1))\nprint(final_prediction1.shape)\n\nfinal_prediction2=final_prediction1[len(df1_test):]\nfinal_prediction2.shape"]}, {"cell_type": "markdown", "id": "b2c558ed", "metadata": {}, "source": ["Checking the RMSE for *feature_1* just to be sure for any Overfitting and plotting the Predicted values of *feature_1* alongwith the Actual values *feature_1* in for test.csv"]}, {"cell_type": "code", "execution_count": 1, "id": "ff2283f8", "metadata": {}, "outputs": [], "source": ["print('rmse for '+i+' is : '+str(math.sqrt(mean_squared_error(final_prediction2['feature_1'],data_test['feature_1']))))\n\nplt.plot(final_prediction2['feature_1'])\nplt.plot(data_test['feature_1'])\nplt.show()"]}, {"cell_type": "markdown", "id": "909bf425", "metadata": {}, "source": ["As we can see that the RMSE for *feature_1* is **33.40936553566021** and the plot also shows that the forecast is quite satisfactory, so we can conclude that even for *feature_2* the RMSE would be nearly accurate, as per the trend."]}, {"cell_type": "markdown", "id": "525f9715", "metadata": {}, "source": ["So, moving ahead with the submission."]}, {"cell_type": "code", "execution_count": 1, "id": "b39fa90f", "metadata": {}, "outputs": [], "source": ["final_prediction2['id'] = index=range(564, 564+len(final_prediction2), 1)\nfinal_prediction2.set_index('id',inplace=True)\nfinal_sol =final_prediction2.drop(['feature_1'],1)\n\nfinal_sol"]}, {"cell_type": "code", "execution_count": 1, "id": "fce76b02", "metadata": {}, "outputs": [], "source": ["final_sol.to_csv('Final_Solution.csv')"]}, {"cell_type": "markdown", "id": "4f9f9ad0", "metadata": {}, "source": ["# Conclusion"]}, {"cell_type": "markdown", "id": "1715fa0f", "metadata": {}, "source": ["I had tested with different types models and different parameters, and this was the model with the best predictions, having comparatively better RMSE score."]}, {"cell_type": "markdown", "id": "37b46122", "metadata": {}, "source": ["Thank you for this opportunity, solving this was a great learning experience for me.\n<br></br>I'll be waiting for the feedback.\n<br></br>Thank You."]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}