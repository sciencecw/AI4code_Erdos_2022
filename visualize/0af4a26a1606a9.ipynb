{"cells": [{"cell_type": "code", "execution_count": 1, "id": "a1d5fdda", "metadata": {}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output."]}, {"cell_type": "code", "execution_count": 1, "id": "e3a84c89", "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR\nfrom sklearn.metrics import mean_absolute_error"]}, {"cell_type": "markdown", "id": "e9382365", "metadata": {}, "source": ["# Introduction\nThis is literally just a forked notebook of the original \"Basic Feature Benchmark\" but I added some comments that I thought may be helpful for others."]}, {"cell_type": "code", "execution_count": 1, "id": "2232939f", "metadata": {}, "outputs": [], "source": ["train = pd.read_csv('../input/train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})"]}, {"cell_type": "code", "execution_count": 1, "id": "e3dfa08f", "metadata": {}, "outputs": [], "source": ["train.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "a41d79d7", "metadata": {}, "outputs": [], "source": ["# pandas doesn't show us all the decimals\npd.options.display.precision = 15"]}, {"cell_type": "code", "execution_count": 1, "id": "6689ce81", "metadata": {}, "outputs": [], "source": ["# much better!\ntrain.head()"]}, {"cell_type": "markdown", "id": "5e5b0056", "metadata": {}, "source": ["So what we have here is 2 columns. The first column is the signal that we will use for prediction and the second column is the `time_to_failure`. It goes down in equal increments and I expect it eventually will reach 0 when there is an earth quake and then back up to count down to the next earth quake. "]}, {"cell_type": "markdown", "id": "d0375464", "metadata": {}, "source": ["Below we break up the training data frame into groups of 150,000 rows. We collect the 150,000 signals and set the target as the `time_to_failure` of the last signal. However, some of the signals may be from previous earthquakes if the 150,000 rows has `time_to_failure` equal to zero in it, but oh well! We just use the 150,000 signals to help predict the time to quake target."]}, {"cell_type": "code", "execution_count": 1, "id": "430a4814", "metadata": {}, "outputs": [], "source": ["# Create a training file with simple derived features\n\nrows = 150_000\nsegments = int(np.floor(train.shape[0] / rows))\n\nX_train = pd.DataFrame(index=range(segments), dtype=np.float64,\n                       columns=['ave', 'std', 'max', 'min'])\ny_train = pd.DataFrame(index=range(segments), dtype=np.float64,\n                       columns=['time_to_failure'])\n\nfor segment in tqdm(range(segments)):\n    seg = train.iloc[segment*rows:segment*rows+rows]\n    x = seg['acoustic_data'].values\n    y = seg['time_to_failure'].values[-1]\n    \n    y_train.loc[segment, 'time_to_failure'] = y\n    \n    X_train.loc[segment, 'ave'] = x.mean()\n    X_train.loc[segment, 'std'] = x.std()\n    X_train.loc[segment, 'max'] = x.max()\n    X_train.loc[segment, 'min'] = x.min()"]}, {"cell_type": "code", "execution_count": 1, "id": "ebd75202", "metadata": {}, "outputs": [], "source": ["X_train.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "d6090477", "metadata": {}, "outputs": [], "source": ["y_train.head()"]}, {"cell_type": "markdown", "id": "08580b83", "metadata": {}, "source": ["> Our features are the average, standard deviation, max and min of the 150,000 signals. Now we standardize these rows using a Z-score and then throw them into a SVM machine learning model."]}, {"cell_type": "code", "execution_count": 1, "id": "ad8c7cf2", "metadata": {}, "outputs": [], "source": ["scaler = StandardScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "c38c3f6e", "metadata": {}, "outputs": [], "source": ["svm = NuSVR()\nsvm.fit(X_train_scaled, y_train.values.flatten())\ny_pred = svm.predict(X_train_scaled)"]}, {"cell_type": "code", "execution_count": 1, "id": "646407d9", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(6, 6))\nplt.scatter(y_train.values.flatten(), y_pred)\nplt.xlim(0, 20)\nplt.ylim(0, 20)\nplt.xlabel('actual', fontsize=12)\nplt.ylabel('predicted', fontsize=12)\nplt.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)])\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "da0975df", "metadata": {}, "outputs": [], "source": ["score = mean_absolute_error(y_train.values.flatten(), y_pred)\nprint(f'Score: {score:0.3f}')"]}, {"cell_type": "code", "execution_count": 1, "id": "6aa78c17", "metadata": {}, "outputs": [], "source": ["submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')"]}, {"cell_type": "markdown", "id": "426614a9", "metadata": {}, "source": ["Now lets look at how to handle the test data. There are a bunch of different files and we need to go through all of them and predict!"]}, {"cell_type": "code", "execution_count": 1, "id": "a20a1372", "metadata": {}, "outputs": [], "source": ["X_test = pd.DataFrame(columns=X_train.columns, dtype=np.float64, index=submission.index)\n"]}, {"cell_type": "code", "execution_count": 1, "id": "1adc1b7c", "metadata": {}, "outputs": [], "source": ["for seg_id in X_test.index:\n    seg = pd.read_csv('../input/test/' + seg_id + '.csv')\n    \n    x = seg['acoustic_data'].values\n    \n    X_test.loc[seg_id, 'ave'] = x.mean()\n    X_test.loc[seg_id, 'std'] = x.std()\n    X_test.loc[seg_id, 'max'] = x.max()\n    X_test.loc[seg_id, 'min'] = x.min()"]}, {"cell_type": "code", "execution_count": 1, "id": "a11eb0d1", "metadata": {}, "outputs": [], "source": ["X_test_scaled = scaler.transform(X_test)\nsubmission['time_to_failure'] = svm.predict(X_test_scaled)\nsubmission.to_csv('submission.csv')"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}