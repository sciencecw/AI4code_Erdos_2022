{"cells": [{"cell_type": "markdown", "id": "8dc955f6", "metadata": {}, "source": ["# Credits to the orginal Authors\nRankGauss: https://www.kaggle.com/kushal1506/moa-pytorch-feature-engineering-0-01846  \nTabNet: https://www.kaggle.com/ludovick/introduction-to-tabnet-kfold-10-training  \nResNet: https://www.kaggle.com/rahulsd91/moa-multi-input-resnet-model  \nLabelSmooth: https://www.kaggle.com/rahulsd91/moa-label-smoothing   \nXGB: https://www.kaggle.com/fchmiel/xgboost-baseline-multilabel-classification  "]}, {"cell_type": "code", "execution_count": 1, "id": "2a1462b5", "metadata": {}, "outputs": [], "source": ["import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport pickle\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping,ModelCheckpoint\nimport random\nimport os\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\nimport gc\nimport json\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold, RepeatedKFold, RepeatedStratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, RobustScaler\n# from category_encoders import CountEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.multioutput import MultiOutputClassifier\nimport xgboost as xgb\nimport warnings\nwarnings.filterwarnings('ignore')"]}, {"cell_type": "markdown", "id": "4c34a7cb", "metadata": {}, "source": ["# Rank Gauss"]}, {"cell_type": "code", "execution_count": 1, "id": "4cada514", "metadata": {}, "outputs": [], "source": ["!pip install ../input/iterative-stratification/iterative_stratification-0.1.6-py3-none-any.whl"]}, {"cell_type": "code", "execution_count": 1, "id": "c97258d9", "metadata": {}, "outputs": [], "source": ["import sys\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold"]}, {"cell_type": "code", "execution_count": 1, "id": "0be8a1ef", "metadata": {}, "outputs": [], "source": ["import os\nimport copy\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom sklearn.preprocessing import QuantileTransformer\n\nimport warnings\nwarnings.filterwarnings('ignore')"]}, {"cell_type": "code", "execution_count": 1, "id": "943a2bdb", "metadata": {}, "outputs": [], "source": ["%%time\ntrain_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_drug = pd.read_csv('../input/lish-moa/train_drug.csv')\nnon_ctl_idx = train_features.loc[train_features['cp_type']!='ctl_vehicle'].index.to_list()\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\nsub_rankgauss = pd.read_csv('../input/lish-moa/sample_submission.csv')"]}, {"cell_type": "code", "execution_count": 1, "id": "d74335d9", "metadata": {}, "outputs": [], "source": ["GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]"]}, {"cell_type": "code", "execution_count": 1, "id": "8037e3b3", "metadata": {}, "outputs": [], "source": ["#RankGauss\n\nfor col in (GENES + CELLS):\n\n    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]"]}, {"cell_type": "code", "execution_count": 1, "id": "043eec00", "metadata": {}, "outputs": [], "source": ["def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)"]}, {"cell_type": "code", "execution_count": 1, "id": "3b6e97c6", "metadata": {}, "outputs": [], "source": ["# GENES\nn_comp = 600  #<--Update\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "c6bec84e", "metadata": {}, "outputs": [], "source": ["#CELLS\nn_comp = 50  #<--Update\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "373ac23d", "metadata": {}, "outputs": [], "source": ["from sklearn.feature_selection import VarianceThreshold\n\n\nvar_thresh = VarianceThreshold(0.8)  #<-- Update\ndata = train_features.append(test_features)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\ntrain_features_transformed = data_transformed[ : train_features.shape[0]]\ntest_features_transformed = data_transformed[-test_features.shape[0] : ]\n\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\ntrain_features.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "e1e31116", "metadata": {}, "outputs": [], "source": ["%%time\nfrom sklearn.cluster import KMeans\ndef fe_cluster(train, test, n_clusters_g = 35, n_clusters_c = 5, SEED = 123):\n    \n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n    \n    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis = 0)\n        kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n        train[f'clusters_{kind}'] = kmeans.labels_[:train.shape[0]]\n        test[f'clusters_{kind}'] = kmeans.labels_[train.shape[0]:]\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n    \n    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n    train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n    return train, test\n\ntrain_features ,test_features=fe_cluster(train_features,test_features)"]}, {"cell_type": "code", "execution_count": 1, "id": "20e2eaf3", "metadata": {}, "outputs": [], "source": ["%%time\ndef fe_stats(train, test):\n    \n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n    \n    for df in train, test:\n        df['g_sum'] = df[features_g].sum(axis = 1)\n        df['g_mean'] = df[features_g].mean(axis = 1)\n        df['g_std'] = df[features_g].std(axis = 1)\n        df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n        df['g_skew'] = df[features_g].skew(axis = 1)\n        df['c_sum'] = df[features_c].sum(axis = 1)\n        df['c_mean'] = df[features_c].mean(axis = 1)\n        df['c_std'] = df[features_c].std(axis = 1)\n        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n        df['c_skew'] = df[features_c].skew(axis = 1)\n        df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n        \n    return train, test\n\ntrain_features,test_features=fe_stats(train_features,test_features)"]}, {"cell_type": "code", "execution_count": 1, "id": "44ac98f7", "metadata": {}, "outputs": [], "source": ["train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntarget = train[train_targets_scored.columns]"]}, {"cell_type": "code", "execution_count": 1, "id": "7fe1feff", "metadata": {}, "outputs": [], "source": ["train = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "c05cb8e1", "metadata": {}, "outputs": [], "source": ["target_cols = target.drop('sig_id', axis=1).columns.values.tolist()"]}, {"cell_type": "code", "execution_count": 1, "id": "db721626", "metadata": {}, "outputs": [], "source": ["def assign_folds(seed,fold):\n    scored = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\n    targets = scored.columns[1:]\n    scored = scored.merge(train_drug, on='sig_id', how='left') \n    scored = scored.iloc[non_ctl_idx]\n\n    # LOCATE DRUGS\n    vc = scored.drug_id.value_counts()\n    vc1 = vc.loc[vc<=18].index.sort_values()\n    vc2 = vc.loc[vc>18].index.sort_values()\n\n    # STRATIFY DRUGS 18X OR LESS\n    dct1 = {}; dct2 = {}\n    skf = MultilabelStratifiedKFold(n_splits=fold, shuffle=True, \n              random_state=seed)\n    tmp = scored.groupby('drug_id')[targets].mean().loc[vc1]\n    for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n        dd = {k:fold for k in tmp.index[idxV].values}\n        dct1.update(dd)\n\n    # STRATIFY DRUGS MORE THAN 18X\n    skf = MultilabelStratifiedKFold(n_splits=fold, shuffle=True, \n              random_state=seed)\n    tmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop=True)\n    for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n        dd = {k:fold for k in tmp.sig_id[idxV].values}\n        dct2.update(dd)\n\n    # ASSIGN FOLDS\n    scored['fold'] = scored.drug_id.map(dct1)\n    scored.loc[scored.fold.isna(),'fold'] =\\\n        scored.loc[scored.fold.isna(),'sig_id'].map(dct2)\n    scored.fold = scored.fold.astype('int8')\n\n    return scored['fold'].values"]}, {"cell_type": "code", "execution_count": 1, "id": "2a5992b7", "metadata": {}, "outputs": [], "source": ["folds = train.copy()\nseed_folds = train.copy()\n\nval_folds = assign_folds(34,5)\n\nfolds['kfold'] = val_folds\nfolds"]}, {"cell_type": "code", "execution_count": 1, "id": "016785a8", "metadata": {}, "outputs": [], "source": ["class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct"]}, {"cell_type": "code", "execution_count": 1, "id": "c8df01ea", "metadata": {}, "outputs": [], "source": ["def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n#         print(inputs.shape)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss /= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds"]}, {"cell_type": "code", "execution_count": 1, "id": "6a127ac5", "metadata": {}, "outputs": [], "source": ["import torch\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss"]}, {"cell_type": "code", "execution_count": 1, "id": "9b0a0148", "metadata": {}, "outputs": [], "source": ["class Model(nn.Module):      \n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.4)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.4)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x"]}, {"cell_type": "code", "execution_count": 1, "id": "f410f3cd", "metadata": {}, "outputs": [], "source": ["def process_data(data):\n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    return data"]}, {"cell_type": "code", "execution_count": 1, "id": "4faa2ed5", "metadata": {}, "outputs": [], "source": ["feature_cols = [c for c in process_data(folds).columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\nlen(feature_cols)"]}, {"cell_type": "code", "execution_count": 1, "id": "4b29a663", "metadata": {}, "outputs": [], "source": ["DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 25\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nNFOLDS = 5            \nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=2048"]}, {"cell_type": "code", "execution_count": 1, "id": "0b900037", "metadata": {}, "outputs": [], "source": ["def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n    train = process_data(folds)\n    test_ = process_data(test)\n    \n    asn_folds = assign_folds(seed,5)\n    seed_folds['kfold'] = asn_folds\n    \n    trn_idx = train[seed_folds['kfold'] != fold].index\n    val_idx = train[seed_folds['kfold'] == fold].index\n    \n    train_df = train[seed_folds['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[seed_folds['kfold'] == fold].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    \n    loss_tr = SmoothBCEwLogits(smoothing =0.001)\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n    \n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    \n    for epoch in range(EPOCHS):\n        \n        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n        print(f\"SEED: {seed}, FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        print(f\"SEED: {seed} ,FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof[val_idx] = valid_preds\n            torch.save(model.state_dict(), f\"RankGauss_{fold}_.pth\")\n        \n        elif(EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n            \n    \n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n\n    )\n    \n    model.load_state_dict(torch.load(f\"RankGauss_{fold}_.pth\"))\n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions"]}, {"cell_type": "code", "execution_count": 1, "id": "f0905529", "metadata": {}, "outputs": [], "source": ["def run_k_fold(NFOLDS, seed):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        oof_, pred_ = run_training(fold, seed)\n        \n        predictions += pred_ / NFOLDS\n        oof += oof_\n        \n    return oof, predictions"]}, {"cell_type": "code", "execution_count": 1, "id": "3453d7e0", "metadata": {}, "outputs": [], "source": ["%%time\n\nSEED = [34, 14, 75]  \noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n    \n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ / len(SEED)\n    predictions += predictions_ / len(SEED)\n\ntrain[target_cols] = oof\ntest[target_cols] = predictions"]}, {"cell_type": "code", "execution_count": 1, "id": "5935b1e3", "metadata": {}, "outputs": [], "source": ["torch.cuda.empty_cache()"]}, {"cell_type": "code", "execution_count": 1, "id": "2084289f", "metadata": {}, "outputs": [], "source": ["valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\n\ny_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ / target.shape[1]\n    \nprint(\"CV log_loss: \", score)"]}, {"cell_type": "code", "execution_count": 1, "id": "ebf33caa", "metadata": {}, "outputs": [], "source": ["sub_gauss = sub_rankgauss.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub_gauss"]}, {"cell_type": "markdown", "id": "96f46b34", "metadata": {}, "source": ["# TabNet"]}, {"cell_type": "code", "execution_count": 1, "id": "d47b2728", "metadata": {}, "outputs": [], "source": ["!pip install --no-index --find-links /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet"]}, {"cell_type": "code", "execution_count": 1, "id": "fbbca441", "metadata": {}, "outputs": [], "source": ["import copy\nimport tqdm\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = '1'\n\nfrom scipy import stats\n\n### Machine Learning ###\nfrom sklearn import preprocessing\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nfrom pickle import load,dump\n\n### Deep Learning ###\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nfrom torch.nn import functional as F\nfrom torch.nn.modules.loss import _WeightedLoss\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n# Tabnet \nfrom pytorch_tabnet.metrics import Metric\nfrom pytorch_tabnet.tab_model import TabNetRegressor"]}, {"cell_type": "code", "execution_count": 1, "id": "cdd6d75e", "metadata": {}, "outputs": [], "source": ["train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\ndf = pd.read_csv('../input/lish-moa/sample_submission.csv')"]}, {"cell_type": "code", "execution_count": 1, "id": "1d7ffe05", "metadata": {}, "outputs": [], "source": ["train_features2=train_features.copy()\ntest_features2=test_features.copy()"]}, {"cell_type": "code", "execution_count": 1, "id": "cd1e973c", "metadata": {}, "outputs": [], "source": ["GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]"]}, {"cell_type": "code", "execution_count": 1, "id": "3d7b26c9", "metadata": {}, "outputs": [], "source": ["qt = QuantileTransformer(n_quantiles=100,random_state=42,output_distribution='normal')\ntrain_features[GENES+CELLS] = qt.fit_transform(train_features[GENES+CELLS])\ntest_features[GENES+CELLS] = qt.transform(test_features[GENES+CELLS])"]}, {"cell_type": "code", "execution_count": 1, "id": "739a7ab8", "metadata": {}, "outputs": [], "source": ["seed = 42\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\nset_seed(seed)"]}, {"cell_type": "code", "execution_count": 1, "id": "c0b9a9a9", "metadata": {}, "outputs": [], "source": ["# GENES\nn_comp = 600  \ngpca= load(open('../input/moa-tabnet-train-inference/gpca.pkl', 'rb'))\ntrain2= (gpca.transform(train_features[GENES]))\ntest2 = (gpca.transform(test_features[GENES]))\n\ntrain_gpca = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest_gpca = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train_gpca), axis=1)\ntest_features = pd.concat((test_features, test_gpca), axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "59072bc6", "metadata": {}, "outputs": [], "source": ["#CELLS\nn_comp = 50 \n\ncpca= load(open('../input/moa-tabnet-train-inference/cpca.pkl', 'rb'))\ntrain2= (cpca.transform(train_features[CELLS]))\ntest2 = (cpca.transform(test_features[CELLS]))\n\ntrain_cpca = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest_cpca = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train_cpca), axis=1)\ntest_features = pd.concat((test_features, test_cpca), axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "d25c02ca", "metadata": {}, "outputs": [], "source": ["from sklearn.feature_selection import VarianceThreshold\n\nc_n = [f for f in list(train_features.columns) if f not in ['sig_id', 'cp_type', 'cp_time', 'cp_dose']]\nmask = (train_features[c_n].var() >= 0.85).values\ntmp = train_features[c_n].loc[:, mask]\ntrain_features = pd.concat([train_features[['sig_id', 'cp_type', 'cp_time', 'cp_dose']], tmp], axis=1)\ntmp = test_features[c_n].loc[:, mask]\ntest_features = pd.concat([test_features[['sig_id', 'cp_type', 'cp_time', 'cp_dose']], tmp], axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "cff75446", "metadata": {}, "outputs": [], "source": ["from sklearn.cluster import KMeans\ndef fe_cluster_genes(train, test, n_clusters_g = 22, SEED = 42):\n    \n    features_g = GENES\n    #features_c = CELLS\n    \n    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        kmeans_genes = load(open('../input/moa-tabnet-train-inference/kmeans_genes.pkl', 'rb'))\n        train[f'clusters_{kind}'] = kmeans_genes.predict(train_.values)\n        test[f'clusters_{kind}'] = kmeans_genes.predict(test_.values)\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n    \n    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n   # train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n    return train, test\n\ntrain_features2 ,test_features2=fe_cluster_genes(train_features2,test_features2)"]}, {"cell_type": "code", "execution_count": 1, "id": "d0825421", "metadata": {}, "outputs": [], "source": ["def fe_cluster_cells(train, test, n_clusters_c = 4, SEED = 42):\n    \n    #features_g = GENES\n    features_c = CELLS\n    \n    def create_cluster(train, test, features, kind = 'c', n_clusters = n_clusters_c):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        kmeans_cells = load(open('../input/moa-tabnet-train-inference/kmeans_cells.pkl', 'rb'))\n        train[f'clusters_{kind}'] = kmeans_cells.predict(train_.values)\n        test[f'clusters_{kind}'] = kmeans_cells.predict(test_.values)\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n    \n    train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n    return train, test\n\ntrain_features2 ,test_features2=fe_cluster_cells(train_features2,test_features2)"]}, {"cell_type": "code", "execution_count": 1, "id": "90a8aecc", "metadata": {}, "outputs": [], "source": ["train_pca=pd.concat((train_gpca,train_cpca),axis=1)\ntest_pca=pd.concat((test_gpca,test_cpca),axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "88a41ce1", "metadata": {}, "outputs": [], "source": ["def fe_cluster_pca(train, test,n_clusters=5,SEED = 42):\n        kmeans_pca = load(open('../input/moa-tabnet-train-inference/kmeans_pca.pkl', 'rb'))\n        train[f'clusters_pca'] = kmeans_pca.predict(train.values)\n        test[f'clusters_pca'] = kmeans_pca.predict(test.values)\n        train = pd.get_dummies(train, columns = [f'clusters_pca'])\n        test = pd.get_dummies(test, columns = [f'clusters_pca'])\n        return train, test\ntrain_cluster_pca ,test_cluster_pca = fe_cluster_pca(train_pca,test_pca)"]}, {"cell_type": "code", "execution_count": 1, "id": "620ebc37", "metadata": {}, "outputs": [], "source": ["train_cluster_pca = train_cluster_pca.iloc[:,650:]\ntest_cluster_pca = test_cluster_pca.iloc[:,650:]"]}, {"cell_type": "code", "execution_count": 1, "id": "65b8a302", "metadata": {}, "outputs": [], "source": ["train_features_cluster=train_features2.iloc[:,876:]\ntest_features_cluster=test_features2.iloc[:,876:]"]}, {"cell_type": "code", "execution_count": 1, "id": "ef8f87bb", "metadata": {}, "outputs": [], "source": ["gsquarecols=['g-574','g-211','g-216','g-0','g-255','g-577','g-153','g-389','g-60','g-370','g-248','g-167','g-203','g-177','g-301','g-332','g-517','g-6','g-744','g-224','g-162','g-3','g-736','g-486','g-283','g-22','g-359','g-361','g-440','g-335','g-106','g-307','g-745','g-146','g-416','g-298','g-666','g-91','g-17','g-549','g-145','g-157','g-768','g-568','g-396']"]}, {"cell_type": "code", "execution_count": 1, "id": "e4666c8f", "metadata": {}, "outputs": [], "source": ["def fe_stats(train, test):\n    \n    features_g = GENES\n    features_c = CELLS\n    \n    for df in train, test:\n        df['g_sum'] = df[features_g].sum(axis = 1)\n        df['g_mean'] = df[features_g].mean(axis = 1)\n        df['g_std'] = df[features_g].std(axis = 1)\n        df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n        df['g_skew'] = df[features_g].skew(axis = 1)\n        df['c_sum'] = df[features_c].sum(axis = 1)\n        df['c_mean'] = df[features_c].mean(axis = 1)\n        df['c_std'] = df[features_c].std(axis = 1)\n        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n        df['c_skew'] = df[features_c].skew(axis = 1)\n        df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n        \n        df['c52_c42'] = df['c-52'] * df['c-42']\n        df['c13_c73'] = df['c-13'] * df['c-73']\n        df['c26_c13'] = df['c-23'] * df['c-13']\n        df['c33_c6'] = df['c-33'] * df['c-6']\n        df['c11_c55'] = df['c-11'] * df['c-55']\n        df['c38_c63'] = df['c-38'] * df['c-63']\n        df['c38_c94'] = df['c-38'] * df['c-94']\n        df['c13_c94'] = df['c-13'] * df['c-94']\n        df['c4_c52'] = df['c-4'] * df['c-52']\n        df['c4_c42'] = df['c-4'] * df['c-42']\n        df['c13_c38'] = df['c-13'] * df['c-38']\n        df['c55_c2'] = df['c-55'] * df['c-2']\n        df['c55_c4'] = df['c-55'] * df['c-4']\n        df['c4_c13'] = df['c-4'] * df['c-13']\n        df['c82_c42'] = df['c-82'] * df['c-42']\n        df['c66_c42'] = df['c-66'] * df['c-42']\n        df['c6_c38'] = df['c-6'] * df['c-38']\n        df['c2_c13'] = df['c-2'] * df['c-13']\n        df['c62_c42'] = df['c-62'] * df['c-42']\n        df['c90_c55'] = df['c-90'] * df['c-55']\n        \n        \n        for feature in features_c:\n             df[f'{feature}_squared'] = df[feature] ** 2     \n                \n        for feature in gsquarecols:\n            df[f'{feature}_squared'] = df[feature] ** 2        \n        \n    return train, test\n\ntrain_features2,test_features2=fe_stats(train_features2,test_features2)"]}, {"cell_type": "code", "execution_count": 1, "id": "554d5d0c", "metadata": {}, "outputs": [], "source": ["train_features_stats=train_features2.iloc[:,902:]\ntest_features_stats=test_features2.iloc[:,902:]"]}, {"cell_type": "code", "execution_count": 1, "id": "ad2e073b", "metadata": {}, "outputs": [], "source": ["train_features = pd.concat((train_features, train_features_cluster,train_cluster_pca,train_features_stats), axis=1)\ntest_features = pd.concat((test_features, test_features_cluster,test_cluster_pca,test_features_stats), axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "adfdc9dd", "metadata": {}, "outputs": [], "source": ["train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntarget = train[train_targets_scored.columns]"]}, {"cell_type": "code", "execution_count": 1, "id": "e83802ee", "metadata": {}, "outputs": [], "source": ["train = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "e568afbe", "metadata": {}, "outputs": [], "source": ["target_cols = target.drop('sig_id', axis=1).columns.values.tolist()"]}, {"cell_type": "code", "execution_count": 1, "id": "b2a51d8f", "metadata": {}, "outputs": [], "source": ["target=target[target_cols]"]}, {"cell_type": "code", "execution_count": 1, "id": "5df601a6", "metadata": {}, "outputs": [], "source": ["train = pd.get_dummies(train, columns=['cp_time','cp_dose'])\ntest_ = pd.get_dummies(test, columns=['cp_time','cp_dose'])"]}, {"cell_type": "code", "execution_count": 1, "id": "4184cd96", "metadata": {}, "outputs": [], "source": ["feature_cols = [c for c in train.columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['sig_id']]"]}, {"cell_type": "code", "execution_count": 1, "id": "704211b7", "metadata": {}, "outputs": [], "source": ["train = train[feature_cols]\ntest = test_[feature_cols]"]}, {"cell_type": "code", "execution_count": 1, "id": "cfa58cf0", "metadata": {}, "outputs": [], "source": ["X_test = test.values"]}, {"cell_type": "code", "execution_count": 1, "id": "558ff3fe", "metadata": {}, "outputs": [], "source": ["class LogitsLogLoss(Metric):\n\n    def __init__(self):\n        self._name = \"logits_ll\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_pred):\n        logits = 1 / (1 + np.exp(-y_pred))\n        aux = (1 - y_true) * np.log(1 - logits + 5e-5) + y_true * np.log(logits + 5e-5)\n        return np.mean(-aux)"]}, {"cell_type": "code", "execution_count": 1, "id": "e90283fa", "metadata": {}, "outputs": [], "source": ["MAX_EPOCH = 200\n\ntabnet_params = dict(\n    n_d = 32,\n    n_a = 32,\n    n_steps = 1,\n    gamma = 1.3,\n    lambda_sparse = 0,\n    optimizer_fn = optim.Adam,\n    optimizer_params = dict(lr = 2e-2, weight_decay = 1e-5),\n    mask_type = \"entmax\",\n    scheduler_params = dict(mode = \"min\", patience = 5, min_lr = 1e-5, factor = 0.9),\n    scheduler_fn = ReduceLROnPlateau,\n    seed = seed,\n    verbose = 10\n)"]}, {"cell_type": "code", "execution_count": 1, "id": "2a09b713", "metadata": {}, "outputs": [], "source": ["%%time\ntest_cv_preds = []\n\nNB_SPLITS = 10\nmskf = MultilabelStratifiedKFold(n_splits = NB_SPLITS, random_state = 0, shuffle = True)\nSEED = [20,21,22]\nfor s in SEED:\n    tabnet_params['seed'] = s\n    for fold_nb, (train_idx, val_idx) in enumerate(mskf.split(train, target)):\n        \n        model = TabNetRegressor()\n        ### Predict on test ###\n        model.load_model(f\"../input/moa-tabnet-train-inference/TabNet_seed_{s}_fold_{fold_nb+1}.zip\")\n        preds_test = model.predict(X_test)\n        test_cv_preds.append(1 / (1 + np.exp(-preds_test)))\n\ntest_preds_all = np.stack(test_cv_preds)"]}, {"cell_type": "code", "execution_count": 1, "id": "66b69ed4", "metadata": {}, "outputs": [], "source": ["all_feat = [col for col in df.columns if col not in [\"sig_id\"]]\n# To obtain the same lenght of test_preds_all and submission\ntest = pd.read_csv(\"../input/lish-moa/test_features.csv\")\nsig_id = test[test[\"cp_type\"] != \"ctl_vehicle\"].sig_id.reset_index(drop = True)\ntmp = pd.DataFrame(test_preds_all.mean(axis = 0), columns = all_feat)\ntmp[\"sig_id\"] = sig_id\n\nsub_tabnet = pd.merge(test[[\"sig_id\"]], tmp, on = \"sig_id\", how = \"left\")\nsub_tabnet.fillna(0, inplace = True)\nsub_tabnet"]}, {"cell_type": "markdown", "id": "a5709ce6", "metadata": {}, "source": ["# ResNet"]}, {"cell_type": "code", "execution_count": 1, "id": "4e5d6c71", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import KFold\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.decomposition import PCA\nfrom tensorflow.keras import layers,regularizers,Sequential,Model,backend,callbacks,optimizers,metrics,losses"]}, {"cell_type": "code", "execution_count": 1, "id": "85a68b56", "metadata": {}, "outputs": [], "source": ["# Import train data, drop sig_id, cp_type\n\ntrain_features = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\nnon_ctl_idx = train_features.loc[train_features['cp_type']!='ctl_vehicle'].index.to_list()\ntrain_features = train_features.drop(['sig_id','cp_type','cp_dose','cp_time'],axis=1)\ntrain_targets_scored = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\ntrain_targets_scored = train_targets_scored.drop('sig_id',axis=1)\nlabels_train = train_targets_scored.values\n\n# Drop training data with ctl vehicle\n\ntrain_features = train_features.iloc[non_ctl_idx]\nlabels_train = labels_train[non_ctl_idx]\n\n# Import test data\n\ntest_features = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\ntest_features = test_features.drop(['sig_id','cp_dose','cp_time'],axis=1)\n\n# Import predictors from public kernel\n\njson_file_path = '../input/t-test-pca-rfe-logistic-regression/main_predictors.json'\n\nwith open(json_file_path, 'r') as j:\n    predictors = json.loads(j.read())\n    predictors = predictors['start_predictors']"]}, {"cell_type": "code", "execution_count": 1, "id": "77a52c73", "metadata": {}, "outputs": [], "source": ["# Create g-mean, c-mean, genes_pca (2 components), cells_pca (all components)\n\ncs = train_features.columns.str.contains('c-')\ngs = train_features.columns.str.contains('g-')\n\ndef preprocessor(train,test):\n    # PCA\n    n_gs = 2 # No of PCA comps to include\n    n_cs = 100 # No of PCA comps to include\n    \n    pca_cs = PCA(n_components = n_cs)\n    pca_gs = PCA(n_components = n_gs)\n\n    train_pca_gs = pca_gs.fit_transform(train[:,gs])\n    train_pca_cs = pca_cs.fit_transform(train[:,cs])\n    test_pca_gs = pca_gs.transform(test[:,gs])\n    test_pca_cs = pca_cs.transform(test[:,cs])\n    \n    # c-mean, g-mean\n    \n    train_c_mean = train[:,cs].mean(axis=1)\n    test_c_mean = test[:,cs].mean(axis=1)\n    train_g_mean = train[:,gs].mean(axis=1)\n    test_g_mean = test[:,gs].mean(axis=1)\n    \n    # Append Features\n    \n    train = np.concatenate((train,train_pca_gs,train_pca_cs,train_c_mean[:,np.newaxis]\n                            ,train_g_mean[:,np.newaxis]),axis=1)\n    test = np.concatenate((test,test_pca_gs,test_pca_cs,test_c_mean[:,np.newaxis],\n                           test_g_mean[:,np.newaxis]),axis=1)\n    \n    # Scaler for numerical values\n\n    # Scale train data\n    scaler = preprocessing.StandardScaler()\n\n    train = scaler.fit_transform(train)\n\n    # Scale Test data\n    test = scaler.transform(test)\n    \n    return train, test"]}, {"cell_type": "code", "execution_count": 1, "id": "63729501", "metadata": {}, "outputs": [], "source": ["n_labels = train_targets_scored.shape[1]\nn_train = train_features.shape[0]\nn_test = test_features.shape[0]\n\n\n# Prediction Clipping Thresholds\n\np_min = 0.0005\np_max = 0.9995\n\n# OOF Evaluation Metric with clipping and no label smoothing\n\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return -backend.mean(y_true*backend.log(y_pred) + (1-y_true)*backend.log(1-y_pred))"]}, {"cell_type": "code", "execution_count": 1, "id": "5926a865", "metadata": {}, "outputs": [], "source": ["def build_model(n_features, n_features_2, n_labels, label_smoothing = 0.0005):    \n    input_1 = layers.Input(shape = (n_features,), name = 'Input1')\n    input_2 = layers.Input(shape = (n_features_2,), name = 'Input2')\n\n    head_1 = Sequential([\n        layers.BatchNormalization(),\n        layers.Dropout(0.2),\n        layers.Dense(512, activation=\"elu\"), \n        layers.BatchNormalization(),\n        layers.Dense(256, activation = \"elu\")\n        ],name='Head1') \n\n    input_3 = head_1(input_1)\n    input_3_concat = layers.Concatenate()([input_2, input_3])\n\n    head_2 = Sequential([\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        layers.Dense(512, \"relu\"),\n        layers.BatchNormalization(),\n        layers.Dense(512, \"elu\"),\n        layers.BatchNormalization(),\n        layers.Dense(256, \"relu\"),\n        layers.BatchNormalization(),\n        layers.Dense(256, \"elu\")\n        ],name='Head2')\n\n    input_4 = head_2(input_3_concat)\n    input_4_avg = layers.Average()([input_3, input_4]) \n\n    head_3 = Sequential([\n        layers.BatchNormalization(),\n        layers.Dense(256, kernel_initializer='lecun_normal', activation='selu'),\n        layers.BatchNormalization(),\n        layers.Dense(n_labels, kernel_initializer='lecun_normal', activation='selu'),\n        layers.BatchNormalization(),\n        layers.Dense(n_labels, activation=\"sigmoid\")\n        ],name='Head3')\n\n    output = head_3(input_4_avg)\n\n\n    model = Model(inputs = [input_1, input_2], outputs = output)\n    model.compile(optimizer='adam', loss=losses.BinaryCrossentropy(label_smoothing=label_smoothing), metrics=logloss)\n    \n    return model"]}, {"cell_type": "code", "execution_count": 1, "id": "d9fcf5ec", "metadata": {}, "outputs": [], "source": ["%%time\n\nn_seeds = 5\nnp.random.seed(1)\nseeds = np.random.randint(0,100,size=n_seeds)\n\n# Training Loop\n\nn_folds = 10\ny_pred = np.zeros((n_test,n_labels))\noof = tf.constant(0.0)\nhists = []\nfor seed in seeds:\n    fold = 0\n    kf = KFold(n_splits=n_folds,shuffle=True,random_state=seed)\n    for train, test in kf.split(train_features):\n        print(f'FOLD: {fold}, SEED: {seed}')\n        X_train, X_test = preprocessor(train_features.iloc[train].values,\n                                       train_features.iloc[test].values)\n        _,data_test = preprocessor(train_features.iloc[train].values,\n                                   test_features.drop('cp_type',axis=1).values)\n        X_train_2 = train_features.iloc[train][predictors].values\n        X_test_2 = train_features.iloc[test][predictors].values\n        data_test_2 = test_features[predictors].values\n        y_train = labels_train[train]\n        y_test = labels_train[test]\n        n_features = X_train.shape[1]\n        n_features_2 = X_train_2.shape[1]\n\n        model = build_model(n_features, n_features_2, n_labels)\n        \n        reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_logloss', factor=0.1, patience=2, mode='min', min_lr=1E-5)\n        early_stopping = callbacks.EarlyStopping(monitor='val_logloss', min_delta=1E-5, patience=10, mode='min',restore_best_weights=True)\n\n        hist = model.fit([X_train,X_train_2],y_train, batch_size=128, epochs=182,verbose=0,validation_data = ([X_test,X_test_2],y_test),\n                         callbacks=[reduce_lr, early_stopping])\n        hists.append(hist)\n        \n        # Save Model\n        model.save('ResNet_seed_'+str(seed)+'_fold_'+str(fold))\n\n        # OOF Score\n        y_val = model.predict([X_test,X_test_2])\n        oof += logloss(tf.constant(y_test,dtype=tf.float32),tf.constant(y_val,dtype=tf.float32))/(n_folds*n_seeds)\n\n        # Run prediction\n        y_pred += model.predict([data_test,data_test_2])/(n_folds*n_seeds)\n\n        fold += 1\n    tf.keras.backend.clear_session()\n    del model, X_train, X_test, X_train_2, X_test_2"]}, {"cell_type": "code", "execution_count": 1, "id": "d9342a15", "metadata": {}, "outputs": [], "source": ["tf.print('\\nOOF score is ',oof)"]}, {"cell_type": "code", "execution_count": 1, "id": "09a4140c", "metadata": {}, "outputs": [], "source": ["sub_resnet = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')\nsub_resnet.iloc[:,1:] = np.clip(y_pred,p_min,p_max)\n# sub_resnet.iloc[:,1:] = y_pred[0]\n\n# Set ctl_vehicle to 0\nsub_resnet.iloc[test_features['cp_type'] == 'ctl_vehicle',1:] = 0\nsub_resnet"]}, {"cell_type": "markdown", "id": "d4e84317", "metadata": {}, "source": ["# Label Smoothing"]}, {"cell_type": "code", "execution_count": 1, "id": "88089daf", "metadata": {}, "outputs": [], "source": ["%%time\ntrain_features = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\nnon_ctl_idx = train_features.loc[train_features['cp_type']!='ctl_vehicle'].index.to_list()\ntrain_features = train_features.drop(['sig_id','cp_type'],axis=1)\ntrain_targets_scored = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\ntrain_targets_scored = train_targets_scored.drop('sig_id',axis=1)\nlabels_train = train_targets_scored.values\n\n# Drop training data with ctl vehicle\n\ntrain_features = train_features.iloc[non_ctl_idx]\nlabels_train = labels_train[non_ctl_idx]\n\n# Import test data\n\ntest_features = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\ntest_features = test_features.drop('sig_id',axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "d00b2044", "metadata": {}, "outputs": [], "source": ["# Label Encoder for categorical cp_dose\n\ncat = 'cp_dose'\nle = preprocessing.LabelEncoder()\nle.fit(train_features[cat])\ntrain_features[cat] = le.transform(train_features[cat])\n\n# Transform categorical\n\ntest_features[cat] = le.transform(test_features[cat])"]}, {"cell_type": "code", "execution_count": 1, "id": "73c855ef", "metadata": {}, "outputs": [], "source": ["# Fit scaler to joint train and test data\nscaler = preprocessing.MinMaxScaler()\nscaler.fit(train_features.append(test_features.drop('cp_type',axis=1)))\n\n# Scale train data\ndata_train = scaler.transform(train_features)\n\n# Scale test data\ndata_test = scaler.transform(test_features.drop('cp_type',axis=1))"]}, {"cell_type": "code", "execution_count": 1, "id": "3382828e", "metadata": {}, "outputs": [], "source": ["%%time\nn_labels = train_targets_scored.shape[1]\nn_features = data_train.shape[1]\nn_train = data_train.shape[0]\nn_test = data_test.shape[0]\n\n\n# Prediction Clipping Thresholds\n\np_min = 0.001\np_max = 0.999\n\n# Evaluation Metric with clipping and no label smoothing\n\n@tf.autograph.experimental.do_not_convert\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return -backend.mean(y_true*backend.log(y_pred) + (1-y_true)*backend.log(1-y_pred))\n\n\n# Generate Seeds\n\nn_seeds = 6\n\nnp.random.seed(1)\nseeds = np.random.randint(0,100,size=n_seeds)\n\n# Training Loop\n\nn_folds = 5\ny_pred = np.zeros((n_test,n_labels))\noof = tf.constant(0.0)\nhists = []\nfor seed in seeds:\n    fold = 0\n    mskf = MultilabelStratifiedKFold(n_splits=n_folds,shuffle=True,random_state=seed)\n    for train, test in mskf.split(data_train,labels_train):\n        \n        X_train = data_train[train]\n        X_test = data_train[test]\n        y_train = labels_train[train]\n        y_test = labels_train[test]\n\n        # Define NN Model\n\n        model = Sequential()\n        model.add(layers.Dense(2048, input_shape=(n_features,)))\n        model.add(layers.Activation('relu'))\n        model.add(layers.BatchNormalization())\n        model.add(layers.Dropout(0.3))\n        model.add(layers.Dense(1024))\n        model.add(layers.Activation('relu'))\n        model.add(layers.BatchNormalization())\n        model.add(layers.Dropout(0.3))\n        model.add(layers.Dense(512))\n        model.add(layers.Activation('relu'))\n        model.add(layers.BatchNormalization())\n        model.add(layers.Dropout(0.3))        \n        model.add(layers.Dense(n_labels))\n        model.add(layers.Activation('sigmoid')) \n        model.compile(optimizer='adam', loss=losses.BinaryCrossentropy(label_smoothing=0.001), metrics=logloss)\n        reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_logloss', factor=0.3, patience=5, mode='min', min_lr=1E-5)\n        early_stopping = callbacks.EarlyStopping(monitor='val_logloss', min_delta=1E-5, patience=15, mode='min',restore_best_weights=True)\n\n        hist = model.fit(X_train,y_train, batch_size=128, epochs=182,verbose=0,validation_data = (X_test,y_test),callbacks=[reduce_lr, early_stopping])\n        hists.append(hist)\n        \n        # Save Model\n        model.save('LabelSmooth_seed_'+str(seed)+'_fold_'+str(fold))\n\n        # OOF Score\n        y_val = model.predict(X_test)\n        oof += logloss(tf.constant(y_test,dtype=tf.float32),tf.constant(y_val,dtype=tf.float32))/(n_folds*n_seeds)\n\n        # Run prediction\n        y_pred += model.predict(data_test)/(n_folds*n_seeds)\n\n        fold += 1\n\ntf.print('OOF score is ',oof)"]}, {"cell_type": "code", "execution_count": 1, "id": "62101df1", "metadata": {}, "outputs": [], "source": ["sub_smooth = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')\nsub_smooth.iloc[:,1:] = np.clip(y_pred,p_min,p_max)\n\nsub_smooth.iloc[test_features['cp_type'] == 'ctl_vehicle',1:] = 0\nsub_smooth"]}, {"cell_type": "markdown", "id": "b3e98b3c", "metadata": {}, "source": ["# XGB"]}, {"cell_type": "code", "execution_count": 1, "id": "80bca964", "metadata": {}, "outputs": [], "source": ["from xgboost import XGBClassifier\nfrom category_encoders import CountEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import log_loss\nfrom sklearn.multioutput import MultiOutputClassifier\n\nSEED = 34\nNFOLDS = 5\nDATA_DIR = '/kaggle/input/lish-moa/'\nnp.random.seed(SEED)\n\ntrain = pd.read_csv(DATA_DIR + 'train_features.csv')\nnon_ctl_idx = train.loc[train['cp_type']!='ctl_vehicle'].index.to_list()\ntrain_drug = pd.read_csv(DATA_DIR + 'train_drug.csv')\ntargets = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\ntest = pd.read_csv(DATA_DIR + 'test_features.csv')\nsub_xgb = pd.read_csv(DATA_DIR + 'sample_submission.csv')\n\n# drop where cp_type==ctl_vehicle (baseline)\nctl_mask = train.cp_type=='ctl_vehicle'\ntrain = train[~ctl_mask]\ntargets = targets[~ctl_mask]\n\n# drop id col\nX = train.iloc[:,1:].to_numpy()\nX_test = test.iloc[:,1:].to_numpy()\ny = targets.iloc[:,1:].to_numpy() \n\nclf = Pipeline([('encode', CountEncoder(cols=[0, 2])),\n                ('classify', MultiOutputClassifier(\n                                 XGBClassifier(tree_method='gpu_hist')))\n               ])\n\nparams = {'classify__estimator__colsample_bytree': 0.6522,\n          'classify__estimator__gamma': 3.6975,\n          'classify__estimator__learning_rate': 0.0503,\n          'classify__estimator__max_delta_step': 2.0706,\n          'classify__estimator__max_depth': 10,\n          'classify__estimator__min_child_weight': 31.5800,\n          'classify__estimator__n_estimators': 166,\n          'classify__estimator__subsample': 0.8639\n         }\n\n_ = clf.set_params(**params)"]}, {"cell_type": "code", "execution_count": 1, "id": "584507cb", "metadata": {}, "outputs": [], "source": ["def assign_folds(seed,fold):\n    scored = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\n    targets = scored.columns[1:]\n    scored = scored.merge(train_drug, on='sig_id', how='left') \n    scored = scored.iloc[non_ctl_idx]\n\n    # LOCATE DRUGS\n    vc = scored.drug_id.value_counts()\n    vc1 = vc.loc[vc<=18].index.sort_values()\n    vc2 = vc.loc[vc>18].index.sort_values()\n\n    # STRATIFY DRUGS 18X OR LESS\n    dct1 = {}; dct2 = {}\n    skf = MultilabelStratifiedKFold(n_splits=fold, shuffle=True, \n              random_state=seed)\n    tmp = scored.groupby('drug_id')[targets].mean().loc[vc1]\n    for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n        dd = {k:fold for k in tmp.index[idxV].values}\n        dct1.update(dd)\n\n    # STRATIFY DRUGS MORE THAN 18X\n    skf = MultilabelStratifiedKFold(n_splits=fold, shuffle=True, \n              random_state=seed)\n    tmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop=True)\n    for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n        dd = {k:fold for k in tmp.sig_id[idxV].values}\n        dct2.update(dd)\n\n    # ASSIGN FOLDS\n    scored['fold'] = scored.drug_id.map(dct1)\n    scored.loc[scored.fold.isna(),'fold'] =\\\n        scored.loc[scored.fold.isna(),'sig_id'].map(dct2)\n    scored.fold = scored.fold.astype('int8')\n\n    return scored['fold'].values"]}, {"cell_type": "code", "execution_count": 1, "id": "b5b25e9c", "metadata": {}, "outputs": [], "source": ["use_xgb = True"]}, {"cell_type": "code", "execution_count": 1, "id": "b1af6d3d", "metadata": {}, "outputs": [], "source": ["%%time\nif use_xgb:\n    oof_preds = np.zeros(y.shape)\n    test_preds = np.zeros((test.shape[0], y.shape[1]))\n    oof_losses = []\n\n    x = assign_folds(SEED,NFOLDS)\n\n    for n in range(NFOLDS):\n        print(f'Inferencing FOLD: {n}')\n        trn_idx = np.where(x != n)[0]\n        val_idx = np.where(x == n)[0]\n\n        X_train, X_val = X[trn_idx], X[val_idx]\n        y_train, y_val = y[trn_idx], y[val_idx]\n\n        # drop where cp_type==ctl_vehicle (baseline)\n        ctl_mask = X_train[:,0]=='ctl_vehicle'\n        X_train = X_train[~ctl_mask,:]\n        y_train = y_train[~ctl_mask]\n\n        clf = pickle.load(open(f'../input/moa-xgb-weights/xgb_cv_fold_{n}.dat', \"rb\"))\n\n        val_preds = clf.predict_proba(X_val) # list of preds per class\n        val_preds = np.array(val_preds)[:,:,1].T # take the positive class\n        oof_preds[val_idx] = val_preds\n\n        loss = log_loss(np.ravel(y_val), np.ravel(val_preds))\n        oof_losses.append(loss)\n        preds = clf.predict_proba(X_test)\n        preds = np.array(preds)[:,:,1].T # take the positive class\n        test_preds += preds / NFOLDS\n\n    print(oof_losses)\n    print('Mean OOF loss across folds', np.mean(oof_losses))\n    print('STD OOF loss across folds', np.std(oof_losses))"]}, {"cell_type": "code", "execution_count": 1, "id": "1cca6366", "metadata": {}, "outputs": [], "source": ["control_mask = train['cp_type']=='ctl_vehicle'\noof_preds[control_mask] = 0\n\nprint('OOF log loss: ', log_loss(np.ravel(y), np.ravel(oof_preds)))"]}, {"cell_type": "code", "execution_count": 1, "id": "7cada736", "metadata": {}, "outputs": [], "source": ["control_mask = test['cp_type']=='ctl_vehicle'\ntest_preds[control_mask] = 0\n\nsub_xgb.iloc[:,1:] = test_preds\nsub_xgb"]}, {"cell_type": "markdown", "id": "5017b630", "metadata": {}, "source": ["# Ensemble"]}, {"cell_type": "code", "execution_count": 1, "id": "7abd7282", "metadata": {}, "outputs": [], "source": ["%%time\nids = sub_tabnet[['sig_id']]\n\nsub_tabnet.drop('sig_id',inplace=True,axis=1)\nsub_resnet.drop('sig_id',inplace=True,axis=1)\nsub_smooth.drop('sig_id',inplace=True,axis=1)\nsub_xgb.drop('sig_id',inplace=True,axis=1)\nsub_gauss.drop('sig_id',inplace=True,axis=1) \n\nsub_en = 0.3*sub_gauss + 0.3*sub_tabnet + 0.25*sub_resnet +  0.1*sub_smooth +0.05*sub_xgb\nsub_en['sig_id'] = ids[['sig_id']]\ncols = sub_en.columns.tolist()\ncols.insert(0, cols.pop(cols.index('sig_id')))\nsub_en = sub_en.reindex(columns=cols)\nsub_en.to_csv('submission.csv',index=False)\nsub_en"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}