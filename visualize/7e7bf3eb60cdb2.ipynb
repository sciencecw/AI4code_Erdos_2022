{"cells": [{"cell_type": "code", "execution_count": 1, "id": "cc383aa6", "metadata": {}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n #   for filename in filenames:\n  #      print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output."]}, {"cell_type": "markdown", "id": "e26ea640", "metadata": {}, "source": ["**PATH Set**"]}, {"cell_type": "markdown", "id": "cd616a5d", "metadata": {}, "source": ["**Data Exploration and Analysis**"]}, {"cell_type": "code", "execution_count": 1, "id": "6868c673", "metadata": {}, "outputs": [], "source": ["dirname = '/kaggle/input'\ntrain_path = os.path.join(dirname, 'kermany2018/OCT2017 /train')\ntrain_normal_pth = os.path.join(train_path, 'NORMAL')\ntrain_dme_pth = os.path.join(train_path, 'DME')\ntrain_drusen_pth = os.path.join(train_path, 'DRUSEN')\ntrain_cnv_pth = os.path.join(train_path, 'CNV')\n    \ntest_path = os.path.join(dirname, 'kermany2018/OCT2017 /test')\ntest_normal_pth = os.path.join(test_path, 'NORMAL')\ntest_dme_pth = os.path.join(test_path, 'DME')\ntest_drusen_pth = os.path.join(test_path, 'DRUSEN')\ntest_cnv_pth = os.path.join(test_path, 'CNV')\n    \nval_path = os.path.join(dirname, 'kermany2018/OCT2017 /val')\nval_normal_pth = os.path.join(val_path, 'NORMAL')\nval_dme_pth = os.path.join(val_path, 'DME')\nval_drusen_pth = os.path.join(val_path, 'DRUSEN')\nval_cnv_pth = os.path.join(val_path, 'CNV')"]}, {"cell_type": "code", "execution_count": 1, "id": "b491e0bc", "metadata": {}, "outputs": [], "source": ["print(test_normal_pth)\nprint(train_drusen_pth)"]}, {"cell_type": "code", "execution_count": 1, "id": "906c4c99", "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt"]}, {"cell_type": "code", "execution_count": 1, "id": "cb4d661c", "metadata": {}, "outputs": [], "source": ["def plot_imgs(item_dir, num_imgs=4):\n    all_item_dirs = os.listdir(item_dir)\n    item_files = [os.path.join(item_dir, file) for file in all_item_dirs][:num_imgs]\n\n    plt.figure(figsize=(16, 16))\n    for idx, img_path in enumerate(item_files):\n        plt.subplot(1, 4, idx+1)\n\n        img = plt.imread(img_path)\n        plt.imshow(img, cmap='bone')\n\n    plt.tight_layout()\n"]}, {"cell_type": "markdown", "id": "ff8b002a", "metadata": {}, "source": ["**NORMAL**"]}, {"cell_type": "code", "execution_count": 1, "id": "46990632", "metadata": {}, "outputs": [], "source": ["plot_imgs(train_normal_pth)"]}, {"cell_type": "markdown", "id": "0d1aae5d", "metadata": {}, "source": ["**DME**"]}, {"cell_type": "code", "execution_count": 1, "id": "e567d634", "metadata": {}, "outputs": [], "source": ["plot_imgs(train_dme_pth)"]}, {"cell_type": "markdown", "id": "ca9c6c41", "metadata": {}, "source": ["**DRUSEN**"]}, {"cell_type": "code", "execution_count": 1, "id": "30656de6", "metadata": {}, "outputs": [], "source": ["plot_imgs(train_drusen_pth)"]}, {"cell_type": "markdown", "id": "ab95b184", "metadata": {}, "source": ["**CNV**"]}, {"cell_type": "code", "execution_count": 1, "id": "6acf42b2", "metadata": {}, "outputs": [], "source": ["plot_imgs(train_cnv_pth)"]}, {"cell_type": "markdown", "id": "d3552b3b", "metadata": {}, "source": ["**Detais about image dimensions**"]}, {"cell_type": "code", "execution_count": 1, "id": "ab0a8b63", "metadata": {}, "outputs": [], "source": ["import glob\nfrom PIL import Image"]}, {"cell_type": "code", "execution_count": 1, "id": "9fe335e8", "metadata": {}, "outputs": [], "source": ["def Images_details_Print_data(data, path):\n    print(\" ----->>>Images in: \", path)    \n    for k, v in data.items():\n        print(\"%s:\\t%s\" % (k, v))\n\ndef Images_details(path):\n    files = [f for f in glob.glob(path + \"**/*.*\", recursive=True)]\n    data = {}\n    data['images_count'] = len(files)\n    data['min_width'] = 10**100  # No image will be bigger than that\n    data['max_width'] = 0\n    data['min_height'] = 10**100  # No image will be bigger than that\n    data['max_height'] = 0\n\n\n    for f in files:\n        im = Image.open(f)\n        width, height = im.size\n        data['min_width'] = min(width, data['min_width'])\n        data['min_height'] = min(height, data['min_height'])\n        data['max_width'] = max(width, data['max_height'])\n        \n        data['max_height'] = max(height, data['max_height'])\n\n    Images_details_Print_data(data, path)"]}, {"cell_type": "markdown", "id": "e064178e", "metadata": {}, "source": ["**TRAIN**"]}, {"cell_type": "code", "execution_count": 1, "id": "bd78a466", "metadata": {}, "outputs": [], "source": ["Images_details(train_normal_pth)\nImages_details(train_dme_pth)\nImages_details(train_drusen_pth)\nImages_details(train_cnv_pth)"]}, {"cell_type": "markdown", "id": "d4c6c1a8", "metadata": {}, "source": ["**TEST**"]}, {"cell_type": "code", "execution_count": 1, "id": "452c98d5", "metadata": {}, "outputs": [], "source": ["Images_details(test_normal_pth)\nImages_details(test_dme_pth)\nImages_details(test_drusen_pth)\nImages_details(test_cnv_pth)"]}, {"cell_type": "markdown", "id": "8e96e403", "metadata": {}, "source": ["**Validation**"]}, {"cell_type": "code", "execution_count": 1, "id": "803738c2", "metadata": {}, "outputs": [], "source": ["Images_details(val_normal_pth)\nImages_details(val_dme_pth)\nImages_details(val_drusen_pth)\nImages_details(val_cnv_pth)"]}, {"cell_type": "code", "execution_count": 1, "id": "33cb1315", "metadata": {}, "outputs": [], "source": ["input_path = \"/kaggle/input/kermany2018/OCT2017 /\"\n\nfor _set in ['train', 'test', 'val']:\n    normal = len(os.listdir(input_path + _set + '/NORMAL'))\n    dme = len(os.listdir(input_path + _set + '/DME'))\n    drusen = len(os.listdir(input_path + _set + '/DRUSEN'))\n    cnv = len(os.listdir(input_path + _set + '/CNV'))\n    print('{}, Normal images: {}, DME images: {}, DRUSEN images: {}, CNV images: {}'.format(_set, normal, dme, drusen, cnv))"]}, {"cell_type": "markdown", "id": "07320955", "metadata": {}, "source": ["dirname = '/kaggle/input'\ntrain_path = os.path.join(dirname, 'kermany2018/OCT2017 /train')\ntrain_normal_pth = os.path.join(train_path, 'NORMAL')\ntrain_dme_pth = os.path.join(train_path, 'DME')\ntrain_drusen_pth = os.path.join(train_path, 'DRUSEN')\ntrain_cnv_pth = os.path.join(train_path, 'CNV')\n    \ntest_path = os.path.join(dirname, 'kermany2018/OCT2017 /test')\ntest_normal_pth = os.path.join(test_path, 'NORMAL')\ntest_dme_pth = os.path.join(test_path, 'DME')\ntest_drusen_pth = os.path.join(test_path, 'DRUSEN')\ntest_cnv_pth = os.path.join(test_path, 'CNV')\n    \nval_path = os.path.join(dirname, 'kermany2018/OCT2017 /val')\nval_normal_pth = os.path.join(val_path, 'NORMAL')\nval_dme_pth = os.path.join(val_path, 'DME')\nval_drusen_pth = os.path.join(val_path, 'DRUSEN')\nval_cnv_pth = os.path.join(val_path, 'CNV')"]}, {"cell_type": "code", "execution_count": 1, "id": "2f61f7b8", "metadata": {}, "outputs": [], "source": ["datadir = '../input/kermany2018/OCT2017 /'\ntraindir = datadir + 'train/'\nvaliddir = datadir + 'val/'\ntestdir = datadir + 'test/'"]}, {"cell_type": "code", "execution_count": 1, "id": "9de1b2cd", "metadata": {}, "outputs": [], "source": ["# Empty lists\ncategories = []\nimg_categories = []\nn_train = []\nn_valid = []\nn_test = []\nhs = []\nws = []\n\n# Iterate through each category\nfor d in os.listdir(traindir):\n    categories.append(d)\n\n    # Number of each image\n    train_imgs = os.listdir(traindir + d)\n    valid_imgs = os.listdir(validdir + d)\n    test_imgs = os.listdir(testdir + d)\n    n_train.append(len(train_imgs))\n    n_valid.append(len(valid_imgs))\n    n_test.append(len(test_imgs))\n\n    # Find stats for train images\n    for i in train_imgs:\n        img_categories.append(d)\n        img = Image.open(traindir + d + '/' + i)\n        img_array = np.array(img)\n        # Shape\n        hs.append(img_array.shape[0])\n        ws.append(img_array.shape[1])\n\n# Dataframe of categories\ncat_df = pd.DataFrame({'category': categories,\n                       'n_train': n_train,\n                       'n_valid': n_valid, 'n_test': n_test}).\\\n    sort_values('category')\n\n# Dataframe of training images\nimage_df = pd.DataFrame({\n    'category': img_categories,\n    'height': hs,\n    'width': ws\n})\n\ncat_df.sort_values('n_train', ascending=False, inplace=True)\ncat_df.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "d259261a", "metadata": {}, "outputs": [], "source": ["cat_df.set_index('category')['n_train'].plot.bar(figsize=(15, 6))\nplt.xticks(rotation=80)\nplt.ylabel('Count')\nplt.title('Training Images by Category')\nplt.show()"]}, {"cell_type": "markdown", "id": "58694a8e", "metadata": {}, "source": ["**Distribution of Images Sizes**\n\nThe images themselves have vastly different shapes. We can see this by looking at the stats of images sizes by category."]}, {"cell_type": "code", "execution_count": 1, "id": "900dabef", "metadata": {}, "outputs": [], "source": ["img_dsc = image_df.groupby('category').describe()\nimg_dsc.head()\n"]}, {"cell_type": "code", "execution_count": 1, "id": "bca2f3fa", "metadata": {}, "outputs": [], "source": ["import seaborn as sns"]}, {"cell_type": "code", "execution_count": 1, "id": "84366ed2", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(10, 6))\nsns.kdeplot(\n    img_dsc['height']['mean'], label='Average Height')\nsns.kdeplot(\n    img_dsc['width']['mean'], label='Average Width')\nplt.xlabel('Pixels')\nplt.ylabel('Density')\nplt.title('Average Size Distribution')"]}, {"cell_type": "markdown", "id": "a8aa5e74", "metadata": {}, "source": ["Retinal optical coherence tomography (OCT) is an imaging technique used to capture high-resolution cross sections of the retinas of living patients. Total 83,484 OCT images are there in the training dataset. Also 1000 OCT images are there in the test dataset.\n\nIt is an imbalanced dataset. Training dataset contains below number of images:\n"]}, {"cell_type": "code", "execution_count": 1, "id": "9bdfebdb", "metadata": {}, "outputs": [], "source": ["import os\nfrom glob import glob\nimport matplotlib.pyplot as plt\nimport random\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nimport zlib\nimport itertools\nimport sklearn\nimport itertools\nimport scipy\nimport skimage\nfrom skimage.transform import resize\nimport csv\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold, GridSearchCV\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import confusion_matrix, make_scorer, accuracy_score, classification_report\nimport keras\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Lambda, MaxPool2D, BatchNormalization\nfrom keras.utils import np_utils\nfrom keras.utils.np_utils import to_categorical\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import models, layers, optimizers\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.utils import class_weight\nfrom keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta, RMSprop\nfrom keras.models import Sequential, model_from_json\nfrom keras.layers import Activation,Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.layers import MaxPooling2D,AveragePooling2D, GlobalAveragePooling2D,BatchNormalization\nfrom keras.preprocessing.image import array_to_img, img_to_array, load_img, ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom keras import backend as K\nfrom keras.applications.vgg16 import VGG16\nfrom keras.models import Model\nfrom keras.applications.mobilenet import MobileNet\nfrom keras.applications.inception_v3 import InceptionV3\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\n%matplotlib inline"]}, {"cell_type": "code", "execution_count": 1, "id": "4b2b7d41", "metadata": {}, "outputs": [], "source": ["# Model parameters\nimage_size = 64\nbatch_size = 32\nnum_classes = 4\nepochs = 20"]}, {"cell_type": "code", "execution_count": 1, "id": "48c63c6a", "metadata": {}, "outputs": [], "source": ["# Baseline Model.\nmodel = Sequential()\nmodel.add(Conv2D(256, kernel_size=(3, 3), activation='relu', input_shape=(image_size, image_size,3)))\nmodel.add(BatchNormalization())\n# model.add(Dropout(0.4))\n\nmodel.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n# model.add(Dropout(0.4))\n\nmodel.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\nmodel.add(BatchNormalization())\n# model.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.4))\n\nmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\nmodel.add(BatchNormalization())\n# model.add(Dropout(0.4))\n\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(num_classes, activation='softmax'))\n\nprint(model.summary())\n\nmodel.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])"]}, {"cell_type": "code", "execution_count": 1, "id": "bd02ec70", "metadata": {}, "outputs": [], "source": ["train_datagen = ImageDataGenerator(validation_split=0.2) # set validation split\n\ntrain_generator = train_datagen.flow_from_directory(traindir,target_size=(image_size, image_size),\n                                                    batch_size=batch_size,\n                                                    class_mode='categorical',\n                                                    subset='training') # set as training data\n\nvalidation_generator = train_datagen.flow_from_directory(traindir,target_size=(image_size,image_size),\n                                                         batch_size=batch_size,\n                                                         class_mode='categorical',\n                                                         subset='validation') # set as validation data\n\n\ntest_datagen = ImageDataGenerator()\ntest_generator = test_datagen.flow_from_directory(testdir,target_size=(image_size, image_size),\n                                                  batch_size=batch_size,\n                                                  class_mode='categorical')"]}, {"cell_type": "code", "execution_count": 1, "id": "6aa9a230", "metadata": {}, "outputs": [], "source": ["class_weights = class_weight.compute_class_weight('balanced',\n                                                  np.unique(train_generator.classes),\n                                                  train_generator.classes)"]}, {"cell_type": "code", "execution_count": 1, "id": "4a044eba", "metadata": {}, "outputs": [], "source": ["from keras.callbacks import ModelCheckpoint, EarlyStopping\nfilepath=\"/kaggle/output/5layered_best.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n# earlystop = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 3,verbose = 1,restore_best_weights = True)\ncallbacks_list = [checkpoint]"]}, {"cell_type": "code", "execution_count": 1, "id": "87683f23", "metadata": {}, "outputs": [], "source": ["history = model.fit_generator(train_generator,\n                              steps_per_epoch = train_generator.samples // batch_size,\n                              validation_data = validation_generator,\n                              validation_steps = validation_generator.samples // batch_size,\n                              epochs = epochs, \n                              callbacks=callbacks_list,\n                              class_weight=class_weights)"]}, {"cell_type": "code", "execution_count": 1, "id": "90bcfd9a", "metadata": {}, "outputs": [], "source": ["import pandas as pd\nhist_df = pd.DataFrame(history.history) \n\n# checkout result with print e.g.:    \nprint(hist_df)\n\nhist_df.to_csv('5Layerfile.csv',index=False)"]}, {"cell_type": "code", "execution_count": 1, "id": "a035b4ce", "metadata": {}, "outputs": [], "source": ["display(model.summary())"]}, {"cell_type": "code", "execution_count": 1, "id": "eb900e12", "metadata": {}, "outputs": [], "source": ["from keras.utils.vis_utils import plot_model\nplot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True,expand_nested=False)"]}, {"cell_type": "code", "execution_count": 1, "id": "47325c4a", "metadata": {}, "outputs": [], "source": ["# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"model_balanced_cnn5layered.json\", \"w\") as json_file:\n    json_file.write(model_json)"]}, {"cell_type": "code", "execution_count": 1, "id": "6c410d4c", "metadata": {}, "outputs": [], "source": ["score = model.evaluate_generator(test_generator,steps = test_generator.samples // batch_size) \nprint(\"\\n\\n\")\nprint('Testing Loss:', score[0])\nprint('Testing accuracy:', score[1])"]}, {"cell_type": "code", "execution_count": 1, "id": "2094bafc", "metadata": {}, "outputs": [], "source": ["def load_test_data(folder):\n    \"\"\"\n    Function to load the images and labels.\n    \"\"\"\n    Image = []\n    Label = []\n    \n    for folder_name in os.listdir(folder):\n        # Reading the labels.\n        if not folder_name.startswith('.'):\n            if folder_name in ['CNV']:\n                label = 0\n            elif folder_name in ['DME']:\n                label = 1\n            elif folder_name in ['DRUSEN']:\n                label = 2\n            elif folder_name in ['NORMAL']:\n                label = 3\n            else:\n                label = 4\n            for image_file_name in tqdm(os.listdir(folder + folder_name)):\n                # Reading the images.\n                image_file = cv2.imread(folder + folder_name + '/' + image_file_name)\n                if image_file is not None:\n                    # Converting images into array.\n                    image_file = skimage.transform.resize(image_file, (image_size, image_size, 3))\n                    image_array = np.asarray(image_file)\n                    Image.append(image_array)\n                    Label.append(label)\n    Image = np.asarray(Image)\n    Label = np.asarray(Label)\n    return Image,Label"]}, {"cell_type": "code", "execution_count": 1, "id": "ebcaa1c7", "metadata": {}, "outputs": [], "source": ["X_test,Y_test= load_test_data(testdir)"]}, {"cell_type": "code", "execution_count": 1, "id": "afe987e5", "metadata": {}, "outputs": [], "source": ["pred_datagen = ImageDataGenerator()\n\npred_generator = pred_datagen.flow_from_directory(testdir,target_size=(image_size, image_size),\n                                                  batch_size=1,\n                                                  class_mode='categorical',\n                                                  shuffle = False)"]}, {"cell_type": "code", "execution_count": 1, "id": "643d7a90", "metadata": {}, "outputs": [], "source": ["\npred_generator.reset()\ny_pred = model.predict_generator(pred_generator,steps = 968)\nY_test = pred_generator.classes[pred_generator.index_array]\nY_pred = np.argmax(y_pred, axis=-1)"]}, {"cell_type": "code", "execution_count": 1, "id": "7fd29a34", "metadata": {}, "outputs": [], "source": ["Y_test"]}, {"cell_type": "code", "execution_count": 1, "id": "bcfe3c06", "metadata": {}, "outputs": [], "source": ["Y_pred"]}, {"cell_type": "code", "execution_count": 1, "id": "bafaca70", "metadata": {}, "outputs": [], "source": ["for i in range(len(Y_test)):\n    if(Y_test[i] != Y_pred[i]):\n        print(\" Difference Found\\n\")\n        print(\"Expected = \", Y_test[i], \" predicted = \", Y_pred[i])\n        print(\"\\n\")"]}, {"cell_type": "code", "execution_count": 1, "id": "aa95dd35", "metadata": {}, "outputs": [], "source": ["Y_pred = np.argmax(y_pred,axis = 1)"]}, {"cell_type": "code", "execution_count": 1, "id": "6ef7dd60", "metadata": {}, "outputs": [], "source": ["import pickle\nwith open('y_pred_baseline_model.pkl','wb') as f:\n    pickle.dump(y_pred, f)"]}, {"cell_type": "code", "execution_count": 1, "id": "66311e73", "metadata": {}, "outputs": [], "source": ["print ('Train Accuracy', np.mean(history.history['accuracy']))\nprint ('Train Loss', np.mean(history.history['loss']))\nprint ('Validation Accuracy', np.mean(history.history['val_accuracy']))\nprint ('Validation Loss', np.mean(history.history['val_loss']))"]}, {"cell_type": "code", "execution_count": 1, "id": "62848f84", "metadata": {}, "outputs": [], "source": ["# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "5d602ec9", "metadata": {}, "outputs": [], "source": ["# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "4d160da0", "metadata": {}, "outputs": [], "source": ["def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')"]}, {"cell_type": "code", "execution_count": 1, "id": "f5bcbbf7", "metadata": {}, "outputs": [], "source": ["cm = confusion_matrix(Y_test, Y_pred)\n \nprint('--------------------')\nprint('| Confusion Matrix |')\nprint('--------------------')\nprint('\\n {}'.format(cm))\n        \n# plot confusin matrix\nplt.figure(figsize=(8,8))\nplt.grid(b=False)\nplot_confusion_matrix(cm, classes=['CNV','DME','DRUSEN','NORMAL'], normalize=False, \n                      title='Confusion matrix', cmap = plt.cm.Blues)"]}, {"cell_type": "code", "execution_count": 1, "id": "40fb25c6", "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import classification_report\nprint(classification_report(Y_test,Y_pred,target_names=['CNV','DME','DRUSEN','Normal']))"]}, {"cell_type": "code", "execution_count": 1, "id": "ad526898", "metadata": {}, "outputs": [], "source": ["FP = cm.sum(axis=0) - np.diag(cm)  \nFN = cm.sum(axis=1) - np.diag(cm)\nTP = np.diag(cm)\nTN = cm.sum() - (FP + FN + TP)\n\nFP = FP.astype(float)\nFN = FN.astype(float)\nTP = TP.astype(float)\nTN = TN.astype(float)\n\n# Sensitivity, hit rate, recall, or true positive rate\nTPR = TP/(TP+FN)\nprint(\"Recall/TPR = {}\".format(TPR))\n\n# Specificity or true negative rate\nTNR = TN/(TN+FP) \nprint(\"Specificity/TNR = {}\".format(TNR))\n\n# Precision or positive predictive value\nPPV = TP/(TP+FP)\nprint(\"Precision/PPV = {}\".format(PPV))\n\n# Negative predictive value\nNPV = TN/(TN+FN)\nprint(\"Negative Predict Value = {}\".format(NPV))\n\n# Fall out or false positive rate\nFPR = FP/(FP+TN)\nprint(\"False Positive Rate = {}\".format(FPR))\n\n# False negative rate\nFNR = FN/(TP+FN)\nprint(\"False Negative Rate = {}\".format(FNR))\n\n# False discovery rate\nFDR = FP/(TP+FP)\nprint(\"False discovery rate = {}\".format(FDR))\n\n# Overall accuracy\nACC = (TP+TN)/(TP+FP+FN+TN)\nprint(\"Overall Accuracy = {}\".format(ACC))"]}, {"cell_type": "markdown", "id": "8b826f73", "metadata": {}, "source": ["**Layer Visualisation**"]}, {"cell_type": "code", "execution_count": 1, "id": "3fc2e7bd", "metadata": {}, "outputs": [], "source": ["import numpy as np   \nfrom keras.preprocessing import image    \nim1_path=\"../input/kermany2018/OCT2017 /test/DME/DME-15208-2.jpeg\"\ntest_image=image.load_img(im1_path,target_size=(64,64))"]}, {"cell_type": "code", "execution_count": 1, "id": "4a341295", "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\nplt.imshow(test_image)\n# now to convert to 3 dimensional from 2d\ntest_image=image.img_to_array(test_image)\nprint(test_image.size)"]}, {"cell_type": "code", "execution_count": 1, "id": "fb09cb0e", "metadata": {}, "outputs": [], "source": ["def get_name_layer_filters(model):\n    filter_whole=[]\n    layer_whole=[]\n    for layer in model.layers:\n        if 'conv' not in layer.name:\n            continue\n        filters,biases=layer.get_weights()\n        filter_whole.append(filters)\n        layer_whole.append(biases)\n        print(layer.name,filters.shape)\n    return filter_whole,layer_whole "]}, {"cell_type": "code", "execution_count": 1, "id": "930f833e", "metadata": {}, "outputs": [], "source": ["filter_whole,layer_whole=get_name_layer_filters(model)"]}, {"cell_type": "code", "execution_count": 1, "id": "a370ab6e", "metadata": {}, "outputs": [], "source": ["filters,biases=model.layers[0].get_weights()"]}, {"cell_type": "code", "execution_count": 1, "id": "a6614cfc", "metadata": {}, "outputs": [], "source": ["f_min,f_max=filters.min(),filters.max()\nfilters=(filters-f_min)/(f_max-f_min)"]}, {"cell_type": "code", "execution_count": 1, "id": "e23357a4", "metadata": {}, "outputs": [], "source": ["from matplotlib import pyplot\nn_filters,ix=6,1\nfor i in range(n_filters):\n    f=filters[:,:,:,i]\n    #Plot each channel\n    for j in range(3):\n        ax=pyplot.subplot(n_filters,3,ix)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        #Plot filter channel\n        pyplot.imshow(f[:,:,j])\n        ix+=1\n        \npyplot.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "7d5f9079", "metadata": {}, "outputs": [], "source": ["from keras.models import Model\nmodel_feature=Model(inputs=model.inputs,outputs=model.layers[4].output)"]}, {"cell_type": "code", "execution_count": 1, "id": "4c878bf5", "metadata": {}, "outputs": [], "source": ["test_image = np.expand_dims(test_image, axis=0)\nfeature_map=model_feature.predict(test_image)"]}, {"cell_type": "code", "execution_count": 1, "id": "42827b13", "metadata": {}, "outputs": [], "source": ["feature_map.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "cfe2e411", "metadata": {}, "outputs": [], "source": ["pyplot.figure(figsize=(10,10))        \n        \nsquare=8\nix=1\nfor _ in range(4):\n    for _ in range(8):\n        ax=pyplot.subplot(square,square,ix)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        pyplot.imshow(feature_map[0,:,:,ix-1])\n        ix+=1\n        \npyplot.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "fc00f8e7", "metadata": {}, "outputs": [], "source": ["def get_convolutional_layers(model):\n    convolutions_models=[]\n    for layer in model.layers:\n        if 'conv2d' not in layer.name:\n            continue\n        model_temp=Model(inputs=model.inputs,outputs=layer.output)\n        convolutions_models.append(model_temp)\n    return convolutions_models    \n        "]}, {"cell_type": "code", "execution_count": 1, "id": "a2b2e5d9", "metadata": {}, "outputs": [], "source": ["def generate_feature_maps(model,test_image):\n    models=get_convolutional_layers(model)#Fetching convolution layers models\n    feature_maps=[]\n    \n    for model_temp in models:\n        feature_map=model_feature.predict(test_image)\n        feature_maps.append(feature_map)\n    return feature_maps,models   "]}, {"cell_type": "code", "execution_count": 1, "id": "501c85c4", "metadata": {}, "outputs": [], "source": ["def plot_graph(feature_map):\n    \n    #plot all 32 maps in an 8*4 squares\n    pyplot.figure(figsize=(10,10))        \n        \n    square=8\n    ix=1\n    for _ in range(4):\n        for _ in range(8):\n            ax=pyplot.subplot(square,square,ix)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            pyplot.imshow(feature_map[0,:,:,ix-1],cmap='gist_gray')\n            ix+=1\n        \n    pyplot.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "7082cbe8", "metadata": {}, "outputs": [], "source": ["def plots_generator(model):\n    print(\"IMAGE UNDER CONSIDERATION\")\n    test_image=image.load_img(im1_path,target_size=(64,64))\n    plt.imshow(test_image)\n    test_image=image.img_to_array(test_image)\n\n    test_image= np.expand_dims(test_image,axis=0)\n    print()\n    feature_maps,models=generate_feature_maps(model,test_image)\n    #ax=pyplot.subplot(square,square,ix)# only 32 filters will be shown of each layer\n    counter=1\n    for each_map in feature_maps:\n        print(\"Convolutional Layer Number {} \".format(counter))\n        counter+=1\n        #ax=pyplot.subplot(square,square,ix)\n        plot_graph(each_map)\n"]}, {"cell_type": "code", "execution_count": 1, "id": "28fd3cf9", "metadata": {}, "outputs": [], "source": ["plots_generator(model)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}