{"cells": [{"cell_type": "markdown", "id": "43b5e5ee", "metadata": {}, "source": ["# House Prices - A brief introduction\n\n\n## Setup"]}, {"cell_type": "code", "execution_count": 1, "id": "a6e9d011", "metadata": {}, "outputs": [], "source": ["import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns    \nimport math\n\nfrom scipy import stats\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom sklearn.preprocessing import  LabelEncoder\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, BayesianRidge, RidgeCV\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom mlxtend.regressor import StackingCVRegressor\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\ntrain = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ntest = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\n\ntrain.columns"]}, {"cell_type": "markdown", "id": "61404c79", "metadata": {}, "source": ["## Data Processing\n### 1. Target\n' SalePrice ' is our target which we want to predict, so the first thing is to check this property in our dataset. "]}, {"cell_type": "code", "execution_count": 1, "id": "a440499b", "metadata": {}, "outputs": [], "source": ["train['SalePrice'].describe()"]}, {"cell_type": "markdown", "id": "3fadb73d", "metadata": {}, "source": ["#### 1.1.  Relationship\nHeatmap is a good way to get a clear overview of our variable's relationships."]}, {"cell_type": "code", "execution_count": 1, "id": "aaf9f1d8", "metadata": {}, "outputs": [], "source": ["corrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);"]}, {"cell_type": "markdown", "id": "a73f2582", "metadata": {}, "source": ["According to heat map, here are some variables more correlated with ' SalePrice ' :\n\n- OverallQual\n- GrLivArea\n- GarageCars\n- GarageArea\n- TotalBsmtSF\n\n\n' GrLivArea ' and ' TotalBsmtSF ' are numerical variables which we can check relationship first.\n\n#### GrLivArea"]}, {"cell_type": "code", "execution_count": 1, "id": "fdaca085", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()"]}, {"cell_type": "markdown", "id": "e22508b9", "metadata": {}, "source": ["As we can see in the image, ' SalePrice ' and ' GrLivArea ' have a good linear relationship, but they include some outliers values about large area with low price at bottom right. Therefor, we delete them.\n"]}, {"cell_type": "code", "execution_count": 1, "id": "4b32cbda", "metadata": {}, "outputs": [], "source": ["train.drop(train[(train['GrLivArea'] > 4000) & (train['SalePrice'] < 300000)].index, inplace = True)"]}, {"cell_type": "markdown", "id": "1ae3779b", "metadata": {}, "source": ["#### TotalBsmtSF"]}, {"cell_type": "code", "execution_count": 1, "id": "55b05074", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots()\nax.scatter(x = train['TotalBsmtSF'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('TotalBsmtSF', fontsize=13)\nplt.show()"]}, {"cell_type": "markdown", "id": "edb98464", "metadata": {}, "source": ["#### 1.2.  Distibution"]}, {"cell_type": "code", "execution_count": 1, "id": "322cc0ae", "metadata": {}, "outputs": [], "source": ["sns.distplot(train['SalePrice'], fit = norm);"]}, {"cell_type": "markdown", "id": "ccdd16d4", "metadata": {}, "source": ["In many classical analytical methods, data is required to follow or approximate a normal distribution. Apparently,'SalePrice' is not. so for our right skewed taget vaiable, it need to transorm its distribution to make it more normally."]}, {"cell_type": "code", "execution_count": 1, "id": "3f1983c5", "metadata": {}, "outputs": [], "source": ["train['SalePrice'] = np.log1p(train['SalePrice'])\nsns.distplot(train['SalePrice'], fit = norm);"]}, {"cell_type": "markdown", "id": "04b54e68", "metadata": {}, "source": ["The ' SalePrice ' corrected more normally distributed now.\n\n\n### 2.  Features Engineering"]}, {"cell_type": "code", "execution_count": 1, "id": "a1ec08dd", "metadata": {}, "outputs": [], "source": ["y = train['SalePrice'].reset_index(drop = True)\ntrain.drop(['SalePrice'], axis = 1,  inplace = True)\n\ntrain.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)\n\ndata = pd.concat([train, test], sort = True).reset_index(drop = True)"]}, {"cell_type": "markdown", "id": "d643013d", "metadata": {}, "source": ["#### 2.1. Missing Data\n"]}, {"cell_type": "code", "execution_count": 1, "id": "5081215e", "metadata": {}, "outputs": [], "source": ["total = data.isnull().sum().sort_values(ascending = False)\npercent = (data.isnull().sum()/data.isnull().count()).sort_values(ascending = False)\nmissing_data = pd.concat([total, percent], axis = 1, keys = ['Total', 'MissingRatio'])\nmissing_data = missing_data[missing_data['MissingRatio'] > 0.0]\nmissing_data.head(10)"]}, {"cell_type": "markdown", "id": "cecfc57e", "metadata": {}, "source": ["When a property has large missing ratio, it can also considered as having lower or no impact on final forecast. So a property should be delete if it has missing ratio more than 20%."]}, {"cell_type": "code", "execution_count": 1, "id": "2cd169ff", "metadata": {}, "outputs": [], "source": ["data.drop(['PoolQC',  'MiscFeature', 'Alley',  'Fence', 'FireplaceQu'], axis=1, inplace = True)"]}, {"cell_type": "markdown", "id": "793052a8", "metadata": {}, "source": ["Other missing values :"]}, {"cell_type": "code", "execution_count": 1, "id": "7b52b216", "metadata": {}, "outputs": [], "source": ["data['LotFrontage'] = data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\ndata['MSZoning'] = data.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\nattributes = ['Electrical', 'Functional', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType', 'Utilities']\nfor attribute in attributes:\n    data[attribute] = data[attribute].fillna(data[attribute].mode()[0])\n# For categorical \nfor col in ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n            'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'MasVnrType']:\n    data[col] = data[col].fillna('None')\n# For numerical \nfor col in ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath',\n            'GarageYrBlt', 'GarageArea', 'GarageCars', 'MasVnrArea']:\n    data[col] = data[col].fillna(0)"]}, {"cell_type": "markdown", "id": "cb1ab768", "metadata": {}, "source": ["Finally:"]}, {"cell_type": "code", "execution_count": 1, "id": "75dcd6d9", "metadata": {}, "outputs": [], "source": ["total = data.isnull().sum().sort_values(ascending = False)\npercent = (data.isnull().sum()/data.isnull().count()).sort_values(ascending = False)\nmissing_data = pd.concat([total, percent], axis = 1, keys = ['Total', 'MissingRatio'])\nmissing_data = missing_data[missing_data['MissingRatio'] > 0.0]\n\nmissing_data"]}, {"cell_type": "markdown", "id": "77140040", "metadata": {}, "source": ["#### 2.2. Skewness"]}, {"cell_type": "code", "execution_count": 1, "id": "38767540", "metadata": {}, "outputs": [], "source": ["numerics = data.dtypes[data.dtypes != \"object\"].index\n\ndata.loc[data['TotalBsmtSF'] > 0, 'TotalBsmtSF'] = boxcox1p(data['TotalBsmtSF'], 0.15)\n\nskewness = data[numerics].apply(lambda x: skew(x))\nskew_index = skewness[abs(skewness) > 0.5].index\nskewness = skewness[skew_index].sort_values(ascending = False)\nfor idx in skew_index:\n    data[idx] = boxcox1p(data[idx], 0.15)"]}, {"cell_type": "markdown", "id": "a3f39811", "metadata": {}, "source": ["#### 2.3. Add and Del"]}, {"cell_type": "code", "execution_count": 1, "id": "e75cc83d", "metadata": {}, "outputs": [], "source": ["data['hasGarage'] = data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ndata['hasBsmt'] = data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ndata['hasPool'] = data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ndata['hasFireplace'] = data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\ndata['has2ndfloor'] = data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)"]}, {"cell_type": "code", "execution_count": 1, "id": "93b35340", "metadata": {}, "outputs": [], "source": ["data = pd.get_dummies(data).reset_index(drop=True)\n\nfeatures = data.keys()\ndata = data.drop(data.loc[:, (data == 0).sum() >= (data.shape[0] * 0.99)], axis = 1)\ndata = data.drop(data.loc[:, (data == 1).sum() >= (data.shape[0] * 0.99)], axis = 1)\nremove = [feat for feat in features if feat not in data.keys()]\n\nprint(\"Del %2d features\"%(len(remove)))"]}, {"cell_type": "markdown", "id": "eb023219", "metadata": {}, "source": ["#### 2.4. Normalize "]}, {"cell_type": "code", "execution_count": 1, "id": "eba517f0", "metadata": {}, "outputs": [], "source": ["data = pd.DataFrame(RobustScaler().fit_transform(data))"]}, {"cell_type": "markdown", "id": "f98ea305", "metadata": {}, "source": ["## Training Model"]}, {"cell_type": "code", "execution_count": 1, "id": "5fa95a4b", "metadata": {}, "outputs": [], "source": ["data_train = np.array(data[:len(train)])\ndata_test = np.array(data[len(train):])\n\nkfolds = KFold(n_splits = 10, shuffle = True, random_state = 42)\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, x = data_train):\n    rmse = np.sqrt(-cross_val_score(model, x, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)\n\nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas = alphas_alt, cv = kfolds))\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter = 1e7, alphas = alphas2, random_state = 42, cv = kfolds))\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter = 1e7, alphas = e_alphas, cv = kfolds, l1_ratio = e_l1ratio))                                \nsvr = make_pipeline(RobustScaler(), SVR(C = 20, epsilon = 0.008, gamma = 0.0003))\n\ngbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, \n        min_samples_split=10, loss='huber', random_state =42)\nxgb = XGBRegressor(learning_rate=0.01,n_estimators=3460, max_depth=3, min_child_weight=0, gamma=0, subsample=0.7, \n        colsample_bytree=0.7, objective ='reg:squarederror', nthread=-1, scale_pos_weight=1, seed=27, reg_alpha=0.00006)\nlgb = LGBMRegressor(objective='regression', num_leaves=4, learning_rate=0.01, n_estimators=5000, max_bin=200, \n        bagging_fraction=0.75, bagging_freq=5, bagging_seed=7, feature_fraction=0.2, feature_fraction_seed=7, verbose=-1)\n                \nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr, xgb, lgb), meta_regressor = xgb, use_features_in_secondary = True)\n\nstack_gen_model = stack_gen.fit(data_train, y)\nelastic_model = elasticnet.fit(data_train, y)\nlasso_model = lasso.fit(data_train, y)\nridge_model = ridge.fit(data_train, y)\nsvr_model = svr.fit(data_train, y)\ngbr_model = gbr.fit(data_train, y)\nxgb_model = xgb.fit(data_train, y)\nlgb_model = lgb.fit(data_train, y)"]}, {"cell_type": "markdown", "id": "80cd6ad1", "metadata": {}, "source": ["## Make Submission"]}, {"cell_type": "code", "execution_count": 1, "id": "a9279e62", "metadata": {}, "outputs": [], "source": ["def models_predict(x):\n    return ((0.1 * ridge_model.predict(x)) + \\\n            (0.05 * lasso_model.predict(x)) + \\\n            (0.1 * elastic_model.predict(x)) + \\\n            (0.1 * svr_model.predict(x)) + \\\n            (0.1 * gbr_model.predict(x)) + \\\n            (0.15 * xgb_model.predict(x)) + \\\n            (0.1 * lgb_model.predict(x)) + \\\n            (0.3 * stack_gen_model.predict(x)))\n\npre =  np.floor(np.expm1(models_predict(data_test)))\nsubmission = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")\nsubmission.iloc[:,1] = pre\nsubmission.to_csv(\"submission.csv\", index=False)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}