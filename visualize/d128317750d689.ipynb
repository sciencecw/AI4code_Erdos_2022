{"cells": [{"cell_type": "markdown", "id": "e7439c2d", "metadata": {}, "source": ["In this work I classify images of sign language letters with convolutional network using PyTorch. \n\nFirst, let's import everything we need for work: "]}, {"cell_type": "code", "execution_count": 1, "id": "7c8df509", "metadata": {}, "outputs": [], "source": ["import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline"]}, {"cell_type": "code", "execution_count": 1, "id": "15f4e4a3", "metadata": {}, "outputs": [], "source": ["\ndata_raw = pd.read_csv('../input/sign_mnist_train.csv', sep=\",\")\ntest_data_raw = pd.read_csv('../input/sign_mnist_test.csv', sep=\",\")\n\nlabels = data_raw['label']\ndata_raw.drop('label', axis=1, inplace=True)\nlabels_test = test_data_raw['label']\ntest_data_raw.drop('label', axis=1, inplace=True)\n\ndata = data_raw.values\nlabels = labels.values\ntest_data = test_data_raw.values\nlabels_test = labels_test.values\n"]}, {"cell_type": "markdown", "id": "ef222ee3", "metadata": {}, "source": ["Before we go on, let's take a peek at the data we're dealing with:"]}, {"cell_type": "code", "execution_count": 1, "id": "4f497690", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(10, 8))\n\npixels = data[10].reshape(28, 28)\nplt.subplot(221)\nsns.heatmap(data=pixels)\n\npixels = data[11].reshape(28, 28)\nplt.subplot(222)\nsns.heatmap(data=pixels)\n\npixels = data[20].reshape(28, 28)\nplt.subplot(223)\nsns.heatmap(data=pixels)\n\npixels = data[32].reshape(28, 28)\nplt.subplot(224)\nsns.heatmap(data=pixels)"]}, {"cell_type": "markdown", "id": "216f5706", "metadata": {}, "source": ["I've created the alphabet dictionary just so I can check the output more easily. That way we can see the actual letter instead of the numeric label representation. "]}, {"cell_type": "code", "execution_count": 1, "id": "233c89b8", "metadata": {}, "outputs": [], "source": ["import string\n\nalph = {0:'a', 1:'b', 2:'c', 3:'d', 4:'e', 5:\"f\", 6:'g', 7:'h', 8:'i', 9:'j', 10:'k', 11:'l', 12:'m', 13:'n',\n        14:'o', 15:'p', 16:'q', 17:'r', 18:'s', 19:'t', 20:'u', 21:'v', 22:'w', 23:'x', 24:'y', 25:'z'}"]}, {"cell_type": "markdown", "id": "76e38e9e", "metadata": {}, "source": ["The  dataset consists of 1D arrays for each image. If we want it to work correctly with CNN, we will need to reshape it into 2D format. In this specific case, we have 28\\*28 images.\n\nI've created a primive function that gets the job done, it takes the dataset and dimention of images, and returnes the reshaped dataset:"]}, {"cell_type": "code", "execution_count": 1, "id": "768e24e1", "metadata": {}, "outputs": [], "source": ["def reshape_to_2d(data, dim):\n    reshaped = []\n    for i in data:\n        reshaped.append(i.reshape(1, dim, dim))\n\n    return np.array(reshaped)"]}, {"cell_type": "markdown", "id": "29f88cb8", "metadata": {}, "source": ["Now we can apply it to the dataset, then divide it into features (x) and labels (y), and construct Tensors. Tensors are PyTorch data structures that work like arrays, but are little bit different. For example, they can work with GPU. [See this](https://pytorch.org/tutorials/beginner/examples_tensor/two_layer_net_tensor.html) for more details, if you want. Tensors in PyTorch also have data type, so watch out for that."]}, {"cell_type": "code", "execution_count": 1, "id": "942af66e", "metadata": {}, "outputs": [], "source": ["data = reshape_to_2d(data, 28)\n\nx = torch.FloatTensor(data)\ny = torch.LongTensor(labels.tolist())\n\ntest_labels = torch.LongTensor(labels_test.tolist())\n\ntest_data_formated = reshape_to_2d(test_data, 28)\ntest_data_formated = torch.FloatTensor(test_data_formated)"]}, {"cell_type": "markdown", "id": "7d4cb69e", "metadata": {}, "source": ["Next, we need to define the hyperparameters. Epoch is basically how much the network is going to train (beware of overfitting). Batch is the number of training samples that will be fed to the network in one iteration. Learning rate defines how fast will the network adjust and, well, learn. This hyperparameter is pretty important and tricky to adjust, 0.001 worked pretty well for me.\nYou can experiment a bit and see what works best for you."]}, {"cell_type": "code", "execution_count": 1, "id": "7e044d09", "metadata": {}, "outputs": [], "source": ["epochs = 50\nbatch_size = 100\nlearning_rate = 0.001"]}, {"cell_type": "markdown", "id": "ca9b1907", "metadata": {}, "source": ["Now we define the architecture of the network. As mentioned before, I will be using a basic CNN. I've stopped at 3 conv. layers with maxpool at first two. I've also added a [dropout](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/dropout_layer.html) after the third conv. layer, which helps with overfitting and is generally nice to have. If you want more details about some specific type of layer or about CNN architecture in general, make sure to take a look at [this](http://cs231n.github.io/convolutional-networks/).\n\nI've also added two custom functions: test and evaluate. Both of them just give feedback about performance of the network. *Test* function compares the number of correct predictions and prints the accuracy, while *evaluate* calculates the accuracy and returns it, which is used for logging while training. "]}, {"cell_type": "code", "execution_count": 1, "id": "e364942b", "metadata": {}, "outputs": [], "source": ["class Network(nn.Module): \n    \n    def __init__(self):\n        super(Network, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, 3)\n        self.pool1 = nn.MaxPool2d(2)\n        \n        self.conv2 = nn.Conv2d(10, 20, 3)\n        self.pool2 = nn.MaxPool2d(2)\n        \n        self.conv3 = nn.Conv2d(20, 30, 3) \n        self.dropout1 = nn.Dropout2d()\n        \n        self.fc3 = nn.Linear(30 * 3 * 3, 270) \n        self.fc4 = nn.Linear(270, 26) \n        \n        self.softmax = nn.LogSoftmax(dim=1)\n    \n    \n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.pool1(x)\n        \n        x = self.conv2(x)\n        x = F.relu(x)\n        x = self.pool2(x)\n        \n        x = self.conv3(x)\n        x = F.relu(x)\n        x = self.dropout1(x)\n                \n        x = x.view(-1, 30 * 3 * 3) \n        x = F.relu(self.fc3(x))\n        x = F.relu(self.fc4(x))\n        \n        return self.softmax(x)\n    \n    \n    def test(self, predictions, labels):\n        \n        self.eval()\n        correct = 0\n        for p, l in zip(predictions, labels):\n            if p == l:\n                correct += 1\n        \n        acc = correct / len(predictions)\n        print(\"Correct predictions: %5d / %5d (%5f)\" % (correct, len(predictions), acc))\n        \n    \n    def evaluate(self, predictions, labels):\n                \n        correct = 0\n        for p, l in zip(predictions, labels):\n            if p == l:\n                correct += 1\n        \n        acc = correct / len(predictions)\n        return(acc)"]}, {"cell_type": "markdown", "id": "5eeaf6a9", "metadata": {}, "source": ["After the network architecture is designed and defined, we need to create an instance of it. We can \"print\" the network to take a look at the structure one more time and to make sure everything is fine. "]}, {"cell_type": "code", "execution_count": 1, "id": "76714949", "metadata": {}, "outputs": [], "source": ["net = Network()\nprint(net)"]}, {"cell_type": "markdown", "id": "42afe7b4", "metadata": {}, "source": ["To enable the network to learn, we also need to define the optimizer and the loss function. I've tried many optimizers, but SGD with momentum seems to work best for me. But I suggest you experimenting with other optimizers, like Adam or Adadelta. \n\nAlso, keep in mind that different optimizers work differently with learning rates. While one learning rate might be perfect for Adam, it migth stop network form learning with SGD. You can read more about learning rates in [this article](https://medium.com/octavian-ai/which-optimizer-and-learning-rate-should-i-use-for-deep-learning-5acb418f9b2), it helped me quite a lot with this project."]}, {"cell_type": "code", "execution_count": 1, "id": "8487d6cf", "metadata": {}, "outputs": [], "source": ["optimizer = optim.SGD(net.parameters(), learning_rate, momentum=0.7)\nloss_func = nn.CrossEntropyLoss()"]}, {"cell_type": "markdown", "id": "e4f368e1", "metadata": {}, "source": ["And now we can finally train the network! There are quite a few of steps that are PyTorch specific, so if it seems confusing, check out that [tutorial](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py), where every step is explained in detail. \n\nI've also created two lists for loss and accuracy just for logging. They will come in handy after the training to see what happened and how the performance has been changing over the training process. "]}, {"cell_type": "code", "execution_count": 1, "id": "f430ec15", "metadata": {}, "outputs": [], "source": ["loss_log = []\nacc_log = []\n\nfor e in range(epochs):\n    for i in range(0, x.shape[0], batch_size):\n        x_mini = x[i:i + batch_size] \n        y_mini = y[i:i + batch_size] \n        \n        optimizer.zero_grad()\n        net_out = net(Variable(x_mini))\n        \n        loss = loss_func(net_out, Variable(y_mini))\n        loss.backward()\n        optimizer.step()\n        \n        if i % 1000 == 0:\n            #pred = net(Variable(test_data_formated))\n            loss_log.append(loss.item())\n            acc_log.append(net.evaluate(torch.max(net(Variable(test_data_formated[:500])).data, 1)[1], \n                                        test_labels[:500]))\n        \n    print('Epoch: {} - Loss: {:.6f}'.format(e + 1, loss.item()))"]}, {"cell_type": "markdown", "id": "7d9da009", "metadata": {}, "source": ["Now the network is trained, and we can take a look at the performance of the network. Blue line represents the loss function, and the orange one represents the accuracy. The data comes from the logs I've defined before."]}, {"cell_type": "code", "execution_count": 1, "id": "2fd8881f", "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.figure(figsize=(10,8))\nplt.plot(loss_log[2:])\nplt.plot(acc_log)\nplt.plot(np.ones(len(acc_log)), linestyle='dashed')"]}, {"cell_type": "markdown", "id": "5fc8455a", "metadata": {}, "source": ["We can also take a look at some data samples and compare the image with prediction of the network. "]}, {"cell_type": "code", "execution_count": 1, "id": "6a69a5a1", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(10, 8))\n\nsample = 30\npixels = test_data[sample].reshape(28, 28)\nplt.subplot(221)\nsns.heatmap(data=pixels)\nlab = labels_test[sample]\ntest_sample = torch.FloatTensor([test_data[sample].reshape(1, 28, 28).tolist()])\ntest_var_sample = Variable(test_sample)\nnet_out_sample = net(test_var_sample)\n\nprint(\"Prediction: {}\".format(alph[torch.max(net_out_sample.data, 1)[1].numpy()[0]]))\nprint(\"Actual Label: {}\".format(alph[lab]))\n\nsample = 42\npixels = test_data[sample].reshape(28, 28)\nplt.subplot(222)\nsns.heatmap(data=pixels)\nlab = labels_test[sample]\ntest_sample = torch.FloatTensor([test_data[sample].reshape(1, 28, 28).tolist()])\ntest_var_sample = Variable(test_sample)\nnet_out_sample = net(test_var_sample)\n\nprint(\"Prediction: {}\".format(alph[torch.max(net_out_sample.data, 1)[1].numpy()[0]]))\nprint(\"Actual Label: {}\".format(alph[lab]))\n\nsample = 100\npixels = test_data[sample].reshape(28, 28)\nplt.subplot(223)\nsns.heatmap(data=pixels)\nlab = labels_test[sample]\ntest_sample = torch.FloatTensor([test_data[sample].reshape(1, 28, 28).tolist()])\ntest_var_sample = Variable(test_sample)\nnet_out_sample = net(test_var_sample)\n\nprint(\"Prediction: {}\".format(alph[torch.max(net_out_sample.data, 1)[1].numpy()[0]]))\nprint(\"Actual Label: {}\".format(alph[lab]))\n\nsample = 22\npixels = test_data[sample].reshape(28, 28)\nplt.subplot(224)\nsns.heatmap(data=pixels)\nlab = labels_test[sample]\ntest_sample = torch.FloatTensor([test_data[sample].reshape(1, 28, 28).tolist()])\ntest_var_sample = Variable(test_sample)\nnet_out_sample = net(test_var_sample)\n\nprint(\"Prediction: {}\".format(alph[torch.max(net_out_sample.data, 1)[1].numpy()[0]]))\nprint(\"Actual Label: {}\".format(alph[lab]))"]}, {"cell_type": "markdown", "id": "44caf305", "metadata": {}, "source": ["And here is the performance in numbers and accuracy:"]}, {"cell_type": "code", "execution_count": 1, "id": "f1918005", "metadata": {}, "outputs": [], "source": ["predictions = net(Variable(test_data_formated))\nnet.test(torch.max(predictions.data, 1)[1], test_labels)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}