{"cells": [{"cell_type": "markdown", "id": "55bacc36", "metadata": {}, "source": ["# Find unique client by using D1 and Card information\n\nI joined this competition two weeka ago, and try to [find the good teammate](https://www.kaggle.com/c/ieee-fraud-detection/discussion/109873#latest-632441), Finally team up with @[Nanashi](https://www.kaggle.com/jesucristo), very great team work experience with him.\n\n## EDA on first week\nBecause we don't have feature, I foucs on EDA, base on Konstantin's feature on this great kernel,and try some FE\n* https://www.kaggle.com/kyakovlev/ieee-simple-lgbm\n* Generate some new feature, single LGBM model, LB 0.9485 to 0.9504\n\n## Below is the leak that we find, implemnt it on submit file, boost ~ 0.002 score \n### [Reference kernel](https://www.kaggle.com/alexanderzv/find-unique-clients)"]}, {"cell_type": "code", "execution_count": 1, "id": "fb8fe2fa", "metadata": {}, "outputs": [], "source": ["import numpy as np # linear algebra \nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom datetime import timedelta\nimport os, sys, gc, warnings, random, datetime\nimport hashlib\nimport matplotlib.pyplot as plt\nimport os\nimport gc\n\"\"\"\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\"\"\"\n# Any results you write to the current directory are saved as output."]}, {"cell_type": "code", "execution_count": 1, "id": "c99828c7", "metadata": {}, "outputs": [], "source": ["pd.options.display.max_rows = 500\npd.options.display.max_columns = 100"]}, {"cell_type": "markdown", "id": "aff2c0d8", "metadata": {}, "source": ["### Read Data"]}, {"cell_type": "code", "execution_count": 1, "id": "c7bccb05", "metadata": {}, "outputs": [], "source": ["train = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_transaction.csv')\ntrain_ind = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_identity.csv')\ntest = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_transaction.csv')\ntest_ind = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_identity.csv')"]}, {"cell_type": "code", "execution_count": 1, "id": "b2881439", "metadata": {}, "outputs": [], "source": ["#train = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_transaction.csv')\ntrain_len = len(train)\n#del train\n#gc.collect()"]}, {"cell_type": "markdown", "id": "ec2e2dcb", "metadata": {}, "source": ["### Merge ID to original training data"]}, {"cell_type": "code", "execution_count": 1, "id": "977f40ea", "metadata": {}, "outputs": [], "source": ["train = train.merge(train_ind, how = 'left', on ='TransactionID' )\ntest = test.merge(test_ind, how = 'left', on ='TransactionID' )\ndel train_ind,test_ind"]}, {"cell_type": "code", "execution_count": 1, "id": "e9443002", "metadata": {}, "outputs": [], "source": ["all_data = pd.concat([train, test])\ndel train,test\ngc.collect()"]}, {"cell_type": "markdown", "id": "4f121943", "metadata": {}, "source": ["### D1 meaning (days from the first transaction of each client)\n* https://www.kaggle.com/akasyanama13/eda-what-s-behind-d-features\n\nAs my experimence of time series base data analysis, time correlate feature is the important information, I read this great kernel and I realize the D1 meaning, using D1 and try to find client information."]}, {"cell_type": "code", "execution_count": 1, "id": "b5e591f8", "metadata": {}, "outputs": [], "source": ["START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\nall_data['DT_time'] = all_data['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\nall_data['count'] = 1\nall_data['diff_days_from_first_transaction'] = all_data['D1'].fillna(0).apply(lambda x: (datetime.timedelta(days = x)))\nall_data['client_firstdate'] = all_data['DT_time']- all_data['diff_days_from_first_transaction']\nall_data['client_firstdate_days'] = (all_data['DT_time']- all_data['diff_days_from_first_transaction']).apply(lambda x:str(x.date()))"]}, {"cell_type": "code", "execution_count": 1, "id": "f39b035a", "metadata": {}, "outputs": [], "source": ["all_data[['DT_time','diff_days_from_first_transaction','client_firstdate_days','D1']].head()"]}, {"cell_type": "markdown", "id": "9ab211d4", "metadata": {}, "source": ["### Remove no need feature"]}, {"cell_type": "code", "execution_count": 1, "id": "931b7ab1", "metadata": {}, "outputs": [], "source": ["anaysis_fea = [ 'TransactionID',\n 'isFraud',\n 'TransactionDT',\n 'TransactionAmt',\n 'ProductCD',\n 'device_hash','card_hash', 'V307','id_30','id_31','id_32','id_33','DeviceType','DeviceInfo',\n 'card1','card2','card3','card4','card5','card6','client_firstdate_days','dist1','dist2','P_emaildomain','addr1','addr2','train_or_test','count','DT_time','diff_days_from_first_transaction','client_firstdate']\n#anaysis_fea+['M'+str(i+1) for i in range(9)]\ndrop_fea = [col for col in all_data.columns if col not in anaysis_fea]"]}, {"cell_type": "code", "execution_count": 1, "id": "30e0d7ab", "metadata": {}, "outputs": [], "source": ["all_data = all_data.drop(drop_fea,axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "c74defba", "metadata": {}, "outputs": [], "source": ["gc.collect()"]}, {"cell_type": "markdown", "id": "1592d66d", "metadata": {}, "source": ["### Functions that will help us find unique cards."]}, {"cell_type": "code", "execution_count": 1, "id": "09b0f93f", "metadata": {}, "outputs": [], "source": ["def card_info_hash(x):\n    s = (str(int(x['card1']))+\n         str(int(x['card2']))+\n         str(int(x['card3']))+\n         str(x['card4'])+\n         str(int(x['card5']))+\n         str(x['card6']))\n    h = hashlib.sha256(s.encode('utf-8')).hexdigest()[0:15]\n    return h"]}, {"cell_type": "code", "execution_count": 1, "id": "4c35cdb1", "metadata": {}, "outputs": [], "source": ["all_data['card1'] = all_data['card1'].fillna(0)\nall_data['card2'] = all_data['card2'].fillna(0)\nall_data['card3'] = all_data['card3'].fillna(0)\nall_data['card5'] = all_data['card5'].fillna(0)\nall_data['card4'] = all_data['card4'].fillna('nan')\nall_data['card6'] = all_data['card6'].fillna('nan')"]}, {"cell_type": "code", "execution_count": 1, "id": "6a06292d", "metadata": {}, "outputs": [], "source": ["all_data['card_hash'] = all_data.apply(lambda x: card_info_hash(x), axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "985a066f", "metadata": {}, "outputs": [], "source": ["def get_data_by_card_hash( data, card_hash):\n    mask = data['card_hash']==card_hash\n    return data.loc[mask,:].copy()\n\n\ndef get_data_by_device_hash( data, device_hash):\n    mask = data['device_hash']==device_hash\n    return data.loc[mask,:].copy()\n\n\ndef get_data_by_card_and_startdate( data, card_hash, device_hash):\n    mask = (data['client_firstdate_days']==card_hash) &(data['card_hash']==device_hash)\n    return data.loc[mask,:].copy()"]}, {"cell_type": "markdown", "id": "9561f665", "metadata": {}, "source": ["## Groupby start_date and unique card to find the unique client"]}, {"cell_type": "code", "execution_count": 1, "id": "c37e31bd", "metadata": {}, "outputs": [], "source": ["all_data['count']=1\ngrp = all_data.iloc[:train_len].groupby(['client_firstdate_days','card_hash'])['count'].agg('sum')\n#Let us display the count >10 client \ndisplay_group = get_data_by_card_and_startdate(all_data,grp[grp>10].index[0][0],grp[grp>10].index[0][1])\ndisplay_group[['DT_time','TransactionAmt','V307','id_30','id_31','id_32','id_33','DeviceType','DeviceInfo','dist1','dist2','P_emaildomain']]"]}, {"cell_type": "markdown", "id": "f836d643", "metadata": {}, "source": ["### What we found \n* You can see the V307 means the \"cumulative sum of amounts\", I don't think it's coincidence.\n* So I think that \"Same start date\" and \"same Card information\" would be the unique client \n* Then we can use this information to featch more things and create more features."]}, {"cell_type": "markdown", "id": "817d1c60", "metadata": {}, "source": ["### Find the client that all Fraud\n* Base on AirmH's [discussion](https://www.kaggle.com/c/ieee-fraud-detection/discussion/109455) here.\n* Once we can find the unique client, and if the client alwasy fraud, and the client also in the test set, then we can just set them to isFraus=1"]}, {"cell_type": "code", "execution_count": 1, "id": "6d219b0a", "metadata": {}, "outputs": [], "source": ["s = all_data.iloc[:train_len].groupby(['client_firstdate_days','card_hash'])['isFraud'].agg(['mean', 'count'])"]}, {"cell_type": "code", "execution_count": 1, "id": "56cc1186", "metadata": {}, "outputs": [], "source": ["s.head()"]}, {"cell_type": "markdown", "id": "397bd95e", "metadata": {}, "source": ["### Get those client in Test set, and record the Transaction ID"]}, {"cell_type": "code", "execution_count": 1, "id": "30185935", "metadata": {}, "outputs": [], "source": ["from tqdm import tqdm\nTest_ID=[]\nfor ind in tqdm(s[(s['mean']==1)].index):\n    very_strange_thing = get_data_by_card_and_startdate(all_data, ind[0],ind[1])\n    Test_ID.extend(very_strange_thing[very_strange_thing['isFraud'].isna()]['TransactionID'].tolist())"]}, {"cell_type": "code", "execution_count": 1, "id": "d400716f", "metadata": {}, "outputs": [], "source": ["Test_ID[:20]"]}, {"cell_type": "code", "execution_count": 1, "id": "87a02fb9", "metadata": {}, "outputs": [], "source": ["np.save(\"test_IDs\", Test_ID)"]}, {"cell_type": "markdown", "id": "67f2b388", "metadata": {}, "source": ["### Set those client as Fraud client"]}, {"cell_type": "code", "execution_count": 1, "id": "ba0bb872", "metadata": {}, "outputs": [], "source": ["submit = pd.read_csv(\"../input/ieeecis-fraud-detection-results/stack_gmean_09_16th_2019_LB0pt9530.csv\")\n"]}, {"cell_type": "code", "execution_count": 1, "id": "9b9b87e5", "metadata": {}, "outputs": [], "source": ["mask = submit['TransactionID'].isin(Test_ID)\nsubmit.loc[mask,'isFraud'] =1"]}, {"cell_type": "code", "execution_count": 1, "id": "cfca13bc", "metadata": {}, "outputs": [], "source": ["submit.loc[mask,:]"]}, {"cell_type": "code", "execution_count": 1, "id": "dddf19f8", "metadata": {}, "outputs": [], "source": ["submit.to_csv('submit_try.csv',index = False)"]}, {"cell_type": "markdown", "id": "a3d3355f", "metadata": {}, "source": ["## Conclusion\n* I found this at last 4 hours, and boost our ensemble model to silver zone, it's the key that we can get the silver.\n* I wish I have more time to use this information to create mroe feature and imporve model.\n* It's really fun and interest competition, I am very regret that I didn't join this competition early.\n* Congrats to all winners! Happy kagglers!"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}