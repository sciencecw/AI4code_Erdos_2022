{"cells": [{"cell_type": "code", "execution_count": 1, "id": "bd67a1b4", "metadata": {}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom fastai.vision.all import *\nimport albumentations\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]}, {"cell_type": "code", "execution_count": 1, "id": "34523a01", "metadata": {}, "outputs": [], "source": ["path='/kaggle/input/expertclass2/'"]}, {"cell_type": "code", "execution_count": 1, "id": "afea6d75", "metadata": {}, "outputs": [], "source": ["%%time\ntrain_df = pd.read_csv(\"/kaggle/input/expertclass2/train_data.csv\", sep=',')\ntrain_df = pd.concat([train_df['emotion'],train_df['pixels'].str.split(' ', expand=True)], axis=1)\ntest_df = pd.read_csv(\"/kaggle/input/expertclass2/test_data.csv\", sep=',')\ntest_df = pd.concat([test_df['pixels'].str.split(' ', expand=True)], axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "031cd6a7", "metadata": {}, "outputs": [], "source": ["submission_df = pd.read_csv(\"/kaggle/input/expertclass2/example_submission.csv\", sep=',')"]}, {"cell_type": "code", "execution_count": 1, "id": "932e06ae", "metadata": {}, "outputs": [], "source": ["train_df = train_df.astype(np.uint8)\ntrain_df.head(1)"]}, {"cell_type": "code", "execution_count": 1, "id": "0d8271bd", "metadata": {}, "outputs": [], "source": ["test_df = test_df.astype(np.uint8)\ntest_df.head(1)"]}, {"cell_type": "code", "execution_count": 1, "id": "243fd832", "metadata": {}, "outputs": [], "source": ["print(f'Train_df shape : {train_df.shape}')\nprint(f'Test_df shape  : {test_df.shape}')"]}, {"cell_type": "code", "execution_count": 1, "id": "3947e8fe", "metadata": {}, "outputs": [], "source": ["def split_df(df):\n    '''return a tuple (X, y) \n    \n        X : the training inputs which is in (samples, height, width, channel) shape\n        y : the label which is flatten\n    '''\n    if 'emotion' in df.columns:\n        y = df['emotion'].values.flatten()\n        X = df.drop('emotion', axis=1).values\n    else:\n        X = df.values\n        y = None\n        \n    X = X.reshape(X.shape[0], 48, 48)\n#     X = np.stack((X,) * 3, axis=-1)\n    return (X,y)\n\nclass AlbumentationsTransform(RandTransform):\n    \"A transform handler for multiple `Albumentation` transforms\"\n    split_idx,order=None,2\n    def __init__(self, train_aug, valid_aug): store_attr()\n    \n    def before_call(self, b, split_idx):\n        self.idx = split_idx\n    \n    def encodes(self, img: PILImage):\n        if self.idx == 0:\n            aug_img = self.train_aug(image=np.array(img))['image']\n        else:\n            aug_img = self.valid_aug(image=np.array(img))['image']\n        return PILImage.create(aug_img)\n            \ndef get_train_aug(): return albumentations.Compose([\n            albumentations.Transpose(p=0.2),\n            albumentations.VerticalFlip(p=0.2),\n            albumentations.ShiftScaleRotate(p=0.2),\n            albumentations.CoarseDropout(p=0.5),\n            albumentations.Cutout(p=0.5)\n])\n\ndef get_valid_aug(): return albumentations.Compose([\n    albumentations.Resize(224,224)\n], p=1.)\nitem_tfms = [Resize(224)]#, AlbumentationsTransform(get_train_aug(), get_valid_aug())]"]}, {"cell_type": "code", "execution_count": 1, "id": "4d7ca8ce", "metadata": {}, "outputs": [], "source": ["X_train, y_train = split_df(train_df)\nX_test, _   = split_df(test_df)\n# X_train = np.stack((X_train,) * 3, axis=-1)\n# X_test  = np.stack((X_test,) * 3, axis=-1)"]}, {"cell_type": "code", "execution_count": 1, "id": "8c5f9123", "metadata": {}, "outputs": [], "source": ["print(f'Train set shape : {X_train.shape, y_train.shape}')\nprint(f'Test  set shape  : {X_test.shape}')"]}, {"cell_type": "code", "execution_count": 1, "id": "85815834", "metadata": {}, "outputs": [], "source": ["emotions = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']"]}, {"cell_type": "code", "execution_count": 1, "id": "c6a2a626", "metadata": {}, "outputs": [], "source": ["index = 3\nplt.imshow(X_train[index,], cmap='gray')\nplt.title(emotions[y_train[index]]);"]}, {"cell_type": "code", "execution_count": 1, "id": "e90744e8", "metadata": {}, "outputs": [], "source": ["# Thanks to nghiaho12 (http://nghiaho.com/?p=2741)\n\ndef make_dataloaders_from_numpy_data(image, label):\n    def pass_index(idx):\n        return idx\n\n    def get_x(i):\n        # NOTE: This is a grayscale image that appears to just work with a network expecting RGB.\n        # I suspect this is due to tensor broadcasting rules.\n        return image[i]\n\n    def get_y(i):\n        return label[i]\n\n    dblock = DataBlock(\n        blocks=(ImageBlock, CategoryBlock),\n        get_items=pass_index,\n#         item_tfms=Resize(224),\n        batch_tfms=aug_transforms(size=224),\n        get_x=get_x,\n        get_y=get_y)\n\n    # pass in a list of index\n    num_images = image.shape[0]\n    dls = dblock.dataloaders(list(range(num_images)), valid_pct=0.2, seed=42)\n\n    return dls\n\ndls = make_dataloaders_from_numpy_data(X_train, y_train)\n\n# sanity check\ndls.train.show_batch(max_n=9, cmap='gray')"]}, {"cell_type": "code", "execution_count": 1, "id": "f02f27b7", "metadata": {}, "outputs": [], "source": ["learn = cnn_learner(dls,\n                    models.resnet34,\n                    metrics=accuracy,\n                    pretrained=False,\n                    n_in=1 # 1 channel - grayscale images\n                   )"]}, {"cell_type": "code", "execution_count": 1, "id": "927e4112", "metadata": {}, "outputs": [], "source": ["learn.unfreeze()"]}, {"cell_type": "code", "execution_count": 1, "id": "3ac6ab70", "metadata": {}, "outputs": [], "source": ["lr_min, lr_step = learn.lr_find()\nprint(lr_min, lr_step)"]}, {"cell_type": "code", "execution_count": 1, "id": "fa07ff71", "metadata": {}, "outputs": [], "source": ["learn.fit_one_cycle(50, lr_max=lr_min*1.1, cbs=[EarlyStoppingCallback(patience=5, monitor='accuracy')])"]}, {"cell_type": "code", "execution_count": 1, "id": "8180efc5", "metadata": {}, "outputs": [], "source": ["learn.show_results(cmap='gray')"]}, {"cell_type": "code", "execution_count": 1, "id": "759c98f2", "metadata": {}, "outputs": [], "source": ["class_interp = ClassificationInterpretation.from_learner(learn)"]}, {"cell_type": "code", "execution_count": 1, "id": "877751c2", "metadata": {}, "outputs": [], "source": ["class_interp.plot_top_losses(9, figsize=(15,10), cmap='gray')"]}, {"cell_type": "code", "execution_count": 1, "id": "f3f6f07e", "metadata": {}, "outputs": [], "source": ["class_interp.plot_confusion_matrix()"]}, {"cell_type": "code", "execution_count": 1, "id": "8cbddcfb", "metadata": {}, "outputs": [], "source": ["learn.save('fer_model_resnet34')"]}, {"cell_type": "code", "execution_count": 1, "id": "d4f56798", "metadata": {}, "outputs": [], "source": ["# predict and submit\n# learn.predict()"]}, {"cell_type": "code", "execution_count": 1, "id": "420c9d8c", "metadata": {}, "outputs": [], "source": ["predictions, *_ = learn.get_preds(dl=dls.test_dl(X_test))\nlabels = np.argmax(predictions, 1)\nCounter(labels.cpu().detach().numpy())"]}, {"cell_type": "markdown", "id": "da92c7e1", "metadata": {}, "source": ["Test Time Augmentation"]}, {"cell_type": "code", "execution_count": 1, "id": "ab4863fd", "metadata": {}, "outputs": [], "source": ["tta, *_ = learn.tta(dl=dls.test_dl(X_test), use_max=True)\nlabels_tta = np.argmax(tta, 1)\nCounter(labels_tta.cpu().detach().numpy())"]}, {"cell_type": "markdown", "id": "4d4843c2", "metadata": {}, "source": ["Submission"]}, {"cell_type": "code", "execution_count": 1, "id": "4cb0d90f", "metadata": {}, "outputs": [], "source": ["submission_df['Emotion'] = labels_tta\nsubmission_df.to_csv(\"submission.csv\", index=False)\nsubmission_df"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}