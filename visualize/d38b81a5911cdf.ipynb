{"cells": [{"cell_type": "markdown", "id": "b6f2e676", "metadata": {}, "source": ["**Introduction and Loading Python Support Libraries**\n\nIn this notebook we would be making prediction for Titanic Survival data set. Objective of the problem is to predict if a person would survive or not looking at the input variables provided. On a high level we would be running through the following steps\n\n* a) Exploratory Data Analysis\n* b) Cleaning Up Data \n* c) Visulization on data set\n* d) Feature Engineering / Scaling data\n* e) Model Generation (Using - KNN,Logisitc,SVM,Gaussain NB, Ensemble Models (Random Forest, Gradient Boost Classifier)\n\nLet's get started with loading the python libraries and train/test data.\n\nSteps - C and D are Overlapping and we go along creating new features while Visualizing the data.\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "0a636b85", "metadata": {}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\n# data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport os\nimport pandas as pd \nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\n## ignore warnings\nimport warnings; warnings.simplefilter('ignore')\npd.set_option('display.width',1000000)\npd.set_option('display.max_columns', 500)"]}, {"cell_type": "code", "execution_count": 1, "id": "a3cf4d1c", "metadata": {}, "outputs": [], "source": ["### some more libraries\n\nimport numpy as np # linear algebra\nimport seaborn as sns\nimport os\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import recall_score,precision_score,f1_score,accuracy_score,confusion_matrix\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom scipy import stats\nimport statistics as s\nfrom sklearn.linear_model import LogisticRegression"]}, {"cell_type": "markdown", "id": "6d43a5ee", "metadata": {}, "source": ["Load train and test data sets"]}, {"cell_type": "code", "execution_count": 1, "id": "f8f22de0", "metadata": {}, "outputs": [], "source": ["\n#print(os.getcwd())\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))"]}, {"cell_type": "code", "execution_count": 1, "id": "9ae3f0ce", "metadata": {}, "outputs": [], "source": ["gender_submission = pd.read_csv(\"../input/titanic/gender_submission.csv\")\ntest = pd.read_csv(\"../input/titanic/test.csv\")\ntrain = pd.read_csv(\"../input/titanic/train.csv\")\n\n#### setting Index of the test data as Passenger Id \n#### which acts as a unique identifier for the data\ntrain.set_index('PassengerId',inplace=True)\ntest.set_index('PassengerId', inplace=True)\n\n### Data frame to hold scores\nscore_df = pd.DataFrame(columns={'Model_Name','Score'})"]}, {"cell_type": "markdown", "id": "76e4129d", "metadata": {}, "source": ["1.a **Exploratory data analysis** - The feed data or data captured in a data science pipeline is bound to have some inconsistencies - missing values, outliers, incorrect information etc .  Even applying a very good prediction model will not provide good results unless the feed data is cleaned."]}, {"cell_type": "markdown", "id": "04088b2c", "metadata": {}, "source": ["Let's start with getting some feel of data with common used methods and also get an idea about Missing/Null values in our train and test data set"]}, {"cell_type": "code", "execution_count": 1, "id": "dd39b588", "metadata": {}, "outputs": [], "source": ["# check the columns in the data set\ntrain.dtypes"]}, {"cell_type": "markdown", "id": "f5bd4461", "metadata": {}, "source": ["Columns in data set along with brief description\n \nsurvival\tSurvival\t    0 = No, 1 = Yes\npclass\t    Ticket class\t1 = 1st, 2 = 2nd, 3 = 3rd\nsex\t        Sex\t\nAge\t        Age in years\t\nsibsp   \t# of siblings / spouses aboard the Titanic\t\nparch\t    # of parents / children aboard the Titanic\t\nticket\t    Ticket number\t\nfare\t    Passenger fare\t\ncabin\t    Cabin number\t\nembarked\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton"]}, {"cell_type": "code", "execution_count": 1, "id": "1beb326c", "metadata": {}, "outputs": [], "source": ["train.isnull().any()"]}, {"cell_type": "code", "execution_count": 1, "id": "5ae9bf12", "metadata": {}, "outputs": [], "source": ["test.isnull().any()"]}, {"cell_type": "code", "execution_count": 1, "id": "18973174", "metadata": {}, "outputs": [], "source": ["## lets look at the initial few rows to see how the data looks \ntrain.head(5)"]}, {"cell_type": "code", "execution_count": 1, "id": "4ee67808", "metadata": {}, "outputs": [], "source": ["test.head(5)"]}, {"cell_type": "code", "execution_count": 1, "id": "42f91484", "metadata": {}, "outputs": [], "source": ["## using describe functions give some usefull stats for the columns in the dataset\ntrain.describe()"]}, {"cell_type": "markdown", "id": "58b3ea2d", "metadata": {}, "source": ["1b ** Cleaning Data **\nWe will start with filling up missing/null values "]}, {"cell_type": "code", "execution_count": 1, "id": "4f47a4ba", "metadata": {}, "outputs": [], "source": ["######################## Column Embarked - Train data \n\n## Only 2 values are NUll in the data for this column \nprint(train[train['Embarked'].isnull()])\n"]}, {"cell_type": "code", "execution_count": 1, "id": "7b127701", "metadata": {}, "outputs": [], "source": ["### check if any relation between where a person has embarked and the ticket fare \nsns.catplot(x='Embarked',y='Fare',hue ='Pclass',data=train,kind='swarm')"]}, {"cell_type": "markdown", "id": "5f7b1164", "metadata": {}, "source": ["From the above it's not clear if we can derive Embarked from the Fare and Pclass fields , for now let us have it as 'S' which has the maximum frequency count."]}, {"cell_type": "code", "execution_count": 1, "id": "25a0cc5c", "metadata": {}, "outputs": [], "source": ["## check how Embarked is distirbuted\nprint(train['Embarked'].value_counts())"]}, {"cell_type": "code", "execution_count": 1, "id": "2d41c571", "metadata": {}, "outputs": [], "source": ["### for now don't see any relation between Embarked so makring this as 'S' which is the \ntrain['Embarked'].fillna('S',inplace=True)\n#train[train['Embarked'].isnull()]"]}, {"cell_type": "code", "execution_count": 1, "id": "76b5c9bf", "metadata": {}, "outputs": [], "source": ["##### Column Cabin \n##### Cabin has a lot of distinct values this is of no use\n##### for building a prediction model rather we can use a bit of feature engineering\nprint(train['Cabin'].value_counts())"]}, {"cell_type": "code", "execution_count": 1, "id": "fe08ddb3", "metadata": {}, "outputs": [], "source": ["#### to make this column usefull , let's only use the Cabin Name (first Alphabet)\ntrain.loc[:,'Cabin'] = train.loc[:,'Cabin'].str[0]\ntest.loc[:,'Cabin'] = test.loc[:,'Cabin'].str[0]\n"]}, {"cell_type": "markdown", "id": "5d4bfaac", "metadata": {}, "source": ["By genreal observation we can assume that Cabin and Fare should be realted to each other"]}, {"cell_type": "code", "execution_count": 1, "id": "92c19e25", "metadata": {}, "outputs": [], "source": ["train[['Cabin','Fare']].groupby(['Cabin'],as_index=False).mean()"]}, {"cell_type": "code", "execution_count": 1, "id": "af1999ad", "metadata": {}, "outputs": [], "source": ["\nsns.catplot(x='Cabin',y='Fare',data=train,kind='bar',ci=None)"]}, {"cell_type": "code", "execution_count": 1, "id": "9ee447d2", "metadata": {}, "outputs": [], "source": ["sns.catplot(x='Cabin',y='Fare',data=test,kind='swarm')"]}, {"cell_type": "markdown", "id": "f3ead43a", "metadata": {}, "source": ["To fill NULL values for cabin we are going to use Fare as a predictor variable and use data that we have with us.Below function is going to clean out the Null values by assigning Cabin by Fare"]}, {"cell_type": "code", "execution_count": 1, "id": "8a1ede0d", "metadata": {}, "outputs": [], "source": ["def calc_cabin_by_fare(df_train):\n    # sns.catplot(x='Cabin',y='Fare',data = df_train,kind='bar')\n    # plt.show()\n    def calculate_cabin(row):\n        if row['Fare'] <= 15:\n            return 'G'\n        elif row['Fare'] <= 19:\n            return 'F'\n        elif row['Fare'] <= 35:\n            return 'T'\n        elif row['Fare'] <= 40:\n            return 'A'\n        elif row['Fare'] <= 45:\n            return 'E'\n        elif row['Fare'] <= 57:\n            return 'D'\n        elif row['Fare'] <= 100:\n            return 'C'\n        else:\n            return 'B'\n\n\n    df_train.loc[df_train['Cabin'].isnull(), 'Cabin'] = df_train[df_train['Cabin'].isnull()].apply(calculate_cabin,\n                                                                                                       axis=1)\n    return df_train\n\ntrain = calc_cabin_by_fare(train)\ntest = calc_cabin_by_fare(test)"]}, {"cell_type": "code", "execution_count": 1, "id": "e93984d9", "metadata": {}, "outputs": [], "source": ["### Column Age \nsns.distplot(train.loc[train['Survived']==1,'Age'].dropna(),color='blue',bins=40)\nsns.distplot(train.loc[train['Survived']==0,'Age'].dropna(),color='yellow',bins=40)"]}, {"cell_type": "markdown", "id": "5f73891b", "metadata": {}, "source": ["For Missing Age values we could either just fill  the mean from train data set or drill down a bit more and instead take mean of Age grouping by Sex and CAbin "]}, {"cell_type": "code", "execution_count": 1, "id": "635651ff", "metadata": {}, "outputs": [], "source": ["\ntemp = train[['Sex','Cabin','Age']].groupby(['Sex','Cabin'],as_index=False).mean()\n\ndef find_mean_age(Sex,Cabin):\n    return temp.loc[(temp['Sex']==Sex)&(temp['Cabin']==Cabin),'Age'].tolist()[0] \n\n\ntrain.loc[train['Age'].isnull(),['Age']] = train.apply(lambda row:find_mean_age(row['Sex']\n                                                                                ,row['Cabin']),\n                                                       axis=1)\n\ntest.loc[test['Age'].isnull(),['Age']] = test.apply(lambda row:find_mean_age(row['Sex']\n                                                                                ,row['Cabin']),\n                                                       axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "fb139000", "metadata": {}, "outputs": [], "source": ["## column - Fare \n## use mean to fill the Fare\n\ntest.loc[test['Fare'].isnull(),]\n\ntest.loc[test['Fare'].isnull(),['Fare']] = np.mean(train['Fare'])\n"]}, {"cell_type": "markdown", "id": "c0c4b3ec", "metadata": {}, "source": ["Along with handling the Null values , Categorical variables need to be converted to numerical values as many of the Models do not support Categorical variables as input in model building"]}, {"cell_type": "code", "execution_count": 1, "id": "68f64b1c", "metadata": {}, "outputs": [], "source": ["# Embarked\nemb = {'S':1,'C':2,'Q':3}\ntrain['Embarked']=train['Embarked'].replace(emb)\ntest['Embarked']=test['Embarked'].replace(emb)\n"]}, {"cell_type": "markdown", "id": "36071925", "metadata": {}, "source": ["Name attribute is not going to have any significance on Model building , but maybe the Title of the name might have some significance . for e.g - a Captain might have a less chance of surviving then a MR/ MS. So below we take out the Title and then numerically encode this "]}, {"cell_type": "code", "execution_count": 1, "id": "4247d2e3", "metadata": {}, "outputs": [], "source": ["### Extract the title from Name and store this in a new column\ntrain['Title']= train['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\ntest['Title'] = test['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n\ntrain['Title'].value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "c9abead2", "metadata": {}, "outputs": [], "source": ["## titles with very few frequency being renamed as Rare\ntrain['Title'] = train['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', \\\n                                                 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\ntest['Title'] = test['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', \\\n                                             'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\ntrain['Title'] = train['Title'].replace('Mlle', 'Miss')\ntrain['Title'] = train['Title'].replace('Ms', 'Miss')\ntrain['Title'] = train['Title'].replace('Mme', 'Mrs')\n\ntest['Title'] = test['Title'].replace('Mlle', 'Miss')\ntest['Title'] = test['Title'].replace('Ms', 'Miss')\ntest['Title'] = test['Title'].replace('Mme', 'Mrs')"]}, {"cell_type": "code", "execution_count": 1, "id": "d3cf96a3", "metadata": {}, "outputs": [], "source": ["# convert titles into numbers\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\ntrain['Title'] = train['Title'].map(titles)\n# filling NaN with 0, to get safe\ntrain['Title'] = train['Title'].fillna(0)\n\n\ntest['Title'] = test['Title'].map(titles)\n# filling NaN with 0, to get safe\ntest['Title'] = test['Title'].fillna(0)"]}, {"cell_type": "code", "execution_count": 1, "id": "61b4eb31", "metadata": {}, "outputs": [], "source": ["## convert sex variable to Numberic\n\ntrain['Sex_var'] = np.where(train['Sex'] == 'male', 1, 0)\ntest['Sex_var'] = np.where(test['Sex'] == 'male', 1, 0)"]}, {"cell_type": "code", "execution_count": 1, "id": "84a96c96", "metadata": {}, "outputs": [], "source": ["## convert Cabin to Numeric\n\nencode = LabelEncoder()\ntrain['Cabin'] = encode.fit_transform(train['Cabin'])\ntest['Cabin'] = encode.transform(test['Cabin'])"]}, {"cell_type": "markdown", "id": "5962318e", "metadata": {}, "source": ["**c) Visulization on data set** - Now that we have imputed missing values , let's see relation between the dependednt valriable(Survived) and other independent variables. Only the attributes which have an impact of Survival value would be used to train the model.\n\n\n**d) Feature Engineering** - As we move along we are also going to probably explore creation of new Features"]}, {"cell_type": "markdown", "id": "0e29c1de", "metadata": {}, "source": ["Plot co-realtion between all variables to see survived via  heat map \n"]}, {"cell_type": "code", "execution_count": 1, "id": "be7d36e5", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(14,12))\nplt.title('Correlation of Features')\ncor = train.corr()\nsns.heatmap(cor, cmap=\"YlOrRd\", annot=True)\nplt.show()"]}, {"cell_type": "markdown", "id": "25ac5deb", "metadata": {}, "source": ["From the above map it's clear that SibSp and Parch don't seem to have much bearing on the person Surviving or not. Let's try to combine these two and see how it goes."]}, {"cell_type": "code", "execution_count": 1, "id": "c39fd947", "metadata": {}, "outputs": [], "source": ["### create family count\ntrain['fmly_count'] = train['SibSp']+train['Parch']\ntest['fmly_count'] = test['SibSp']+test['Parch']\n\n##create is_alone\n\ntrain['is_alone'] = np.where((train['SibSp'] + train['Parch']) == 0, 1, 0)\ntest['is_alone'] = np.where((test['SibSp'] + test['Parch']) == 0, 1, 0)\n\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "db432400", "metadata": {}, "outputs": [], "source": ["\n### let's draw the co-realtion matrix again to include new features\n\ncor = train.corr()\nplt.figure(figsize=(14,12))\nplt.title('Correlation of Features')\nsns.heatmap(cor, cmap=\"YlOrRd\", annot_kws={'fontsize':8},annot=True,linewidths=0.1,vmax=1.0)\nplt.show()\n"]}, {"cell_type": "code", "execution_count": 1, "id": "a565c85c", "metadata": {}, "outputs": [], "source": ["#print(train.dtypes)\n#col_list=['Pclass','Sex_var','Age','Fare','Cabin','fmly_count','is_alone']\n## selecting the column names that we would use for prediction modelling \ncol_list = ['Pclass', 'Sex_var', 'Age', 'Fare', 'Cabin', 'fmly_count','Title']\nX = train[col_list].copy()\ny = train['Survived'].copy()\n\ntest_orig = test.copy()\ntest = test[col_list]\n#print(test.describe())"]}, {"cell_type": "markdown", "id": "4009c8b4", "metadata": {}, "source": ["For prediction modelling you would want to test your model prediction accuracy , to do this it is an agreed practice to divide your dataset into 2 parts\n\nTrain Data -> data fed while training model\n\nTest Data -> the model trained on train data is used to test the accuracy \n\n(Some cases)\n\nwe might also keep an unseen set of data called \nValidate Data set -> this is used to test final prediction model's accuracy "]}, {"cell_type": "code", "execution_count": 1, "id": "4b43e6b0", "metadata": {}, "outputs": [], "source": ["## by deafult 75% train data and 25% test data divison \nX_train, X_test, y_train, y_test = train_test_split(X, y,random_state=100)"]}, {"cell_type": "markdown", "id": "e084d64b", "metadata": {}, "source": ["**Knn (K nearest neighbour)** - would be a simple model to apply for this.\n\nImportant Parameters - n_neighbours (this tells the algo how many nearest number of neighbours to consider) , there are other parameters which can be used to tweak the model, Discussion for those would be beyond the scope here. Similarly for other models as well we would be tweaking the most significant parameters as per our problem's context.\n"]}, {"cell_type": "markdown", "id": "52a2b3d3", "metadata": {}, "source": ["Check the example below , depending on value of n_neighbours the new point would be classified  \n\n![image.png](attachment:image.png)"]}, {"cell_type": "code", "execution_count": 1, "id": "90e214c3", "metadata": {}, "outputs": [], "source": ["## repeated import here \n## just to increase readability\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nscale = MinMaxScaler()\nX_scaled=pd.DataFrame(scale.fit_transform(X),columns=X.columns)\ntest_scaled = pd.DataFrame(scale.fit_transform(test), columns=test.columns)\ntest_scaled.index=test.index\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y,random_state=100)\n\n\n\n## fit the model \nknn_model = KNeighborsClassifier(n_neighbors=6).fit(X_train,y_train)\nknn_score = knn_model.score(X_test,y_test)\nprint('Score of fitted model is: ')\nprint(knn_score)\n\n### let's get the scores now\npredict_df = pd.DataFrame()\npredict_df['PassengerId'] = test.index\npredict_df['Survived'] = knn_model.predict(test_scaled)\n#predict_df.to_csv('submission100_knn_nvalue_6.csv', index=False)\nscore_df = score_df.append({'Model_Name':'K Nearest Neighbour','Score':knn_score},ignore_index=True)\n"]}, {"cell_type": "markdown", "id": "f7be159a", "metadata": {}, "source": ["Based on our model let's look at survival rate in test set"]}, {"cell_type": "code", "execution_count": 1, "id": "dc2f64f7", "metadata": {}, "outputs": [], "source": ["temp = test.copy()\ntemp['Survived'] = predict_df['Survived'].tolist() \nprint(temp)\nprint('Percentage of People who Survived - %3f'%(temp['Survived'].mean()*100))"]}, {"cell_type": "markdown", "id": "a1098352", "metadata": {}, "source": ["Output of ML model depends heavily on parameters hence parameter tuning plays an important role. It has been observed that with proper parameter values set, model\u2019s performance increase reasonably."]}, {"cell_type": "markdown", "id": "052cc6c6", "metadata": {}, "source": ["Above we have just used a single parameter for fitting KNN model, which is n_neighbours and used a value of 6 . It is interesting to find out how did I reach that value or know that at value 6 we get a realtively optimal model"]}, {"cell_type": "code", "execution_count": 1, "id": "53ba4bd7", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import validation_curve\n\n### this is how you do a validation curve\ntrain_score,test_score = validation_curve(KNeighborsClassifier(),X,y,param_name='n_neighbors',param_range=range(1,11),cv=5)\ntrain_score_mean = np.mean(train_score,axis=1)\ntest_score_mean = np.mean(test_score, axis=1)\nplt.plot(range(1,11),train_score_mean,'-o',label='train score')\nplt.plot(range(1, 11), test_score_mean, '-o', label='train score')\nplt.xlabel('N_neighbours values')\nplt.ylabel('Accuracy')\nplt.title('Variation of Accuracy with input parameter n_neighbour')\nplt.legend(loc='best')\nplt.show()\n"]}, {"cell_type": "markdown", "id": "80655a39", "metadata": {}, "source": ["**Logistic Regression ** -  \n\nCan be used for regression and classification as well. It is similar to linear regression , only difference being before giving the output it passes it through a function which gives an output b/w 0 and 1.\n\nImportant Parameters - penalty norm - l1 or l2 etc\nc - Regularization paramter higher values means less penalties and would lead to overfitting"]}, {"cell_type": "markdown", "id": "064d3b16", "metadata": {}, "source": ["Worth adding that we would be using GridSearch utility for Hyperparameter Optimization - while training a model you want to tweak the input parametes so as to achieve the optimal Accuracy. You could either do this manually by passing differnet values to input parameters and checking result which is going to be time consuming , so to avoid this heavy loading we use GridSearch Utility which can test the accuracy over combination of multiple input values and also uses cross validation to give more robust results"]}, {"cell_type": "code", "execution_count": 1, "id": "d8ffcff5", "metadata": {}, "outputs": [], "source": ["from sklearn.linear_model import LogisticRegression \nfrom sklearn.model_selection import GridSearchCV\n\nlr_model = LogisticRegression(random_state=100)\ngrid_values = {'penalty':['l1', 'l2'],'C':[0.01, 0.1, 1, 10, 100]}\ngrid_search_model = GridSearchCV(lr_model,param_grid=grid_values,cv=3)\ngrid_search_model.fit(X_scaled,y)\nprint(grid_search_model.best_estimator_)\nprint('Model Accuracy')\nprint(grid_search_model.best_score_)\nprint(grid_search_model.best_params_)\npredict_df = pd.DataFrame()\npredict_df['PassengerId'] = test_scaled.index\npredict_df['Survived'] = grid_search_model.predict(test_scaled)\npredict_df.to_csv('submission101_lr_gsearch_opt.csv', index=False)\nscore_df = score_df.append({'Model_Name':'LR - with Grid Search','Score':grid_search_model.best_score_},ignore_index=True)\n"]}, {"cell_type": "markdown", "id": "b7a0b114", "metadata": {}, "source": ["**Support Vector Machine ** \n\nThis is similar to logisitc regression but applies a different objective function , while in LR the algo aims at probabilities , this model is mode inclined towards finding a hyperplane with a wide margin between various classes.\n\nC-> Regularization Parameter\n"]}, {"cell_type": "code", "execution_count": 1, "id": "c4fe9885", "metadata": {}, "outputs": [], "source": ["from sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\n\nsvc_model = SVC(C=100,random_state=100).fit(X_train,y_train)\nscv_score = svc_model.score(X_test,y_test) \n\nprint('SVC Model score is ')\nprint(scv_score)\n\npredict_df = pd.DataFrame()\npredict_df['PassengerId'] = test_scaled.index\npredict_df['Survived'] = svc_model.predict(test_scaled)\n#predict_df.to_csv('submission101_lr_gsearch_opt.csv', index=False)\nscore_df = score_df.append({'Model_Name':'SVC Model - C 100','Score':scv_score},ignore_index=True)\n"]}, {"cell_type": "markdown", "id": "837e43cd", "metadata": {}, "source": ["**Gaussian NB ** \n"]}, {"cell_type": "code", "execution_count": 1, "id": "9a50b8c3", "metadata": {}, "outputs": [], "source": ["from sklearn.naive_bayes import GaussianNB\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,random_state=100)\ngaussnb_model = GaussianNB().fit(X_train,y_train)\nprint('Naive Bayes Accuracy')\nprint(gaussnb_model.score(X_test,y_test))\ngb_model_pred = gaussnb_model.predict(test)\nscore_df = score_df.append({'Model_Name':'Gaussian Naive Bayes','Score':scv_score},ignore_index=True)\n\n"]}, {"cell_type": "markdown", "id": "0f724371", "metadata": {}, "source": ["Decision Trees - As the name suggests this model builds a tree format to reach the calssification result by using a series of if and else questions."]}, {"cell_type": "code", "execution_count": 1, "id": "03d9e55b", "metadata": {}, "outputs": [], "source": ["from sklearn.tree import DecisionTreeClassifier\ndt_model = DecisionTreeClassifier(max_depth=3,random_state=100).fit(X_train,y_train)\ndt_model_score = dt_model.score(X_test,y_test)\nprint('Decision Tree Accuracy')\nprint(dt_model_score)\nscore_df = score_df.append({'Model_Name':'Decision Tree','Score':dt_model_score},ignore_index=True)\n"]}, {"cell_type": "markdown", "id": "42806049", "metadata": {}, "source": ["**Ensemble Models**"]}, {"cell_type": "markdown", "id": "7ec6f24d", "metadata": {}, "source": ["**Gradient Boosting Classifier**\n\nImportant Parameters \nlearning_rate\nn_estimators\n"]}, {"cell_type": "code", "execution_count": 1, "id": "0421acfa", "metadata": {}, "outputs": [], "source": ["from sklearn.ensemble import GradientBoostingClassifier\n\n## fitting the model on complete data set \ngbf_model = GradientBoostingClassifier(learning_rate=0.001,n_estimators=3000).fit(X,y)\ngbf_score = gbf_model.score(X_test,y_test)\nprint('GBF score -')\nprint(gbf_score)\ngbf_model_pred = gbf_model.predict(test)\nscore_df = score_df.append({'Model_Name':'GradientBoostingClassifier','Score':gbf_score},ignore_index=True)\n\n\npredict_df = pd.DataFrame()\npredict_df['PassengerId'] = test.index\npredict_df['Survived'] = gbf_model.predict(test)\npredict_df.to_csv('submission_temp.csv', index=False,line_terminator=\"\")\n    \n### some more file clearning\nfile_data = open('submission_temp.csv', 'rb').read()\nopen('submission_gbfc_model.csv', 'wb').write(file_data[:-1])"]}, {"cell_type": "markdown", "id": "7aa1ed81", "metadata": {}, "source": ["**Random Forest Classifier **\n"]}, {"cell_type": "code", "execution_count": 1, "id": "244c93d9", "metadata": {}, "outputs": [], "source": ["from sklearn.ensemble import RandomForestClassifier\n\nrf_model= RandomForestClassifier(n_estimators=1000,max_depth=7)\nrf_model.fit(X,y)\nrf_score = rf_model.score(X_test,y_test)\nprint('RF Model Score')\nprint(rf_score)\nscore_df = score_df.append({'Model_Name':'RandomForestClassifier','Score':rf_score},ignore_index=True)\nrf_model_pred = rf_model.predict(test)\n\n\n\npredict_df = pd.DataFrame()\npredict_df['PassengerId'] = test.index\npredict_df['Survived'] = rf_model.predict(test)\npredict_df.to_csv('submission_temp.csv', index=False,line_terminator=\"\")\n    \n### some more file clearning\nfile_data = open('submission_temp.csv', 'rb').read()\nopen('submission_rf_model.csv', 'wb').write(file_data[:-1])"]}, {"cell_type": "markdown", "id": "20a0e0f2", "metadata": {}, "source": ["Checking the final scores"]}, {"cell_type": "code", "execution_count": 1, "id": "2e576b8f", "metadata": {}, "outputs": [], "source": ["print(score_df[['Model_Name','Score']].sort_values(by='Score'))"]}, {"cell_type": "markdown", "id": "6cc89419", "metadata": {}, "source": ["Voting Approach - Combining More models together and picking the output with majorit votes\n\nUse Voting along with \n1) GB model\n2) RF model\n3) GBF model"]}, {"cell_type": "code", "execution_count": 1, "id": "2237c31c", "metadata": {}, "outputs": [], "source": ["final_pred = []\nfor i in range(0,len(test)):\n   final_pred.append(s.mode(np.array([rf_model_pred[i],gb_model_pred[i],gbf_model_pred[i]])))\n\npredict_df = pd.DataFrame()\npredict_df['PassengerId'] = test.index\npredict_df['Survived'] = final_pred\npredict_df.to_csv('submission_temp.csv', index=False,line_terminator=\"\")\n    \n### some more file clearning\nfile_data = open('submission_temp.csv', 'rb').read()\nopen('submission_final.csv', 'wb').write(file_data[:-1])"]}, {"cell_type": "markdown", "id": "7cfe2bf5", "metadata": {}, "source": ["**Please UpVote if you found this Notebook Usefull**"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}