{"cells": [{"cell_type": "markdown", "id": "29f1a412", "metadata": {}, "source": ["***Iris Flower Model*** \n\nIn this kernal I am going to explore following\n* Basic ploting\n* different classification algorithems\n* Cross_validation\n* GridSearchCV\n* RandamizedSearchCV\n* Converting categorical variable (different approaches)\n"]}, {"cell_type": "markdown", "id": "d609b852", "metadata": {}, "source": ["> **Import required libraries using import**"]}, {"cell_type": "code", "execution_count": 1, "id": "0f7a964f", "metadata": {}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # using for plots\nimport seaborn as sns #using for plots\n%matplotlib inline \n\nfrom sklearn.model_selection import train_test_split # split train and test sets\nfrom sklearn.preprocessing import StandardScaler # for scaling \n\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\n\n# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n# Gradient Boosting Machine\nfrom sklearn.ensemble import GradientBoostingClassifier\n# Cross Validation Score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom time import time\nfrom scipy.stats import randint as sp_randint\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nprint(os.listdir(\"../input\"))\n\n\n# Any results you write to the current directory are saved as output."]}, {"cell_type": "markdown", "id": "e06fccb0", "metadata": {}, "source": ["**Reading Iris flower dataset using pandas read_csv menthod**"]}, {"cell_type": "code", "execution_count": 1, "id": "f93146e5", "metadata": {}, "outputs": [], "source": ["iris_ds= pd.read_csv(\"../input/Iris.csv\")\niris_ds.columns,iris_ds.shape\n#print(iris_ds.shape)"]}, {"cell_type": "markdown", "id": "cd874661", "metadata": {}, "source": ["**Dataset contains 6 columns and 150 observations.**\n\nChecking top 5 rows of the dataset using head. We can show number of rows based on parameter. By default it shows 5 rows."]}, {"cell_type": "code", "execution_count": 1, "id": "cac61e1f", "metadata": {}, "outputs": [], "source": ["iris_ds.head()"]}, {"cell_type": "markdown", "id": "4c5217ba", "metadata": {}, "source": ["checking last 5 rows using tail. By default tail shows last 5 rows."]}, {"cell_type": "code", "execution_count": 1, "id": "23d1c60b", "metadata": {}, "outputs": [], "source": ["iris_ds.tail()"]}, {"cell_type": "markdown", "id": "36146498", "metadata": {}, "source": ["Check any null values in the datase using isnull and sum. Out data set not have any null/empty values."]}, {"cell_type": "code", "execution_count": 1, "id": "b056e939", "metadata": {}, "outputs": [], "source": ["iris_ds.isnull().sum()"]}, {"cell_type": "markdown", "id": "aa7b7419", "metadata": {}, "source": ["checking column data types using dtypes. Our dataset contains one int, 4 float and 1 object variables. 'Species' is our target varible."]}, {"cell_type": "code", "execution_count": 1, "id": "324dfdff", "metadata": {}, "outputs": [], "source": ["iris_ds.dtypes"]}, {"cell_type": "markdown", "id": "05f0f1e5", "metadata": {}, "source": ["We have total 6 columns in the data set. Id column we can delete. 'Species' is the catagorical column\n\nLet check distribution of target vriable uisng value_counts\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "2af54a47", "metadata": {}, "outputs": [], "source": ["iris_ds.Species.value_counts()"]}, {"cell_type": "markdown", "id": "cd132cd3", "metadata": {}, "source": ["We have equal distribution values in Target variable. This is balenced dataset. In case of imbalenced datasets we have to use other techniques to handle. \n\nI am deleting Id column. This wont help our model. using drop function I am deleting Id variable from dataset. \n\naxis=1 indicated column level and inplace=True applies dataset."]}, {"cell_type": "code", "execution_count": 1, "id": "da303f06", "metadata": {}, "outputs": [], "source": ["iris_ds.drop(columns=['Id'],axis=1,inplace=True)\n"]}, {"cell_type": "markdown", "id": "eb2c4947", "metadata": {}, "source": ["Id column has been deleted from dataset. Now you can check how many columns present in the dataset"]}, {"cell_type": "code", "execution_count": 1, "id": "2ed3d439", "metadata": {}, "outputs": [], "source": ["iris_ds.columns.values"]}, {"cell_type": "markdown", "id": "171815ce", "metadata": {}, "source": ["using hist function from pandas we can check numarical column distribution."]}, {"cell_type": "code", "execution_count": 1, "id": "24c2f67c", "metadata": {}, "outputs": [], "source": ["iris_ds.hist(figsize=(20,10))"]}, {"cell_type": "markdown", "id": "0e89d81d", "metadata": {}, "source": ["Using seaboarns package we can show rich plots. I used pairplot function to show scatter plots."]}, {"cell_type": "code", "execution_count": 1, "id": "40d1d1ed", "metadata": {}, "outputs": [], "source": ["sns.pairplot(iris_ds,hue='Species')"]}, {"cell_type": "markdown", "id": "ab553112", "metadata": {}, "source": ["Box plot for all numarical variables"]}, {"cell_type": "code", "execution_count": 1, "id": "cd2eb11f", "metadata": {}, "outputs": [], "source": ["sns.boxplot(data=iris_ds)"]}, {"cell_type": "markdown", "id": "47187d44", "metadata": {}, "source": ["Using describe we can see min/max/std/quartail distribution for all numarical columns"]}, {"cell_type": "code", "execution_count": 1, "id": "27967d23", "metadata": {}, "outputs": [], "source": ["iris_ds.describe()"]}, {"cell_type": "markdown", "id": "ec9b376e", "metadata": {}, "source": ["Info shows more info like how many observations there in the dataset\n\nIt is also very powerfull. It show how many missing values also in each column level.\n\nIt shows column data type also. Lot information with single command"]}, {"cell_type": "code", "execution_count": 1, "id": "b787e0e7", "metadata": {}, "outputs": [], "source": ["iris_ds.info()"]}, {"cell_type": "markdown", "id": "37dee58e", "metadata": {}, "source": ["Now I am going to convert target variable as category.\nwe can follow different approaches for this convertion. different approaches as follows\n* LableEncoder()\n* map (We use map for ordinal category columns. You can specify the order)\n* type conversion using astype"]}, {"cell_type": "code", "execution_count": 1, "id": "cf6cc5ed", "metadata": {}, "outputs": [], "source": ["iris_ds.Species = iris_ds.Species.astype('category')\n"]}, {"cell_type": "markdown", "id": "b3a4d6e6", "metadata": {}, "source": ["Now we check again data types using info command. now it shows type as category"]}, {"cell_type": "code", "execution_count": 1, "id": "4c51f415", "metadata": {}, "outputs": [], "source": ["iris_ds.info()"]}, {"cell_type": "markdown", "id": "3f7f77e3", "metadata": {}, "source": ["We can access converted values using codes property."]}, {"cell_type": "code", "execution_count": 1, "id": "4a5eb853", "metadata": {}, "outputs": [], "source": ["iris_ds.Species.cat.codes.head()"]}, {"cell_type": "markdown", "id": "33fe3829", "metadata": {}, "source": ["Placing converted values back to Species column."]}, {"cell_type": "code", "execution_count": 1, "id": "9ed0f9a7", "metadata": {}, "outputs": [], "source": ["iris_ds.Species = iris_ds.Species.cat.codes"]}, {"cell_type": "code", "execution_count": 1, "id": "2c508447", "metadata": {}, "outputs": [], "source": ["iris_ds.Species.tail()"]}, {"cell_type": "markdown", "id": "72cf4bdc", "metadata": {}, "source": ["Displaying columns in the dataset using columns property"]}, {"cell_type": "code", "execution_count": 1, "id": "61340a5f", "metadata": {}, "outputs": [], "source": ["iris_ds.columns.values"]}, {"cell_type": "markdown", "id": "3d1f23b8", "metadata": {}, "source": ["Now I am copying target variable to y and all remaining columns to X variable. without this also you can pass directly to train_test_split method. "]}, {"cell_type": "code", "execution_count": 1, "id": "1838d860", "metadata": {}, "outputs": [], "source": ["X = iris_ds[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]\ny = iris_ds.Species"]}, {"cell_type": "markdown", "id": "01c5611e", "metadata": {}, "source": ["Split dataset for train and test. We use train for training purpose and test for validation."]}, {"cell_type": "code", "execution_count": 1, "id": "cf3180f4", "metadata": {}, "outputs": [], "source": ["x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\nx_train.shape[0],y_train.shape[0]"]}, {"cell_type": "markdown", "id": "6fef6a48", "metadata": {}, "source": ["Using StandardScalar function scale all numarical variables. If we have any categorival variables we use OneHotEncoder() for create dummy variables. We can use pandas get_dummies also for to create dummy variables."]}, {"cell_type": "code", "execution_count": 1, "id": "c0477f30", "metadata": {}, "outputs": [], "source": ["scalar = StandardScaler()\nx_train = scalar.fit_transform(x_train)\nx_test = scalar.transform(x_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "c70d9856", "metadata": {}, "outputs": [], "source": ["x_train"]}, {"cell_type": "markdown", "id": "d03b675b", "metadata": {}, "source": ["I written normal_prediction() fuction for predicting output using different algorithems. \nFollowing algorithems I used for predict\n* LogisticRegression\n* SVM\n* KNN\n* DecisionTree\n* ReandomForest\n* GradientBoosting"]}, {"cell_type": "code", "execution_count": 1, "id": "acc7584b", "metadata": {}, "outputs": [], "source": ["def normal_prediction():\n    logis = LogisticRegression()\n    logis.fit(x_train,y_train)\n    print(\"logistic regression::\\n\",confusion_matrix(y_test,logis.predict(x_test)),\"\\n\")\n    \n    svm = SVC()\n    svm.fit(x_train,y_train)\n    print(\"SVM ::\\n\",confusion_matrix(y_test,logis.predict(x_test)),\"\\n\")\n    \n    knn = KNeighborsClassifier()\n    knn.fit(x_train,y_train)\n    print(\"KNN ::\\n\",confusion_matrix(y_test,knn.predict(x_test)),\"\\n\")\n    \n    dTmodel = DecisionTreeClassifier()\n    dTmodel.fit(x_train,y_train)\n    print(\"DecisionTree ::\\n\",confusion_matrix(y_test,dTmodel.predict(x_test)),\"\\n\")\n    \n    rForest = RandomForestClassifier()\n    rForest.fit(x_train,y_train)\n    print(\"RandomForest ::\\n\",confusion_matrix(y_test,rForest.predict(x_test)),\"\\n\")\n\n    grBoosting = GradientBoostingClassifier()\n    grBoosting.fit(x_train,y_train)\n    print(\"GradientBoosting ::\\n\",confusion_matrix(y_test,grBoosting.predict(x_test)),\"\\n\")"]}, {"cell_type": "markdown", "id": "7a050f6a", "metadata": {}, "source": ["Calling normal_prediction() function"]}, {"cell_type": "code", "execution_count": 1, "id": "4dcbc870", "metadata": {}, "outputs": [], "source": ["normal_prediction()"]}, {"cell_type": "markdown", "id": "7ee41e67", "metadata": {}, "source": ["Using Cross_val_score() function to predict output. This way we can use KFold cross validation. Here I used cv=5. This creates 5 folds."]}, {"cell_type": "code", "execution_count": 1, "id": "532679aa", "metadata": {}, "outputs": [], "source": ["#using cross_val_score\nlogis = LogisticRegression()\nsvm = SVC()\nknn = KNeighborsClassifier()\ndTmodel = DecisionTreeClassifier()\nrForest = RandomForestClassifier()\ngrBoosting = GradientBoostingClassifier()\n    \nscores = cross_val_score(logis,x_train,y_train,cv=5)\nprint(\"Accuracy for logistic regresion: mean: {0:.2f} 2sd: {1:.2f}\".format(scores.mean(),scores.std() * 2))\nprint(\"Scores::\",scores)\nprint(\"\\n\")\n\nscores2 = cross_val_score(svm,x_train,y_train,cv=5)\nprint(\"Accuracy for SVM: mean: {0:.2f} 2sd: {1:.2f}\".format(scores2.mean(),scores2.std() * 2))\nprint(\"Scores::\",scores)\nprint(\"\\n\")\n\nscores3 = cross_val_score(knn,x_train,y_train,cv=5)\nprint(\"Accuracy for KNN: mean: {0:.2f} 2sd: {1:.2f}\".format(scores3.mean(),scores3.std() * 2))\nprint(\"Scores::\",scores)\nprint(\"\\n\")\n\nscores4 = cross_val_score(dTmodel,x_train,y_train,cv=5)\nprint(\"Accuracy for Decision Tree: mean: {0:.2f} 2sd: {1:.2f}\".format(scores4.mean(),scores4.std() * 2))\nprint(\"Scores::\",scores4)\nprint(\"\\n\")\n\nscores5 = cross_val_score(rForest,x_train,y_train,cv=5)\nprint(\"Accuracy for Random Forest: mean: {0:.2f} 2sd: {1:.2f}\".format(scores5.mean(),scores5.std() * 2))\nprint(\"Scores::\",scores5)\nprint(\"\\n\")\n\nscores6 = cross_val_score(grBoosting,x_train,y_train,cv=5)\nprint(\"Accuracy for Gradient Boosting: mean: {0:.2f} 2sd: {1:.2f}\".format(scores6.mean(),scores6.std() * 2))\nprint(\"Scores::\",scores6)\nprint(\"\\n\")"]}, {"cell_type": "markdown", "id": "0807ab15", "metadata": {}, "source": ["GridSearchCV and RandomizedSearchCV are ways to tune hyper parameters."]}, {"cell_type": "code", "execution_count": 1, "id": "26fdd9b0", "metadata": {}, "outputs": [], "source": ["clf = RandomForestClassifier()\n#Random Forest\nparam_dist = {\"max_depth\": [3, None],\n              \"max_features\": sp_randint(1, 4),\n              \"min_samples_split\": sp_randint(2, 4),\n              \"bootstrap\": [True, False],\n              \"criterion\": [\"gini\", \"entropy\"]}\n\n# run randomized search\nn_iter_search = 5\nrandom_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n                                   n_iter=n_iter_search, cv=5)\n\nrandom_search.fit(x_train, y_train)\nprint(random_search.best_params_)\nprint(random_search.best_estimator_)\nconfusion_matrix(y_test,random_search.predict(x_test))\n"]}, {"cell_type": "markdown", "id": "8f9b271e", "metadata": {}, "source": ["Using GridSearchCV to tune parameters"]}, {"cell_type": "code", "execution_count": 1, "id": "230593ca", "metadata": {}, "outputs": [], "source": ["# use a full grid over all parameters\nparam_grid = {\"max_depth\": [3, None],\n              \"max_features\": [1, 3, 4],\n              \"min_samples_split\": [2, 3, 4],\n              \"bootstrap\": [True, False],\n              \"criterion\": [\"gini\", \"entropy\"]}\n\n# run grid search\ngrid_search = GridSearchCV(clf, param_grid=param_grid, cv=5)\n\ngrid_search.fit(x_train, y_train)\nprint(grid_search.best_params_)\nprint(grid_search.best_estimator_)\nconfusion_matrix(y_test,grid_search.predict(x_test))"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}