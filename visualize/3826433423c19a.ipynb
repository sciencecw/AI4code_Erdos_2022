{"cells": [{"cell_type": "markdown", "id": "8ff5381f", "metadata": {}, "source": ["#### PS: Tune-in to the summary at last for what worked best for me, if you dont have much time for this big notebook!"]}, {"cell_type": "markdown", "id": "d3ad23a4", "metadata": {}, "source": ["### Problem Statement"]}, {"cell_type": "markdown", "id": "fe297d7d", "metadata": {}, "source": ["Supplement Sales Prediction\nYour Client WOMart is a leading nutrition and supplement retail chain that offers a comprehensive range of products for all your wellness and fitness needs. \n\nWOMart follows a multi-channel distribution strategy with 350+ retail stores spread across 100+ cities. \n\nEffective forecasting for store sales gives essential insight into upcoming cash flow, meaning WOMart can more accurately plan the cashflow at the store level.\n\nSales data for 18 months from 365 stores of WOMart is available along with information on Store Type, Location Type for each store, Region Code for every store, Discount provided by the store on every day, Number of Orders everyday etc.\n\nYour task is to predict the store sales for each store in the test set for the next two months."]}, {"cell_type": "markdown", "id": "a8a4cbf4", "metadata": {}, "source": ["### Data Dictionary"]}, {"cell_type": "markdown", "id": "77cdfcbc", "metadata": {}, "source": ["#### Train Data\n- ID: Unique Identifier for a row\n\n- Store_id: Unique id for each Store\n\n- Store_Type: Type of the Store\n\n- Location_Type: Type of the location where Store is located\n\n- Region_Code: Code of the Region where Store is located\n\n- Date: Information about the Date\n\n- Holiday: If there is holiday on the given Date, 1 : Yes, 0 : No\n\n- Discount: If discount is offered by store on the given Date, Yes/ No\n\n- Orders: Number of Orders received by the Store on the given Day\n\n- Sales: Total Sale for the Store on the given Day"]}, {"cell_type": "markdown", "id": "69634051", "metadata": {}, "source": ["#### Test Data\n- ID: Unique Identifier for a row\n\n- Store_id: Unique id for each Store\n\n- Store_Type: Type of the Store\n\n- Location_Type: Type of the location where Store is located\n\n- Region_Code: Code of the Region where Store is located\n\n- Date: Information about the Date\n\n- Holiday: If there is holiday on the given Date, 1 : Yes, 0 : No\n\n- Discount: If discount is offered by store on the given Date, Yes/ No"]}, {"cell_type": "code", "execution_count": 1, "id": "2ccb5d23", "metadata": {}, "outputs": [], "source": ["import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt"]}, {"cell_type": "code", "execution_count": 1, "id": "89dc7cf4", "metadata": {}, "outputs": [], "source": ["train = pd.read_csv(\"TRAIN.csv\")\ntrain.head(5)"]}, {"cell_type": "code", "execution_count": 1, "id": "ecdba441", "metadata": {}, "outputs": [], "source": ["test = pd.read_csv(\"TEST_FINAL.csv\")\ntest.head(5)"]}, {"cell_type": "code", "execution_count": 1, "id": "026c7e58", "metadata": {}, "outputs": [], "source": ["train.shape , test.shape"]}, {"cell_type": "markdown", "id": "6b6bd5fc", "metadata": {}, "source": ["### Data Pre-processing"]}, {"cell_type": "code", "execution_count": 1, "id": "1fee8b33", "metadata": {}, "outputs": [], "source": ["train.info(),test.info()"]}, {"cell_type": "code", "execution_count": 1, "id": "d16d4725", "metadata": {}, "outputs": [], "source": ["train.isnull().sum(), test.isnull().sum()"]}, {"cell_type": "code", "execution_count": 1, "id": "d780579d", "metadata": {}, "outputs": [], "source": ["train.duplicated().sum(), test.duplicated().sum()"]}, {"cell_type": "code", "execution_count": 1, "id": "a9f3c372", "metadata": {}, "outputs": [], "source": ["# train_t = train.T\n# test_t = test.T\n# train_t.duplicated().sum(), test_t.duplicated().sum()"]}, {"cell_type": "code", "execution_count": 1, "id": "7d5b9f53", "metadata": {}, "outputs": [], "source": ["test.head()\ntrain.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "ddfafef4", "metadata": {}, "outputs": [], "source": ["y = train.Sales\ntrain_new = train.drop([\"Sales\"],axis=1)"]}, {"cell_type": "markdown", "id": "413fc937", "metadata": {}, "source": ["#### Differentiating Numericala and Categorical Data for further processing"]}, {"cell_type": "code", "execution_count": 1, "id": "875a078f", "metadata": {}, "outputs": [], "source": ["#We have 2 types of data in our dataset : int64 and object\n\ntrain_categorical = train_new.select_dtypes(exclude = ['int64'])\ntest_categorical = test.select_dtypes(exclude = ['int64'])\n\ntrain_numerical = train_new.select_dtypes(include = ['int64'])\ntest_numerical = test.select_dtypes(include = ['int64'])\n\n#Defining column names for numerical data\nnumcol_names_train = train_numerical.columns.values\nnumcol_names_test = test_numerical.columns.values\nnumcol_names_train"]}, {"cell_type": "code", "execution_count": 1, "id": "ed4fea21", "metadata": {}, "outputs": [], "source": ["#Converting these to list from array\nnumcol_names_train.tolist(), numcol_names_test.tolist()"]}, {"cell_type": "markdown", "id": "18cf8114", "metadata": {}, "source": ["#### Checking Skewness and Kurtosis for Numerical Columns"]}, {"cell_type": "code", "execution_count": 1, "id": "2e68d60b", "metadata": {}, "outputs": [], "source": ["sns.kdeplot(train_numerical['#Order'], bw=0.5)    #bw is smoothing parameter\nplt.show()"]}, {"cell_type": "markdown", "id": "6bd59367", "metadata": {}, "source": ["#### Encoding categorical Data"]}, {"cell_type": "code", "execution_count": 1, "id": "422c3684", "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import LabelEncoder\n\ntrain_categorical = train_categorical.apply(LabelEncoder().fit_transform)\ntest_categorical = test_categorical.apply(LabelEncoder().fit_transform)\n\n#Combining the Numnerical and Categorical Database\ntrain_new= pd.concat([train_categorical,train_numerical,y],axis=1)\n\ntest_new = pd.concat([test_categorical,test_numerical],axis=1)\ntrain_new.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "f29e183b", "metadata": {}, "outputs": [], "source": ["test_new.head(5)"]}, {"cell_type": "markdown", "id": "2e9e2fcc", "metadata": {}, "source": ["#### Correlations:"]}, {"cell_type": "code", "execution_count": 1, "id": "8d1e5a03", "metadata": {}, "outputs": [], "source": ["#For coplete Database\ncorr_train = train_new.corr()\nplt.figure(figsize=(13,5)) \n\nax = sns.heatmap(corr_train,annot=True)\nplt.show"]}, {"cell_type": "code", "execution_count": 1, "id": "4a2b7a05", "metadata": {}, "outputs": [], "source": ["imp = train_new.drop(\"Sales\", axis=1).apply(lambda x: x.corr(train_new[\"Sales\"]))\nindices = np.argsort(imp)\nprint(imp[indices])     #Sorted in ascending order"]}, {"cell_type": "markdown", "id": "590b9b9b", "metadata": {}, "source": ["#### Removing Variable with Low correlation with Target Variables"]}, {"cell_type": "code", "execution_count": 1, "id": "19c63f56", "metadata": {}, "outputs": [], "source": ["for i in range(0, len(indices)):\n    if np.abs(imp[i]) < 0.1:\n        print(train_new.columns[i])"]}, {"cell_type": "code", "execution_count": 1, "id": "ded1f3fa", "metadata": {}, "outputs": [], "source": ["train_new.drop([\"Date\",\"Store_id\"],axis=1,inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "a0e47d25", "metadata": {}, "outputs": [], "source": ["train_new.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "4976d508", "metadata": {}, "outputs": [], "source": ["test_new.drop([\"Date\",\"Store_id\"],axis=1,inplace=True)\ntest_new.head()"]}, {"cell_type": "markdown", "id": "0a7df7b5", "metadata": {}, "source": ["#### Checking Predictors Co-relation With each other"]}, {"cell_type": "code", "execution_count": 1, "id": "cff6181a", "metadata": {}, "outputs": [], "source": ["for i in range(0,len(train_new.columns)):\n    for j in  range(0,len(train_new.columns)):\n        if i!=j:\n            corr_1=np.abs(train_new[train_new.columns[i]].corr(train_new[train_new.columns[j]]))\n            if corr_1 <0.3:\n                print( train_new.columns[i] , \" is not correlated  with \", train_new.columns[j])\n            elif corr_1>0.75:\n                print( train_new.columns[i] , \" is highly  correlated  with \", train_new.columns[j])"]}, {"cell_type": "markdown", "id": "86905a6b", "metadata": {}, "source": ["### Dealing with Outliers:`"]}, {"cell_type": "code", "execution_count": 1, "id": "e2d9331f", "metadata": {}, "outputs": [], "source": ["out = sns.boxplot(train_new[\"Sales\"])\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "1962f3e4", "metadata": {}, "outputs": [], "source": ["Q1 = train_new[\"Sales\"].quantile(0.25)\nQ3 = train_new[\"Sales\"].quantile(0.75)\nIQR = Q3 - Q1\n\nfilter = (train_new[\"Sales\"] >= Q1 - 1.5 * IQR) & (train_new[\"Sales\"] <= Q3 + 1.5 *IQR)\ntrain2 = train_new.loc[filter]  \nprint(\"data loss percentage {}%\".format(((len(train_new) - len(train2))/len(train_new))*100))"]}, {"cell_type": "markdown", "id": "5d8077e7", "metadata": {}, "source": ["### Modelling"]}, {"cell_type": "code", "execution_count": 1, "id": "e854262c", "metadata": {}, "outputs": [], "source": ["# Split the Train data into predictors and target\n\nx = train_new.drop(['Sales',\"ID\",\"#Order\"],axis=1)\ny = train_new['Sales']\n\ntest_set = test_new.drop(\"ID\",axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "ad2ab0ce", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 100)"]}, {"cell_type": "markdown", "id": "d6591829", "metadata": {}, "source": ["#### Model 1: Linear regression"]}, {"cell_type": "code", "execution_count": 1, "id": "df5190aa", "metadata": {}, "outputs": [], "source": ["from sklearn.linear_model import LinearRegression\nmodel1 = LinearRegression()  \nmodel1.fit(x_train, y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "394f495e", "metadata": {}, "outputs": [], "source": ["#Prediction from validation set\ny_pred_model1= model1.predict(x_test)\n\nprint(\"Prediction for test set: {}\".format(y_pred_model1))"]}, {"cell_type": "code", "execution_count": 1, "id": "830d2cf5", "metadata": {}, "outputs": [], "source": ["model1.score(x_test,y_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "e895bd31", "metadata": {}, "outputs": [], "source": ["import sklearn.preprocessing\ny_pred_model1_p = sklearn.preprocessing.minmax_scale(y_pred_model1, feature_range=(0,1))\ny_test_p = sklearn.preprocessing.minmax_scale(y_test, feature_range=(0,1))\n\nfrom sklearn.metrics import mean_squared_log_error\nmean_squared_log_error(y_test_p, y_pred_model1_p)"]}, {"cell_type": "code", "execution_count": 1, "id": "4439f072", "metadata": {}, "outputs": [], "source": ["#Prediction from test set\ntest_pred_model1= model1.predict(test_set)\nprint(\"Prediction for test set: {}\".format(test_pred_model1))"]}, {"cell_type": "code", "execution_count": 1, "id": "2732905d", "metadata": {}, "outputs": [], "source": ["frame = { 'ID': test.ID, 'Sales': test_pred_model1 }\nsub1 = pd.DataFrame(frame)\nsub1.set_index('ID', inplace=True)\nsub1 = sub1.to_csv(\"sub1.csv\")"]}, {"cell_type": "markdown", "id": "88957236", "metadata": {}, "source": ["#### Model 2: Decision Tree:"]}, {"cell_type": "code", "execution_count": 1, "id": "ea90e8c5", "metadata": {}, "outputs": [], "source": ["from sklearn.tree import DecisionTreeRegressor\nmodel2 = DecisionTreeRegressor() \nmodel2.fit(x_train, y_train)\n\n#Prediction for validation set\ny_pred_model2 = model2.predict(x_test)\n\n#MSLE FOR Validation data\nmean_squared_log_error(y_test, y_pred_model2)"]}, {"cell_type": "code", "execution_count": 1, "id": "ed373151", "metadata": {}, "outputs": [], "source": ["model2.score(x_test,y_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "0ddcae62", "metadata": {}, "outputs": [], "source": ["#Prediction from test set\ntest_pred_model2= model2.predict(test_set)\n\nframe = { 'ID': test.ID, 'Sales': test_pred_model2}\nsub2 = pd.DataFrame(frame)\nsub2.set_index('ID', inplace=True)\nsub2 = sub2.to_csv(\"sub2.csv\")"]}, {"cell_type": "markdown", "id": "a1053fc3", "metadata": {}, "source": ["#### Model 4:  Random Forest regressor"]}, {"cell_type": "code", "execution_count": 1, "id": "03d26dc7", "metadata": {}, "outputs": [], "source": ["from sklearn.ensemble import RandomForestRegressor\nmodel4 = RandomForestRegressor()\nmodel4.fit(x_train, y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "97e26740", "metadata": {}, "outputs": [], "source": ["model4.score(x_test,y_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "9b05f9d7", "metadata": {}, "outputs": [], "source": ["#Prediction for validation set\ny_pred_model4 = model4.predict(x_test)\n\n#MSLE FOR Validation data\nmean_squared_log_error(y_test, y_pred_model4)"]}, {"cell_type": "code", "execution_count": 1, "id": "90a865c0", "metadata": {}, "outputs": [], "source": ["#Prediction from test set\ntest_pred_model4 = model4.predict(test_set)\n\nframe = { 'ID': test.ID, 'Sales': test_pred_model4}\nsub4 = pd.DataFrame(frame)\nsub4.set_index('ID', inplace=True)\nsub4 = sub4.to_csv(\"sub4.csv\")"]}, {"cell_type": "markdown", "id": "87b09056", "metadata": {}, "source": ["#### Model 5: XG Boost"]}, {"cell_type": "code", "execution_count": 1, "id": "ce89b6ea", "metadata": {}, "outputs": [], "source": ["import xgboost as xg\nmodel5 = xg.XGBRegressor(n_estimators = 500, seed = 100)\nmodel5.fit(x_train, y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "1422c1ca", "metadata": {}, "outputs": [], "source": ["model5.score(x_test,y_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "32e8cc30", "metadata": {}, "outputs": [], "source": ["#Prediction for validation set\ny_pred_model5 = model5.predict(x_test)\n\n#MSLE FOR Validation data\nmean_squared_log_error(y_test, y_pred_model5)"]}, {"cell_type": "code", "execution_count": 1, "id": "0f4f2900", "metadata": {}, "outputs": [], "source": ["#Prediction from test set\ntest_pred_model5 = model5.predict(test_set)\n\nframe = { 'ID': test.ID, 'Sales': test_pred_model5}\nsub5 = pd.DataFrame(frame)\nsub5.set_index('ID', inplace=True)\nsub5 = sub5.to_csv(\"sub5.csv\")"]}, {"cell_type": "markdown", "id": "814221f6", "metadata": {}, "source": ["#### Model 6: Following data centric approach : Inproving data\n\npredicting number of orders first before predicting sales number"]}, {"cell_type": "code", "execution_count": 1, "id": "05d03272", "metadata": {}, "outputs": [], "source": ["x1 = train_new.drop(['Sales',\"ID\",\"#Order\"],axis=1)\ny1 = train_new['#Order']\n\nx1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1, test_size = 0.2, random_state = 100)\n\nmodel61 = RandomForestRegressor(n_estimators=500)\nmodel61.fit(x1_train, y1_train)\n\n#Prediction for validation set\ny_pred_model61 = model61.predict(x1_test)\n\n#MSLE FOR Validation data\naccuracy_pred_orders = model61.score(x1_test,y1_test)\n\naccuracy_pred_orders "]}, {"cell_type": "code", "execution_count": 1, "id": "b2b6ad60", "metadata": {}, "outputs": [], "source": ["y_pred_model61[0].dtype"]}, {"cell_type": "code", "execution_count": 1, "id": "2978b740", "metadata": {}, "outputs": [], "source": ["#Prediction of orders for test data:\npred_order_test = model61.predict(test_set)\npred_order_test = pd.Series(pred_order_test)\n\ntest_set_new = pd.concat([test_set,pred_order_test],axis=1)\n\ntest_set_new.rename({0: '#Order'}, axis=1,inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "34308268", "metadata": {}, "outputs": [], "source": ["pred_order_test.head(3)"]}, {"cell_type": "code", "execution_count": 1, "id": "29e529f6", "metadata": {}, "outputs": [], "source": ["test_set_new.head()"]}, {"cell_type": "markdown", "id": "26374725", "metadata": {}, "source": ["#### Applying new test dataset on model6:"]}, {"cell_type": "code", "execution_count": 1, "id": "02e79b28", "metadata": {}, "outputs": [], "source": ["from sklearn.ensemble import RandomForestRegressor\nmodel6 = RandomForestRegressor()\n\nx2 = train_new.drop([\"ID\",\"Sales\"],axis=1)\ny2 = train_new['Sales']\n\nx2_train, x2_test, y2_train, y2_test = train_test_split(x2, y2, test_size = 0.2, random_state = 100)\n\nmodel6.fit(x2_train, y2_train)\n\n#Prediction for validation set\ny_pred_model6 = model6.predict(x2_test)\n\n#MSLE FOR Validation data\nmean_squared_log_error(y2_test, y_pred_model6)"]}, {"cell_type": "code", "execution_count": 1, "id": "60bd68c3", "metadata": {}, "outputs": [], "source": ["#Prediction from test set\ntest_pred_model6 = model6.predict(test_set_new)\n\nframe = { 'ID': test.ID, 'Sales': test_pred_model6}\nsub6 = pd.DataFrame(frame)\nsub6.set_index('ID', inplace=True)\nsub6 = sub6.to_csv(\"sub6.csv\")"]}, {"cell_type": "markdown", "id": "2c05d889", "metadata": {}, "source": ["#### Model 7 : Without Removing Columns that had Less corelation with target:"]}, {"cell_type": "code", "execution_count": 1, "id": "7aa38839", "metadata": {}, "outputs": [], "source": ["x2 = train.drop([\"ID\",\"Sales\"],axis=1)\nx2_test = test.drop(\"ID\",axis=1)\ny2 = train.Sales\n\nx2_categorical = x2.select_dtypes(exclude = ['int64'])\nx2_numerical = x2.select_dtypes(include = ['int64'])\n\nx2_test_categorical = x2_test.select_dtypes(exclude = ['int64'])\nx2_test_numerical = x2_test.select_dtypes(include = ['int64'])\n\n\nx2_categorical = x2_categorical.apply(LabelEncoder().fit_transform)\nx2_test_categorical = x2_test_categorical.apply(LabelEncoder().fit_transform)\n\n#Combining the Numnerical and Categorical Database\nx2 = pd.concat([x2_categorical,x2_numerical],axis=1)\nx2_test = pd.concat([x2_test_categorical,x2_test_numerical,pred_order_test],axis=1)\nx2_test.rename({0: '#Order'}, axis=1,inplace=True)\n\nx2_test.shape\nfrom sklearn.ensemble import RandomForestRegressor\nmodel7 = RandomForestRegressor()\n\nx2_train, x22_test, y2_train, y22_test = train_test_split(x2, y2, test_size = 0.2, random_state = 100)\n\nmodel7.fit(x2_train, y2_train)\n\n#Prediction for validation set\ny_pred_model7 = model7.predict(x22_test)\n\n#MSLE FOR Validation data\nmean_squared_log_error(y22_test, y_pred_model7)"]}, {"cell_type": "code", "execution_count": 1, "id": "b69e9679", "metadata": {}, "outputs": [], "source": ["model7.score(x22_test,y22_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "2d924415", "metadata": {}, "outputs": [], "source": ["#Prediction from test set\ntest_pred_model7 = model7.predict(x2_test)\n\nframe = { 'ID': test.ID, 'Sales': test_pred_model7}\nsub7 = pd.DataFrame(frame)\nsub7.set_index('ID', inplace=True)\nsub7 = sub7.to_csv(\"sub7.csv\")"]}, {"cell_type": "code", "execution_count": 1, "id": "f5232f12", "metadata": {}, "outputs": [], "source": ["test_pred_model7.shape  "]}, {"cell_type": "markdown", "id": "af73a6b7", "metadata": {}, "source": ["the above model gave worst score on test data ! hence , overfitting. my best bet until now is removing less correlational columns and predicting the orders forst than predicting sales gave almost equal results on test data as that of not considering order column"]}, {"cell_type": "code", "execution_count": 1, "id": "0f37ba07", "metadata": {}, "outputs": [], "source": ["#Generating Pandas Profiling Report for better analysis:\nimport pandas_profiling\nprofile = train.profile_report()\nprofile"]}, {"cell_type": "markdown", "id": "a6c4aace", "metadata": {}, "source": ["from the above survey of profile , i can see that their is huge discrepancy in the holiday column, hence i will apply upsampling for this column and then predict results."]}, {"cell_type": "markdown", "id": "634ab701", "metadata": {}, "source": ["#### Oversampling to balance Holiday Column:"]}, {"cell_type": "code", "execution_count": 1, "id": "3fbcfbd9", "metadata": {}, "outputs": [], "source": ["from sklearn.utils import resample\n\ny = train_new.Holiday\nx = train_new.drop(\"Holiday\",axis=1)\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 100)\n\nd = pd.concat([x_train,y_train],axis=1)\n\nholi = train_new[train_new.Holiday==1]\nnot_holi = train_new[train_new.Holiday==0]\n\n#resample minority\nholi_resampled = resample(holi,replace=True,n_samples=len(not_holi))\n\nresampled = pd.concat([holi_resampled,not_holi])\n\nresampled.Holiday.value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "2d6494be", "metadata": {}, "outputs": [], "source": ["resampled.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "b1701607", "metadata": {}, "outputs": [], "source": ["x_train = resampled.drop([\"ID\",\"Sales\",\"#Order\"],axis=1)\ny_train = resampled.Sales"]}, {"cell_type": "code", "execution_count": 1, "id": "cf1d52a5", "metadata": {}, "outputs": [], "source": ["x_train.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "12bbb738", "metadata": {}, "outputs": [], "source": ["test_new.drop(\"ID\",axis=1,inplace=True)"]}, {"cell_type": "markdown", "id": "df7be4d2", "metadata": {}, "source": ["#### Model 8: Applying Model on resampled data"]}, {"cell_type": "code", "execution_count": 1, "id": "34190740", "metadata": {}, "outputs": [], "source": ["model8 = RandomForestRegressor()\n\nmodel8.fit(x_train, y_train)\n\n# #Prediction from test set\n# test_pred_model8 = model8.predict(test_new)\n\n# frame = { 'ID': test.ID, 'Sales': test_pred_model8}\n# sub8 = pd.DataFrame(frame)\n# sub8.set_index('ID', inplace=True)\n# sub8 = sub8.to_csv(\"sub8.csv\")"]}, {"cell_type": "markdown", "id": "41e83a82", "metadata": {}, "source": ["#### resampling didnt perform that well!"]}, {"cell_type": "markdown", "id": "8290c42d", "metadata": {}, "source": ["### Model 9: ARIMA MODEL:"]}, {"cell_type": "code", "execution_count": 1, "id": "4802534e", "metadata": {}, "outputs": [], "source": ["train = pd.read_csv(\"TRAIN.csv\",parse_dates=True)\ntest = pd.read_csv(\"TEST_FINAL.csv\",parse_dates=True)\n\ntrain['Date'] =  pd.to_datetime(train['Date'])\n\n#Leaving date and label column out of category encoding\ny = train[[\"Sales\",\"Date\"]]\ntrain_new = train.drop([\"Sales\",\"Date\",\"ID\"],axis=1)\n\ny1 = test[[\"Date\"]]\ntest_new = test.drop([\"Date\",\"ID\"],axis=1)\n\n#We have 2 types of data in our dataset : int64 and object\n\ntrain_categorical = train_new.select_dtypes(exclude = ['int64'])\ntest_categorical = test_new.select_dtypes(exclude = ['int64'])\n\ntrain_numerical = train_new.select_dtypes(include = ['int64'])\ntest_numerical = test_new.select_dtypes(include = ['int64'])\n\n#Defining column names for numerical data\nnumcol_names_train = train_numerical.columns.values\nnumcol_names_test = test_numerical.columns.values\nnumcol_names_train\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\ntrain_categorical = train_categorical.apply(LabelEncoder().fit_transform)\ntest_categorical = test_categorical.apply(LabelEncoder().fit_transform)\n\n#Combining the Numnerical and Categorical Database\ntrain_new= pd.concat([train_categorical,train_numerical,y],axis=1)\n\ntest_new = pd.concat([test_categorical,test_numerical,y1],axis=1)\n\n\n#Removing order as it will not be present in test dataset\ntrain_new.drop([\"#Order\"],axis=1,inplace=True)\n\n\n\ntrain2 = train_new[train_new[\"Store_id\"] == 100]\ntrain2\n\nfrom matplotlib.pyplot import figure\nfigure(figsize=(8, 6))\nplt.scatter(train2.Date,train2.Sales)\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "377ae60f", "metadata": {}, "outputs": [], "source": ["# Updating the header\n\ndata1 = train_new[(train_new[\"Store_id\"] == 1)]\ndata11 = data1[[\"Sales\",\"Date\"]]\ndata11.columns=[\"Sales\",\"Date\"]\n\ndata11 = data11.set_index('Date')\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10, 5\ndata11.plot()"]}, {"cell_type": "markdown", "id": "bbef8390", "metadata": {}, "source": ["#### For ARIMA first thing we do is identify if the data is stationary or non \u2013 stationary. if data is non-stationary we will try to make them stationary then we will process further"]}, {"cell_type": "markdown", "id": "ab49cd01", "metadata": {}, "source": ["- Ho: It is non-stationary\n- H1: It is stationary\n\nWe will be considering the null hypothesis that data is not stationary and the alternate hypothesis that data is stationary."]}, {"cell_type": "code", "execution_count": 1, "id": "2e532951", "metadata": {}, "outputs": [], "source": ["from statsmodels.tsa.stattools import adfuller\ntest_result = adfuller(data11['Sales'])\n\ndef adfuller_test(sales):\n    result = adfuller(sales)\n    labels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations']\n    for value,label in zip(result,labels):\n        print(label+' : '+str(value) )\n\n    if result[1] <= 0.05:\n        print(\"Data is stationary\")\n    else:\n        print(\"Data is non-stationary \")\n        "]}, {"cell_type": "code", "execution_count": 1, "id": "89063912", "metadata": {}, "outputs": [], "source": ["adfuller_test(data11['Sales'])"]}, {"cell_type": "markdown", "id": "2b00d6fd", "metadata": {}, "source": ["#### Auto regressive model"]}, {"cell_type": "code", "execution_count": 1, "id": "bcafcba4", "metadata": {}, "outputs": [], "source": ["from pandas.plotting import autocorrelation_plot\nautocorrelation_plot(data11['Sales'])\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "2636e783", "metadata": {}, "outputs": [], "source": ["import statsmodels\nimport statsmodels.api as sm\nfrom statsmodels.graphics.tsaplots import plot_acf,plot_pacf"]}, {"cell_type": "code", "execution_count": 1, "id": "6a5e9d39", "metadata": {}, "outputs": [], "source": ["fig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(data11['Sales'].iloc[13:],lags=40,ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(data11['Sales'].iloc[13:],lags=40,ax=ax2)"]}, {"cell_type": "code", "execution_count": 1, "id": "689a6c08", "metadata": {}, "outputs": [], "source": ["from statsmodels.tsa.arima_model import ARIMA\nimport warnings\nwarnings.filterwarnings(\"ignore\")"]}, {"cell_type": "code", "execution_count": 1, "id": "b7318253", "metadata": {}, "outputs": [], "source": ["def prediction_for_store(store_id=None, train_new = train_new, test_new = test_new):\n    train = train_new[(train_new[\"Store_id\"] == store_id)]\n    train = train[[\"Sales\",\"Date\"]]\n    train.columns=[\"Sales\",\"Date\"]\n    train = train.set_index('Date')\n    \n    test = test_new[(test_new[\"Store_id\"] == store_id)]\n    \n    test_result = adfuller(train['Sales'])\n    \n    start = len(train)\n    end = len(train) + len(test) - 1\n\n    p_values = range(1,2)\n    d_values = range(0,1)\n    q_values = range(0,1)\n    \n    \n    prediction_store = []\n    for p in p_values:\n        for d in d_values:\n            for q in q_values:\n                order = (p,d,q)\n                for i in range(len(test)):\n                    try:\n                        model=ARIMA(train['Sales'],order)\n                        model_fit=model.fit(disp=0)\n                        pred_y = model_fit.forecast()[0]\n                        prediction_store.append(pred_y)\n                    except:\n                        continue\n    return prediction_store"]}, {"cell_type": "code", "execution_count": 1, "id": "07da9a0c", "metadata": {}, "outputs": [], "source": ["predictions = []\nfor i in range(365):\n    pred = prediction_for_store(i+1)\n    predictions.append(pred)"]}, {"cell_type": "code", "execution_count": 1, "id": "09b403fa", "metadata": {}, "outputs": [], "source": ["result=[]\nfor prediction in predictions:\n    for data in prediction:\n        result.extend(data)\n        \nlen(result)"]}, {"cell_type": "code", "execution_count": 1, "id": "43a33855", "metadata": {}, "outputs": [], "source": ["frame = { 'ID': test.ID, 'Sales': result}\narima = pd.DataFrame(frame)\narima.set_index('ID', inplace=True)\narima = arima.to_csv(\"arima.csv\")"]}, {"cell_type": "markdown", "id": "c4ca7d75", "metadata": {}, "source": ["### Model 10: Model by grouping Store_id values together:"]}, {"cell_type": "code", "execution_count": 1, "id": "e852a97a", "metadata": {}, "outputs": [], "source": ["sales_mean = train_new.groupby(['Store_id', 'Discount',\"Holiday\",\"Store_Type\",\"Region_Code\",\"Location_Type\"]).agg({\"Sales\":\"mean\"})\nsales_mean.reset_index(inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "325d8822", "metadata": {}, "outputs": [], "source": ["sales_mean.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "7f417e04", "metadata": {}, "outputs": [], "source": ["x = sales_mean.drop([\"Sales\",\"Region_Code\",\"Location_Type\",\"Store_Type\"],axis=1)\ny = sales_mean.Sales"]}, {"cell_type": "code", "execution_count": 1, "id": "1a8d887e", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 99)"]}, {"cell_type": "code", "execution_count": 1, "id": "37546934", "metadata": {}, "outputs": [], "source": ["from sklearn.ensemble import RandomForestRegressor\nmodel14 = RandomForestRegressor()\nmodel14.fit(x_train,y_train)\n\npred= model14.predict(x_test)\n\nmodel14.score(x_test,y_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "ecc025d0", "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import mean_squared_log_error\nmean_squared_log_error(pred,y_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "5705928a", "metadata": {}, "outputs": [], "source": ["# pred14 = model14.predict(test_set)\n# frame = { 'ID': test.ID, 'Sales': pred14}\n# sales_mean = pd.DataFrame(frame)\n# sales_mean.set_index('ID', inplace=True)\n# sales_mean = sales_mean.to_csv(\"sales_mean.csv\")"]}, {"cell_type": "markdown", "id": "1b254bcc", "metadata": {}, "source": ["### Model 11: Mean of  Outcomes from 2 models"]}, {"cell_type": "code", "execution_count": 1, "id": "94ce3e03", "metadata": {}, "outputs": [], "source": ["x = train_new.drop([\"Sales\",\"Date\"],axis=1)\ny = train_new.Sales"]}, {"cell_type": "code", "execution_count": 1, "id": "95b3f947", "metadata": {}, "outputs": [], "source": ["# from catboost import CatBoostRegressor\n\n# from sklearn.model_selection import train_test_split\n# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 99)\n\n# model10 = CatBoostRegressor()\n# # Fit model\n# model10.fit(x_train,y_train)\n\n# # Get predictions\n# pred10 = model10.predict(x_test)\n# preds_catboost = model10.predict(test_set)\n\n# accuracy_train10 = model10.score(x_test,y_test)\n\n# accuracy_train10"]}, {"cell_type": "code", "execution_count": 1, "id": "5b5eb4d2", "metadata": {}, "outputs": [], "source": ["# model11 = xg.XGBRegressor()\n  \n\n# from sklearn.model_selection import train_test_split\n# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 99)\n\n# # Fitting the model\n# model11.fit(x_train,y_train)\n  \n# # Predict the model\n# pred11 = model11.score(x_test,y_test)\n# preds_xgboost = model11.predict(test_set)"]}, {"cell_type": "code", "execution_count": 1, "id": "8f1701da", "metadata": {}, "outputs": [], "source": ["# sum1 = []\n# for i in range(len(preds_xgboost)):\n#     sum1.append(np.sum(preds_xgboost[i]+ preds_catboost[i])/2)\n# sum1 "]}, {"cell_type": "markdown", "id": "a1a27a8d", "metadata": {}, "source": ["#### Same results as with single model"]}, {"cell_type": "markdown", "id": "aeda0b33", "metadata": {}, "source": ["### Model 12: With grid search params"]}, {"cell_type": "code", "execution_count": 1, "id": "9430efbe", "metadata": {}, "outputs": [], "source": ["# n_estimators = [ int(x) for x in np.linspace(start=100,stop=1000,num=10)]\n# max_features = [ \"auto\",\"sqrt\"]\n# max_depth= [2,4]\n\n# # min_sample_split= [2,4]\n# min_samples_leaf = [1,6]\n# bootstrap = [True,False]\n\n# #Creating param grid\n# param_grid = {\"n_estimators\":n_estimators,\n#     \"max_features\" : max_features,\n#     \"max_depth\" : max_depth,\n# #     \"min_sample_split\" : min_sample_split,\n#     \"min_samples_leaf\" : min_samples_leaf,\n#     \"bootstrap\" : bootstrap}\n        \n# print(param_grid)\n    \n# from sklearn.ensemble import RandomForestRegressor\n# model9 = RandomForestRegressor()\n# from sklearn.model_selection import GridSearchCV\n\n# grid_search = GridSearchCV(estimator= model9,param_grid=param_grid,cv=3,verbose=2,n_jobs=4)\n\n# grid_search.fit(x_train,y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "be2abcb4", "metadata": {}, "outputs": [], "source": ["# grid_search.best_params_"]}, {"cell_type": "code", "execution_count": 1, "id": "7eb2f2a2", "metadata": {}, "outputs": [], "source": ["# grid_train = grid_search.score(x_test,y_test)\n# grid13 = grid_search.predict(test_set)\n\n# grid_train"]}, {"cell_type": "code", "execution_count": 1, "id": "b0a24e71", "metadata": {}, "outputs": [], "source": ["# frame = { 'ID': test.ID, 'Sales': grid13}\n# sub13 = pd.DataFrame(frame)\n# sub13.set_index('ID', inplace=True)\n# sub13 = sub13.to_csv(\"sub13.csv\")"]}, {"cell_type": "markdown", "id": "c565de8e", "metadata": {}, "source": ["#### grid search recommended features:\n    \nbootstrap= True,max_depth= 4,max_features= \"sqrt\",min_samples_leaf= 6, n_estimators= 1000"]}, {"cell_type": "markdown", "id": "5d4f621d", "metadata": {}, "source": ["### Final Outcome"]}, {"cell_type": "markdown", "id": "cbf1471e", "metadata": {}, "source": ["After exhaustive attempts of 3 days and giving my best, I received the best MSLE*1000 Error of 225 (rank 78!). I tried multiple approaches, used the data-centric approach by making changes to data multiple times (as many ways as my brain could think of) apart from changing models. \n\nI tried multiple models (approx 10), also implemented an algorithm I never used before ARIMA (in this hectic pressures!), much to my disappointment the model didn\u2019t perform that well as compared to ordinary ones.\n\nI also tried to group the data for same-store and make prediction store-wise instead of generic, which also didn\u2019t result in good results test results though performed exceptionally on my train dataset(overfitting because of fewer train data when clubbed).\n\nMy best results were in the most simple approach, (Simplicity at its best!) all the good models gave me almost equal results, with all features except Date, the params when kept at default performed best in my case(Still don\u2019t know how, but open to wonders)!\n\nAs this hackathon nears the closure, my mind is still restless with what can be the approach (eagerly waiting for top coder code file!!)\n\nStill, I feel satisfied as I gave my 100% at every hour of this weekend challenge and learned a lot!\n\nPeace :)\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}