{"cells": [{"cell_type": "markdown", "id": "b85d1392", "metadata": {}, "source": ["# Introduction\n\nHello everyone,\n\nThis noteboot is an assignment of CBD Robotics Intern to utilize my acknowledge. It entails two main sections.\n\n***Cleaning data***, includes: dealing with missing data, outliers, scaling, and PCA.\n\n***Building and Tuning Linear Regression*** to get the best predictions. "]}, {"cell_type": "code", "execution_count": 1, "id": "e47c2280", "metadata": {}, "outputs": [], "source": ["import numpy as np \nimport pandas as pd \nimport scipy\nimport random\nrandom.seed(10)\nnp.random.seed(11)\n\n\nfrom scipy import stats\nfrom scipy.stats import norm\nimport missingno as msno\nimport datetime\n\n#from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import KNNImputer\n\nfrom sklearn.model_selection import train_test_split,cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.linear_model import LinearRegression, Ridge, RidgeCV\n\n# Ploting libs\n\nfrom plotly.offline import iplot, plot\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport plotly.io as pio\npio.renderers.default = \"notebook\" \n# As after installing vscode, renderer changed to vscode, \n# which made graphs no more showed in jupyter.\n\nfrom yellowbrick.regressor import ResidualsPlot\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nsns.set_palette('RdBu')"]}, {"cell_type": "markdown", "id": "4a304cbc", "metadata": {}, "source": ["# 1. Take a look at the Dataset"]}, {"cell_type": "code", "execution_count": 1, "id": "b006cf24", "metadata": {}, "outputs": [], "source": ["df = pd.read_csv('../input/melbourne-housing-market/Melbourne_housing_FULL.csv')"]}, {"cell_type": "code", "execution_count": 1, "id": "f0269c8d", "metadata": {}, "outputs": [], "source": ["print('Observations                 : ', df.shape[0])\nprint('Features -- exclude the Price: ', df.shape[1] - 1)"]}, {"cell_type": "code", "execution_count": 1, "id": "a1190092", "metadata": {}, "outputs": [], "source": ["# Datatypes\ndf.info()"]}, {"cell_type": "code", "execution_count": 1, "id": "111e39e3", "metadata": {}, "outputs": [], "source": ["df.head(5)"]}, {"cell_type": "markdown", "id": "11935835", "metadata": {}, "source": ["We have some zero in Landsize, let get a closer survey on zeroes."]}, {"cell_type": "code", "execution_count": 1, "id": "a1d41517", "metadata": {}, "outputs": [], "source": ["# zero values\n(df==0).sum().sort_values(ascending=False).head(6)"]}, {"cell_type": "markdown", "id": "0eedff6b", "metadata": {}, "source": ["## Comments\n* ***Landsize and BuildingArea*** where equal zeros must be missing data. Convert them.\n* ***Date*** is time series, which is a big deal for Linear Regression, so better extract Month and Year from Date then delete it.\n* ***Suburb, Address, SellerG***: full of text with too many distinct values, should be removed, as Linear Regression can not deal with them."]}, {"cell_type": "code", "execution_count": 1, "id": "3fa4d541", "metadata": {}, "outputs": [], "source": ["# Zeroes to Missing in Landsize and BuildingArea\ndf['Landsize'].replace(0, np.nan, inplace=True)\ndf['BuildingArea'].replace(0, np.nan, inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "64f7d3a6", "metadata": {}, "outputs": [], "source": ["# Extract Month & Year from Date, then drop Date\n\ndf['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\n\ndf['Month'] = df['Date'].dt.month\ndf['Year'] = df['Date'].dt.year\n\ndf.drop('Date', axis=1, inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "35ea030d", "metadata": {}, "outputs": [], "source": ["# Drop: Texts\ndf.drop(['Suburb', 'Address', 'SellerG'], axis=1, inplace=True)"]}, {"cell_type": "markdown", "id": "3f25bd79", "metadata": {}, "source": ["# 2. Missing Data: A Quick Glance"]}, {"cell_type": "code", "execution_count": 1, "id": "805f80ac", "metadata": {}, "outputs": [], "source": ["# A Brief of Missing data\ntotal_miss   = df.isnull().sum().sort_values(ascending=False)\n\npercent      = total_miss / df.shape[0]\n\ntable = pd.concat([total_miss, percent], axis=1, keys=['Numbers', 'Percent'])\nprint(table.head(15))"]}, {"cell_type": "markdown", "id": "3fc33cf1", "metadata": {}, "source": ["***More 40 percents*** must be unbearble for any imputation. I would drop those columns.\n\n***Bathroom and Bedroom2*** look like a twins. Notice them in EDA latter.\n\n***The target Price*** has 21% data missing. Should I try to impute them? No. I would not take risks of predicting missing of things already mystic. Lets listwise remove them."]}, {"cell_type": "code", "execution_count": 1, "id": "af1e4916", "metadata": {}, "outputs": [], "source": ["# Drop: Missing > 40%\ndf.drop(['BuildingArea', 'YearBuilt', 'Landsize'], axis=1, inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "c900bb75", "metadata": {}, "outputs": [], "source": ["# Drop: Missing in Price\ndf.dropna(subset=['Price'], axis=0, inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "670de57f", "metadata": {}, "outputs": [], "source": ["# Drop: Minorities\ndf.dropna(subset=['Propertycount', 'Regionname', 'CouncilArea', 'Postcode', 'Distance'],\n          axis=0, inplace=True)"]}, {"cell_type": "markdown", "id": "83496718", "metadata": {}, "source": ["# 3. Descriptive Statistic "]}, {"cell_type": "code", "execution_count": 1, "id": "bd9e3bc9", "metadata": {}, "outputs": [], "source": ["df.describe(percentiles=[0.01, 0.25, 0.75, 0.99])"]}, {"cell_type": "markdown", "id": "6db24818", "metadata": {}, "source": ["## Comments on Numerics\n***Datatype***\n * ***Postcode and Propertycount:*** first, they should have been categorical by nature, but being numerical. Second, Postcode has 211 uniques and Propertycount has 342, so even with converts into categorical, one-hot-encode will be useless. I will remove them.\n \n***Abnormality***\n * ***Some palaces on sale*** with: 30 Bedroom 2, 26 slots of Car parking."]}, {"cell_type": "code", "execution_count": 1, "id": "91b754b4", "metadata": {}, "outputs": [], "source": ["# Texts with too many of uniques\ndf.drop(['Postcode', 'Propertycount'], axis=1, inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "c4304efa", "metadata": {}, "outputs": [], "source": ["df.describe(include='O').sort_values(axis=1, by=['unique'], ascending=False)"]}, {"cell_type": "markdown", "id": "c0329154", "metadata": {}, "source": ["## Comments on Categories\n* ***CouncilArea*** has 33 of unique values, though still are able to apply one-hot-encode, but it will burden Linear Regression performance. It would be removed.\n* ***Regionname, Type, Method*** has pretty small number of distinct values. They are deserved to one-hot-encode."]}, {"cell_type": "code", "execution_count": 1, "id": "ad835c52", "metadata": {}, "outputs": [], "source": ["df.drop('CouncilArea', axis=1, inplace=True)"]}, {"cell_type": "markdown", "id": "ccb6593a", "metadata": {}, "source": ["## To-do latter\n* ***Regionname, Type, Method***: one-hot-encode."]}, {"cell_type": "markdown", "id": "f845d480", "metadata": {}, "source": ["# 4. EDA"]}, {"cell_type": "code", "execution_count": 1, "id": "982515ea", "metadata": {}, "outputs": [], "source": ["#Classify features based on Datatypes, helpful for EDA.\n\ncontinuous_features = ['Price',      'Distance']\n\ndiscrete_features  = ['Bathroom',    'Bedroom2',       'Car',        'Rooms']\n\ncategory_features  = ['Type',        'Method',         'Regionname']\n                     "]}, {"cell_type": "markdown", "id": "343f52fb", "metadata": {}, "source": ["# 4.1 The Target: Price"]}, {"cell_type": "code", "execution_count": 1, "id": "fef05521", "metadata": {}, "outputs": [], "source": ["sns.distplot(df['Price'], fit=norm);"]}, {"cell_type": "markdown", "id": "cd1b5710", "metadata": {}, "source": ["Price is ***skewed***, but be able to nomalized by removing extreme high points on the right."]}, {"cell_type": "markdown", "id": "6848dca5", "metadata": {}, "source": ["# 4.2 Univariate analyze: Features"]}, {"cell_type": "code", "execution_count": 1, "id": "3e7bcca5", "metadata": {}, "outputs": [], "source": ["df[continuous_features].hist(bins=40, figsize=(18,9))\nplt.show()"]}, {"cell_type": "markdown", "id": "57445d2b", "metadata": {}, "source": ["* ***Distance*** are skewed."]}, {"cell_type": "code", "execution_count": 1, "id": "45a0458f", "metadata": {}, "outputs": [], "source": ["df[discrete_features].hist(bins=40, figsize=(20,20))\nplt.show()"]}, {"cell_type": "markdown", "id": "10c6b3c8", "metadata": {}, "source": ["## Comments on Discrete Features\n***Potential Outliers***\nMost of observations have:\n* Bathroom < 5,\n* Bedroom2 < 10,\n* Car      < 10,\n* Rooms    < 6.  \n\nSo, points standing out of these boundaries probaly are outliers.\n"]}, {"cell_type": "markdown", "id": "8f9a8e85", "metadata": {}, "source": ["# 4.3 Bivariate analyze"]}, {"cell_type": "code", "execution_count": 1, "id": "4d68aa0f", "metadata": {}, "outputs": [], "source": ["# First try for Total sales per Region\n\n# plotly.offline.init_notebook_mode(connected=True)\n\nregions = df.Regionname.unique()\ntotal_values_per_region = [df['Price'][df.Regionname==region].sum() for region in regions]\n\nfig = px.bar(y=regions, x=total_values_per_region,\n             title='Total Sales per Regions', orientation='h',\n             template='plotly_white')\n\nfig.update_layout(xaxis={'title':'Price'},\n                  yaxis={'title':'Regions'})\n\nfig.show()"]}, {"cell_type": "markdown", "id": "003934ec", "metadata": {}, "source": ["***Regions*** somehow play an important role in Sales."]}, {"cell_type": "code", "execution_count": 1, "id": "93aace5d", "metadata": {}, "outputs": [], "source": ["fig = px.box(df, x='Regionname', y='Price', template='simple_white')\nfig.update_layout(title='Price by Regions')"]}, {"cell_type": "markdown", "id": "ec926b8a", "metadata": {}, "source": ["***Strange 'Outliers'***   \nA considerable number of Price are far from their quartiles, I afraid that Z-score, 3-sigma, or IQR - detecting outlier strategies will remove a lot of data, lets see."]}, {"cell_type": "code", "execution_count": 1, "id": "2f1f089d", "metadata": {}, "outputs": [], "source": ["# IQR score\ndef IQR_outlier_detect(data=df, features=[]):\n    for feature in features:\n        Q1 = data[feature].quantile(0.25)\n        Q3 = data[feature].quantile(0.75)\n        IQR = Q3 - Q1\n        outside_IQR = (data[feature]<=(Q1-1.5*IQR)) | ((Q3+1.5*IQR)<=data[feature])  \n        outside_IQR = outside_IQR.sum()        \n        \n        print('Outside of IQR: %s -- Total: %d -- percent %2.2f'% (feature, outside_IQR, outside_IQR/df.shape[0]))\n    return\n\nIQR_outlier_detect(df, features=['Price'])"]}, {"cell_type": "markdown", "id": "db927c08", "metadata": {}, "source": ["***No problem, Price is fine***."]}, {"cell_type": "code", "execution_count": 1, "id": "830c60e5", "metadata": {}, "outputs": [], "source": ["fig = px.scatter(df, x='Longtitude', y='Lattitude', color='Price')\nfig.update_layout(title='Price by Locations')"]}, {"cell_type": "markdown", "id": "d47ddbf3", "metadata": {}, "source": ["Sale houses tend to locate in the map central.  "]}, {"cell_type": "markdown", "id": "2838b928", "metadata": {}, "source": ["# 4.4 Multivariate analyze"]}, {"cell_type": "code", "execution_count": 1, "id": "0803d4f2", "metadata": {}, "outputs": [], "source": ["# Price vs Continuous Features\n\ncorr_matrix = df[continuous_features].corr()\n\nfigure = plt.figure(figsize=(16,12))\n\nmask = np.triu(corr_matrix) # Hide the upper part.\nsns.heatmap(corr_matrix, annot=True, fmt='.2f', linewidths=0.5, cmap=\"YlGnBu\", mask=mask)\n\nplt.show()"]}, {"cell_type": "markdown", "id": "bf26a79a", "metadata": {}, "source": ["Nothing seems to be meaningful."]}, {"cell_type": "code", "execution_count": 1, "id": "dc3e764e", "metadata": {}, "outputs": [], "source": ["# Price vs Discrete Features\n\ncorr_matrix = df[discrete_features + ['Price']].corr()\n\nfigure = plt.figure(figsize=(16,12))\n\nmask = np.triu(corr_matrix) # Hide the upper part.\nsns.heatmap(corr_matrix, annot=True, fmt='.2f', linewidths=0.5, cmap=\"YlGnBu\", mask=mask)\n\nplt.show()\n"]}, {"cell_type": "markdown", "id": "a4983cca", "metadata": {}, "source": ["***Rooms and Bedroom2*** is a twins, so keep Rooms and drop the latter."]}, {"cell_type": "code", "execution_count": 1, "id": "74821ebb", "metadata": {}, "outputs": [], "source": ["df.drop('Bedroom2', axis=1, inplace=True)\n\ndiscrete_features.remove('Bedroom2')"]}, {"cell_type": "markdown", "id": "2a8e713f", "metadata": {}, "source": ["# 5. Outliers"]}, {"cell_type": "markdown", "id": "c2afc8ac", "metadata": {}, "source": ["## Detection by IQR Rule\n\n***IQR Rule***  \n\nThis is a renowned technique to detecting outliers. To apply this rule, first we need to define several stuffs.\n\n***Q1***: the quantile at 25%.\n\n***Q3***: the quantile at 75%.\n\n***IQR*** = Q3 - Q1.\n\nThen, any value stands out of range **[Q1 - 1.5 IQR, Q3 + 1.5 IQR]** would be considered an outlier.\n\nThe IQR rule would be praticed on numerical features only."]}, {"cell_type": "code", "execution_count": 1, "id": "ab0cdca2", "metadata": {}, "outputs": [], "source": ["# First, detect Outliers\nfeatures = continuous_features + discrete_features\nIQR_outlier_detect(df, features)"]}, {"cell_type": "code", "execution_count": 1, "id": "9ed47a6c", "metadata": {}, "outputs": [], "source": ["# Remove Outliers\ndef IQR_outlier_remove(data=df, features=[]):\n    for feature in features:\n        Q1 = data[feature].quantile(0.25)\n        Q3 = data[feature].quantile(0.75)\n        IQR = Q3 - Q1\n        \n        # the core: the ~ is a must to avoid removing NaN.\n        outside_IQR = (data[feature]<=(Q1-1.5*IQR)) | ((Q3+1.5*IQR)<=data[feature])\n        data = data[~outside_IQR]\n        print('Cleaning: ', feature)\n        print('Q1: %2.2f', Q1)\n        print('Q2: %2.2f', Q3)\n        print('After cleaning, data left: %d \\n' % (data.shape[0]))\n        \n        # debug\n        #inside_IQR = ((Q1-1.5*IQR)<= data[feature]) & (data[feature]<=(Q3+1.5*IQR))\n        \n    return data\n\n# Driving code\nfeatures = continuous_features + discrete_features\ndf = IQR_outlier_remove(df, features)"]}, {"cell_type": "code", "execution_count": 1, "id": "7f1e4cdc", "metadata": {}, "outputs": [], "source": ["# How much observations left?\ndf.shape"]}, {"cell_type": "markdown", "id": "40ee40aa", "metadata": {}, "source": ["# 6. Standardization"]}, {"cell_type": "code", "execution_count": 1, "id": "e4cdce91", "metadata": {}, "outputs": [], "source": ["df.dtypes"]}, {"cell_type": "code", "execution_count": 1, "id": "8945682c", "metadata": {}, "outputs": [], "source": ["features_to_scaler = ['Rooms', 'Distance', 'Bathroom', 'Car',\n                        'Lattitude', 'Longtitude',\n                        'Month', 'Year']"]}, {"cell_type": "code", "execution_count": 1, "id": "4d82fdd0", "metadata": {}, "outputs": [], "source": ["df_std = df"]}, {"cell_type": "code", "execution_count": 1, "id": "3708d3ee", "metadata": {}, "outputs": [], "source": ["scaler = StandardScaler()\n\nfor feature in features_to_scaler:\n    df_std[feature] = scaler.fit_transform(df_std[feature].values.reshape(-1, 1))"]}, {"cell_type": "code", "execution_count": 1, "id": "0d7a2fef", "metadata": {}, "outputs": [], "source": ["df_std.head()"]}, {"cell_type": "markdown", "id": "0bc89c79", "metadata": {}, "source": ["# 7. One Hot Encode\nWith intending to knn imputing on missing data, but knn only works with numerical, not categorical, so the encoding is performed up front."]}, {"cell_type": "code", "execution_count": 1, "id": "1878957a", "metadata": {}, "outputs": [], "source": ["df_std.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "f8b6833c", "metadata": {}, "outputs": [], "source": ["df_encode = pd.get_dummies(df_std)"]}, {"cell_type": "code", "execution_count": 1, "id": "592e30b5", "metadata": {}, "outputs": [], "source": ["df_encode.dtypes"]}, {"cell_type": "markdown", "id": "89d3b7f4", "metadata": {}, "source": ["# 8. Missing Data"]}, {"cell_type": "markdown", "id": "661d710c", "metadata": {}, "source": ["## Strategies for  Missing Data\n<a href=\"https://ibb.co/fXC5QMG\"><img src=\"https://i.ibb.co/TwHSrcq/Missing-Data.png\" alt=\"Missing-Data\" border=\"0\"></a>"]}, {"cell_type": "markdown", "id": "7bd6dc59", "metadata": {}, "source": ["## Assumption: all MCAR.\n\nSelection of methods must base on the nature of missing data, whether they are MCAR, MAR, or MNAR. I know a research on those are essential, but in this entry-level assignment, I will skip it to focus on the major section Modelling.\n\nTherefore, let assumpt all columns are MCAR."]}, {"cell_type": "code", "execution_count": 1, "id": "bb2504c5", "metadata": {}, "outputs": [], "source": ["# A Brief of Missing data\n\ntotal_miss   = df.isnull().sum().sort_values(ascending=False)\n\npercent      = total_miss / df.shape[0]\n\ntable = pd.concat([total_miss, percent], axis=1, keys=['Numbers', 'Percent'])\nprint(table.head(8))"]}, {"cell_type": "markdown", "id": "9fdaa0bb", "metadata": {}, "source": ["## Strategies on Choices\n\n * ***Hand-in-hand pattern***: If a row lacks Car value, moreoften lacks values in Bathroom, Longtitude, Lattitude, and vice versa. Please scroll a half page down then look at the graph, you'll see that most of cells are around 1, which means our missing data are very centralized in specific rows.\n      \n \n * ***K-nn Imputation is by far the best***. Reason is the way real estate market working: houses with similar specifications, close-by location, usually sold in the same price level. So, k-nn is a nice choice.\n"]}, {"cell_type": "code", "execution_count": 1, "id": "09650d75", "metadata": {}, "outputs": [], "source": ["msno.heatmap(df)"]}, {"cell_type": "markdown", "id": "874ddee9", "metadata": {}, "source": ["## Simply Put\n***Listwise deleting***: Region Name and Distance.\n\n***K-nn approach***: all the rest.\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "e3befd87", "metadata": {}, "outputs": [], "source": ["df_encode.dtypes"]}, {"cell_type": "code", "execution_count": 1, "id": "a6243486", "metadata": {}, "outputs": [], "source": ["# K-nn imputation\nneighbors = 10\nimputer = KNNImputer(n_neighbors=neighbors)\n\ndf_filled = imputer.fit_transform(df_encode)\n\n# to Dataframe\ndf_filled = pd.DataFrame(df_filled)"]}, {"cell_type": "code", "execution_count": 1, "id": "c39f879a", "metadata": {}, "outputs": [], "source": ["df_filled.head()"]}, {"cell_type": "markdown", "id": "813f214a", "metadata": {}, "source": ["# 9. Assign to X, y"]}, {"cell_type": "code", "execution_count": 1, "id": "db12a9db", "metadata": {}, "outputs": [], "source": ["y = df_filled[1]\n\nX = df_filled.drop(labels=1, axis=1)"]}, {"cell_type": "markdown", "id": "5b625d36", "metadata": {}, "source": ["# 10. Linear Regression"]}, {"cell_type": "markdown", "id": "d9db9c50", "metadata": {}, "source": ["### Assumptions of Linear Regression\n\nBeforehand modeling or tuning, firstly we need to acknowledge of Assumptions of Linear Regression.\n\n* ***Normality of X and y***. Or by a more specific term: multivariate normality. Hum, dangerous-look words..\n* ***Linearity of X and y***. Capital X means a plural of features, columns.\n* ***Homoscedasticity***. Namely, variance of residuals are constant. There are several others explanation of homoscedasticity, but this one is nice and simple at most, especially for Residual plots.\n\n\nIt seems like a lot of works, but fortunely could be done just by Residual plots."]}, {"cell_type": "markdown", "id": "050bac5c", "metadata": {}, "source": ["### Searching for Normality"]}, {"cell_type": "code", "execution_count": 1, "id": "0c186ea8", "metadata": {}, "outputs": [], "source": ["# Normality of y\nsns.distplot(y, fit=norm);\nfig = plt.figure()\nres = stats.probplot(y, plot=plt)"]}, {"cell_type": "markdown", "id": "0567d14a", "metadata": {}, "source": ["The Price is not very normal. It shows peakedness, skewness and does not follow the diagonal line.\n\nLet's transform it.\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "39926df8", "metadata": {}, "outputs": [], "source": ["y = np.log(y)\n\n# Check again\n# Normality of y\nsns.distplot(y, fit=norm);\nfig = plt.figure()\nres = stats.probplot(y, plot=plt)"]}, {"cell_type": "markdown", "id": "21ac264d", "metadata": {}, "source": ["#### Normality of X\n\nTake a look at X then we'll see lots of negative values, which are incompetence for log transformations. Of course, we can still perform np.log() for X, but it will return NaN for negative values, and damages our dataset."]}, {"cell_type": "code", "execution_count": 1, "id": "a4e67c90", "metadata": {}, "outputs": [], "source": ["X.head()"]}, {"cell_type": "markdown", "id": "e2aa1766", "metadata": {}, "source": ["### Searching for Linearity and Homoscedasticity\n\nI will leave them blank because I don't know how to do it for now. Sorry, it must be a gap in my knowledge."]}, {"cell_type": "markdown", "id": "6f8f5487", "metadata": {}, "source": ["### Modeling\n\nThe most important part of this sections is ***B - Linear Regression with Cross Validation***, that I am carrying on both modeling and tuning carefully. The reasons:\n* Linear regression with Holdout, aka 1-fold cross validation, are highly dependent on luck that I dislike, so it is removal.\n* Linear regression with PCA is just a CV linear regression with additional steps. It's better to detail the CV linear regression then assuming the PCA one are similiar.  \n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "cf9c4f2a", "metadata": {}, "outputs": [], "source": ["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11)"]}, {"cell_type": "markdown", "id": "84f11294", "metadata": {}, "source": ["X_train and y_train for models to learn with folding by the cross validation, where X_test, y_test would be untouch till the final scoring. "]}, {"cell_type": "markdown", "id": "700f5f4f", "metadata": {}, "source": ["## A - Linear Regression with Holdout"]}, {"cell_type": "code", "execution_count": 1, "id": "dcd1266a", "metadata": {}, "outputs": [], "source": ["A = Ridge(alpha=0)\n\nA.fit(X_train, y_train)\nprint(\"A's score: %2.4f\" % A.score(X_test, y_test))"]}, {"cell_type": "markdown", "id": "4bc2ccfc", "metadata": {}, "source": ["Oh, a nice number.Then, 0.6959 would be our baseline for further tuning."]}, {"cell_type": "markdown", "id": "f3485b43", "metadata": {}, "source": ["## B - Linear Regression with Cross Validation"]}, {"cell_type": "code", "execution_count": 1, "id": "2f2f9433", "metadata": {}, "outputs": [], "source": ["# B is the same as A but with CV\n\nB = RidgeCV(alphas=[0], cv=5, scoring='r2')\n\nB.fit(X_train, y_train)\nprint(\"A's score: %2.4f\" % B.score(X_test, y_test))"]}, {"cell_type": "code", "execution_count": 1, "id": "7ca1dc20", "metadata": {}, "outputs": [], "source": ["# Finding the best k-folds\n\nB_score = []\ncv = []\n\nfor i in range(2, 11):\n    model = Ridge(alpha=0, normalize=True)\n    score = cross_val_score(model, X_train, y_train, cv=i).mean()\n    if score<0 : score = 0\n    B_score.append(round(score, 5))\n    cv.append(i)\n    \n    print(\"cv: %d --- score: %2.5f\" % (i, score))\n    \nB_score = [0 if score<0 else score for score in B_score]\nprint(B_score)\n\npx.line(x=cv, y=B_score, \n        template='simple_white', \n        title='<b>K-fold vs R2</b>',\n        labels={'x':'K-fold', 'y':'R2'})\n"]}, {"cell_type": "markdown", "id": "e589daf0", "metadata": {}, "source": ["*** 8 K-fold is the best***, though not the by far best. Let fix the k-fold down. "]}, {"cell_type": "code", "execution_count": 1, "id": "249b9b83", "metadata": {}, "outputs": [], "source": ["cv = 8"]}, {"cell_type": "markdown", "id": "d16dd7b2", "metadata": {}, "source": ["## Tuning B - Linear Regression with cv=8"]}, {"cell_type": "markdown", "id": "fe87138f", "metadata": {}, "source": ["We have 2 parameters to tune:\n* How strong the regularization.\n* Should we normalize the data?"]}, {"cell_type": "code", "execution_count": 1, "id": "1b31f232", "metadata": {}, "outputs": [], "source": ["params = {'alpha':[100, 30, 21, 20, 19.5, 19, 18.5, 18, 17, 17.5, 16, 15, 14, 13.5, 13, 12.5, 12, 11, 10.5, 10, 9.5, 9, 8.5, 8, 7.7, 7.6, 7.5, 7.4, 7.3, 7, 6, 5, 4.5, 4, 3.5, 3, 1, 0.3, 0.1, 0.03, 0.01, 0],\n          'normalize': (True, False)}\n\nmodel = Ridge()\ngsc = GridSearchCV(estimator=model, param_grid=params, n_jobs=-1)\ngsc.fit(X_train, y_train)\n\nbest = gsc.best_params_\nscore = gsc.score(X_test, y_test)\nprint('With : ', best)\nprint('Score: %2.4f' % score)"]}, {"cell_type": "markdown", "id": "53df68f4", "metadata": {}, "source": ["The best choice: alpha 7.6 and normalize False. \n\nYes, normalize must be False - turned off, cause we perform a standard scaling already."]}, {"cell_type": "code", "execution_count": 1, "id": "8db06c73", "metadata": {}, "outputs": [], "source": ["# With those best params, plot: Residuals vs Prediction\n\nB = gsc.best_estimator_\nB.fit(X_train, y_train)\nprint(\"B's score: %2.4f\" % B.score(X_test, y_test))\n\nvisualizer = ResidualsPlot(B)\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show() "]}, {"cell_type": "markdown", "id": "20aa89d2", "metadata": {}, "source": ["#### Comments\n* Homoscedastic. Yes, we got it. The shape is not fan-out, not spreading. Points locate within a quite parallel limits.\n* Normality. We got it, too. On the scatter plot, there are a bit outliers on the upper, but no any dense on one side. Then, look at the histogram on the right, quite perfect nomarl, huh.\n* Linearity between X and y. I am not so sure. It is worth an extensive study.\n\n* Outliers. There are some of them on the higher top. I afraid that somehow these outliers sneaked into data after all the scaling and normalizing to destroy our normality."]}, {"cell_type": "markdown", "id": "eb9227ef", "metadata": {}, "source": ["***Comments for the OLDER version of B***\n\n> For my presentation at class.\n\nIn this graph of Residuals against Predicted values, the ***distribution*** is:\n1. In fan-out shape: an identify of not constant variance of residuals, or namely ***Heteroscedasticity***.\n2. A little curve or bend: probably is a proof of ***non-linear***.\n\nSo we got two violations here: ***non-homoscedasticity*** and ***non-linearity*** of X and y. Mention that both problems lay in natural of data, not in the linear model. Nothing in hell we can do with it."]}, {"cell_type": "code", "execution_count": 1, "id": "4c0e33ad", "metadata": {}, "outputs": [], "source": ["from yellowbrick.regressor import PredictionError\nfrom sklearn.linear_model import Lasso\n\nmodel = PredictionError(B)\nmodel.fit(X_train, y_train)\nmodel.score(X_test, y_test)\nmodel.show()"]}, {"cell_type": "markdown", "id": "014439e0", "metadata": {}, "source": ["## C - LR with PCA and Holdout"]}, {"cell_type": "code", "execution_count": 1, "id": "5c7e5a7b", "metadata": {}, "outputs": [], "source": ["pca = PCA()\npca.fit(X_train)\n\ncumsum = pca.explained_variance_ratio_.cumsum() // 0.01\nn_comp = [i for i in range(1, len(cumsum)+1, 1)]\n\nprint(cumsum)\npx.bar(y=cumsum, x=n_comp, text=cumsum)"]}, {"cell_type": "markdown", "id": "4e0155c7", "metadata": {}, "source": ["According to those numbers, with 10 principal components, we can loss no more than 10% information. Let's choose ***n_components=10***."]}, {"cell_type": "code", "execution_count": 1, "id": "a77d43c0", "metadata": {}, "outputs": [], "source": ["pipe = Pipeline([\n                ('PCA', PCA(n_components=10)),\n                ('Linear Regression', Ridge(alpha=0, normalize=True))])\npipe.fit(X_train, y_train)\npipe.score(X_test, y_test)"]}, {"cell_type": "markdown", "id": "14464e5b", "metadata": {}, "source": ["Oh, dimensionality reduction means lossing in information as well as lossing in our R2 score."]}, {"cell_type": "markdown", "id": "7110041c", "metadata": {}, "source": ["## D - LR with PCA and Cross Validation"]}, {"cell_type": "code", "execution_count": 1, "id": "6da4f030", "metadata": {}, "outputs": [], "source": ["# D\nstep = [( 'PCA'     , PCA()   ),\n        ( 'Lin_Reg' , RidgeCV(alphas=[0], cv=7) )]\n\nD = Pipeline(step)\nD.fit(X_train, y_train)\nscore = D.score(X_test, y_test)\nprint(\"D's score: %2.4f\" % score)"]}, {"cell_type": "markdown", "id": "19225727", "metadata": {}, "source": ["### Tuning D"]}, {"cell_type": "markdown", "id": "ed2d5650", "metadata": {}, "source": ["We have 3 parameters to tune:\n* Number of Principle components in PCA,\n* How strong the regularization.\n* Should we normalize the data?"]}, {"cell_type": "code", "execution_count": 1, "id": "214e33ac", "metadata": {}, "outputs": [], "source": ["step = [( 'PCA'     , PCA()   ),\n        ( 'Lin_Reg' , Ridge() )]\npipe = Pipeline(step)\n\nparams = {'PCA__n_components' : range(1,24),\n          'Lin_Reg__alpha'    : [100, 30, 21, 20, 19.5, 19, 18.5, 18, 17, 17.5, 16, 15, 14, 13.5, 13, 12.5, 12, 11, 10.5, 10, 9.5, 9, 8.5, 8, 7.7, 7.6, 7.5, 7.4, 7.3, 7, 6, 5, 4.5, 4, 3.5, 3, 1, 0.3, 0.1, 0.03, 0.01, 0],\n          'Lin_Reg__normalize': [True, False]}\n\ngsc = GridSearchCV(pipe, param_grid=params, cv=7)\ngsc.fit(X_train, y_train)\n\nbest = gsc.best_params_\nscore = gsc.score(X_test, y_test)\nprint('With : ', best)\nprint('Score: %2.4f' % score)"]}, {"cell_type": "code", "execution_count": 1, "id": "761e1e8f", "metadata": {}, "outputs": [], "source": ["D = gsc.best_estimator_\nD.fit(X_train, y_train)\nprint(\"B's score: %2.4f\" % D.score(X_test, y_test))\n\nvisualizer = ResidualsPlot(D)\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show() "]}, {"cell_type": "markdown", "id": "3115305c", "metadata": {}, "source": ["With tons of tuning, the model advance only 0.0001 R2 score. It is not worth the effort."]}, {"cell_type": "markdown", "id": "94fa4e0e", "metadata": {}, "source": ["# 11. Conclusion.\n\n1. Cleaning data is very challenging. It took me dozens of hours and bunchs of effor to get data in shape. I doubt when people say most of time in data science, you will deal with data cleaning. Now I sadly know it's true.\n2. In searching for Normality of X, I was unable to log transform X because of negative values. These negatives came from standard scaling performed beforehand. I am thinking that if I first perform log-transformation, then scale the data later, so I could get benefits from both process with nicer distributions of X features.\n3. Tuning did not make sense as much as I hoped. The baseline of very simple linear regression was 0.69 in R2. Then I did not get any improvement after all the tuning. There are some reasonable explains:\n    * I perfectly cleaned the data. Oh, I hope so.\n    * Ridge - a linear regression with L2 regularization was too simple for predictions.\n    * Is there a better transform than a log, like a square root?"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}