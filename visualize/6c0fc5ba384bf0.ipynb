{"cells": [{"cell_type": "code", "execution_count": 1, "id": "98e8ebb4", "metadata": {}, "outputs": [], "source": ["import pandas\nimport numpy\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import RidgeClassifier\n"]}, {"cell_type": "markdown", "id": "b71556c7", "metadata": {}, "source": ["![image.png](attachment:c43ade77-1681-4dc2-8323-0b3057c74722.png)"]}, {"cell_type": "markdown", "id": "2d55028d", "metadata": {}, "source": ["\n<h2> Stacking ! We collect all the train and test predicted files <h2>"]}, {"cell_type": "markdown", "id": "4c0de924", "metadata": {}, "source": ["<h4> Inspired by Craig Thomas Notebook. With my \"weak\" laptop, it is easier to tune, and train the first layer of algo with separate notebook (during the night). then I can upload the output files for the second layer (stacking). The more files you get the more improve the final score, but it is getting a bit boring and score should not be the main goal but learning :-) <h4>"]}, {"cell_type": "code", "execution_count": 1, "id": "c8b19761", "metadata": {}, "outputs": [], "source": ["#each file is the output of a tuned model \n\ntrain1=pandas.read_csv('../input/mars-competition/xgb1_train_preds.csv',sep=';') # XGBM\ntrain2=pandas.read_csv('../input/mars-competition/xgb2_train_preds.csv',sep=';') # XGBM\ntrain3=pandas.read_csv('../input/mars-competition/lgb1_train_preds.csv',sep=';') # LGBM\ntrain4=pandas.read_csv('../input/mars-competition/lgb2_train_preds.csv',sep=';') # LGBM\ntrain5=pandas.read_csv('../input/mars-competition/cat_train_preds.csv',sep=';') # Catboost\ntrain6=pandas.read_csv('../input/mars-competition/hist_train_preds.csv',sep=';') # HistGradientBoostingClassifier\ntrain7=pandas.read_csv('../input/mars-competition/NHG_train_preds.csv',sep=';') # GradientBoostingClassifier\n\n# We build a final train file \n\nl1_train=pandas.DataFrame()\nl1_train['xgb1']=train1['0']\nl1_train['xgb2']=train2['0']\nl1_train['lgb1']=train3['0']\nl1_train['lgb2']=train4['0']\nl1_train['cat']=train5['0']\nl1_train['hist']=train6['0']\nl1_train['NHG']=train7['0']\n\n\n# We build a final train file to fit. \n\ntest1=pandas.read_csv('../input/mars-competition/xgb1_test_preds.csv',sep=';')\ntest2=pandas.read_csv('../input/mars-competition/xgb2_test_preds.csv',sep=';')\ntest3=pandas.read_csv('../input/mars-competition/lgb1_test_preds.csv',sep=';')\ntest4=pandas.read_csv('../input/mars-competition/lgb2_test_preds.csv',sep=';')\ntest5=pandas.read_csv('../input/mars-competition/cat_test_preds.csv',sep=';')\ntest6=pandas.read_csv('../input/mars-competition/hist_test_preds.csv',sep=';')\ntest7=pandas.read_csv('../input/mars-competition/NHG_test_preds.csv',sep=';')\n\n# We build a validation file from the test prediction of each model.\n\nl1_test=pandas.DataFrame()\nl1_test['xgb1']=test1['0']\nl1_test['xgb2']=test2['0']\nl1_test['lgb1']=test3['0']\nl1_test['lgb2']=test4['0']\nl1_test['cat']=test5['0']\nl1_test['hist']=test6['0']\nl1_test['NHG']=test7['0']\n\n\ntarget=pandas.read_csv('../input/marscompetition/1y.csv')\ndisplay(l1_train.dtypes,l1_train.head(3))\ndisplay(l1_test.dtypes,l1_test.head(3))\n\ntrain_preds = numpy.zeros(len(l1_train.index), )\ntest_preds = numpy.zeros(len(l1_test.index), )\n"]}, {"cell_type": "markdown", "id": "0d20676d", "metadata": {}, "source": ["<h2> META MODEL <h2>"]}, {"cell_type": "code", "execution_count": 1, "id": "9ce6db0c", "metadata": {}, "outputs": [], "source": ["random_state = 2021\nn_folds = 15\nk_fold = StratifiedKFold(n_splits=n_folds, random_state=random_state, shuffle=True)\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(l1_train, target)):\n    print(\"--> Fold {}\".format(fold + 1))\n   \n    x_train = pandas.DataFrame(l1_train.iloc[train_index])\n    y_train = target.iloc[train_index]\n    \n    x_valid = pandas.DataFrame(l1_train.iloc[test_index])\n    y_valid = target.iloc[test_index]\n    \n    model = CalibratedClassifierCV(RidgeClassifier(random_state=random_state),cv=3)\n    model.fit(x_train,y_train)\n\n    train_oof_preds = model.predict_proba(x_valid)[:,-1]\n    test_oof_preds = model.predict_proba(l1_test)[:,-1] # test prediction for the fold\n    \n    train_preds[test_index] = train_oof_preds # We need it to measure the AUC score of the fold\n    test_preds += test_oof_preds / n_folds    # We get test prediction for the current fold \n                                              # (For the final test file we need /n_folds X n_folds = 1)\n    \n    print(\": ROC AUC Score = {}\".format(roc_auc_score(y_valid, train_oof_preds, average=\"micro\")))\n    print(\"\")\n    \nprint(\"--> Overall metrics\")\nprint(\": ROC AUC Score = {}\".format(roc_auc_score(target, train_preds, average=\"micro\")))"]}, {"cell_type": "code", "execution_count": 1, "id": "afbe707f", "metadata": {}, "outputs": [], "source": ["test_preds[:5]"]}, {"cell_type": "code", "execution_count": 1, "id": "8e5314e5", "metadata": {}, "outputs": [], "source": ["submission = pandas.read_csv(\"../input/tabular-playground-series-mar-2021/sample_submission.csv\")"]}, {"cell_type": "code", "execution_count": 1, "id": "4618b3eb", "metadata": {}, "outputs": [], "source": ["submission[\"target\"] = test_preds"]}, {"cell_type": "code", "execution_count": 1, "id": "f9789c9b", "metadata": {}, "outputs": [], "source": ["submission.to_csv(\"./submission25.csv\",index=False)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}