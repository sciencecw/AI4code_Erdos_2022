{"cells": [{"cell_type": "markdown", "id": "67bb7a23", "metadata": {}, "source": ["<a class=\"anchor\" id=\"0\"></a>\n# [NLP : Reports & News Classification](https://www.kaggle.com/vbmokin/nlp-reports-news-classification)\n## Automatic Environmental Reports & News Classification (Ukranian)\n\n### Thanks to [@vbmokin](https://www.kaggle.com/vbmokin)"]}, {"cell_type": "markdown", "id": "704cdb5c", "metadata": {}, "source": ["# Acknowledgements\n\nThis notebook uses such good notebooks: \n* BERT model from the paper with notebook [A Visual Guide to Using BERT for the First Time](http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)\n* EDA from the notebook [NLP - EDA, Bag of Words, TF IDF, GloVe, BERT](https://www.kaggle.com/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert)\n* Classification model from the notebook [Heart Disease - Automatic AdvEDA & FE & 20 models](https://www.kaggle.com/vbmokin/heart-disease-automatic-adveda-fe-20-models)\n* Similar notebook [NLP for EN : BERT Classification for Water Report](https://www.kaggle.com/vbmokin/nlp-for-en-bert-classification-for-water-report)\n* Source [NLP for UA : BERT Classification for Water Report](https://www.kaggle.com/vbmokin/nlp-for-ua-bert-classification-for-water-report)\n\nDataset [NLP : Reports & News Classification](https://www.kaggle.com/vbmokin/nlp-reports-news-classification)\n\nSource of models:\nhttps://huggingface.co/transformers/pretrained_models.html"]}, {"cell_type": "markdown", "id": "2a2225c9", "metadata": {}, "source": ["<a class=\"anchor\" id=\"0.1\"></a>\n## Table of Contents\n\n1. [Import libraries](#1)\n1. [Download data](#2)\n1. [BERT: Data Prepairing and Modeling](#3)\n1. [Text Classification and Prediction](#4)"]}, {"cell_type": "markdown", "id": "eac5fcd7", "metadata": {}, "source": ["## 1. Import libraries <a class=\"anchor\" id=\"1\"></a>\n\n[Back to Table of Contents](#0.1)"]}, {"cell_type": "code", "execution_count": 1, "id": "ed29fd55", "metadata": {}, "outputs": [], "source": ["#!pip install transformers"]}, {"cell_type": "code", "execution_count": 1, "id": "d3ccf679", "metadata": {}, "outputs": [], "source": ["import numpy as np\nimport pandas as pd\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\n\nimport torch\nimport transformers as ppb\n\nimport warnings\nwarnings.filterwarnings('ignore')"]}, {"cell_type": "markdown", "id": "64edaec0", "metadata": {}, "source": ["## 2. Download data <a class=\"anchor\" id=\"2\"></a>\n\n[Back to Table of Contents](#0.1)"]}, {"cell_type": "code", "execution_count": 1, "id": "d014bd57", "metadata": {}, "outputs": [], "source": ["df = pd.read_csv('../input/nlp-reports-news-classification/water_problem_nlp_ua_for_Kaggle_100.csv', delimiter=';', \n                 header=0, encoding='cp1251')\ndf = df.fillna(0)\n\nconvert_dict = {'text': str, \n                'env_problems': int,\n                'pollution': int, \n                'treatment': int,\n                'climate': int,\n                'biomonitoring': int} \n  \ndf = df.astype(convert_dict)\ndf"]}, {"cell_type": "code", "execution_count": 1, "id": "620c9a5b", "metadata": {}, "outputs": [], "source": ["df.info()"]}, {"cell_type": "markdown", "id": "534f5829", "metadata": {}, "source": ["## 3. BERT: Data Prepairing and Modeling <a class=\"anchor\" id=\"3\"></a>\n\n[Back to Table of Contents](#0.1)"]}, {"cell_type": "code", "execution_count": 1, "id": "49619add", "metadata": {}, "outputs": [], "source": ["# For pre-trained DistilBERT:\nmodel_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-multilingual-cased')\n\n# Other models: https://huggingface.co/transformers/pretrained_models.html\n\n# Load pretrained model/tokenizer\ntokenizer = tokenizer_class.from_pretrained(pretrained_weights)\nmodel = model_class.from_pretrained(pretrained_weights)"]}, {"cell_type": "code", "execution_count": 1, "id": "0b5e6629", "metadata": {}, "outputs": [], "source": ["# Tokenization the sentences - break them up into word and subwords in the format BERT is comfortable with\ntokenized = df['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n\nmax_len = 0\nfor i in tokenized.values:\n    if len(i) > max_len:\n        max_len = len(i)\n\npadded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\nnp.array(padded).shape"]}, {"cell_type": "code", "execution_count": 1, "id": "51d006ea", "metadata": {}, "outputs": [], "source": ["# Creation variable to ignore (mask) the data padding\nattention_mask = np.where(padded != 0, 1, 0)\nprint(attention_mask.shape)\nattention_mask"]}, {"cell_type": "code", "execution_count": 1, "id": "c89e5921", "metadata": {}, "outputs": [], "source": ["# Modeling\ninput_ids = torch.tensor(padded).to(torch.int64)\nattention_mask = torch.tensor(attention_mask).to(torch.int64)\n\nwith torch.no_grad():\n    last_hidden_states = model(input_ids, attention_mask=attention_mask)"]}, {"cell_type": "code", "execution_count": 1, "id": "f62caa03", "metadata": {}, "outputs": [], "source": ["# Last hidden states\nfeatures = last_hidden_states[0][:,0,:].numpy()"]}, {"cell_type": "markdown", "id": "56228d36", "metadata": {}, "source": ["## 4. Text Classification and Prediction <a class=\"anchor\" id=\"4\"></a>\n\n[Back to Table of Contents](#0.1)"]}, {"cell_type": "code", "execution_count": 1, "id": "320ee05c", "metadata": {}, "outputs": [], "source": ["def target_prediction(df, features, target, test_size=0.2):\n    # Text classification model and prediction for given feature \"target\" (with labels) in df\n    \n    # Target\n    labels = df[target]\n    \n    # EDA\n    print()\n    # Extracting the number of examples of each class\n    Relevant_len = df[df[target] == 1].shape[0]\n    Not_len = df[df[target] == 0].shape[0]\n    # Draw bar plot\n    plt.rcParams['figure.figsize'] = (7, 5)\n    plt.bar(10, Relevant_len, 3, label=\"Relevant\", color='green')\n    plt.bar(15, Not_len, 3, label=\"Not\", color='red')\n    plt.legend(loc='upper center')\n    plt.ylabel('Number of examples')\n    plt.title('Propertion of examples for ' + target)\n    plt.show()\n    \n    # Train, test split \n    train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=test_size)\n    \n    # Train a simple model\n    print(f'Classification for {col}:')\n    parameters = {'C': np.linspace(0.0001, 100, 50)}\n    model = GridSearchCV(LogisticRegression(), parameters)\n    model.fit(train_features, train_labels)\n\n    print('best parameters: ', model.best_params_)\n    print('best scores: ', model.best_score_)\n    \n    # Test prediction\n    test_pred = model.predict(test_features)\n    print('Score of the test prediction -', accuracy_score(test_labels, test_pred),'\\n\\n')"]}, {"cell_type": "code", "execution_count": 1, "id": "5ab4e1bf", "metadata": {}, "outputs": [], "source": ["# List of the target features in df\ncols = df.columns.tolist()[1:]\nprint('Target columns:', cols)"]}, {"cell_type": "code", "execution_count": 1, "id": "300fcb13", "metadata": {}, "outputs": [], "source": ["%%time\n# Solving NLP Classification tasks\nprint('Solving NLP Classification tasks')\nfor col in cols:\n    target_prediction(df, features, col, test_size=0.1)"]}, {"cell_type": "markdown", "id": "34fdc82e", "metadata": {}, "source": ["I hope you find this kernel useful and enjoyable."]}, {"cell_type": "markdown", "id": "96217c3f", "metadata": {}, "source": ["Your comments and feedback are most welcome."]}, {"cell_type": "markdown", "id": "8dc758dd", "metadata": {}, "source": ["[Go to Top](#0)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}