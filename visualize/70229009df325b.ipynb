{"cells": [{"cell_type": "code", "execution_count": 1, "id": "61244162", "metadata": {}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]}, {"cell_type": "markdown", "id": "2ad6babe", "metadata": {}, "source": ["## Loading required packages"]}, {"cell_type": "code", "execution_count": 1, "id": "47960d78", "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics \nfrom sklearn import preprocessing\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nimport cv2\nfrom keras.preprocessing.image import img_to_array\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical\nfrom keras.layers import Dense,GlobalAveragePooling2D\nfrom keras.applications import MobileNet\nfrom keras.applications.mobilenet import preprocess_input\nfrom keras.optimizers import Adam"]}, {"cell_type": "markdown", "id": "ec09af3e", "metadata": {}, "source": ["## Loading training data"]}, {"cell_type": "code", "execution_count": 1, "id": "5cfac37e", "metadata": {}, "outputs": [], "source": ["df = pd.read_csv('../input/fast-furious-and-insured/Fast_Furious_Insured/train.csv')\ndf.head()"]}, {"cell_type": "markdown", "id": "880a19de", "metadata": {}, "source": ["## Loading testing data"]}, {"cell_type": "code", "execution_count": 1, "id": "4e05365c", "metadata": {}, "outputs": [], "source": ["df_test = pd.read_csv('../input/fast-furious-and-insured/Fast_Furious_Insured/test.csv')\ndf_test"]}, {"cell_type": "markdown", "id": "9889f9c7", "metadata": {}, "source": ["## Pre-processing"]}, {"cell_type": "code", "execution_count": 1, "id": "ce318dd2", "metadata": {}, "outputs": [], "source": ["# Get the number of missing data points per column\nmissing_values_count_train = df.isnull().sum()\nprint(missing_values_count_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "967e06a0", "metadata": {}, "outputs": [], "source": ["# Get the number of missing data points per column\nmissing_values_count_test = df_test.isnull().sum()\nprint(missing_values_count_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "8dcb64d7", "metadata": {}, "outputs": [], "source": ["# Filling missing values\ndf = df.fillna(method='bfill', axis=0).fillna(0)"]}, {"cell_type": "code", "execution_count": 1, "id": "71fdd454", "metadata": {}, "outputs": [], "source": ["# Checking different values in Insurance company in the training set\ndf['Insurance_company'].value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "8f9f8f35", "metadata": {}, "outputs": [], "source": ["# Checking different values in Insurance company in the testing set\ndf_test['Insurance_company'].value_counts()"]}, {"cell_type": "markdown", "id": "39cf197a", "metadata": {}, "source": ["# Label encoding and scaling"]}, {"cell_type": "code", "execution_count": 1, "id": "204588a4", "metadata": {}, "outputs": [], "source": ["features_num = ['Cost_of_vehicle', 'Min_coverage', 'Max_coverage']\nfeatures_cat = ['Insurance_company']\n\nle= LabelEncoder()   \ndf['Insurance_company'] = le.fit_transform(df['Insurance_company'])\ndf_test['Insurance_company'] = le.transform(df_test['Insurance_company'])\n\npreprocessor = make_column_transformer(\n    (StandardScaler(), features_num),\n)\n\ny = df['Amount']\ntrain_imputed = df.loc[:,['Cost_of_vehicle', 'Min_coverage', 'Max_coverage', 'Insurance_company']]\nX = preprocessor.fit_transform(train_imputed)\n\ntest_imputed = df_test.loc[:,['Cost_of_vehicle', 'Min_coverage',  'Max_coverage', 'Insurance_company']]\ntest_X = preprocessor.transform(test_imputed)\n\ntrain_imputed.columns"]}, {"cell_type": "code", "execution_count": 1, "id": "a19fc2c0", "metadata": {}, "outputs": [], "source": ["#Train-test split\ntrain_X, val_X, train_y, val_y = train_test_split(X,y,random_state=1,test_size=0.2)"]}, {"cell_type": "markdown", "id": "b4dd27b8", "metadata": {}, "source": ["## Train a random forest regressor"]}, {"cell_type": "code", "execution_count": 1, "id": "c9b56d93", "metadata": {}, "outputs": [], "source": ["rf_model = RandomForestRegressor(random_state=1, n_estimators = 1000, max_depth=3)\n# fit your model\nrf_model.fit(train_X,train_y)\nval_preds = rf_model.predict(val_X)\n# Calculate the mean absolute error of your Random Forest model on the validation data\nrf_val_mae = mean_absolute_error(val_y,val_preds)\n\nprint(\"Validation MAE for Random Forest Model: {}\".format(rf_val_mae))"]}, {"cell_type": "markdown", "id": "5cf3630e", "metadata": {}, "source": ["## Get the predictions for amount"]}, {"cell_type": "code", "execution_count": 1, "id": "4fb8a2d7", "metadata": {}, "outputs": [], "source": ["amount_predictions = rf_model.predict(test_X)"]}, {"cell_type": "markdown", "id": "fb99c032", "metadata": {}, "source": ["## Prepare the images"]}, {"cell_type": "code", "execution_count": 1, "id": "05965ad8", "metadata": {}, "outputs": [], "source": ["X = df.loc[:,['Image_path']]\ny = df.loc[:,['Condition']]    \nX_test = df.loc[:,['Image_path']]\nprint('train set shape:', X.shape)\nprint('test set shape:', X_test.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "eac3979d", "metadata": {}, "outputs": [], "source": ["data = []\nlabels = []\nfor (index_label, row_series) in df.iterrows():\n        img_path = row_series.values[0]\n        condition = row_series.values[-2]\n        labels.append(int(condition))\n        # load the image, pre-process it, and store it in the data list\n        originalImage = cv2.imread('/kaggle/input/fast-furious-and-insured/Fast_Furious_Insured/trainImages/' + img_path)\n        image = cv2.resize(originalImage, (224, 224))\n        image = img_to_array(image)\n        data.append(image)"]}, {"cell_type": "markdown", "id": "b95b183b", "metadata": {}, "source": ["## Transfer learning with MobileNet"]}, {"cell_type": "code", "execution_count": 1, "id": "97538f04", "metadata": {}, "outputs": [], "source": ["base_model=MobileNet(weights='imagenet',include_top=False) #imports the mobilenet model and discards the last 1000 neuron layer.\nx=base_model.output\nx=GlobalAveragePooling2D()(x)\nx=Dense(256,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.\npreds=Dense(2,activation='softmax')(x) #final layer with softmax activation\nmodel=Model(inputs=base_model.input,outputs=preds)\n   # we want to set the first 20 layers of the network to be non-trainable\nfor layer in model.layers[:80]:\n    layer.trainable=False\nfor layer in model.layers[80:]:\n    layer.trainable=True"]}, {"cell_type": "code", "execution_count": 1, "id": "44102215", "metadata": {}, "outputs": [], "source": ["from tensorflow.keras import optimizers\n\nprint(len(data),len(labels))\ndata = np.array(data, dtype=\"float\")\nlabels = np.array(labels)\n    \n# partition the data into training and testing splits using 80% of\n# the data for training and the remaining 20% for testing\n(train_images, test_images, train_labels, test_labels) = train_test_split(data,labels, test_size=0.2, random_state=42)\n\n#(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n# Normalize pixel values to be between 0 and 1\ntrain_images, test_images = (train_images / 255.0)-0.5, (test_images / 255.0) -0.5\n\ntrain_labels = to_categorical(train_labels, 2)\ntest_labels = to_categorical(test_labels, 2)\n\n#compile and train the model\nadam=optimizers.Adam(\n                lr=0.002,\n                beta_1=0.9,\n                beta_2=0.999,\n                epsilon=None,\n                decay=0.0001,\n                amsgrad=False\n                )\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n#callback = callbacks.LearningRateScheduler(scheduler)\nhistory = model.fit(train_images, train_labels, batch_size=32,epochs=5,shuffle=True, validation_data=(test_images, test_labels))"]}, {"cell_type": "markdown", "id": "df54b295", "metadata": {}, "source": ["## Getting the test prediction"]}, {"cell_type": "code", "execution_count": 1, "id": "f95b71fd", "metadata": {}, "outputs": [], "source": ["condition_predictions = []\nfor (index_label, row_series) in df_test.iterrows():\n        img_path = row_series.values[0]\n        # load the image, pre-process it, and store it in the data list\n        originalImage = cv2.imread('/kaggle/input/fast-furious-and-insured/Fast_Furious_Insured/testImages/' + img_path)\n        image = cv2.resize(originalImage, (224, 224))\n        image = img_to_array(image)\n        image = image.reshape((1,224, 224, 3))\n        image = np.array(image, dtype=\"float\") / 255.0 - 0.5\n        prediction = model.predict(image)\n        prediction = prediction[0]\n        condition_predictions.append(np.argmax(prediction))\n       "]}, {"cell_type": "markdown", "id": "6ebe6312", "metadata": {}, "source": ["## Preparing the submission"]}, {"cell_type": "code", "execution_count": 1, "id": "54b7b5fc", "metadata": {}, "outputs": [], "source": ["submission = pd.DataFrame({'Image_path': df_test.Image_path, 'Condition': condition_predictions, \n                          'Amount': amount_predictions})\nsubmission.to_csv('submission.csv', index=False)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}