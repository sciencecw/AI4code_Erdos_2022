{"cells": [{"cell_type": "markdown", "id": "71f29b56", "metadata": {}, "source": ["# Changelog\n\n### Version 3\n\n* Convert series to dataframe before save to parquet format\n\n### Version 1\n\n* Initialize code"]}, {"cell_type": "markdown", "id": "5413606f", "metadata": {}, "source": ["# Library"]}, {"cell_type": "code", "execution_count": 1, "id": "da924c24", "metadata": {}, "outputs": [], "source": ["!pip install pyenchant pysastrawi"]}, {"cell_type": "code", "execution_count": 1, "id": "027db2c6", "metadata": {}, "outputs": [], "source": ["!wget http://archive.ubuntu.com/ubuntu/pool/main/libr/libreoffice-dictionaries/hunspell-id_6.4.3-1_all.deb\n!dpkg -i hunspell-id_6.4.3-1_all.deb"]}, {"cell_type": "code", "execution_count": 1, "id": "e77f0a93", "metadata": {}, "outputs": [], "source": ["!apt update && apt install -y enchant libenchant1c2a hunspell hunspell-en-us libhunspell-1.6-0"]}, {"cell_type": "code", "execution_count": 1, "id": "4c0f95fc", "metadata": {}, "outputs": [], "source": ["import re\nimport os\nimport gc\nimport random\n\nimport numpy as np\nimport pandas as pd\nimport sklearn\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nimport enchant"]}, {"cell_type": "code", "execution_count": 1, "id": "6394a79a", "metadata": {}, "outputs": [], "source": ["!pip freeze > requirements.txt"]}, {"cell_type": "code", "execution_count": 1, "id": "adacaa19", "metadata": {}, "outputs": [], "source": ["print('Numpy version:', np.__version__)\nprint('Pandas version:', pd.__version__)\nprint('Scikit-Learn version:', sklearn.__version__)\nprint('Matplotlib version:', matplotlib.__version__)\nprint('Seaborn version:', sns.__version__)\nprint('NLTK version:', nltk.__version__)"]}, {"cell_type": "code", "execution_count": 1, "id": "8fc91adb", "metadata": {}, "outputs": [], "source": ["SEED = 42\n\nos.environ['PYTHONHASHSEED']=str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)"]}, {"cell_type": "code", "execution_count": 1, "id": "e1b175b5", "metadata": {}, "outputs": [], "source": ["nltk.download('wordnet')"]}, {"cell_type": "markdown", "id": "4cee8ae2", "metadata": {}, "source": ["# Dataset"]}, {"cell_type": "code", "execution_count": 1, "id": "95f396c8", "metadata": {}, "outputs": [], "source": ["!ls -lha /kaggle/input\n!ls -lha /kaggle/input/student-shopee-code-league-sentiment-analysis"]}, {"cell_type": "code", "execution_count": 1, "id": "f70325fd", "metadata": {}, "outputs": [], "source": ["df_train = pd.read_csv('/kaggle/input/student-shopee-code-league-sentiment-analysis/train.csv')\ndf_train.sample(10)"]}, {"cell_type": "code", "execution_count": 1, "id": "89f0366c", "metadata": {}, "outputs": [], "source": ["df_train2 = pd.read_csv('/kaggle/input/shopee-reviews/shopee_reviews.csv')\n\ndef to_int(r):\n    try:\n        return np.int32(r)\n    except:\n        return np.nan\n\ndf_train2['label'] = df_train2['label'].apply(to_int)\ndf_train2 = df_train2.dropna()\ndf_train2['label'] = df_train2['label'].astype(np.int32)\ndf_train2"]}, {"cell_type": "code", "execution_count": 1, "id": "3ff3c867", "metadata": {}, "outputs": [], "source": ["df_test = pd.read_csv('/kaggle/input/student-shopee-code-league-sentiment-analysis/test.csv')\ndf_test.sample(10)"]}, {"cell_type": "code", "execution_count": 1, "id": "d10482c6", "metadata": {}, "outputs": [], "source": ["X_train = pd.concat([df_train['review'], df_train2['text']], axis=0)\nX_train = X_train.reset_index(drop=True)\ny_train = pd.concat([df_train['rating'], df_train2['label']], axis=0)\ny_train = y_train.reset_index(drop=True)\n\nX_test = df_test['review']"]}, {"cell_type": "markdown", "id": "4095043a", "metadata": {}, "source": ["# Class weight"]}, {"cell_type": "code", "execution_count": 1, "id": "44b27e27", "metadata": {}, "outputs": [], "source": ["rating_count = y_train.value_counts().sort_index().to_list()\ntotal_rating = sum(rating_count)\nlowest_rating_count = min(rating_count)\nrating_weight = [lowest_rating_count/rc for rc in rating_count]\n\nprint(rating_count)\nprint(total_rating)\nprint(rating_weight)"]}, {"cell_type": "code", "execution_count": 1, "id": "def413b9", "metadata": {}, "outputs": [], "source": ["class_weight = np.empty((total_rating,))\nfor i in range(total_rating):\n    class_weight[i] = rating_weight[y_train[i] - 1]"]}, {"cell_type": "markdown", "id": "a8740de6", "metadata": {}, "source": ["# Preprocess"]}, {"cell_type": "code", "execution_count": 1, "id": "86581244", "metadata": {}, "outputs": [], "source": ["from nltk.stem import WordNetLemmatizer\nfrom Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n\nlemmatizer = WordNetLemmatizer() # for en\nfactory = StemmerFactory() # for id\nstemmer = factory.create_stemmer() # for id\n\ntweet_tokenizer = nltk.tokenize.TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n\neng_dict = enchant.Dict('en')\nind_dict = enchant.Dict('id_ID')\n\ndef remove_char(text):\n    text = re.sub(r'[^a-z ]', ' ', text)\n    return text\n\n\ndef stem_lemma(tokens):\n    new_token = []\n    for token in tokens:\n        if eng_dict.check(token):\n            new_token.append(lemmatizer.lemmatize(token))\n        elif ind_dict.check(token):\n            new_token.append(stemmer.stem(token))\n        else:\n            new_token.append(token)\n    return new_token\n\ndef upper_or_lower(tokens):\n    new_token = []\n    for token in tokens:\n        total_lower = len(re.findall(r'[a-z]',token))\n        total_upper = len(re.findall(r'[A-Z]',token))\n        if total_lower == 0 or total_upper == 0:\n            new_token.append(token)\n        elif total_lower > total_upper:\n            new_token.append(token.lower())\n        else:\n            new_token.append(token.upper())\n    return new_token\n    \n\ndef preprocess(X):\n    X = X.apply(tweet_tokenizer.tokenize)\n    X = X.apply(lambda token: [t for t in token if t != ''])\n    X = X.apply(upper_or_lower)\n    X = X.apply(stem_lemma)\n#     X = X.apply(lambda token: ' '.join(token)) # need to join token because sklearn tf-idf only accept string, not list of string\n    \n#     X = X.apply(remove_char)\n    return X"]}, {"cell_type": "code", "execution_count": 1, "id": "89e0789b", "metadata": {}, "outputs": [], "source": ["X_train = preprocess(X_train)\nX_test = preprocess(X_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "5123249d", "metadata": {}, "outputs": [], "source": ["X_train.sample(10)"]}, {"cell_type": "markdown", "id": "a5d60603", "metadata": {}, "source": ["# Save to parquet"]}, {"cell_type": "code", "execution_count": 1, "id": "0ca252a2", "metadata": {}, "outputs": [], "source": ["X_train = pd.DataFrame({'X': X_train})\nX_train.to_parquet('X_train.parquet', engine='pyarrow')"]}, {"cell_type": "code", "execution_count": 1, "id": "1bd7591c", "metadata": {}, "outputs": [], "source": ["X_test = pd.DataFrame({'X': X_test})\nX_test.to_parquet('X_test.parquet', engine='pyarrow')"]}, {"cell_type": "code", "execution_count": 1, "id": "cd19e6ad", "metadata": {}, "outputs": [], "source": ["y_train = pd.DataFrame({'y': y_train})\ny_train.to_parquet('y_train.parquet', engine='pyarrow')"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}