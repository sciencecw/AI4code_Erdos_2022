{"cells": [{"cell_type": "markdown", "id": "bb598877", "metadata": {}, "source": ["## Load Libraries and Dataset\nLoad libraries, dataset and take a look at what we got!"]}, {"cell_type": "code", "execution_count": 1, "id": "8c9380fb", "metadata": {}, "outputs": [], "source": ["import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom pprint import pprint\n\ndf = pd.read_csv('/kaggle/input/nyc-property-sales/nyc-rolling-sales.csv')\ndf = df.drop('Unnamed: 0', 1)\ndf = df.replace(' -  ', np.nan) # empty data points are not set up properly\ndf = df.replace(' ', np.nan)\ndf"]}, {"cell_type": "markdown", "id": "345334f9", "metadata": {}, "source": ["Let's see if there any *rows* which need to be filtered. We'll look at the columns later."]}, {"cell_type": "code", "execution_count": 1, "id": "2e2618f4", "metadata": {}, "outputs": [], "source": ["print(df[df.duplicated() == True].shape)\nprint(df[df.isnull().all(axis=1)].shape) "]}, {"cell_type": "markdown", "id": "6408122b", "metadata": {}, "source": ["765 duplicate rows; let's get rid of those. Zero rows with all `NaN` entries which is good."]}, {"cell_type": "code", "execution_count": 1, "id": "396f23c3", "metadata": {}, "outputs": [], "source": ["df.drop_duplicates(inplace=True)"]}, {"cell_type": "markdown", "id": "08fc9c5a", "metadata": {}, "source": ["## Inspect dependent variable (`SALE PRICE`)"]}, {"cell_type": "code", "execution_count": 1, "id": "fde3c48a", "metadata": {}, "outputs": [], "source": ["df['SALE PRICE'] = df['SALE PRICE'].fillna(0).astype('int')\ndf['SALE PRICE'].describe()"]}, {"cell_type": "markdown", "id": "f7eb2434", "metadata": {}, "source": ["Quite a few 0 entries there. Let's visually inspect price against gross square feet:"]}, {"cell_type": "code", "execution_count": 1, "id": "197429cc", "metadata": {}, "outputs": [], "source": ["df['GROSS SQUARE FEET'] = df['GROSS SQUARE FEET'].fillna(0).astype('int')\n\nplt.rcParams['figure.figsize'] = (20, 10)\n\nfig, ax = plt.subplots()\nsns.scatterplot(x='GROSS SQUARE FEET', y='SALE PRICE', data=df)\nax.set_xlim([-10**3, 10**4.5])\nax.set_ylim([.5, 10**9])\nax.set_yscale(\"log\")\nplt.show()"]}, {"cell_type": "markdown", "id": "c9f7c91a", "metadata": {}, "source": ["We see some unrealistic prices at `10**0`, `10**1` etc. These are probably set arbitrarily and not representative for the true value of the property."]}, {"cell_type": "code", "execution_count": 1, "id": "a6f86731", "metadata": {}, "outputs": [], "source": ["df = df[df['SALE PRICE'] != 0]\ndf = df[df['SALE PRICE'] != 10**0]\ndf = df[df['SALE PRICE'] != 10**1]\ndf = df[df['SALE PRICE'] != 10**2]\ndf = df[df['SALE PRICE'] != 10**3]\ndf = df[df['SALE PRICE'] != 10**4]"]}, {"cell_type": "markdown", "id": "4a7c4de5", "metadata": {}, "source": ["How are prices distributed?"]}, {"cell_type": "code", "execution_count": 1, "id": "d600f9da", "metadata": {}, "outputs": [], "source": ["sns.distplot(df['SALE PRICE'])"]}, {"cell_type": "markdown", "id": "cd102a58", "metadata": {}, "source": ["There are some crazy outliers, let's get rid of the top x% (and then, to be fair, the bottom y%). We could instead try and set a 'reasonable' minimum/maximum but let's not forget we are looking at prices for both properties such as parking lots and for whole skyscrapers. A skewed distribution is to be expected.\n\nI define these percentages as hyperparameters to be able to tweak them once the model is in place."]}, {"cell_type": "code", "execution_count": 1, "id": "15f27b1f", "metadata": {}, "outputs": [], "source": ["HYP = {\n    'PRICE_UPPER_Q': .995,\n    'PRICE_LOWER_Q': 0.01,\n    'N_NEIGHBORS': 50,\n    'N_BUILD_CAT': 50,\n    'BUILD_LETTER': False\n}"]}, {"cell_type": "code", "execution_count": 1, "id": "1b5f26be", "metadata": {}, "outputs": [], "source": ["df = df[df['SALE PRICE'] < df['SALE PRICE'].quantile(HYP['PRICE_UPPER_Q'])]\ndf = df[df['SALE PRICE'] > df['SALE PRICE'].quantile(HYP['PRICE_LOWER_Q'])]"]}, {"cell_type": "code", "execution_count": 1, "id": "fe03eca1", "metadata": {}, "outputs": [], "source": ["sns.distplot(df['SALE PRICE'])"]}, {"cell_type": "markdown", "id": "4fcd0834", "metadata": {}, "source": ["Looking better now."]}, {"cell_type": "markdown", "id": "6e22368b", "metadata": {}, "source": ["## Feature Extraction and Engineering"]}, {"cell_type": "markdown", "id": "c6a95a63", "metadata": {}, "source": ["I will just browse through all columns and keep, drop or adjust them accordingly."]}, {"cell_type": "code", "execution_count": 1, "id": "d191b880", "metadata": {}, "outputs": [], "source": ["df.columns"]}, {"cell_type": "code", "execution_count": 1, "id": "ffca2a9f", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots()\nsns.scatterplot(x='GROSS SQUARE FEET', y='SALE PRICE', data=df, hue='BOROUGH')\nax.set_xlim([-10**3, 10**4.5])\nax.set_ylim([10**4, 10**7.5])\nax.set_yscale(\"log\")\nplt.show()"]}, {"cell_type": "markdown", "id": "d0074174", "metadata": {}, "source": ["`BOROUGH` is a no-brainer. It has five clean categories and shows nice clustering in the scatterplot. Keeper!"]}, {"cell_type": "code", "execution_count": 1, "id": "82913aae", "metadata": {}, "outputs": [], "source": ["df['BOROUGH'] = df['BOROUGH'].astype('category')"]}, {"cell_type": "markdown", "id": "38680048", "metadata": {}, "source": ["`NEIGHBORHOOD` is trickier. This looks like another clean categorical but this time with 253 unique values. Let's only consider the top `N` most used categories for now but at the same time keep this number adjustable as a hyperparameter."]}, {"cell_type": "code", "execution_count": 1, "id": "3956479c", "metadata": {}, "outputs": [], "source": ["# had to call in the help from a ninja to get this two liner to work:\n# https://stackoverflow.com/questions/58494476/pandas-category-keep-only-most-common-ones-and-replace-rest-with-nan/\ntop = df['NEIGHBORHOOD'].value_counts().head(HYP['N_NEIGHBORS']).index.tolist()\ndf.loc[~df['NEIGHBORHOOD'].isin(top), 'NEIGHBORHOOD'] = 'OTHER'\n\ndf['NEIGHBORHOOD'] = df['NEIGHBORHOOD'].astype('category')"]}, {"cell_type": "code", "execution_count": 1, "id": "c579baf0", "metadata": {}, "outputs": [], "source": ["df['NEIGHBORHOOD'].value_counts()"]}, {"cell_type": "markdown", "id": "2fe76d69", "metadata": {}, "source": ["I'm thinking of a similar tactic for `BUILDING CLASS CATEGORY`. Top `N` for now."]}, {"cell_type": "code", "execution_count": 1, "id": "28c04432", "metadata": {}, "outputs": [], "source": ["df['BUILDING CLASS CATEGORY'].value_counts()\ntop = df['BUILDING CLASS CATEGORY'].value_counts().head(HYP['N_BUILD_CAT']).index.tolist()\ndf.loc[~df['BUILDING CLASS CATEGORY'].isin(top), 'BUILDING CLASS CATEGORY'] = '00 OTHER'\ndf['BUILDING CLASS CATEGORY'] = df['BUILDING CLASS CATEGORY'].str.strip().astype('category')"]}, {"cell_type": "code", "execution_count": 1, "id": "59d4b223", "metadata": {}, "outputs": [], "source": ["df['BUILDING CLASS CATEGORY']"]}, {"cell_type": "markdown", "id": "3bf46d35", "metadata": {}, "source": ["`BUILDING CLASS AT PRESENT` can be simplified by taking only the letter and not the number. This reduces this categrorical from 140 to 24 unique entries. Judging by these [building code descriptions](https://www1.nyc.gov/assets/finance/jump/hlpbldgcode.html) the different subcategories per letter are much less important for our purpose anyway. For tweaking purposes I did add a boolean switch to the hyperparameters."]}, {"cell_type": "code", "execution_count": 1, "id": "3f6318e0", "metadata": {}, "outputs": [], "source": ["if HYP['BUILD_LETTER']:\n    df['BUILDING CLASS AT PRESENT'] = df['BUILDING CLASS AT PRESENT'].str[0].astype('category')\nelse:\n    df['BUILDING CLASS AT PRESENT'] = df['BUILDING CLASS AT PRESENT'].astype('category')"]}, {"cell_type": "markdown", "id": "24f98a7c", "metadata": {}, "source": ["`TAX CLASS AT PRESENT` looks to be concise enough to keep as is."]}, {"cell_type": "code", "execution_count": 1, "id": "46982d14", "metadata": {}, "outputs": [], "source": ["df['TAX CLASS AT PRESENT'] = df['TAX CLASS AT PRESENT'].astype('category')"]}, {"cell_type": "markdown", "id": "38ef2da9", "metadata": {}, "source": ["`BLOCK` and `LOT` seem way too specific for our predictive purposes. We could see if any properties were sold more than once? Going to leave that for now. The same goes for `ADDRESS` and `APARTMENT NUMBER`; too specific. `ZIP CODE` has too many unique values: `182`. If we would simplify it (first two or three numbers for example) we'd be recreating something close to `BOROUGH` is my guess. So not worth it. `EASE-MENT` is (as good as) empty. Drop drop drop."]}, {"cell_type": "code", "execution_count": 1, "id": "4bbf682c", "metadata": {}, "outputs": [], "source": ["df = df.drop(['BLOCK', 'LOT', 'ADDRESS', 'APARTMENT NUMBER', 'ZIP CODE', 'EASE-MENT'], 1)"]}, {"cell_type": "markdown", "id": "bccaa495", "metadata": {}, "source": ["The different `UNIT` features are another reason we see such a skewed distribution in `SALE PRICE`. The biggest property consists of `1844` units! All of these unit features are definitely important. `16313` entries have `0` units though. Not sure what to think of this now---filtering them out."]}, {"cell_type": "code", "execution_count": 1, "id": "bbb29e35", "metadata": {}, "outputs": [], "source": ["df['RESIDENTIAL UNITS'] = df['RESIDENTIAL UNITS'].astype('int')\ndf['COMMERCIAL UNITS'] = df['COMMERCIAL UNITS'].astype('int')\ndf['TOTAL UNITS'] = df['TOTAL UNITS'].astype('int')\ndf = df[df['TOTAL UNITS'] > 0]\ndf = df[df['RESIDENTIAL UNITS'] + df['COMMERCIAL UNITS'] == df['TOTAL UNITS']] # obviously these have to be equal"]}, {"cell_type": "markdown", "id": "c4882260", "metadata": {}, "source": ["We already saw that the amount of square feet correlates the strongest with the price so we keep it as a feature. Again of lot of `0` entries here though :-("]}, {"cell_type": "code", "execution_count": 1, "id": "14e43162", "metadata": {}, "outputs": [], "source": ["df['LAND SQUARE FEET'] = df['LAND SQUARE FEET'].fillna(0).astype('int')\ndf['GROSS SQUARE FEET'] = df['GROSS SQUARE FEET'].fillna(0).astype('int')\ndf = df[df['LAND SQUARE FEET'] > 0]\ndf = df[df['GROSS SQUARE FEET'] > 0]"]}, {"cell_type": "markdown", "id": "cfdbfbcd", "metadata": {}, "source": ["`YEAR BUILT` has some too low values. Let's get rid of all observations with a year built before first European contact (1524)."]}, {"cell_type": "code", "execution_count": 1, "id": "32380f26", "metadata": {}, "outputs": [], "source": ["sns.boxplot(df['YEAR BUILT'])\ndf = df[df['YEAR BUILT'] > 1524]"]}, {"cell_type": "markdown", "id": "b01a5796", "metadata": {}, "source": ["`TAX CLASS AT TIME OF SALE` has no new information compared to `TAX CLASS AT PRESENT`. The same goes for `BUILDING CLASS AT TIME OF SALE`."]}, {"cell_type": "code", "execution_count": 1, "id": "fc64c7ae", "metadata": {}, "outputs": [], "source": ["df = df.drop(['TAX CLASS AT TIME OF SALE', 'BUILDING CLASS AT TIME OF SALE'], 1)"]}, {"cell_type": "markdown", "id": "e684b890", "metadata": {}, "source": ["`SALE DATE` gives lots of options for feature engineering: month of sale, season etc. For now we leave it though."]}, {"cell_type": "code", "execution_count": 1, "id": "ead16f40", "metadata": {}, "outputs": [], "source": ["df = df.drop('SALE DATE', 1)"]}, {"cell_type": "code", "execution_count": 1, "id": "979d3f37", "metadata": {}, "outputs": [], "source": ["df.info()"]}, {"cell_type": "markdown", "id": "81d7e635", "metadata": {}, "source": ["Sweet, a concise and clean feature set! Had to drop quite some observations though; 27882 left out of 84548 (33.0%)."]}, {"cell_type": "markdown", "id": "d52edfcb", "metadata": {}, "source": ["## Fitting a Linear Regression Model"]}, {"cell_type": "code", "execution_count": 1, "id": "f630332f", "metadata": {}, "outputs": [], "source": ["from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\n# http://rasbt.github.io/mlxtend/api_subpackages/mlxtend.feature_selection/#sequentialfeatureselector\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs"]}, {"cell_type": "markdown", "id": "df728603", "metadata": {}, "source": ["First we have to prepare the categorical features to dummy variables in order to be able to feed them to the model:"]}, {"cell_type": "code", "execution_count": 1, "id": "5f4226a9", "metadata": {}, "outputs": [], "source": ["def one_hot(df, cat):\n    one_hots = pd.get_dummies(df[cat], prefix=cat)\n    del df[cat]\n    return pd.concat([df, one_hots], axis=1)\n\nfor cat in df.select_dtypes(include='category').columns:\n    df = one_hot(df, cat)\n\ndf.columns"]}, {"cell_type": "markdown", "id": "91a9b5d4", "metadata": {}, "source": ["Now before we do anything else we need to split our data into train, test and validation sets. The first is to fit the model. \n\nTo make sure we are not overfitting the model on the train set, we test it on 'unseen' data from the test set afterwards. Adjusting the hyperparameters of the preprocessing steps can then again overfit the cleaning on the test set. Only when completely done with these adjustments will we do a run on the never seen validation set to get a final performance measure."]}, {"cell_type": "code", "execution_count": 1, "id": "29cbfa53", "metadata": {}, "outputs": [], "source": ["y = df.pop('SALE PRICE')\nX = df\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.5, random_state=55)\nX_test, X_valid, y_test, y_valid = train_test_split(\n    X_test, y_test, test_size=0.4, random_state=55)\n\nprint(X_train.shape, X_test.shape, X_valid.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "a2bf8dde", "metadata": {}, "outputs": [], "source": ["lr = LinearRegression()\n\nsfs = SFS(lr,\n          k_features='parsimonious',\n          verbose=1,\n          scoring='r2',\n          cv=5,\n          n_jobs=-1)\n\nsfs = sfs.fit(X_train, y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "c24075d5", "metadata": {}, "outputs": [], "source": ["pd.DataFrame.from_dict(sfs.get_metric_dict()).T"]}, {"cell_type": "markdown", "id": "6b781165", "metadata": {}, "source": ["`scikit-learn` doesn't offer adjusted $R^2$ out of the box so manually calculating it through $\\bar{R}^2 = 1 - (1 - R^2)\\frac{n - 1}{n - p - 1}$."]}, {"cell_type": "code", "execution_count": 1, "id": "7f1f0100", "metadata": {}, "outputs": [], "source": ["X_train_sfs = sfs.transform(X_train)\nX_test_sfs = sfs.transform(X_test)\n\nlr.fit(X_train_sfs, y_train)\ny_pred = lr.predict(X_test_sfs)\n\nk = len(sfs.k_feature_names_)\nn = X_train.shape[0]\nr2 = sfs.k_score_\nadj_r2 = 1 - (1 - sfs.k_score_) * ((n - 1) / (n - k - 1))\n\npprint(HYP)\nprint(f'TRAIN R2: {r2}')\nprint(f'TRAIN ADJUSTED R2: {adj_r2}')\nprint(f'TEST R2: {r2_score(y_test, y_pred)}')\nprint(f'k: {k}')\npprint(list(sfs.k_feature_names_))"]}, {"cell_type": "markdown", "id": "8bdcd40c", "metadata": {}, "source": ["## Conclusion"]}, {"cell_type": "markdown", "id": "75e026e1", "metadata": {}, "source": ["* So first result gives an $R^2$ = .51 for both train and test set with $k$ = 16. Most interesting fact is that there is no square feet nor units features got included! Going back to the cleaning hyperparameters to see if tweaking those have any effect on these results.\n* Increasing the upper and lower price filtering quantiles decreased $R^2$ and increased $k$. Maybe not filter out upper and lower quantiles at all?\n* No! Sweet spot seems to be between 0.01 and 0.995, So filtering out some lowest values and a couple of the highest.\n* Turns out simplifying all the features through the hyperparameters is actually only good for increasing training time.\n* Better results could be obtained with more features. Potential columns could be `SALE DATE` and `ZIPCODE`. Main bottleneck becomes the processing time needed to let SFS find the best subset.\n* Within reasonable time (~3 hours) this kernel was able to search through a feature space of 257 independent variables. This resulted in a model with $R^2 = 0.58$ on the train set and 0.52 on the test set. Validated at $R^2 = 0.56$:"]}, {"cell_type": "markdown", "id": "36a0554c", "metadata": {}, "source": ["## Validation"]}, {"cell_type": "code", "execution_count": 1, "id": "fad1a04d", "metadata": {}, "outputs": [], "source": ["X_valid_sfs = sfs.transform(X_valid)\ny_pred_v = lr.predict(X_valid_sfs)\n\nprint(f'TRAIN R2:\\t {r2}')\nprint(f'TEST R2:\\t {r2_score(y_test, y_pred)}')\nprint(f'VALIDATION R2:\\t {r2_score(y_valid, y_pred_v)}')"]}, {"cell_type": "code", "execution_count": 1, "id": "7d66b76f", "metadata": {}, "outputs": [], "source": ["fig = plot_sfs(sfs.get_metric_dict(), ylabel='R^2')\nplt.ylim([0.2, 0.7])\nplt.title('Sequential Forward Selection (w. StdDev)')\nplt.grid()\nplt.show()"]}, {"cell_type": "markdown", "id": "a12b392d", "metadata": {}, "source": ["Because we are runnning SFS with `k_features='parsimonious'` the \"smallest feature subset that is within one standard error of the cross-validation performance will be selected\". [[docs]](http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/#api)\n\nThe above graph clearly shows plateauing around $k > 60$. Best model that SFS returns has indeed $k = 40$."]}, {"cell_type": "markdown", "id": "d83b7649", "metadata": {}, "source": ["## Bonus: TPOT"]}, {"cell_type": "markdown", "id": "beae3f3d", "metadata": {}, "source": ["[TPOT](https://github.com/EpistasisLab/tpot) uses genetic programming to optimise a classifier or regressor without too much manual setup. Feature selection, feature construction, model selection, parameter optimisation etc. are all handled by TPOT. We only have to give it our clean dataset!"]}, {"cell_type": "code", "execution_count": 1, "id": "c1cd2ba2", "metadata": {}, "outputs": [], "source": ["from tpot import TPOTRegressor\nfrom tpot.builtins import StackingEstimator\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.ensemble import GradientBoostingRegressor\n\ntpot = TPOTRegressor(generations=10, population_size=50, verbosity=2, scoring='r2', \n                     warm_start=True, n_jobs=-1)"]}, {"cell_type": "code", "execution_count": 1, "id": "f8351202", "metadata": {}, "outputs": [], "source": ["# not turning the teapot on again!\n#tpot.fit(X_train, y_train)\n#print(tpot.score(X_test, y_test))\n#tpot.export('tpot_pipeline.py')\n\n# yields:\ntpot_model = GradientBoostingRegressor(alpha=0.95, learning_rate=0.1, loss=\"huber\", \n                                       max_depth=8, max_features=0.7, \n                                       min_samples_leaf=2, min_samples_split=19, \n                                       n_estimators=100, subsample=0.6)"]}, {"cell_type": "markdown", "id": "066aeddb", "metadata": {}, "source": ["Took this Kaggle kernel 19006.03 seconds on a dedicated CPU thread to come up with the model above.\n\nCup of tea anyone?!"]}, {"cell_type": "code", "execution_count": 1, "id": "42bad038", "metadata": {}, "outputs": [], "source": ["print(f'TRAIN R2:\\t 0.7094491358671142')\ntpot_model.fit(X_train, y_train)\ntpot_y_pred = tpot_model.predict(X_test)\nprint(f'TEST R2:\\t {r2_score(y_test, tpot_y_pred)}')\ntpot_y_pred_v = tpot_model.predict(X_valid)\nprint(f'VALIDATION R2:\\t {r2_score(y_valid, tpot_y_pred_v)}')"]}, {"cell_type": "code", "execution_count": 1, "id": "5e166024", "metadata": {}, "outputs": [], "source": ["# results log\n\n#first attempt\n{'BUILD_LETTER': True,\n 'N_BUILD_CAT': 10,\n 'N_NEIGHBORS': 10,\n 'PRICE_LOWER_Q': 0.01,\n 'PRICE_UPPER_Q': 0.99}\nTRAIN R2: 0.509000260340717\nTEST R2: 0.5066320072800943\nk: 16\n['BOROUGH_1',\n 'BOROUGH_3',\n 'BOROUGH_4',\n 'NEIGHBORHOOD_OTHER',\n 'NEIGHBORHOOD_UPPER EAST SIDE (59-79)',\n 'NEIGHBORHOOD_UPPER EAST SIDE (79-96)',\n 'NEIGHBORHOOD_UPPER WEST SIDE (79-96)',\n 'BUILDING CLASS CATEGORY_00 OTHER',\n 'BUILDING CLASS CATEGORY_03 THREE FAMILY DWELLINGS',\n 'BUILDING CLASS CATEGORY_10 COOPS - ELEVATOR APARTMENTS',\n 'TAX CLASS AT PRESENT_1',\n 'TAX CLASS AT PRESENT_2',\n 'TAX CLASS AT PRESENT_2A',\n 'BUILDING CLASS AT PRESENT_D',\n 'BUILDING CLASS AT PRESENT_F',\n 'BUILDING CLASS AT PRESENT_S']\n\n# second\n{'BUILD_LETTER': True,\n 'N_BUILD_CAT': 10,\n 'N_NEIGHBORS': 10,\n 'PRICE_LOWER_Q': 0.05,\n 'PRICE_UPPER_Q': 0.95}\nTRAIN R2: 0.3558789829142702\nTEST R2: 0.35023793944264014\nk: 22\n['BOROUGH_1',\n 'BOROUGH_3',\n 'BOROUGH_4',\n 'BOROUGH_5',\n 'NEIGHBORHOOD_BEDFORD STUYVESANT',\n 'NEIGHBORHOOD_EAST NEW YORK',\n 'NEIGHBORHOOD_JACKSON HEIGHTS',\n 'NEIGHBORHOOD_MIDTOWN EAST',\n 'NEIGHBORHOOD_OTHER',\n 'BUILDING CLASS CATEGORY_01 ONE FAMILY DWELLINGS',\n 'BUILDING CLASS CATEGORY_02 TWO FAMILY DWELLINGS',\n 'BUILDING CLASS CATEGORY_09 COOPS - WALKUP APARTMENTS',\n 'BUILDING CLASS CATEGORY_10 COOPS - ELEVATOR APARTMENTS',\n 'TAX CLASS AT PRESENT_1',\n 'TAX CLASS AT PRESENT_2',\n 'TAX CLASS AT PRESENT_2A',\n 'BUILDING CLASS AT PRESENT_A',\n 'BUILDING CLASS AT PRESENT_B',\n 'BUILDING CLASS AT PRESENT_E',\n 'BUILDING CLASS AT PRESENT_I',\n 'BUILDING CLASS AT PRESENT_S',\n 'BUILDING CLASS AT PRESENT_W']\n\n# third (current)\n{'BUILD_LETTER': False,\n 'N_BUILD_CAT': 50,\n 'N_NEIGHBORS': 50,\n 'PRICE_LOWER_Q': 0.01,\n 'PRICE_UPPER_Q': 0.995}\nTRAIN R2: 0.5814129194095372\nTRAIN ADJUSTED R2: 0.5802083522711474\nTEST R2: 0.5248503759027321\nk: 40\n['BOROUGH_1',\n 'BOROUGH_3',\n 'BOROUGH_4',\n 'NEIGHBORHOOD_ASTORIA',\n 'NEIGHBORHOOD_CANARSIE',\n 'NEIGHBORHOOD_EAST NEW YORK',\n 'NEIGHBORHOOD_FLATBUSH-EAST',\n 'NEIGHBORHOOD_FLUSHING-NORTH',\n 'NEIGHBORHOOD_GRAMERCY',\n 'NEIGHBORHOOD_GREENWICH VILLAGE-CENTRAL',\n 'NEIGHBORHOOD_GREENWICH VILLAGE-WEST',\n 'NEIGHBORHOOD_HARLEM-CENTRAL',\n 'NEIGHBORHOOD_MIDTOWN WEST',\n 'NEIGHBORHOOD_MURRAY HILL',\n 'NEIGHBORHOOD_PARK SLOPE',\n 'NEIGHBORHOOD_UPPER EAST SIDE (59-79)',\n 'NEIGHBORHOOD_UPPER EAST SIDE (79-96)',\n 'NEIGHBORHOOD_UPPER WEST SIDE (79-96)',\n 'BUILDING CLASS CATEGORY_01 ONE FAMILY DWELLINGS',\n 'BUILDING CLASS CATEGORY_08 RENTALS - ELEVATOR APARTMENTS',\n 'BUILDING CLASS CATEGORY_09 COOPS - WALKUP APARTMENTS',\n 'BUILDING CLASS CATEGORY_10 COOPS - ELEVATOR APARTMENTS',\n 'BUILDING CLASS CATEGORY_26 OTHER HOTELS',\n 'BUILDING CLASS CATEGORY_29 COMMERCIAL GARAGES',\n 'BUILDING CLASS CATEGORY_37 RELIGIOUS FACILITIES',\n 'TAX CLASS AT PRESENT_2',\n 'TAX CLASS AT PRESENT_2A',\n 'TAX CLASS AT PRESENT_2B',\n 'TAX CLASS AT PRESENT_4',\n 'BUILDING CLASS AT PRESENT_A3',\n 'BUILDING CLASS AT PRESENT_A4',\n 'BUILDING CLASS AT PRESENT_A7',\n 'BUILDING CLASS AT PRESENT_C5',\n 'BUILDING CLASS AT PRESENT_E2',\n 'BUILDING CLASS AT PRESENT_K4',\n 'BUILDING CLASS AT PRESENT_K9',\n 'BUILDING CLASS AT PRESENT_L1',\n 'BUILDING CLASS AT PRESENT_O4',\n 'BUILDING CLASS AT PRESENT_O8',\n 'BUILDING CLASS AT PRESENT_W9']\n\n# maxing out (processing time becomes bottleneck)\n{'BUILD_LETTER': False,\n 'N_BUILD_CAT': 999,\n 'N_NEIGHBORS': 999,\n 'PRICE_LOWER_Q': 0.01,\n 'PRICE_UPPER_Q': 0.995}\nTRAIN R2: 0.610984514559609\nTRAIN ADJUSTED R2: 0.6082325101148565\nTEST R2: 0.5995258969805612\nk: 98\n['YEAR BUILT',\n 'BOROUGH_1',\n 'BOROUGH_3',\n 'BOROUGH_4',\n 'NEIGHBORHOOD_ARVERNE',\n 'NEIGHBORHOOD_ASTORIA',\n 'NEIGHBORHOOD_BAYSIDE',\n 'NEIGHBORHOOD_BERGEN BEACH',\n 'NEIGHBORHOOD_BOERUM HILL',\n 'NEIGHBORHOOD_BROOKLYN HEIGHTS',\n 'NEIGHBORHOOD_BROWNSVILLE',\n 'NEIGHBORHOOD_CAMBRIA HEIGHTS',\n 'NEIGHBORHOOD_CANARSIE',\n 'NEIGHBORHOOD_CARROLL GARDENS',\n 'NEIGHBORHOOD_CLINTON HILL',\n 'NEIGHBORHOOD_COBBLE HILL',\n 'NEIGHBORHOOD_COBBLE HILL-WEST',\n 'NEIGHBORHOOD_CONEY ISLAND',\n 'NEIGHBORHOOD_CYPRESS HILLS',\n 'NEIGHBORHOOD_DOWNTOWN-FULTON FERRY',\n 'NEIGHBORHOOD_DOWNTOWN-FULTON MALL',\n 'NEIGHBORHOOD_EAST NEW YORK',\n 'NEIGHBORHOOD_EAST RIVER',\n 'NEIGHBORHOOD_ELMHURST',\n 'NEIGHBORHOOD_FAR ROCKAWAY',\n 'NEIGHBORHOOD_FLATBUSH-EAST',\n 'NEIGHBORHOOD_FLATBUSH-NORTH',\n 'NEIGHBORHOOD_FLATLANDS',\n 'NEIGHBORHOOD_FLUSHING-NORTH',\n 'NEIGHBORHOOD_FOREST HILLS',\n 'NEIGHBORHOOD_FORT GREENE',\n 'NEIGHBORHOOD_GERRITSEN BEACH',\n 'NEIGHBORHOOD_GOWANUS',\n 'NEIGHBORHOOD_GRAMERCY',\n 'NEIGHBORHOOD_GRAVESEND',\n 'NEIGHBORHOOD_GREENPOINT',\n 'NEIGHBORHOOD_GREENWICH VILLAGE-CENTRAL',\n 'NEIGHBORHOOD_GREENWICH VILLAGE-WEST',\n 'NEIGHBORHOOD_HARLEM-CENTRAL',\n 'NEIGHBORHOOD_HARLEM-EAST',\n 'NEIGHBORHOOD_HARLEM-UPPER',\n 'NEIGHBORHOOD_HOLLIS',\n 'NEIGHBORHOOD_LAURELTON',\n 'NEIGHBORHOOD_LONG ISLAND CITY',\n 'NEIGHBORHOOD_MARINE PARK',\n 'NEIGHBORHOOD_MIDTOWN WEST',\n 'NEIGHBORHOOD_MILL BASIN',\n 'NEIGHBORHOOD_OCEAN HILL',\n 'NEIGHBORHOOD_OLD MILL BASIN',\n 'NEIGHBORHOOD_OZONE PARK',\n 'NEIGHBORHOOD_PARK SLOPE',\n 'NEIGHBORHOOD_PARK SLOPE SOUTH',\n 'NEIGHBORHOOD_QUEENS VILLAGE',\n 'NEIGHBORHOOD_RICHMOND HILL',\n 'NEIGHBORHOOD_ROSEDALE',\n 'NEIGHBORHOOD_SEAGATE',\n 'NEIGHBORHOOD_SHEEPSHEAD BAY',\n 'NEIGHBORHOOD_SO. JAMAICA-BAISLEY PARK',\n 'NEIGHBORHOOD_SOHO',\n 'NEIGHBORHOOD_SOUTH JAMAICA',\n 'NEIGHBORHOOD_SOUTH OZONE PARK',\n 'NEIGHBORHOOD_SPRING CREEK',\n 'NEIGHBORHOOD_SPRINGFIELD GARDENS',\n 'NEIGHBORHOOD_ST. ALBANS',\n 'NEIGHBORHOOD_SUNNYSIDE',\n 'NEIGHBORHOOD_UPPER EAST SIDE (59-79)',\n 'NEIGHBORHOOD_UPPER WEST SIDE (79-96)',\n 'NEIGHBORHOOD_WASHINGTON HEIGHTS UPPER',\n 'NEIGHBORHOOD_WILLIAMSBURG-NORTH',\n 'NEIGHBORHOOD_WILLIAMSBURG-SOUTH',\n 'BUILDING CLASS CATEGORY_01 ONE FAMILY DWELLINGS',\n 'BUILDING CLASS CATEGORY_08 RENTALS - ELEVATOR APARTMENTS',\n 'BUILDING CLASS CATEGORY_09 COOPS - WALKUP APARTMENTS',\n 'BUILDING CLASS CATEGORY_10 COOPS - ELEVATOR APARTMENTS',\n 'BUILDING CLASS CATEGORY_29 COMMERCIAL GARAGES',\n 'BUILDING CLASS CATEGORY_32 HOSPITAL AND HEALTH FACILITIES',\n 'TAX CLASS AT PRESENT_2',\n 'TAX CLASS AT PRESENT_2A',\n 'TAX CLASS AT PRESENT_2B',\n 'TAX CLASS AT PRESENT_4',\n 'BUILDING CLASS AT PRESENT_A1',\n 'BUILDING CLASS AT PRESENT_A3',\n 'BUILDING CLASS AT PRESENT_A4',\n 'BUILDING CLASS AT PRESENT_A7',\n 'BUILDING CLASS AT PRESENT_C2',\n 'BUILDING CLASS AT PRESENT_C3',\n 'BUILDING CLASS AT PRESENT_C5',\n 'BUILDING CLASS AT PRESENT_C7',\n 'BUILDING CLASS AT PRESENT_E2',\n 'BUILDING CLASS AT PRESENT_E9',\n 'BUILDING CLASS AT PRESENT_F1',\n 'BUILDING CLASS AT PRESENT_H8',\n 'BUILDING CLASS AT PRESENT_I5',\n 'BUILDING CLASS AT PRESENT_K4',\n 'BUILDING CLASS AT PRESENT_K7',\n 'BUILDING CLASS AT PRESENT_L1',\n 'BUILDING CLASS AT PRESENT_O4',\n 'BUILDING CLASS AT PRESENT_P2']"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}