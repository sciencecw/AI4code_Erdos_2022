{"cells": [{"cell_type": "markdown", "id": "4ca0dc8f", "metadata": {}, "source": ["#                                      ICMR Healthcare  Cancer Type Detections  and Gene Type Analysis \n\n* Data Analyst : Sheri Prashanth Reddy \n\n## DESCRIPTION\n\n### Problem Statement: \n\nICMR wants to analyze different types of cancers, such as breast cancer, renal cancer, colon cancer, lung cancer, and prostate cancer becoming a cause of worry in recent years. They would like to identify the probable cause of these cancers in terms of genes responsible for each cancer type. This would lead us to early identification of each type of cancer reducing the fatality rate.\n\n### Dataset Details: \n\nThe input dataset contains 802 samples for the corresponding 802 people who have been detected with different types of cancer. Each sample contains expression values of more than 20K genes. Samples have one of the types of tumors: BRCA, KIRC, COAD, LUAD, and PRAD.\n\n\n### Project Tasks are divided in 4 weeks "]}, {"cell_type": "markdown", "id": "c910fb8e", "metadata": {}, "source": ["## Week 1:-  Exploratory Data Analysis\n\n\n#### Project Task: Week 1:\n\nExploratory Data Analysis:\n\nMerge both the datasets.\n\nPlot the merged dataset as a hierarchically-clustered heatmap.\n\nPerform Null-hypothesis testing.\n "]}, {"cell_type": "code", "execution_count": 1, "id": "335852b0", "metadata": {}, "outputs": [], "source": ["import pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\ncolors = ['royalblue','red','deeppink', 'maroon', 'mediumorchid', 'tan', 'forestgreen', 'olive', 'goldenrod', 'lightcyan', 'navy']\nvectorizer = np.vectorize(lambda x: colors[x % len(colors)])\n\nimport warnings\nwarnings.filterwarnings(action='ignore',category=DeprecationWarning)\nwarnings.filterwarnings(action='ignore',category=FutureWarning)"]}, {"cell_type": "markdown", "id": "21b8ecc3", "metadata": {}, "source": ["### Week1: Load Data in dataframe for labels and the data "]}, {"cell_type": "code", "execution_count": 1, "id": "27b2b600", "metadata": {}, "outputs": [], "source": ["label = pd.read_csv('/kaggle/input/icmr-health-care/labels.csv',delimiter=',',engine='python')\ndata = pd.read_csv('/kaggle/input/icmr-health-care/data.csv',delimiter=',',engine='python')\ndata.describe()"]}, {"cell_type": "markdown", "id": "b4f2a62c", "metadata": {}, "source": ["### Week1:- Merge data set "]}, {"cell_type": "code", "execution_count": 1, "id": "a7e6ab58", "metadata": {}, "outputs": [], "source": ["master_data = pd.merge(label,data)\nmaster_data.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "f79e909d", "metadata": {}, "outputs": [], "source": ["master_data.isnull().sum()"]}, {"cell_type": "code", "execution_count": 1, "id": "2083f726", "metadata": {}, "outputs": [], "source": ["master_data.describe()"]}, {"cell_type": "markdown", "id": "0d305e0d", "metadata": {}, "source": ["### Week1:- Plot the merged dataset as a hierarchically-clustered heatmap."]}, {"cell_type": "code", "execution_count": 1, "id": "8bd50389", "metadata": {}, "outputs": [], "source": ["heatmap_data = pd.pivot_table(master_data, index=['Class'])\n                              \nheatmap_data.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "1ec08f1a", "metadata": {}, "outputs": [], "source": ["sns.clustermap(heatmap_data)\nplt.savefig('heatmap_with_Seaborn_clustermap_python.jpg',\n            dpi=150, figsize=(8,12))"]}, {"cell_type": "code", "execution_count": 1, "id": "3b6aca6a", "metadata": {}, "outputs": [], "source": ["sns.clustermap(heatmap_data, figsize=(18,12))\nplt.savefig('clustered_heatmap_with_dendrograms_Seaborn_clustermap_python.jpg',dpi=150)\n"]}, {"cell_type": "markdown", "id": "50acdcc7", "metadata": {}, "source": ["### Week1:- Perform Null Hypothesis testing "]}, {"cell_type": "markdown", "id": "2a69624e", "metadata": {}, "source": ["### Checking histogram to check if the data is normally distributed "]}, {"cell_type": "code", "execution_count": 1, "id": "ea1b59d3", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(14,6))\nplt.hist(master_data['Class'])\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "47b50b66", "metadata": {}, "outputs": [], "source": ["non_cat_data = master_data.drop(['Unnamed: 0'], axis=1)\nnon_cat_data"]}, {"cell_type": "markdown", "id": "2e8ea959", "metadata": {}, "source": ["### Week1:- F Test\n\nF-tests are named after its test statistic, F, which was named in honor of Sir Ronald Fisher. The F-statistic is simply a ratio of two variances. Variances are a measure of dispersion, or how far the data are scattered from the mean. Larger values represent greater dispersion.\n\nVariance is the square of the standard deviation. For us humans, standard deviations are easier to understand than variances because they\u2019re in the same units as the data rather than squared units. However, many analyses actually use variances in the calculations.\n\nF-statistics are based on the ratio of mean squares. The term \u201cmean squares\u201d may sound confusing but it is simply an estimate of population variance that accounts for the degrees of freedom (DF) used to calculate that estimate."]}, {"cell_type": "code", "execution_count": 1, "id": "c00e0e9d", "metadata": {}, "outputs": [], "source": ["df_f_test=master_data"]}, {"cell_type": "code", "execution_count": 1, "id": "305d849c", "metadata": {}, "outputs": [], "source": ["def f_test(df_f_test,gene):  \n    df_anova = df_f_test[[gene,'Class']]\n    grps = pd.unique(df_anova.Class.values)\n    grps\n    d_data = {grp:df_anova[gene][df_anova.Class == grp] for grp in grps}\n    F, p = stats.f_oneway(d_data['LUAD'], d_data['PRAD'], d_data['BRCA'], d_data['KIRC'], d_data['COAD'])\n    print(\"p_values:-\",p)\n    if p<0.05:\n        print(\"reject null hypothesis\")\n    else:\n        print(\"accept null hypothesis\")\n        \n    return "]}, {"cell_type": "code", "execution_count": 1, "id": "06bfd975", "metadata": {}, "outputs": [], "source": ["f_test(df_f_test,\"gene_3\")"]}, {"cell_type": "code", "execution_count": 1, "id": "4f39089f", "metadata": {}, "outputs": [], "source": ["f_test(df_f_test,\"gene_7\")"]}, {"cell_type": "code", "execution_count": 1, "id": "a3f18a33", "metadata": {}, "outputs": [], "source": ["f_test(df_f_test,\"gene_20524\")"]}, {"cell_type": "code", "execution_count": 1, "id": "caad6b50", "metadata": {}, "outputs": [], "source": ["f_test(df_f_test,\"gene_5\")"]}, {"cell_type": "code", "execution_count": 1, "id": "0954f8a2", "metadata": {}, "outputs": [], "source": ["f_test(df_f_test,\"gene_5\")"]}, {"cell_type": "code", "execution_count": 1, "id": "2e250ae9", "metadata": {}, "outputs": [], "source": ["df_cat_data = master_data\ndf_cat_data['Class'] = df_cat_data['Class'].map({'PRAD': 1, 'LUAD': 2, 'BRCA': 3, 'KIRC': 4, 'COAD': 5}) \ndf_cat_data = df_cat_data.drop(['Unnamed: 0'],axis=1)"]}, {"cell_type": "markdown", "id": "ffdf5d5b", "metadata": {}, "source": ["### Shapiro test \n\n#### The null hypothesis for the Shapiro-Wilk test is that a variable is normally distributed in some population. A different way to say the same is that a variable's values are a simple random sample from a normal distribution. As a rule of thumb, we reject the null hypothesis if p < 0.05"]}, {"cell_type": "code", "execution_count": 1, "id": "50c90d75", "metadata": {}, "outputs": [], "source": ["from scipy.stats import shapiro\nstat, p = shapiro(df_cat_data)\nprint('stat=%.2f, p=%.30f' %(stat, p))\n\nif p > 0.05:\n    print('Normal Distribution')\nelse:\n    print('Not Normal')"]}, {"cell_type": "markdown", "id": "96f497a4", "metadata": {}, "source": ["### k2test - In statistics, D'Agostino's K2 test, named for Ralph D'Agostino, is a goodness-of-fit measure of departure from normality, that is the test aims to establish whether or not the given sample comes from a normally distributed population"]}, {"cell_type": "code", "execution_count": 1, "id": "da86c309", "metadata": {}, "outputs": [], "source": ["#K2 normality test \nfrom scipy.stats import normaltest\nk2_test = df_cat_data['Class']\n\nstat, p = normaltest(k2_test)\nprint('stat=%.2f, p=%.30f' %(stat, p))\n\nif p > 0.05:\n    print('Normal Distribution')\nelse:\n    print('Not Normal')\n"]}, {"cell_type": "markdown", "id": "8ba348e9", "metadata": {}, "source": ["## Week 2:- Dimensionality Reduction\n\n#### Project Task: Week 2: \n\nDimensionality Reduction:\n\nEach sample has expression values for around 20K genes. However, it may not be necessary to include all 20K genes expression values to analyze each cancer type. Therefore, we will identify a smaller set of attributes which will then be used to fit multiclass classification models. So, the first task targets the dimensionality reduction using various techniques such as,\nPCA, LDA, and t-SNE.\nInput: Complete dataset including all genes (20531)\nOutput: Selected Genes from each dimensionality reduction method\n "]}, {"cell_type": "markdown", "id": "067dedbe", "metadata": {}, "source": ["### Dimensionality Reduction using PCA "]}, {"cell_type": "code", "execution_count": 1, "id": "d0ae9da9", "metadata": {}, "outputs": [], "source": ["# Define data \ndf_pca = master_data.drop(['Unnamed: 0'], axis=1)\ndf_pca = df_pca.drop(['Class'], axis=1)\ndf_pca.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "1ca10553", "metadata": {}, "outputs": [], "source": ["df_pca.values.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "3f8f999a", "metadata": {}, "outputs": [], "source": ["x_pca = df_pca.values"]}, {"cell_type": "markdown", "id": "d36caee8", "metadata": {}, "source": ["### Week2:- Scaling the data using standard scaler method"]}, {"cell_type": "code", "execution_count": 1, "id": "53f9d835", "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_Scaled = scaler.fit_transform(x_pca)\nX_Scaled"]}, {"cell_type": "markdown", "id": "61ab803f", "metadata": {}, "source": ["### Week2:- Perform PCA with n_components=2\n\n#### Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\n\n#### Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process.\n\n#### So to sum up, the idea of PCA is simple \u2014 reduce the number of variables of a data set, while preserving as much information as possible.\n"]}, {"cell_type": "code", "execution_count": 1, "id": "8e7dd08f", "metadata": {}, "outputs": [], "source": ["# Import PCA from sklearn and define the n_components as 2 \nfrom sklearn.decomposition import PCA\npca_with_2=PCA(n_components=2)"]}, {"cell_type": "code", "execution_count": 1, "id": "8805b74a", "metadata": {}, "outputs": [], "source": ["#Perform fit transform on the scaled data\nX_pca_with_2 = pca_with_2.fit_transform(X_Scaled)\nX_pca_with_2.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "194cab50", "metadata": {}, "outputs": [], "source": ["X_pca_with_2"]}, {"cell_type": "code", "execution_count": 1, "id": "64347e2e", "metadata": {}, "outputs": [], "source": ["# Put the data back on the 2 columns defined \ndf_pca = pd.DataFrame(X_pca_with_2)\ndf_pca.columns = ['pca1','pca2']\n\n# Add the convereted categorical data for \ndf_pca['cancer_type']=df_cat_data['Class']\ndf_pca"]}, {"cell_type": "code", "execution_count": 1, "id": "88ff7181", "metadata": {}, "outputs": [], "source": ["# Present the data on the 5 clusters using seaborn maps \nsns.scatterplot(x='pca1',y='pca2', hue = 'cancer_type',data=df_pca)"]}, {"cell_type": "markdown", "id": "99e6699b", "metadata": {}, "source": ["### Week2:- PCA with n_components=.995"]}, {"cell_type": "code", "execution_count": 1, "id": "a48cd7d0", "metadata": {}, "outputs": [], "source": ["pca_with_995=PCA(.995)\nX_pca_with_995 = pca_with_995.fit_transform(x_pca)\nX_pca_with_995.shape\nX_pca_with_995"]}, {"cell_type": "code", "execution_count": 1, "id": "6a9acea5", "metadata": {}, "outputs": [], "source": ["df_pca_995 = pd.DataFrame(X_pca_with_995)\ndf_pca_995['cancer_type']=df_cat_data['Class']\ndf_pca_995"]}, {"cell_type": "code", "execution_count": 1, "id": "c722dc17", "metadata": {}, "outputs": [], "source": ["sns.scatterplot(x=0,y=1,hue = 'cancer_type', data=df_pca_995)"]}, {"cell_type": "markdown", "id": "897c469d", "metadata": {}, "source": ["### Week2:- Dimensionality reduction using  TSNE\n\nT-SNE is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with different initializations we can get different results.\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "d4fe121a", "metadata": {}, "outputs": [], "source": ["df_tsne_data = master_data\nnon_numeric = ['Unnamed: 0','Class']\ndf_tsne_data = df_tsne_data.drop(non_numeric, axis=1)\ndf_tsne_data"]}, {"cell_type": "code", "execution_count": 1, "id": "22d215be", "metadata": {}, "outputs": [], "source": ["#import T-SNE from sklearn\nfrom sklearn.manifold import TSNE\nm = TSNE(learning_rate=50)"]}, {"cell_type": "code", "execution_count": 1, "id": "44aadc81", "metadata": {}, "outputs": [], "source": ["tnse_features = m.fit_transform(df_tsne_data)\ntnse_features[1:4,:]"]}, {"cell_type": "code", "execution_count": 1, "id": "afc0003b", "metadata": {}, "outputs": [], "source": ["df_tsne_data['x'] = tnse_features[:,0]\ndf_tsne_data['y'] = tnse_features[:,1]\n\nimport seaborn as sns\nsns.scatterplot(x='x',y='y',data=df_tsne_data)\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "6d9b812b", "metadata": {}, "outputs": [], "source": ["df_tsne_data['cancer_type']=df_cat_data['Class']\nsns.scatterplot(x='x',y='y',hue = 'cancer_type', data=df_tsne_data)\nplt.show()"]}, {"cell_type": "markdown", "id": "6ac4a546", "metadata": {}, "source": ["### Week2:- Dimensionality reduction using LDA \n\n#### Linear Discriminant Analysis, or LDA for short, is a predictive modeling algorithm for multi-class classification. It can also be used as a dimensionality reduction technique, providing a projection of a training dataset that best separates the examples by their assigned class.\n\n#### The ability to use Linear Discriminant Analysis for dimensionality reduction often surprises most practitioners."]}, {"cell_type": "code", "execution_count": 1, "id": "eca431f3", "metadata": {}, "outputs": [], "source": ["df_lda = master_data.drop(['Unnamed: 0'], axis=1)\ndf_lda = df_lda.drop(['Class'], axis=1)\nx_lda = df_lda\nx_lda"]}, {"cell_type": "code", "execution_count": 1, "id": "64415a27", "metadata": {}, "outputs": [], "source": ["x_lda.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "6a811cdc", "metadata": {}, "outputs": [], "source": ["y_lda = master_data['Class']\ny_lda.values"]}, {"cell_type": "code", "execution_count": 1, "id": "bddbf33e", "metadata": {}, "outputs": [], "source": ["from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nlda = LDA(n_components=2)\nx_r2 = lda.fit(x_lda,y_lda).transform(x_lda)"]}, {"cell_type": "code", "execution_count": 1, "id": "284c3c7f", "metadata": {}, "outputs": [], "source": ["lda.explained_variance_ratio_"]}, {"cell_type": "code", "execution_count": 1, "id": "a226c82d", "metadata": {}, "outputs": [], "source": ["x_r3 = pd.DataFrame(data=x_r2)\nx_r3['y']=y_lda\nx_r3"]}, {"cell_type": "code", "execution_count": 1, "id": "e946aacc", "metadata": {}, "outputs": [], "source": ["sns.scatterplot(x=0,y=1,hue = 'y', data=x_r3)"]}, {"cell_type": "markdown", "id": "a91bf7ba", "metadata": {}, "source": ["## Project Task: Week 3: Clustering Genes and Samples:\n\n#### Project Task: Week 3: \n\nClustering Genes and Samples:\n\nOur next goal is to identify groups of genes that behave similarly across samples and identify the distribution of samples corresponding to each cancer type. Therefore, this task focuses on applying various clustering techniques, e.g., k-means, hierarchical, and mean-shift clustering, on genes and samples.\n\n \n\nFirst, apply the given clustering technique on all genes to identify:\n\nGenes whose expression values are similar across all samples\n\nGenes whose expression values are similar across samples of each cancer type \n\n \n\nNext, apply the given clustering technique on all samples to identify:\n\nSamples of the same class (cancer type) which also correspond to the same cluster\n\nSamples identified to be belonging to another cluster but also to the same class (cancer type)\n\n \n\n\n### KMEANS Clustering with PCA = 2"]}, {"cell_type": "code", "execution_count": 1, "id": "c91e2276", "metadata": {}, "outputs": [], "source": ["from sklearn.cluster import KMeans\nclusters = KMeans(5, n_init = 5)\nclusters.fit(X_pca_with_2)\n\nclusters.labels_"]}, {"cell_type": "code", "execution_count": 1, "id": "6a63352f", "metadata": {}, "outputs": [], "source": ["pca_with_2_data_frame = pd.DataFrame(data=X_pca_with_2,columns=['pca1','pca2'])\npca_with_2_data_frame.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "a4cde031", "metadata": {}, "outputs": [], "source": ["pca_with_2_data_frame['Cls_label'] = clusters.labels_\npca_with_2_data_frame['given_cancer_type'] = label.Class.values\npca_with_2_data_frame"]}, {"cell_type": "code", "execution_count": 1, "id": "ca06c51d", "metadata": {}, "outputs": [], "source": ["brca = pca_with_2_data_frame.groupby('given_cancer_type').get_group('BRCA')\nbrca.Cls_label.value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "eccc24a8", "metadata": {}, "outputs": [], "source": ["luad = pca_with_2_data_frame.groupby('given_cancer_type').get_group('LUAD')\nluad.Cls_label.value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "440058b8", "metadata": {}, "outputs": [], "source": ["coad = pca_with_2_data_frame.groupby('given_cancer_type').get_group('COAD')\ncoad.Cls_label.value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "9b5b2ba8", "metadata": {}, "outputs": [], "source": ["prad = pca_with_2_data_frame.groupby('given_cancer_type').get_group('PRAD')\nprad.Cls_label.value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "8318a303", "metadata": {}, "outputs": [], "source": ["kirc = pca_with_2_data_frame.groupby('given_cancer_type').get_group('KIRC')\nkirc.Cls_label.value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "dc56bd9f", "metadata": {}, "outputs": [], "source": ["clusters.cluster_centers_"]}, {"cell_type": "code", "execution_count": 1, "id": "144dc396", "metadata": {}, "outputs": [], "source": ["kmeans = KMeans(n_clusters=5, init='k-means++', max_iter=300, n_init=10, random_state=0)\npred_y = kmeans.fit_predict(X_pca_with_2)\nplt.scatter(X_pca_with_2[:,0], X_pca_with_2[:,1])\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')\nplt.show()"]}, {"cell_type": "markdown", "id": "312fb6c0", "metadata": {}, "source": ["## KMEANS Clustering with PCA = .995"]}, {"cell_type": "code", "execution_count": 1, "id": "3dd7219d", "metadata": {}, "outputs": [], "source": ["from sklearn.cluster import KMeans\nclusters_995 = KMeans(5, n_init = 5)\nclusters_995.fit(X_pca_with_995)\nclusters_995.labels_"]}, {"cell_type": "code", "execution_count": 1, "id": "a58e317e", "metadata": {}, "outputs": [], "source": ["pca_with_995_data_frame = pd.DataFrame(data=X_pca_with_995)\npca_with_995_data_frame.head()\npca_with_995_data_frame['Cls_label'] = clusters.labels_\npca_with_995_data_frame['given_cancer_type'] = label.Class.values"]}, {"cell_type": "code", "execution_count": 1, "id": "f764902e", "metadata": {}, "outputs": [], "source": ["pca_with_995_data_frame.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "ac1de5d2", "metadata": {}, "outputs": [], "source": ["brca_995 = pca_with_995_data_frame.groupby('given_cancer_type').get_group('BRCA')\nbrca_995.Cls_label.value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "1bdeaaf8", "metadata": {}, "outputs": [], "source": ["luad_995 = pca_with_995_data_frame.groupby('given_cancer_type').get_group('LUAD')\nluad_995.Cls_label.value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "55698e4e", "metadata": {}, "outputs": [], "source": ["coad_995 = pca_with_995_data_frame.groupby('given_cancer_type').get_group('COAD')\ncoad_995.Cls_label.value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "b2f3a70c", "metadata": {}, "outputs": [], "source": ["prad_995 = pca_with_995_data_frame.groupby('given_cancer_type').get_group('PRAD')\nprad_995.Cls_label.value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "acb0b82b", "metadata": {}, "outputs": [], "source": ["kirc_995 = pca_with_995_data_frame.groupby('given_cancer_type').get_group('KIRC')\nkirc_995.Cls_label.value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "7d320f54", "metadata": {}, "outputs": [], "source": ["kmeans = KMeans(n_clusters=5, init='k-means++', max_iter=300, n_init=10, random_state=0)\npred_y = kmeans.fit_predict(X_pca_with_995)\nplt.scatter(X_pca_with_995[:,0], X_pca_with_995[:,1])\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')\nplt.show()"]}, {"cell_type": "markdown", "id": "e9b0546f", "metadata": {}, "source": ["## Week 4: Build Classification Models\n\n#### Project Task: Week 4: \n\nBuilding Classification Model(s) with Feature Selection:\n\nOur final task is to build a robust classification model(s) for identifying each type of cancer.\n\nSub-tasks:\n\nBuild a classification model(s) using multiclass SVM, Random Forest, and Deep Neural Network to classify the input data into five cancer types\n\nApply the feature selection algorithms, forward selection, and backward elimination to refine selected attributes (selected in Task-2) using the classification model from the previous step\n\nValidate the genes selected from the last step using statistical significance testing (t-test for one vs. all and F-test)\n "]}, {"cell_type": "markdown", "id": "8f22f345", "metadata": {}, "source": ["### Build decision tree clasifier\n\n#### Decision Tree is a Supervised Machine Learning Algorithm that uses a set of rules to make decisions, similarly to how humans make decisions. One way to think of a Machine Learning classification algorithm is that it is built to make decisions. You usually say the model predicts the class of the new, never-seen-before input but, behind the scenes, the algorithm has to decide which class to assign.\n"]}, {"cell_type": "code", "execution_count": 1, "id": "c73ee001", "metadata": {}, "outputs": [], "source": ["ml_x = x_lda\nml_y = y_lda\nml_x.shape,ml_y.shape\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(ml_x,ml_y,test_size=0.30,random_state=30)"]}, {"cell_type": "code", "execution_count": 1, "id": "1d54e4f2", "metadata": {}, "outputs": [], "source": ["from sklearn import tree\ndt_clf = tree.DecisionTreeClassifier(max_depth=5)\ndt_clf.fit(x_train,y_train)\ndt_clf.score(x_test,y_test)\n\ny_pred=(dt_clf.predict(x_test))\ndt_clf.score(x_test,y_test)"]}, {"cell_type": "markdown", "id": "64bcc2c3", "metadata": {}, "source": ["### SVM \n\n#### Support vector machine algorithm is used to find a hyperplane in an N-dimensional space(N \u2014 the number of features) that distinctly classifies the data points."]}, {"cell_type": "code", "execution_count": 1, "id": "2802d1ed", "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\nsv_clf = SVC(probability=True, kernel='linear')\nsv_clf.fit(x_train,y_train)\nsv_clf.score(x_test,y_test)\n\n\ny_pred = sv_clf.predict(x_test)\nprint(accuracy_score(y_test,y_pred))\n"]}, {"cell_type": "markdown", "id": "8796c044", "metadata": {}, "source": ["### Random Forest \n\n#### Random forest, like its name implies, consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model\u2019s prediction. The fundamental concept behind random forest is a simple but powerful one \u2014 the wisdom of crowds. In data science speak, the reason that the random forest model works so well is: A large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models."]}, {"cell_type": "code", "execution_count": 1, "id": "a213f881", "metadata": {}, "outputs": [], "source": ["from sklearn import ensemble\nrf_clf = ensemble.RandomForestClassifier(n_estimators=100)\nrf_clf.fit(x_train,y_train)\nrf_clf.score(x_test,y_test)"]}, {"cell_type": "markdown", "id": "b9cda679", "metadata": {}, "source": ["### Naive Bayes Classifier \n\n#### A Naive Bayes classifier is a probabilistic machine learning model that\u2019s used for classification task. The crux of the classifier is based on the Bayes theorem.\n\n#### Bayes Theorem:\n\n#### Using Bayes theorem, we can find the probability of A happening, given that B has occurred. Here, B is the evidence and A is the hypothesis. The assumption made here is that the predictors/features are independent. That is presence of one particular feature does not affect the other. Hence it is called naive."]}, {"cell_type": "code", "execution_count": 1, "id": "e2c6a5cc", "metadata": {}, "outputs": [], "source": ["from sklearn.naive_bayes import GaussianNB\ngb_clf = GaussianNB()\ngb_clf.fit(x_train,y_train)\ngb_clf.score(x_test,y_test)"]}, {"cell_type": "markdown", "id": "f1b47c81", "metadata": {}, "source": ["gb_clf = ensemble.GradientBoostingClassifier(n_estimators=40)\ngb_clf.fit(x_train,y_train)\ngb_clf.score(x_test,y_test)"]}, {"cell_type": "markdown", "id": "fafe5f42", "metadata": {}, "source": ["### KNN Classifier\n\n#### K-nearest neighbors (KNN) algorithm is a type of supervised ML algorithm which can be used for both classification as well as regression predictive problems. However, it is mainly used for classification predictive problems in industry. The following two properties would define KNN well \u2212"]}, {"cell_type": "code", "execution_count": 1, "id": "af7998c7", "metadata": {}, "outputs": [], "source": ["from sklearn.neighbors import KNeighborsClassifier\nknn_clf = KNeighborsClassifier(n_neighbors=5)\nknn_clf.fit(x_train,y_train)\nknn_clf.score(x_test,y_test)"]}, {"cell_type": "markdown", "id": "b8078cf5", "metadata": {}, "source": ["## Recurcive Feature Elimination "]}, {"cell_type": "code", "execution_count": 1, "id": "9b1906ad", "metadata": {}, "outputs": [], "source": ["# automatically select the number of features for RFE\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n# create pipeline\nrfe = RFECV(estimator=DecisionTreeClassifier())\nmodel = DecisionTreeClassifier()\npipeline = Pipeline(steps=[('s',rfe),('m',model)])\n# evaluate model\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n# report performance\nprint('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"]}, {"cell_type": "markdown", "id": "73f85cc8", "metadata": {}, "source": ["## One way F test "]}, {"cell_type": "code", "execution_count": 1, "id": "a0786407", "metadata": {}, "outputs": [], "source": ["df_tsne = pd.DataFrame(data=tnse_features,columns=['tsne1','tsne2'])\ndf_tsne['cancer_type']=label['Class']\ndf_tsne"]}, {"cell_type": "code", "execution_count": 1, "id": "7780f433", "metadata": {}, "outputs": [], "source": ["df_anova_tsne = df_tsne[['tsne2','cancer_type']]\ngrps_tsne = pd.unique(df_anova_tsne.cancer_type.values)\n\nd_data = {grp:df_anova_tsne['tsne2'][df_anova_tsne.cancer_type == grp] for grp in grps_tsne}\n\nF, p = stats.f_oneway(d_data['LUAD'], d_data['PRAD'], d_data['BRCA'], d_data['KIRC'], d_data['COAD'])\n\nif p<0.05:\n    print(\"reject null hypothesis\")\nelse:\n    print(\"accept null hypothesis\")"]}, {"cell_type": "code", "execution_count": 1, "id": "bbd68a0b", "metadata": {}, "outputs": [], "source": ["df_anova_tsne = df_tsne[['tsne1','cancer_type']]\ngrps_tsne = pd.unique(df_anova_tsne.cancer_type.values)\n\nd_data = {grp:df_anova_tsne['tsne1'][df_anova_tsne.cancer_type == grp] for grp in grps_tsne}\n\nF, p = stats.f_oneway(d_data['LUAD'], d_data['PRAD'], d_data['BRCA'], d_data['KIRC'], d_data['COAD'])\n\nif p<0.05:\n    print(\"reject null hypothesis\")\nelse:\n    print(\"accept null hypothesis\")"]}, {"cell_type": "markdown", "id": "9d65036c", "metadata": {}, "source": ["## DNN \n\nThe neural network needs to learn all the time to solve tasks in a more qualified manner or even to use various methods to provide a better result. When it gets new information in the system, it learns how to act accordingly to a new situation.\n\nLearning becomes deeper when tasks you solve get harder. Deep neural network represents the type of machine learning when the system uses many layers of nodes to derive high-level functions from input information. It means transforming the data into a more creative and abstract component.\n\nIn order to understand the result of deep learning better, let's imagine a picture of an average man. Although you have never seen this picture and his face and body before, you will always identify that it is a human and differentiate it from other creatures. This is an example of how the deep neural network works. Creative and analytical components of information are analyzed and grouped to ensure that the object is identified correctly. These components are not brought to the system directly, thus the ML system has to modify and derive them. "]}, {"cell_type": "code", "execution_count": 1, "id": "b98b8d1a", "metadata": {}, "outputs": [], "source": ["features=master_data.drop(['Unnamed: 0'],axis=1)\nfeatures=features.drop(['Class'],axis=1)\ntarget=master_data['Class']\nfeatures.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "0337e991", "metadata": {}, "outputs": [], "source": ["target.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "d8071679", "metadata": {}, "outputs": [], "source": ["f1=features.values"]}, {"cell_type": "code", "execution_count": 1, "id": "b2bf83b2", "metadata": {}, "outputs": [], "source": ["y1 = pd.get_dummies(y_lda)"]}, {"cell_type": "code", "execution_count": 1, "id": "cf219f8e", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n\n#y1 = pd.get_dummies(Xg_fea.Pos_Neg)\n\nX1_train, X1_valid, y1_train, y1_valid = train_test_split(f1,y1, test_size = 0.10, random_state=42)"]}, {"cell_type": "code", "execution_count": 1, "id": "6d6632eb", "metadata": {}, "outputs": [], "source": ["X1_train.shape,X1_valid.shape,y1_valid.shape,y1_train.shape"]}, {"cell_type": "markdown", "id": "f9bf4d61", "metadata": {}, "source": ["### Define the model \n\n#### The ReLU function is f(x)=max(0,x). Usually this is applied element-wise to the output of some other function, such as a matrix-vector product. In MLP usages, rectifier units replace all other activation functions except perhaps the readout layer. But I suppose you could mix-and-match them if you'd like. One way ReLUs improve neural networks is by speeding up training. The gradient computation is very simple (either 0 or 1 depending on the sign of x). Also, the computational step of a ReLU is easy: any negative elements are set to 0.0 -- no exponentials, no multiplication or division operations. Gradients of logistic and hyperbolic tangent networks are smaller than the positive portion of the ReLU. This means that the positive portion is updated more rapidly as training progresses. However, this comes at a cost. The 0 gradient on the left-hand side is has its own problem, called \"dead neurons,\" in which a gradient update sets the incoming values to a ReLU such that the output is always zero; modified ReLU units such as ELU (or Leaky ReLU etc.) can minimize this. Source : StackExchange\n\n#### Optimizer is chosen SGD\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "b9b659b8", "metadata": {}, "outputs": [], "source": ["import tensorflow as tf"]}, {"cell_type": "code", "execution_count": 1, "id": "2afc9b8b", "metadata": {}, "outputs": [], "source": ["#Initialize Sequential model\nmodel = tf.keras.models.Sequential()\n\n#adding layers of inout\nmodel.add(tf.keras.layers.Dense(10000, input_dim=20531, activation='relu', kernel_initializer='he_uniform'))\n\n\n#Normalize the data\nmodel.add(tf.keras.layers.BatchNormalization())\n\n#Add 1st hidden layer\nmodel.add(tf.keras.layers.Dense(5000, activation='relu'))\n\n#Add 2nd hidden layer\nmodel.add(tf.keras.layers.Dense(2000, activation='relu'))\n\n#Add 3rd hidden layer\nmodel.add(tf.keras.layers.Dense(1000, activation='relu'))\n\n#Add 4th hidden layer\nmodel.add(tf.keras.layers.Dense(500, activation='relu'))\n\n#Add 5th hidden layer\nmodel.add(tf.keras.layers.Dense(200, activation='relu'))\n\n#Add 6th hidden layer\nmodel.add(tf.keras.layers.Dense(100, activation='relu'))\n\n#Add OUTPUT layer\nmodel.add(tf.keras.layers.Dense(5, activation='softmax'))\n\n#Create optimizer with non-default learning rate\nsgd_optimizer = tf.keras.optimizers.SGD(learning_rate=0.03)\n\n#Compile the model\nmodel.compile(optimizer=sgd_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n"]}, {"cell_type": "code", "execution_count": 1, "id": "39287914", "metadata": {}, "outputs": [], "source": ["model.summary()"]}, {"cell_type": "code", "execution_count": 1, "id": "8531c911", "metadata": {}, "outputs": [], "source": ["history = model.fit(X1_train,y1_train,          \n          validation_data=(X1_valid,y1_valid),\n          epochs=5,\n          batch_size=32)"]}, {"cell_type": "code", "execution_count": 1, "id": "918fcca7", "metadata": {}, "outputs": [], "source": ["xyz = model.predict(X1_valid)"]}, {"cell_type": "code", "execution_count": 1, "id": "3dda4d48", "metadata": {}, "outputs": [], "source": ["y_pr=[]\nfor k in xyz:\n    #np.argmax(k)\n    #print(np.argmax(k))\n    y_pr.append(np.argmax(k))\n    \ny_val=[]\nfor k in y1_valid.values:\n    #np.argmax(k)\n    #print(np.argmax(k))\n    y_val.append(np.argmax(k))"]}, {"cell_type": "code", "execution_count": 1, "id": "e84f378d", "metadata": {}, "outputs": [], "source": ["# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_val, y_pr)"]}, {"cell_type": "markdown", "id": "02e9706a", "metadata": {}, "source": ["### Evaluvate the model "]}, {"cell_type": "code", "execution_count": 1, "id": "5c0442ea", "metadata": {}, "outputs": [], "source": ["_, train_acc = model.evaluate(X1_train, y1_train, verbose=0)\n_, test_acc = model.evaluate(X1_valid, y1_valid, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"]}, {"cell_type": "markdown", "id": "caca37ea", "metadata": {}, "source": ["### Plot History "]}, {"cell_type": "code", "execution_count": 1, "id": "3ab04f54", "metadata": {}, "outputs": [], "source": ["plt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.xlabel('# of epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}