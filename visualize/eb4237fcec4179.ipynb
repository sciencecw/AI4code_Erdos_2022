{"cells": [{"cell_type": "markdown", "id": "0106729d", "metadata": {}, "source": ["# Data analysis and hypothesis testing on Singapore used car data"]}, {"cell_type": "markdown", "id": "c90c8df4", "metadata": {}, "source": ["### Singapore used car market background"]}, {"cell_type": "markdown", "id": "2169a300", "metadata": {}, "source": ["Singapore is a country where there is a unique price structure where by several policies are put in place by government to curb vehicle population due to land constraint. Growth rate are determined by a ratio of car population vs road built. The growth rate is controlled by a quota system where only certain amount of cars can be registered in a month. The system is called the Certificate of Entitlement, also known as COE in short, a paper licence to legally own a car in Singapore. It is only valid for ten years after which one may choose to renew this licence for a 5 years or 10 years to keep ownership. Otherwise, the vehicle will need to be scrapped or exported to make way for a new car to be registered by other owners.\n\nBesides the COE, there are also other taxes to discourage car ownership. To put in context, a popular sedan such as a new Toyota Corolla may cost 28,000 (in singapore dollars) to 40,000 elsewhere in the world, but will cost 80,000 to 120,000 depending on quota available on the certificates, which its price mark is a variable that fluctuates along supply and demand. \n\nMore information can be found at https://en.wikipedia.org/wiki/Driving_in_Singapore\n\nFor this data analysis, we would like to find out more about Singapore used car market. While new car price fluctuates with the quota of COEs, used car price too are affected when new car price changes. But besides COEs, we would also like to know if there are any other contributing factors which could affect a used car resale price. Some brands may be popular and are selling at higher price, or perhaps a lesser mileage clocked car may attract a better price?"]}, {"cell_type": "markdown", "id": "bbc2c84b", "metadata": {}, "source": ["### Dataset"]}, {"cell_type": "markdown", "id": "11fd1e20", "metadata": {}, "source": ["This dataset is obtained from one of the online used car portal in May 2021.\n\n### Information about the data set\n\n<b>Brand</b> - Brand of the vehicle\n\n<b>Type</b> - Model type classification eg, sedan, SUV\n\n<b>Reg_date</b> - The registration date of the vehicle. Also mean its COE will end ten years from this date\n\n<b>Coe_left</b> - The balance lifespan of the vehicle. Due to renewal of COE or to be scrapped\n\n<b>Dep</b> - The yearly depreciation amount. Differs from all vehicle even if of same age and model.\n\n<b>Mileage</b> - The mileage clocked on the vehicle\n\n<b>Road Tax</b> - The yearly taxable amount for usage of public roads\n\n<b>Dereg Value</b> - The value of the vehicle if it is deregistered today, an amount which is given back for vehicle scrapped\n\n<b>COE</b> - The price of COE paid when the car is registered\n\n<b>Engine Cap</b> - The engine capacity size in CC\n\n<b>Curb Weight</b> - The unladen weight of the vehicle\n\n<b>Manufactured</b> - The year in which the vehicle is manufactured\n\n<b>Transmission</b> - Gearbox type of vehicle\n\n<b>OMV</b> -Open market value, a valuation determined by car manufacturer, supposing the cost price from factory\n\n<b>ARF</b> - Additional parf value, an additional tax paid on top of car price, COE and some misc license\n\n<b>Power</b> - Power rating of vehicle in horsepower\n\n<b>No. of Owners</b> - Number of previous owners\n\n<b>Price</b> - The asking price of the vehicle\n"]}, {"cell_type": "markdown", "id": "888c7030", "metadata": {}, "source": ["### Data analysis and hypothesis testing\n\nUsing the variables available in the data set, we will perform a EDA and some statistical testing to gain new insights. In order to keep the data as original as possbile, we will drop most of the null values and impute data on those available to be calculated from other available variables. The content of the analysis is as follows\n\n* Load data\n* Preprocess data and data cleaning\n* EDA \n* Determine business question for testing to gain new insights\n* Investigation to conclusion"]}, {"cell_type": "markdown", "id": "232c3b7d", "metadata": {}, "source": ["# Data Loading"]}, {"cell_type": "code", "execution_count": 1, "id": "3d733309", "metadata": {}, "outputs": [], "source": ["import pandas as pd\nimport numpy as np\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nimport statistics\nimport scipy.stats as stats\nimport statsmodels.api as smi \nfrom sklearn.model_selection import StratifiedShuffleSplit\n"]}, {"cell_type": "code", "execution_count": 1, "id": "4dad20f0", "metadata": {}, "outputs": [], "source": ["# loading of dataset from webscrapping\ndf = pd.read_csv('../input/singapore-used-car/SG_usedcar.csv')\ndf = df.drop(['Unnamed: 18'],axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "4929cb7a", "metadata": {}, "outputs": [], "source": ["df"]}, {"cell_type": "code", "execution_count": 1, "id": "6e9135e4", "metadata": {}, "outputs": [], "source": ["# Replacing the missing values with np.nan\ndf = df.replace('N.A',np.nan)\ndf = df.replace('N.A.',np.nan)\ndf.info()"]}, {"cell_type": "markdown", "id": "a35c6a49", "metadata": {}, "source": ["There are some null values in the data set which preprocess and validate."]}, {"cell_type": "code", "execution_count": 1, "id": "4acb9327", "metadata": {}, "outputs": [], "source": ["# get the number of missing data points per column\nmissing_values_count = df.isnull().sum()\nmissing_values_count"]}, {"cell_type": "code", "execution_count": 1, "id": "cf586040", "metadata": {}, "outputs": [], "source": ["# total missing values\ntotal_cells = np.product(df.shape)\ntotal_missing = missing_values_count.sum()\n\n# percentage of missing values\npercent_missing = (total_missing/total_cells) * 100\nprint(percent_missing)"]}, {"cell_type": "code", "execution_count": 1, "id": "4f82c940", "metadata": {}, "outputs": [], "source": ["# dropping the null value in some of the columns as without them the entry is not very useful in the analysis\ndf.dropna(subset=['Price','Mileage','Reg_date','No. of Owners','COE'],inplace=True)\ndf = df.reset_index(drop=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "797893a9", "metadata": {}, "outputs": [], "source": ["# removing cars aged above 10 years, Off peak cars and commercial vehicles\nfor i, v in enumerate(df['Brand']):\n    desp = str(v)\n    if 'COE' in desp or 'OPC' in desp:\n        df=df.drop([i])\ndf = df.reset_index(drop=True)\nfor i, v in enumerate(df['Type']):\n    desp = str(v)\n    if desp == 'Van' or desp == 'Bus/Mini Bus' or desp == 'Truck' or desp =='Others':\n        df=df.drop([i])\ndf = df.reset_index(drop=True)"]}, {"cell_type": "markdown", "id": "03273426", "metadata": {}, "source": ["Due to a ten year limit on car lifespan, we decided to only validate private cars of the age below ten years."]}, {"cell_type": "code", "execution_count": 1, "id": "b75c68f8", "metadata": {}, "outputs": [], "source": ["df.info()"]}, {"cell_type": "markdown", "id": "b0f315d7", "metadata": {}, "source": ["# Preprocessing and Data cleaning"]}, {"cell_type": "markdown", "id": "12b97461", "metadata": {}, "source": ["Objective to preliminary checks and cleaning on data set is to retain as much of the original information and extraction of any possible calculated fields within the data set."]}, {"cell_type": "code", "execution_count": 1, "id": "196b9dec", "metadata": {}, "outputs": [], "source": ["df.Reg_date"]}, {"cell_type": "code", "execution_count": 1, "id": "5f02f24d", "metadata": {}, "outputs": [], "source": ["# Change date column to datetime format\ndf.Reg_date = pd.to_datetime(df.Reg_date,format=\"%d-%b-%y\")"]}, {"cell_type": "code", "execution_count": 1, "id": "b6964bbf", "metadata": {}, "outputs": [], "source": ["# Successful change date type to datetime format\ndf.Reg_date"]}, {"cell_type": "markdown", "id": "9c5c2c1f", "metadata": {}, "source": ["Next we will do a new calculated field for balance of COE, to have it in months instead of years and months for easier visualization."]}, {"cell_type": "code", "execution_count": 1, "id": "7f1adff0", "metadata": {}, "outputs": [], "source": ["import warnings \nwarnings.simplefilter(action='ignore')\n\ndf['Coe_left_mths'] = 0\n\n# Calculating for COE balance field\nfor i, v in enumerate(df['Coe_left']):\n    coe_string = str(v).split()\n    years = 0\n    if 'yrs' in coe_string[0]:\n        years = 0\n        y = coe_string[0].split('yrs')\n        years = int(y[0])\n    elif 'yr' in coe_string[0]:\n        years = 0\n        y = coe_string[0].split('yr')\n        years = int(y[0])\n    if 'mths' in coe_string[0]:\n        mths = 0\n        y = coe_string[0].split('mths')\n        mths = int(y[0])\n    elif 'mth' in coe_string[0]:\n        mths = 0\n        y = coe_string[0].split('mth')\n        mths = int(y[0])\n    if 'mths' in coe_string[1]:\n        mths = 0\n        y = coe_string[1].split('mths')\n        mths = int(y[0])\n    elif 'mth' in coe_string[1]:\n        mths = 0\n        y = coe_string[1].split('mth')\n        mths = int(y[0])\n    if 'days' in coe_string[0]:\n        print('Flag car with COE lesser than a month')\n        mths = 0\n        years = 0\n    months = (12*years) + mths\n    df['Coe_left_mths'].loc[i] = months"]}, {"cell_type": "markdown", "id": "d613a1e5", "metadata": {}, "source": ["We will remove the entries with less than two months on its lifespan as these car are likely to be scrapped if no one buy them."]}, {"cell_type": "code", "execution_count": 1, "id": "282cb963", "metadata": {}, "outputs": [], "source": ["for i,v in enumerate(df['Coe_left_mths']):\n    if v <= 2:\n        df.drop([i],inplace=True)\ndf.reset_index(drop=True,inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "cef56485", "metadata": {}, "outputs": [], "source": ["df.info()"]}, {"cell_type": "code", "execution_count": 1, "id": "12bfe515", "metadata": {}, "outputs": [], "source": ["df_dep = df[df['Dep'].isnull()]\nfor i,v in enumerate(df_dep['Dep']):\n    price = int(df_dep['Price'].iloc[i])\n    bal = df_dep['Coe_left_mths'].iloc[i]\n    df_dep['Dep'].iloc[i] = price / bal * 12"]}, {"cell_type": "code", "execution_count": 1, "id": "97e63f75", "metadata": {}, "outputs": [], "source": ["index = df_dep.index\nindex"]}, {"cell_type": "code", "execution_count": 1, "id": "a4344a6f", "metadata": {}, "outputs": [], "source": ["i = 0\nfor idx in index:\n    value = str(int(df_dep['Dep'].iloc[i]))\n    df['Dep'].loc[idx] = value \n    i += 1\n    \ndf_dep"]}, {"cell_type": "code", "execution_count": 1, "id": "abfc94df", "metadata": {}, "outputs": [], "source": ["df.info()"]}, {"cell_type": "markdown", "id": "e9567204", "metadata": {}, "source": ["We will try to look up the curb weight of the vehicle with missing values. Imputing them with the same model within the data."]}, {"cell_type": "code", "execution_count": 1, "id": "763f6508", "metadata": {}, "outputs": [], "source": ["df['Curb Weight'] = df['Curb Weight'].replace(np.nan,'0')"]}, {"cell_type": "code", "execution_count": 1, "id": "d65cd758", "metadata": {}, "outputs": [], "source": ["# find the curb weight of the same model with the data set\nfor i,v in enumerate(df['Curb Weight']):\n    if v == '0':\n        for t,s in enumerate(df['Brand']):\n            if df['Brand'].loc[i] == s:\n                df['Curb Weight'].loc[i] = df['Curb Weight'].loc[t]"]}, {"cell_type": "code", "execution_count": 1, "id": "3d0d6b98", "metadata": {}, "outputs": [], "source": ["df[df['Curb Weight'] == '0']"]}, {"cell_type": "markdown", "id": "07c6143b", "metadata": {}, "source": ["We have found the remaining missing value without identical model in the data. These are newly registered car sold as second owner's car problably due to reason such as demo vehicles or test drive vehicles. We will remove them since they are new cars."]}, {"cell_type": "code", "execution_count": 1, "id": "2fd23936", "metadata": {}, "outputs": [], "source": ["# will drop all remaining missing values\ndf['Curb Weight'] = df['Curb Weight'].replace('0',np.nan)\ndf.dropna(inplace=True)\ndf = df.reset_index(drop=True)\ndf.info()"]}, {"cell_type": "markdown", "id": "8de30e0e", "metadata": {}, "source": ["We are left with 1885 private car entries within 1 to 9.8 years old, which is still a good amount of sample for testing. Next we will try to get more details out of our data columns separating brand and models."]}, {"cell_type": "code", "execution_count": 1, "id": "e6383d54", "metadata": {}, "outputs": [], "source": ["# Renaming columns\ndf.rename({'Brand':'Model'},axis='columns',inplace=True)\n\n# Create a new column for data\ndf['Brand'] = 0\n"]}, {"cell_type": "code", "execution_count": 1, "id": "36fd3ce8", "metadata": {}, "outputs": [], "source": ["for i,v in enumerate(df.Model):\n    wordstr = v.split()\n    print(wordstr)\n    df['Brand'].loc[i] = str(wordstr[0]) "]}, {"cell_type": "code", "execution_count": 1, "id": "6cb00c9d", "metadata": {}, "outputs": [], "source": ["df['Brand']"]}, {"cell_type": "code", "execution_count": 1, "id": "6abf56cc", "metadata": {}, "outputs": [], "source": ["df['Brand'].value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "20f2aaf6", "metadata": {}, "outputs": [], "source": ["df.columns"]}, {"cell_type": "code", "execution_count": 1, "id": "e4cf4794", "metadata": {}, "outputs": [], "source": ["# create a list of fields to be converted to int\nconvert_dict = {'Dep': 'int64','Mileage': 'int64','Road Tax': 'int64',\n               'COE': 'int64','OMV': 'int64','ARF': 'int64', 'Manufactured': 'int64',\n               'No. of Owners': 'int64','Price': 'int64','Coe_left_mths': 'int64',\n                'Dereg Value':'int64','Engine Cap':'int64','Curb Weight':'int64',\n                'Power':'int64','Type':'category'}"]}, {"cell_type": "code", "execution_count": 1, "id": "9c489b23", "metadata": {}, "outputs": [], "source": ["df = df.astype(convert_dict)"]}, {"cell_type": "code", "execution_count": 1, "id": "6fda8e78", "metadata": {}, "outputs": [], "source": ["df.info()"]}, {"cell_type": "markdown", "id": "ef3c3792", "metadata": {}, "source": ["The data are now cleaned and ready for Exploratory data analysis."]}, {"cell_type": "markdown", "id": "7eb72763", "metadata": {}, "source": ["# EDA"]}, {"cell_type": "code", "execution_count": 1, "id": "aa1b62c1", "metadata": {}, "outputs": [], "source": ["num_value_col = df.select_dtypes(include=['int64','float64']).columns.tolist()\nfig, axes = plt.subplots(7,2, figsize=(16, 14))\nfig.subplots_adjust(hspace=0.5)\nfig.suptitle('Numerical value histogram',fontsize=18)\n\ni=0\nn=0\n\nfor x in num_value_col:\n    sb.distplot(df[x],fit=stats.norm,ax=axes[i,n])\n\n    if n < 1:\n        n+=1\n    else:\n        n=0\n        i+=1\nplt.show()"]}, {"cell_type": "markdown", "id": "f76d8714", "metadata": {}, "source": ["The numerical histogram shows quite a number of fields with long tails. Suggesting there are quite a number of outlier presence."]}, {"cell_type": "code", "execution_count": 1, "id": "177fdf9b", "metadata": {}, "outputs": [], "source": ["plt.rcParams[\"figure.figsize\"] = [10,6]\nsb.set_style(\"darkgrid\")\nsb.set_context(\"notebook\", font_scale=1.5, rc={\"font.size\":16,\"axes.titlesize\":16,\"axes.labelsize\":16})\np = sb.boxplot(df['Price'],x='Price')\np.set_xlabel('Price', fontsize = 15)\np.set_title(\"Numerical value boxplot\")\nplt.show()\n"]}, {"cell_type": "markdown", "id": "ac5e9533", "metadata": {}, "source": ["A boxplot indicates outliers for car prices over 220000 dollars."]}, {"cell_type": "code", "execution_count": 1, "id": "8c83d4b2", "metadata": {}, "outputs": [], "source": ["sb.set_style(\"white\")\nnum_value_col = df.select_dtypes(include=['int64','float64']).columns.tolist()\nfig, axes = plt.subplots(5, 3, figsize=(18, 16))\nfig.suptitle('Numerical value scatterplot',fontsize=18)\n\ni=0\nn=0\n\nfor x,y in enumerate(num_value_col):\n    p = sb.scatterplot(data=df,x=num_value_col[x],y='Price',ax=axes[i,n])\n    p.set(xlabel = num_value_col[x])\n\n    if n < 2:\n        n+=1\n    else:\n        n=0\n        i+=1\nplt.show()"]}, {"cell_type": "markdown", "id": "5df55af2", "metadata": {}, "source": ["The numerical scatterplot above shows a group of outliers scattered widely from the main cluster. Visually it suggest this group of cars are highly priced from the regular cars."]}, {"cell_type": "code", "execution_count": 1, "id": "b69a0c33", "metadata": {}, "outputs": [], "source": ["df[df['Price'] > 400000].head()"]}, {"cell_type": "code", "execution_count": 1, "id": "3da80fee", "metadata": {}, "outputs": [], "source": ["df[df['Dep'] > 25000].head()"]}, {"cell_type": "markdown", "id": "ef11179b", "metadata": {}, "source": ["A check shows this group consist of supercars and collector cars"]}, {"cell_type": "code", "execution_count": 1, "id": "f2aac1bd", "metadata": {}, "outputs": [], "source": ["df_super = df[df['Price'] > 400000]\nnum_value_col = df_super.select_dtypes(include=['int64','float64']).columns.tolist()\nfig, axes = plt.subplots(5, 3, figsize=(18, 16))\nfig.subplots_adjust(hspace=0.5)\nfig.suptitle('Numerical value scatterplot')\n\ni=0\nn=0\n\nfor x,y in enumerate(num_value_col):\n    p = sb.scatterplot(data=df_super,x=num_value_col[x],y='Price',ax=axes[i,n])\n    p.set(xlabel = num_value_col[x])\n\n    if n < 2:\n        n+=1\n    else:\n        n=0\n        i+=1\nplt.show()"]}, {"cell_type": "markdown", "id": "024261a4", "metadata": {}, "source": ["For the outliers in the data, they appears to be scattered without a clear pattern, this suggest they might belong to a separate distribution from the mass."]}, {"cell_type": "code", "execution_count": 1, "id": "5bb13722", "metadata": {}, "outputs": [], "source": ["df_normal = df[df['Dep'] < 25000]\nnum_value_col = df_normal.select_dtypes(include=['int64','float64']).columns.tolist()\nfig, axes = plt.subplots(5, 3, figsize=(18, 16))\nfig.subplots_adjust(hspace=0.5)\nfig.suptitle('Numerical value scatterplot')\n\ni=0\nn=0\n\nfor x,y in enumerate(num_value_col):\n    p = sb.scatterplot(data=df_normal,x=num_value_col[x],y='Price',ax=axes[i,n])\n    p.set(xlabel = num_value_col[x])\n\n    if n < 2:\n        n+=1\n    else:\n        n=0\n        i+=1\nplt.show()"]}, {"cell_type": "markdown", "id": "67f1e716", "metadata": {}, "source": ["With the removal of outliers, the rest of the data points appears more linear."]}, {"cell_type": "code", "execution_count": 1, "id": "4fe3dc38", "metadata": {}, "outputs": [], "source": ["sb.set_style(\"darkgrid\")\nnum_value_col = df_normal.select_dtypes(include=['int64','float64']).columns.tolist()\nfig, axes = plt.subplots(5, 3, figsize=(18, 16))\nfig.suptitle('Numerical value histogram')\n\ni=0\nn=0\n\nfor x in num_value_col:\n    sb.distplot(df_normal[x],fit=stats.norm,ax=axes[i,n])\n\n    if n < 2:\n        n+=1\n    else:\n        n=0\n        i+=1\nplt.show()"]}, {"cell_type": "markdown", "id": "815d8f60", "metadata": {}, "source": ["With the removal of outliers, the distribution appears much normal."]}, {"cell_type": "code", "execution_count": 1, "id": "7673aca2", "metadata": {}, "outputs": [], "source": ["df_normal.boxplot(column='Price',by='Type')\nplt.title('Price vs Type',fontsize=12)\nplt.xlabel('Type',fontsize=16)\nplt.xticks(rotation=45)\nplt.ylabel('Price',fontsize=16)\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "638ea0e0", "metadata": {}, "outputs": [], "source": ["def color_negative_red(val):\n    color = 'red' if val < 0 else 'black'\n    return 'color: %s' % color\n\ndf_normal.corr().style.applymap(color_negative_red)"]}, {"cell_type": "code", "execution_count": 1, "id": "e5b0fcad", "metadata": {}, "outputs": [], "source": ["df_normal = df_normal.reset_index(drop=True)\ndf_normal"]}, {"cell_type": "code", "execution_count": 1, "id": "51b08d14", "metadata": {}, "outputs": [], "source": ["(stat, p_value) = stats.spearmanr(df_normal['Mileage'],df_normal['Price'])\n\nprint(\"The correlation coefficient is: \", '{:.2f}'.format(stat))\nprint(\"The p-value is:\",'{:.5f}'.format(p_value))\n\nif p_value > 0.05:\n    print('The two variables are likely independent')\nelse:\n    print('The two variables are likely dependent')"]}, {"cell_type": "markdown", "id": "391ea163", "metadata": {}, "source": ["After performing EDA, we have a question we would like to test on.\n\nSome of the variables are fixed from the day the vehicle are registered. Furthermore, variables such as ARF, Dep, Road Tax are calculated fields from OMV, COE month left, engine capacity. They are similar across similar models and make. All except mileage is the only difference which no two cars shares the same. \n\nMileage and price have a moderate co-relationship, suggesting price may be affected by milege of a car. However in singapore, each car has a lifespan of 10 years due to certificate or entitlement. Unlike other countries which has no limit on the years you can use a car, mileage pose a important factor for used car as it determines how frequently the car has been used. Higher mileage would suggest more usage and repair cost for one might likely to be higher. Thus we would like to find out if there is a significant difference between the groups.\n\n"]}, {"cell_type": "markdown", "id": "94813b62", "metadata": {}, "source": ["#  Running statistical test"]}, {"cell_type": "markdown", "id": "a356100f", "metadata": {}, "source": ["Here we create another category of mileage difference. According to Singapore Land transport authority data, it shows a private vehicle in singapore has an average of 20000km clocked annually. As such, we are dividing the classes to three category of low, normal and high. "]}, {"cell_type": "code", "execution_count": 1, "id": "3bfcf1ea", "metadata": {}, "outputs": [], "source": ["df_normal['Mileage_cat'] = 0\nfor i,v in enumerate(df_normal['Mileage']):\n    mileage = int(v)\n    mths_left = df_normal['Coe_left_mths'].loc[i]\n    avg_miles = mileage / (120-mths_left) * 12\n    if avg_miles <= 14000: \n        df_normal['Mileage_cat'].loc[i] = 'Low'\n    elif avg_miles > 14000 and avg_miles <= 24000:\n        df_normal['Mileage_cat'].loc[i] = 'Normal'\n    elif avg_miles > 24000:\n        df_normal['Mileage_cat'].loc[i] = 'High'\ndf_normal"]}, {"cell_type": "code", "execution_count": 1, "id": "a61ce768", "metadata": {}, "outputs": [], "source": ["df_normal.boxplot(column='Price',by='Mileage_cat')\nplt.title('Price vs Mileage_cat',fontsize=12)\nplt.xlabel('Mileage_cat',fontsize=16)\nplt.xticks(rotation=45)\nplt.ylabel('Price',fontsize=16)\nplt.show()"]}, {"cell_type": "markdown", "id": "11914a8a", "metadata": {}, "source": ["The boxplot above shows similar median prices for all vehicles. However, there are more vehicles of higher pricing in the low and normal class. This is quite normal considering these are perceived to be better maintain. "]}, {"cell_type": "code", "execution_count": 1, "id": "ecd6412f", "metadata": {}, "outputs": [], "source": ["import warnings\ndef changing_room(df,fit_val):\n    # Code by Melquiades Ochoa from stack overflow\n    # for more information please visit the site below\n    # https://stackoverflow.com/questions/6620471/fitting-empirical-distribution-to-theoretical-ones-with-scipy-python\n\n    plt.figure(figsize=(12,8))\n    ax = df[fit_val].plot(kind='hist', bins=50, alpha=0.5)\n    \n    dataYLim = ax.get_ylim()\n\n    best_fit = best_fit_distribution(df[fit_val], 200, ax)\n    \n    plt.show()\n\n    plt.rcParams[\"figure.figsize\"] = [10,6]\n    sb.set_style(\"darkgrid\")\n    sb.set_context(\"notebook\", font_scale=1.5, rc={\"font.size\":16,\"axes.titlesize\":16,\"axes.labelsize\":16})\n    p = sb.distplot(df[fit_val],fit=best_fit,kde=True)\n    p.set_xlabel(fit_val, fontsize = 15)\n    p.set_title(best_fit.name+\" distribution of \"+fit_val)\n    #plt.show()\n    return \n\ndef best_fit_distribution(data, bins=200, ax=None):\n    # Code by Melquiades Ochoa from stack overflow\n    # for more information please visit the site below\n    # https://stackoverflow.com/questions/6620471/fitting-empirical-distribution-to-theoretical-ones-with-scipy-python\n    \"\"\"Model data by finding best fit distribution to data\"\"\"\n    # Get histogram of original data\n    y, x = np.histogram(data, bins=bins, density=True)\n    x = (x + np.roll(x, -1))[:-1] / 2.0\n\n    # Distributions to check\n    DISTRIBUTIONS = [        \n        stats.alpha,stats.anglit,stats.arcsine,stats.beta,stats.betaprime,stats.bradford,stats.burr,stats.burr12,stats.cauchy,\n        stats.chi,stats.chi2,stats.cosine,stats.dgamma,stats.dweibull,stats.erlang,stats.expon,stats.exponweib,stats.exponpow,\n        stats.fatiguelife,stats.fisk,stats.foldcauchy,stats.foldnorm,stats.f,stats.gamma,stats.genlogistic,stats.genpareto,\n        stats.genexpon, stats.genextreme, stats.gengamma, stats.genhalflogistic, stats.geninvgauss, stats.gennorm, stats.gilbrat,\n        stats.gompertz, stats.gumbel_r,stats.gumbel_l,stats.halfcauchy,stats.halfnorm,stats.halflogistic,stats.hypsecant,\n        stats.gausshyper, stats.invgamma, stats.invgauss, stats.invweibull,stats.johnsonsb,stats.johnsonsu,stats.ksone,\n        stats.kstwo, stats.kstwobign, stats.laplace, stats.levy_l,stats.levy,stats.logistic,\n        stats.loglaplace, stats.loggamma, stats.lognorm,stats.loguniform,stats.maxwell,stats.mielke,stats.nakagami,\n        stats.ncx2,stats.nct,stats.norm,stats.norminvgauss,stats.pareto,stats.lomax,stats.powerlognorm,\n        stats.powernorm,stats.powerlaw,stats.rdist,stats.rayleigh,stats.rice,stats.recipinvgauss,stats.semicircular,\n        stats.t,stats.triang,stats.truncexpon,stats.truncnorm,stats.tukeylambda,stats.uniform,stats.vonmises,\n        stats.wald,stats.weibull_max,stats.weibull_min,stats.wrapcauchy\n    ] # excluded distribution due to slow speed in fitting : stats.ncf stats.laplace_asymmetric stats.trapezoid\n\n    # Best holders\n    best_distribution = stats.norm\n    best_params = (0.0, 1.0)\n    best_sse = np.inf\n\n    # Estimate distribution parameters from data\n    for distribution in DISTRIBUTIONS:\n        \n        print('Trying out '+str(distribution.name)+' distribution')\n        # Try to fit the distribution\n        try:\n            # Ignore warnings from data that can't be fit\n            with warnings.catch_warnings():\n                warnings.filterwarnings('ignore')\n\n                # fit dist to data\n                params = distribution.fit(data)\n\n                # Separate parts of parameters\n                arg = params[:-2]\n                loc = params[-2]\n                scale = params[-1]\n\n                # Calculate fitted PDF and error with fit in distribution\n                pdf = distribution.pdf(x, loc=loc, scale=scale, *arg)\n                sse = np.sum(np.power(y - pdf, 2.0))\n\n                # if axis pass in add to plot\n                try:\n                    if ax:\n                        pd.Series(pdf, x).plot(ax=ax)\n                    end\n                except Exception:\n                    pass\n\n                # identify if this distribution is better\n                if best_sse > sse > 0:\n                    best_distribution = distribution\n                    best_params = params\n                    best_sse = sse\n                    print('This is a better fit')\n\n        except Exception:\n            pass\n    return best_distribution"]}, {"cell_type": "code", "execution_count": 1, "id": "82c6b3d9", "metadata": {}, "outputs": [], "source": ["changing_room(df_normal,'Price')"]}, {"cell_type": "markdown", "id": "fdae84e5", "metadata": {}, "source": ["We are trying to find a nearest normal distribution fit for our pricing. Here we found the Mielke distribution to be a good fit."]}, {"cell_type": "code", "execution_count": 1, "id": "3494cef3", "metadata": {}, "outputs": [], "source": ["def normal_test(df,test_val):\n    n_test_count = 0\n    print('\\n',' Normality test '.center(40,'*'))\n    shap_stat,shap_p = stats.shapiro(df[test_val])\n    DAgostino_stat, DAgostino_p = stats.normaltest(df[test_val])\n    result = stats.anderson(df[test_val])\n    print('\\n',' Shapiro-Wilk test '.center(40,'*'))\n    print((test_val).center(40,'*'))\n    print('stat=%.3f' % (shap_stat))\n    if shap_p > 0.05:\n        print('P-value : ',shap_p)\n        print('Distribution is Gaussion')\n    else:\n        print('P-value : ',shap_p)\n        print('Distribution is not Gaussion')\n        n_test_count += 1\n    print('\\n',\" D'Agostino's test \".center(40,'*'))\n    print((test_val).center(40,'*'))\n    print('stat=%.3f' % (DAgostino_stat))\n    if DAgostino_p > 0.05:\n        print('P-value : ',DAgostino_p)\n        print('Distribution is Gaussion') \n    else:\n        print('P-value : ',DAgostino_p)\n        print('Distribution is not Gaussion')\n        n_test_count += 1 \n    print('\\n',\" Anderson-Darling test \".center(40,'*'))\n    print((test_val).center(40,'*'))\n    print('stat=%.2f' % (result.statistic))\n    ad = 0\n    for i in range(len(result.critical_values)):\n        significance_level, critical_Value = result.significance_level[i], result.critical_values[i]\n        if result.statistic < critical_Value:\n            print('Approximately Normally Distributed at %.2f%% level' % (significance_level))\n        else:\n            print('Not Approximately Normally Distributed %.2f%% level' % (significance_level))\n            if n_test_count < 3 and ad == 0:\n                n_test_count += 1\n                ad += 1\n    \n    if n_test_count == 3:\n        print('Your distribution is not normal.')\n        print('You might want to try other transformation.')\n    if n_test_count == 2 or n_test_count == 1:\n        print('Your distribution is approximately normal.')\n        print('You might want to try other transformation or proceed to parametric test.')\n    if n_test_count == 0:\n        print('Your distribution is now normal.')\n        print('You can proceed to parametric test.')\n    "]}, {"cell_type": "code", "execution_count": 1, "id": "c0263e28", "metadata": {}, "outputs": [], "source": ["normal_test(df_normal,'Price')"]}, {"cell_type": "markdown", "id": "e7ef92ce", "metadata": {}, "source": ["A standard test of normality shows the current distribution is not normal. We will try to transform the data next."]}, {"cell_type": "code", "execution_count": 1, "id": "35f66689", "metadata": {}, "outputs": [], "source": ["price_transform = np.log10(df_normal.Price)"]}, {"cell_type": "code", "execution_count": 1, "id": "af40a4e6", "metadata": {}, "outputs": [], "source": ["df_normal['log_10_Price'] = price_transform"]}, {"cell_type": "code", "execution_count": 1, "id": "502b77a4", "metadata": {}, "outputs": [], "source": ["print('\\n',' Before Transformation '.center(40,'*'))\nplt.rcParams[\"figure.figsize\"] = [10,6]\nsb.set_style(\"darkgrid\")\nsb.set_context(\"notebook\", font_scale=1.5, rc={\"font.size\":16,\"axes.titlesize\":16,\"axes.labelsize\":16})\np = sb.distplot(df_normal['Price'],fit=stats.mielke,kde=True)\np.set_xlabel('Price', fontsize = 15)\np.set_title(\"Histogram distribution of Price\")\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "c164d5cf", "metadata": {}, "outputs": [], "source": ["skew_b4 = np.round(df_normal['Price'].skew(),2)\nvariance = statistics.variance(df_normal['Price'])\nstdev = statistics.stdev(df_normal['Price'])\nkurt = stats.kurtosis(df_normal['Price'], bias=False)\n\nprint('Skewness : ',skew_b4)\nprint('Varience : ',variance)\nprint('Standard deviation : ',stdev)\nprint('Kurtosis : ',kurt)"]}, {"cell_type": "code", "execution_count": 1, "id": "ee17cd3f", "metadata": {}, "outputs": [], "source": ["print('\\n',' After log 10 Transformation '.center(40,'*'))\nplt.rcParams[\"figure.figsize\"] = [10,6]\nsb.set_style(\"darkgrid\")\nsb.set_context(\"notebook\", font_scale=1.5, rc={\"font.size\":16,\"axes.titlesize\":16,\"axes.labelsize\":16})\np = sb.distplot(df_normal['log_10_Price'],fit=stats.mielke,kde=True)\np.set_xlabel('Log_10_Price', fontsize = 15)\np.set_title(\"Histogram distribution of Log 10 Price\")\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "5a6e1360", "metadata": {}, "outputs": [], "source": ["skew_b4 = np.round(df_normal['log_10_Price'].skew(),2)\nvariance = statistics.variance(df_normal['log_10_Price'])\nstdev = statistics.stdev(df_normal['log_10_Price'])\nkurt = stats.kurtosis(df_normal['log_10_Price'], bias=False)\n\nprint('Skewness : ',skew_b4)\nprint('Varience : ',variance)\nprint('Standard deviation : ',stdev)\nprint('Kurtosis : ',kurt)"]}, {"cell_type": "markdown", "id": "1f887227", "metadata": {}, "source": ["A skewness of near zero and estimation of 68 percent of the data falling within 1 st deviation suggest we may be getting a normal distribution after a log10 transformation."]}, {"cell_type": "code", "execution_count": 1, "id": "218622ba", "metadata": {}, "outputs": [], "source": ["normal_test(df_normal,'log_10_Price')"]}, {"cell_type": "markdown", "id": "67f27da4", "metadata": {}, "source": ["We did not get a standard, standard distribution but test suggest we may assume normality here. A qqplot will be able to show us more."]}, {"cell_type": "code", "execution_count": 1, "id": "86cbef3a", "metadata": {}, "outputs": [], "source": ["fig = plt.figure()\nfig.subplots_adjust(hspace=0.4)\nax1 = fig.add_subplot(211)\nprob = stats.probplot(df_normal['Price'], plot=ax1)\nax1.set_xlabel('')\nax1.set_title('Probplot against normal distribution')\nax2 = fig.add_subplot(212)\nprob = stats.probplot(df_normal['log_10_Price'], plot=ax2)\nax2.set_title('Probplot after log10 transformation')\nplt.show()\n"]}, {"cell_type": "markdown", "id": "3e6ad2d5", "metadata": {}, "source": ["We have most of the data falling almost on a straight line. We are safe to assume normality for the distribution. We will move to the next assumption for parametric test."]}, {"cell_type": "code", "execution_count": 1, "id": "b4d4fe8e", "metadata": {}, "outputs": [], "source": ["df_high = df_normal[df_normal['Mileage_cat'] == 'High']\ndf_norm = df_normal[df_normal['Mileage_cat'] == 'Normal']\ndf_low = df_normal[df_normal['Mileage_cat'] == 'Low']"]}, {"cell_type": "code", "execution_count": 1, "id": "c820cbb0", "metadata": {}, "outputs": [], "source": ["high_variance = statistics.variance(df_high['log_10_Price'])\nnorm_variance = statistics.variance(df_norm['log_10_Price'])\nlow_variance = statistics.variance(df_low['log_10_Price'])\n\nprint('High Variance : ',high_variance)\nprint('Norm Variance : ',norm_variance)\nprint('Low Variance : ',low_variance)"]}, {"cell_type": "code", "execution_count": 1, "id": "c43cdb6f", "metadata": {}, "outputs": [], "source": ["(test_statistic, p_value) = stats.levene(df_low['log_10_Price'], df_norm['log_10_Price'],df_high['log_10_Price'],center='mean')\nprint('\\n',' Levene Test '.center(40,'*'))\nprint(\"The test statistic is: \", round(test_statistic,5))\nprint(\"The p-value is: \", round(p_value,5))\nif p_value > 0.05:\n    print('The group have approximate equal variance')\nif p_value < 0.05:\n    print('The group variance is not equal')\nif p_value == 1:\n    print('The group have equal variance')"]}, {"cell_type": "markdown", "id": "742c08b6", "metadata": {}, "source": ["The group variance is not homogenious, we will need to resample from the group to achieve homogenity."]}, {"cell_type": "code", "execution_count": 1, "id": "d7c76bd9", "metadata": {}, "outputs": [], "source": ["def stratified_samples(df,num,sV):\n    num = num/100  \n    stratifiedSampling = StratifiedShuffleSplit(n_splits=1, test_size=num,random_state=3)\n    sort_value = df.select_dtypes(include=['object','int64','float64']).columns.tolist()\n    try:\n        for x, y in stratifiedSampling.split(df, df[sV]):\n            stratified_random_sample = df.iloc[y].sort_values(by=sV)\n    except:\n        print('Error - Unable to run with chosen strata')\n        return\n    stratified_random_sample.info()\n    return stratified_random_sample\n    \ndef random_samples(df,num):\n    sample = df.sample(num,random_state=1)\n    sample = sample.reset_index(drop=True)\n    return sample   "]}, {"cell_type": "code", "execution_count": 1, "id": "13888fe5", "metadata": {}, "outputs": [], "source": ["df_sample = stratified_samples(df_normal,60,'Mileage_cat')"]}, {"cell_type": "code", "execution_count": 1, "id": "1117451e", "metadata": {}, "outputs": [], "source": ["df_high = df_sample[df_sample['Mileage_cat'] == 'High']\ndf_norm = df_sample[df_sample['Mileage_cat'] == 'Normal']\ndf_low = df_sample[df_sample['Mileage_cat'] == 'Low']"]}, {"cell_type": "code", "execution_count": 1, "id": "da0ad747", "metadata": {}, "outputs": [], "source": ["(test_statistic, p_value) = stats.levene(df_low['log_10_Price'], df_norm['log_10_Price'],df_high['log_10_Price'],center='mean')\nprint('\\n',' Levene Test '.center(40,'*'))\nprint(\"The test statistic is: \", round(test_statistic,5))\nprint(\"The p-value is: \", round(p_value,5))\nif p_value > 0.05:\n    print('The group have approximate equal variance')\nif p_value < 0.05:\n    print('The group variance is not equal')\nif p_value == 1:\n    print('The group have equal variance')"]}, {"cell_type": "markdown", "id": "b26dd46d", "metadata": {}, "source": ["We have homogenious variance for our stratified samples."]}, {"cell_type": "code", "execution_count": 1, "id": "f42312be", "metadata": {}, "outputs": [], "source": ["high_variance = statistics.variance(df_high['log_10_Price'])\nnorm_variance = statistics.variance(df_norm['log_10_Price'])\nlow_variance = statistics.variance(df_low['log_10_Price'])\n\nprint('High Variance : ',high_variance)\nprint('Norm Variance : ',norm_variance)\nprint('Low Variance : ',low_variance)"]}, {"cell_type": "code", "execution_count": 1, "id": "cb690d03", "metadata": {}, "outputs": [], "source": ["df_Rsample = random_samples(df_normal,1000)"]}, {"cell_type": "code", "execution_count": 1, "id": "0c46c5fa", "metadata": {}, "outputs": [], "source": ["dfr_high = df_Rsample[df_Rsample['Mileage_cat'] == 'High']\ndfr_norm = df_Rsample[df_Rsample['Mileage_cat'] == 'Normal']\ndfr_low = df_Rsample[df_Rsample['Mileage_cat'] == 'Low']"]}, {"cell_type": "code", "execution_count": 1, "id": "ff292f59", "metadata": {}, "outputs": [], "source": ["(test_statistic, p_value) = stats.levene(dfr_low['log_10_Price'], dfr_norm['log_10_Price'],dfr_high['log_10_Price'],center='mean')\nprint('\\n',' Levene Test '.center(40,'*'))\nprint(\"The test statistic is: \", round(test_statistic,5))\nprint(\"The p-value is: \", round(p_value,5))\nif p_value > 0.05:\n    print('The group have approximate equal variance')\nif p_value < 0.05:\n    print('The group variance is not equal')\nif p_value == 1:\n    print('The group have equal variance')"]}, {"cell_type": "markdown", "id": "3dba9ef2", "metadata": {}, "source": ["There is no homogenity in variance in our random samples."]}, {"cell_type": "code", "execution_count": 1, "id": "ba8331bf", "metadata": {}, "outputs": [], "source": ["high_variance = statistics.variance(dfr_high['log_10_Price'])\nnorm_variance = statistics.variance(dfr_norm['log_10_Price'])\nlow_variance = statistics.variance(dfr_low['log_10_Price'])\n\nprint('High Variance : ',high_variance)\nprint('Norm Variance : ',norm_variance)\nprint('Low Variance : ',low_variance)"]}, {"cell_type": "code", "execution_count": 1, "id": "0dfeda7b", "metadata": {}, "outputs": [], "source": ["import statsmodels.formula.api as smf\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats.anova import anova_lm\nimport statsmodels.stats.multicomp as multi\n\nprint('\\n',' MANOVA '.center(40,'*'))\nmodel = smf.ols('log_10_Price ~ C(Mileage_cat)', data=df_sample).fit()\naov_table = anova_lm(model, typ=2)\nprint(aov_table)\nprint('\\n')\nmcTreatment    = multi.MultiComparison(df_sample['log_10_Price'], df_sample['Mileage_cat'])\nresults_Treatment  = mcTreatment.tukeyhsd()\nprint(results_Treatment.summary())\n    \ndf_sample.boxplot(column='log_10_Price',by='Mileage_cat')\nplt.title('log_10_Price vs Mileage_cat',fontsize=12)\nplt.xlabel('Mileage_cat',fontsize=16)\nplt.xticks(rotation=45)\nplt.ylabel('Log_10_Price',fontsize=16)\nplt.show()\n\nresiduals = model.resid\n#smi.qqplot(residuals, line='s')\nfig = smi.qqplot(residuals, line='s')\nplt.show()"]}, {"cell_type": "markdown", "id": "c2bdad31", "metadata": {}, "source": ["Check with a non parametric test"]}, {"cell_type": "code", "execution_count": 1, "id": "158bd42e", "metadata": {}, "outputs": [], "source": ["dfm_low = df_normal[df_normal['Mileage_cat'] == 'Low']\ndfm_norm = df_normal[df_normal['Mileage_cat'] == 'Normal']\ndfm_high = df_normal[df_normal['Mileage_cat'] == 'High']"]}, {"cell_type": "code", "execution_count": 1, "id": "3075277c", "metadata": {}, "outputs": [], "source": ["# Man Whitney test\nprint('The Mean of low mileage cars:', dfm_low.Price.mean(axis=0))\nprint('The Mean of normal mileage cars:', dfm_norm.Price.mean(axis=0))\nprint('The Mean of high mileage cars:', dfm_high.Price.mean(axis=0))\n"]}, {"cell_type": "code", "execution_count": 1, "id": "cf5f24a2", "metadata": {}, "outputs": [], "source": ["import scipy.stats as stats\n(test_statistic, p_value) = stats.mannwhitneyu(dfm_high['Price'], dfm_low['Price'], alternative='two-sided')\nprint(\"The test statistic is: \", '{:.5f}'.format(test_statistic))\nprint(\"The p-value between high and low mileage is:\",'{:.5f}'.format(p_value))\n(test_statistic, p_value) = stats.mannwhitneyu(dfm_low['Price'], dfm_norm['Price'], alternative='two-sided')\nprint(\"The test statistic is: \", '{:.5f}'.format(test_statistic))\nprint(\"The p-value between low and normal mileage is:\",'{:.5f}'.format(p_value))\n(test_statistic, p_value) = stats.mannwhitneyu(dfm_norm['Price'], dfm_high['Price'], alternative='two-sided')\nprint(\"The test statistic is: \", '{:.5f}'.format(test_statistic))\nprint(\"The p-value between normal and high mileage is:\",'{:.5f}'.format(p_value))"]}, {"cell_type": "markdown", "id": "102a70f3", "metadata": {}, "source": ["The above result suggest that in between groups, there is no significant difference in treatment for groups except for low to normal mileage cars, which cars in the low mileage category may offer a better drive condition thus, a higher asking price which is logical.\n\nBut there is a point of interest, there is no significant difference between low and high. Which is quite abnormal. We will investigate further."]}, {"cell_type": "markdown", "id": "fb0e2b73", "metadata": {}, "source": ["# Investigation"]}, {"cell_type": "code", "execution_count": 1, "id": "a709637b", "metadata": {}, "outputs": [], "source": ["df_normal[df_normal['Coe_left_mths']<24]"]}, {"cell_type": "markdown", "id": "acf9022d", "metadata": {}, "source": ["Higher mileage would be expected from cars of older age, but a check above show most car of eight years old and above mostly have low mileage clocked."]}, {"cell_type": "code", "execution_count": 1, "id": "5f2f132f", "metadata": {}, "outputs": [], "source": ["df_normal[df_normal['Model']=='Toyota Camry 2.0A']"]}, {"cell_type": "markdown", "id": "5110da2c", "metadata": {}, "source": ["We do a comparison of a particular car model of different age.\nThe oldest car in the group actually have a much higher depreciation even thou the compared price is lower."]}, {"cell_type": "code", "execution_count": 1, "id": "f8c8d095", "metadata": {}, "outputs": [], "source": ["df_normal[df_normal['Mileage_cat'] == 'High']"]}, {"cell_type": "markdown", "id": "70ae13ad", "metadata": {}, "source": ["Most car of high mileage are relatively new with an average of 2 to 3 years old."]}, {"cell_type": "code", "execution_count": 1, "id": "d09e1f26", "metadata": {}, "outputs": [], "source": ["df_normal[df_normal['Model'] == 'Kia Cerato 1.6A EX']"]}, {"cell_type": "markdown", "id": "a6627899", "metadata": {}, "source": ["A check on a popular make of mid size sedan, the depreciation from cars of different mileage group did not show much gap in between pricing."]}, {"cell_type": "code", "execution_count": 1, "id": "07c91055", "metadata": {}, "outputs": [], "source": ["df_normal[df_normal['Model'] == 'Mercedes-Benz E-Class E200 Avantgarde']"]}, {"cell_type": "markdown", "id": "a50f647b", "metadata": {}, "source": ["The price gap is much more apparent in a more expensive make. It shows a 10000 dollar difference for similar age but with a low mileage clocked."]}, {"cell_type": "markdown", "id": "1fbf34ed", "metadata": {}, "source": ["### How real are the mileage recorded in the listing?"]}, {"cell_type": "markdown", "id": "0f42a175", "metadata": {}, "source": ["The national average mileage of a private car in singapore is estimated to be 20000km per year. From the data above, we found most of our high mileage category to be form by newer cars. This does not make sense if older cars are actually clocking lower mileages. We will conduct a comparison between sub category of the car age and see what we can find from the analysis."]}, {"cell_type": "code", "execution_count": 1, "id": "9531bd01", "metadata": {}, "outputs": [], "source": ["# Creating a new segmentation of car age\ndf_normal['Age_seg'] = '0'\nfor i,v in enumerate(df_normal['Coe_left_mths']):\n    if v >= 90:\n        df_normal['Age_seg'].loc[i] = 'Near New'\n    elif v < 90 and v >= 60:\n        df_normal['Age_seg'].loc[i] = 'Upper Mid Age'\n    elif v < 60 and v >= 30:\n        df_normal['Age_seg'].loc[i] = 'Lower Mid Age'\n    else:\n        df_normal['Age_seg'].loc[i] = 'Old'"]}, {"cell_type": "code", "execution_count": 1, "id": "88bcb161", "metadata": {}, "outputs": [], "source": ["# Creating a new average_mileage_per_annum\ndf_normal['Avg_mileage'] = 0\nfor i,v in enumerate(df_normal['Mileage']):\n    df_normal['Avg_mileage'].loc[i] = v/(120-df_normal['Coe_left_mths'].loc[i])*12"]}, {"cell_type": "code", "execution_count": 1, "id": "97b7b3a6", "metadata": {}, "outputs": [], "source": ["df_type = df_normal.groupby(['Type','Age_seg'])\ndf_type['Avg_mileage'].describe()"]}, {"cell_type": "markdown", "id": "01feb807", "metadata": {}, "source": ["Points to note on the above analysis\n- Cars with less than 2.5 years on lifespan appears to be the lowest in most category car type\n- All vehicle average is at least 25 to 50 percent lesser than national average"]}, {"cell_type": "markdown", "id": "00ebbce2", "metadata": {}, "source": ["# Conclusion"]}, {"cell_type": "markdown", "id": "012616b1", "metadata": {}, "source": ["A check with a car dealer, they confirm that depending on current COE trend, used car price may differ. But usually cars of older age lesser than 3 years will have a higher depreciation due to its affordability. As lower cash outlay is usually required, thus the high depreciation on average.\n\nHere we conclude that in a case of singapore used car market, mileage affects pricing in car in different car age segment. It may be applicable to newer cars or some specific higher end models. For bread and butter cars, the difference is rather insignificant.\n\nOr that the mileage in the list may not be true due human error. We will need more factual data to ascertain this claim."]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}