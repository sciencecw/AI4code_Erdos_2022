{"cells": [{"cell_type": "markdown", "id": "89782c6c", "metadata": {}, "source": ["<center><h1 class=\"list-group-item list-group-item-success\">HR Analytics: Job Change Prediction</h1></center>\n\n<br>\n<center><img src = \"https://www.realmediahub.com/wp-content/uploads/2021/08/buzz.jpg\"></center>\n\n## Problem Statement<br>\n<font size = 4>   \nA company which is active in Big Data and Data Science wants to hire data scientists among people who successfully pass some courses which was conducted by the company. Many people signup for their training. Company wants to know which of these candidates really wants to work for the company after training or looking for a new employment because it helps to reduce the cost and time as well as the quality of training or planning the courses and categorization of candidates.\n\nUsing the demographics, education, experience data, predict the probability of a candidate to look for a new job or will work for the company using machine learning techniques, as well as interpreting affected factors on employee decision. </font>\n    \n## Team Members <br>\n  \n<font size = 4>\n1.   L Vishal (E0119010) <br><br> \n2.   V Karan (E0119039)<br><br>\n3.   M Sathishkumar (E0119052)<br>\n    \n    \n## Contents<br>\n<font size = 4 color = \"blue\">\n<li>Importing Packages</li><br>\n<li>Importing Data</li><br>\n<li>Data Overview</li><br>\n<li>Exploratory Data Analysis</li><br>\n<li>Data Preprocessing</li><br>\n<li>Data Upscaling</li><br>\n<li>CO1 - Model Selection and Performance Comparison</li>\n    <ul>\n        <li>1.1 Investigation on Base Learners</li>\n        <li>1.2 Metrics & Measures</li>\n        <li>1.3 Merits of Demerits of Base Learners</li>\n    </ul><br>\n\n<li>CO2 - Ensembling & Tree based Learning Methods</li>\n     <ul>\n        <li>2.1 Identification of Meta Learners</li>\n        <li>2.2 Ensemble Design Techniques</li>\n    </ul><br>\n    \n<li>CO3 - Feature Selection</li>\n     <ul>\n        <li>3.1 Feature Selection & Extraction</li>\n        <li>3.2 Dimensionality Reduction</li>\n        <li>3.3 Discussion on Results</li>\n    </ul><br>\n    \n<li>CO4 - Artificial Neural Network</li>\n    <ul>\n        <li>4.1 Development of ANN</li>\n        <li>4.2 Ensembling of ANN</li>\n        <li>4.3 Discussion on Results</li>\n    </ul><br>\n<li>CO5 - Model Selection and Performance comparison</li>\n     <ul>\n        <li>5.1 Report of Results & Discussions</li>\n        <li>5.2 Project outcome</li>\n    </ul><br>"]}, {"cell_type": "markdown", "id": "cb6f70b2", "metadata": {}, "source": ["# Importing Packages"]}, {"cell_type": "code", "execution_count": 1, "id": "b0d1774c", "metadata": {}, "outputs": [], "source": ["import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport category_encoders as ce\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nfrom sklearn.metrics import accuracy_score,roc_auc_score, confusion_matrix, classification_report,precision_score,recall_score,plot_roc_curve,log_loss\nfrom sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,StackingClassifier,VotingClassifier,BaggingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.svm import SVC\nfrom xgboost import plot_tree,plot_importance\nimport statistics as st\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nimport tensorflow as tf\nimport plotly.express as px\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom tensorflow.keras.regularizers import l2\nfrom sklearn.pipeline import make_pipeline\nfrom pycaret.classification import *\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")"]}, {"cell_type": "markdown", "id": "470c2caa", "metadata": {}, "source": ["# Importing Data"]}, {"cell_type": "code", "execution_count": 1, "id": "816c36d8", "metadata": {}, "outputs": [], "source": ["# Reading the data using read_csv function from pandas\ndf = pd.read_csv(\"../input/hr-analytics-job-change-of-data-scientists/aug_train.csv\")"]}, {"cell_type": "markdown", "id": "a26a4a21", "metadata": {}, "source": ["# Data Overview"]}, {"cell_type": "code", "execution_count": 1, "id": "f98cd51d", "metadata": {}, "outputs": [], "source": ["# Random records\ndf.sample(5)"]}, {"cell_type": "markdown", "id": "82e1575a", "metadata": {}, "source": ["## Features<br>\n<font size = 4 color = \"blue\">\n<li>Enrollee_id : Unique ID for candidate</li><br>\n\n<li>city: City code</li><br>\n\n<li>city_ development _index : Developement index of the city (scaled)</li><br>\n\n<li>gender: Gender of candidate</li><br>\n\n<li>relevent_experience: Relevant experience of candidate</li><br>\n\n<li>enrolled_university: Type of University course enrolled if any</li><br>\n\n<li>education_level: Education level of candidate</li><br>\n\n<li>major_discipline :Education major discipline of candidate</li><br>\n\n<li>experience: Candidate total experience in years</li><br>\n\n<li>company_size: No of employees in current employer's company</li><br>\n\n<li>company_type : Type of current employer</li><br>\n\n<li>lastnewjob: Difference in years between previous job and current job</li><br>\n\n<li>training_hours: training hours completed</li><br>\n\n<li>target: 0 \u2013 Not looking for job change, 1 \u2013 Looking for a job change</li><br>"]}, {"cell_type": "code", "execution_count": 1, "id": "2f63db50", "metadata": {}, "outputs": [], "source": ["# Shape of the dataset\nprint(\"Dataset Shape :\",df.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "f9379cec", "metadata": {}, "outputs": [], "source": ["# Returns Null counts & Data Types for all features\ndf.info()"]}, {"cell_type": "code", "execution_count": 1, "id": "035c2ca4", "metadata": {}, "outputs": [], "source": ["# Missing Values \ndf.isnull().sum()"]}, {"cell_type": "markdown", "id": "c23d0efa", "metadata": {}, "source": ["### More NA Values \ud83d\ude2b"]}, {"cell_type": "code", "execution_count": 1, "id": "bfdeabc4", "metadata": {}, "outputs": [], "source": ["# Missing  Values Percentage\npercent_missing = df.isnull().sum() * 100 / len(df)\nmissing_value_df = pd.DataFrame({'column_name': df.columns,\n                                 'percent_missing': percent_missing})\n\nplt.figure(figsize = (18,8))\nplt.rcParams.update({'font.size': 15})\nplt.xticks(rotation=90)\nplt.title(\"Missing Value Analysis\")\nplt.xlabel(\"Features\")\nplt.ylabel(\"% of missing values\")\nplt.bar(missing_value_df[\"column_name\"],missing_value_df[\"percent_missing\"],color=(0.1, 0.1, 0.1, 0.1),edgecolor='blue')"]}, {"cell_type": "markdown", "id": "956aefdb", "metadata": {}, "source": ["# Exploratory Data Analaysis"]}, {"cell_type": "code", "execution_count": 1, "id": "2536c1d1", "metadata": {}, "outputs": [], "source": ["# Splitting up numerical & categorical columns\ndf_categorical = df.select_dtypes(include='object')\ndf_numerical = df.select_dtypes(exclude='object')"]}, {"cell_type": "code", "execution_count": 1, "id": "68abd39c", "metadata": {}, "outputs": [], "source": ["# Returns five point summary for  numerical columns\ndf_numerical.describe()"]}, {"cell_type": "code", "execution_count": 1, "id": "16a2a813", "metadata": {}, "outputs": [], "source": ["# Returns summary statistics for categorical columns\ndf_categorical.describe()"]}, {"cell_type": "markdown", "id": "5b2aabca", "metadata": {}, "source": ["### Distribution of different educational levels"]}, {"cell_type": "code", "execution_count": 1, "id": "8763400b", "metadata": {}, "outputs": [], "source": ["import plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\nel = df['education_level'].value_counts().reset_index()\nel.columns = [\n    'education_level', \n    'percent'\n]\nel['percent'] /= len(df)\n\nfig = px.pie(\n    el, \n    names='education_level', \n    values='percent', \n    title='Education_level', \n    width=800,\n    height=500 \n)\n\nfig.show()"]}, {"cell_type": "markdown", "id": "0460c74e", "metadata": {}, "source": ["### Countplots with respect to educational level \n\n#### This dataset contains 5 education level:<br>\n\n<li>Graduate<br></li>\n<li>Masters<br></li>\n<li>High School<br></li>\n<li>PhD<br></li>\n<li>Primary School</li>"]}, {"cell_type": "code", "execution_count": 1, "id": "1b88a90c", "metadata": {}, "outputs": [], "source": ["#Countplots showing the frequency of each category with respect to education level \n\nplt.figure(figsize=[15,35])\nplot=[\"relevent_experience\", \"education_level\",\"major_discipline\", \"experience\",\"company_size\",\"company_type\",\"target\"]\nn=1\nfor f in plot:\n    plt.subplots_adjust(hspace=2)\n    plt.subplot(7,1,n)\n    \n    sns.countplot(x=f, hue='education_level', edgecolor=\"black\", alpha=0.7, data=df)\n    sns.despine()\n    plt.title(\"Countplot of {}  by education_level\".format(f))\n    n=n+1\nplt.tight_layout()\nplt.show()"]}, {"cell_type": "markdown", "id": "1ea14217", "metadata": {}, "source": ["### Pairplot with numerical values"]}, {"cell_type": "code", "execution_count": 1, "id": "343e50e9", "metadata": {}, "outputs": [], "source": ["sns.pairplot(df, hue ='target')"]}, {"cell_type": "markdown", "id": "7f62be69", "metadata": {}, "source": ["### Distribution of experience of employees"]}, {"cell_type": "code", "execution_count": 1, "id": "0e77b86d", "metadata": {}, "outputs": [], "source": ["import plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\nep = df['experience'].value_counts().reset_index()\nep.columns = [\n    'experience', \n    'percent'\n]\nep['percent'] /= len(df)\n\nfig = px.pie(\n    ep, \n    names='experience', \n    values='percent', \n    title='Experience', \n    width=800,\n    height=500 \n)\n\nfig.show()"]}, {"cell_type": "markdown", "id": "a6e5d067", "metadata": {}, "source": ["### Distribution of Training Hours"]}, {"cell_type": "code", "execution_count": 1, "id": "004ce119", "metadata": {}, "outputs": [], "source": ["# Taining_hours\nf, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(df[\"training_hours\"], color=\"blue\",ax = axes)\nplt.title(\"Distributional of training_hours\")"]}, {"cell_type": "markdown", "id": "dbba0843", "metadata": {}, "source": ["### Education Level Vs Training Hours"]}, {"cell_type": "code", "execution_count": 1, "id": "ac82f1ad", "metadata": {}, "outputs": [], "source": ["# education_level:training_hours\net = df.sort_values(by='training_hours', ascending=True)[:7000]\nfigure = plt.figure(figsize=(10,6))\nsns.barplot(y=et.education_level, x=et.training_hours)\nplt.xticks()\nplt.xlabel('training_hours')\nplt.ylabel('education_level')\nplt.title('education_level:training_hours ')\nplt.show()"]}, {"cell_type": "markdown", "id": "fe9bc217", "metadata": {}, "source": ["## Essential Functions"]}, {"cell_type": "code", "execution_count": 1, "id": "c0a1c4af", "metadata": {}, "outputs": [], "source": ["def show_me_details(df):\n    print(\"Shape of the Dataframe \",df.shape)\n    print(\"\\n\")\n    print(\"Null Value Counts of the features in Dataframe \\n\\n\",df.isna().sum())\n    print(\"\\n\")\n    print(\"Datatypes of the features in Dataframe \\n\\n\",df.dtypes)"]}, {"cell_type": "markdown", "id": "80c7a397", "metadata": {}, "source": ["# Data Preprocessing "]}, {"cell_type": "code", "execution_count": 1, "id": "aa8560fa", "metadata": {}, "outputs": [], "source": ["# Removing outlier in expereience feature \ndef clean_experience(df):\n    for i in df[\"experience\"]:\n        if(i==\">20\"):\n            df[\"experience\"][df[\"experience\"]==i]=27\n        if(i == \"<1\"):\n            df[\"experience\"][df[\"experience\"]==i]=0\n\n    df[\"experience\"] = df[\"experience\"].fillna(0)\n    df[\"experience\"] = df['experience'].astype('int')"]}, {"cell_type": "code", "execution_count": 1, "id": "25f2cfeb", "metadata": {}, "outputs": [], "source": ["# Filling NA with new class (\"unknown\")\ndef clean_NAN(df):\n    df[\"gender\"] = df[\"gender\"].fillna(\"Unknown\")\n    df[\"education_level\"]=df[\"education_level\"].fillna(\"Unknown\")\n    df[\"major_discipline\"].fillna(value=\"Unknown\", inplace=True)\n    df[\"experience\"] = df[\"experience\"].fillna(df[\"experience\"].mean())\n    df[\"company_type\"] = df[\"company_type\"].fillna(\"Unknown\")"]}, {"cell_type": "markdown", "id": "ef0ff8e1", "metadata": {}, "source": ["### NAN Values are replaced with unknown"]}, {"cell_type": "code", "execution_count": 1, "id": "67889d67", "metadata": {}, "outputs": [], "source": ["# Changing range value to fixed integers in company_size feature\ndef clean_company_size_1(df):\n    converted_list_1 = []\n    converted_list_2 = []\n    converted_list_3 = []\n    for i in df[\"company_size\"]:\n        if i == \"10/49\":\n            i = \"10-49\"\n            converted_list_1.append(i)\n        converted_list_1.append(i)\n        if i == \"<10\":\n            i = '1-9'\n            converted_list_2.append(i)\n        converted_list_2.append(i)\n        if i == \"10000+\":\n            i = '10000-20000'\n            converted_list_3.append(i)\n        converted_list_3.append(i)\n    df[\"company_size\"]=pd.Series(converted_list_1)\n    df[\"company_size\"]=pd.Series(converted_list_2)\n    df[\"company_size\"]=pd.Series(converted_list_3)\n    df[\"company_size\"]=df[\"company_size\"].fillna(\"0-0\")\n    new = df['company_size'].str.split(\"-\", n = 1, expand = True) \n    df['company_size_min']= new[0]\n    df['company_size_max']= new[1] \n    df[\"company_size_max\"] = df['company_size_max'].astype('int')\n    df[\"company_size_min\"] = df['company_size_min'].astype('int')"]}, {"cell_type": "markdown", "id": "91f9847c", "metadata": {}, "source": ["### Cleaning company_size to attain the required format and split them into min and max company_size "]}, {"cell_type": "code", "execution_count": 1, "id": "70523f58", "metadata": {}, "outputs": [], "source": ["# Changing some classes with string values to integers\ndef clean_last_new_job(df):\n    converted_list_1 = []\n    converted_list_2 = []\n    for i in df[\"last_new_job\"]:\n        if i == \"never\" or i == np.NaN:\n            i = 0\n            converted_list_1.append(i)\n        converted_list_1.append(i)\n        if i == \">4\":\n            i = 6\n            converted_list_2.append(i)\n        converted_list_2.append(i)\n    df[\"last_new_job\"]=pd.Series(converted_list_1)\n    df[\"last_new_job\"]=pd.Series(converted_list_2)\n    df[\"last_new_job\"] = df[\"last_new_job\"].fillna(0)\n    df[\"last_new_job\"] = df[\"last_new_job\"].astype('int')"]}, {"cell_type": "markdown", "id": "f337a453", "metadata": {}, "source": ["### Cleaning company_size to attain the required format for modelling"]}, {"cell_type": "code", "execution_count": 1, "id": "022aa804", "metadata": {}, "outputs": [], "source": ["# Replacing \"city_\" in all records of city feature to get only the city name\ndef clean_city(df):\n    converted_list_1 = []\n    for i in range(len(df[\"city\"])):\n        j = df[\"city\"][i].replace(\"city_\",\"\")\n        converted_list_1.append(j)\n    df[\"city\"]=pd.Series(converted_list_1)\n    df[\"city\"] = df[\"city\"].astype('int')"]}, {"cell_type": "code", "execution_count": 1, "id": "ade900eb", "metadata": {}, "outputs": [], "source": ["# Nominal Encoding on relevent_experience feature\ndef clean_relevent_experience(df):\n    converted_list_1 = []\n    converted_list_2 = []\n    for i in df[\"relevent_experience\"]:\n        if i == \"Has relevent experience\":\n            i = 1\n            converted_list_1.append(i)\n        converted_list_1.append(i)\n        if i == \"No relevent experience\":\n            i = 0\n            converted_list_2.append(i)\n        converted_list_2.append(i)\n    df[\"relevent_experience\"]=pd.Series(converted_list_1)\n    df[\"relevent_experience\"]=pd.Series(converted_list_2)"]}, {"cell_type": "markdown", "id": "ef1f2351", "metadata": {}, "source": ["## One Hot encoding\nIt is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction."]}, {"cell_type": "code", "execution_count": 1, "id": "255e1534", "metadata": {}, "outputs": [], "source": ["# One Hot encoding base function\ndef one_hot_encoding(df,col):\n    one_hot_encoder=ce.OneHotEncoder(cols=col,return_df=True,use_cat_names=True)\n    df_final = one_hot_encoder.fit_transform(df)\n    return df_final\n\n# Applying one hot encoding on all required columns\ndef apply_one_hot_encoding(df):\n    df_final = one_hot_encoding(df,\"enrolled_university\")\n    df_final = one_hot_encoding(df_final,\"gender\")\n    df_final = one_hot_encoding(df_final,\"education_level\")\n    df_final = one_hot_encoding(df_final,\"major_discipline\")\n    df_final = one_hot_encoding(df_final,\"company_type\")\n    return df_final"]}, {"cell_type": "code", "execution_count": 1, "id": "a0e3f08f", "metadata": {}, "outputs": [], "source": ["def clean_company_size_2(df):\n    converted_list_1 = []\n    converted_list_2 = []\n    for i in df[\"company_size_min\"]:\n        if i == 0:\n            i = int(df[\"company_size_min\"].mean())\n            converted_list_1.append(i)\n        converted_list_1.append(i)\n    for i in df[\"company_size_max\"]:\n        if i == 0:\n            i = int(df[\"company_size_max\"].mean())\n            converted_list_2.append(i)\n        converted_list_2.append(i)\n    df[\"company_size_min\"]=pd.Series(converted_list_1)\n    df[\"company_size_max\"]=pd.Series(converted_list_2)\n    df[\"company_size_min\"] = df[\"company_size_min\"].fillna(int(df[\"company_size_min\"].mean()))\n    df[\"company_size_max\"] = df[\"company_size_max\"].fillna(int(df[\"company_size_max\"].mean()))"]}, {"cell_type": "code", "execution_count": 1, "id": "7f63a976", "metadata": {}, "outputs": [], "source": ["# Function to remove unwanted columns\ndef remove_columns(df):\n    df_final = df.drop(['enrollee_id','company_size'],axis=1)\n    return df_final"]}, {"cell_type": "code", "execution_count": 1, "id": "df426738", "metadata": {}, "outputs": [], "source": ["# Independent & Dependent Variables Split\ndef XY_Split(df):\n    X = df.drop(\"target\",axis=1)\n    Y = pd.DataFrame(df[\"target\"])\n    return X,Y"]}, {"cell_type": "code", "execution_count": 1, "id": "6a861d09", "metadata": {}, "outputs": [], "source": ["# Fuction to apply all transoformations applied in the above functions\ndef clean_data(df):\n    clean_experience(df)\n    clean_NAN(df)\n    clean_company_size_1(df)\n    clean_last_new_job(df)\n    clean_city(df)\n    clean_relevent_experience(df)\n    df_one_hot = apply_one_hot_encoding(df)\n    clean_company_size_2(df_one_hot)\n    df_final = remove_columns(df_one_hot)\n    X,Y = XY_Split(df_final)\n    return df_final,X,Y"]}, {"cell_type": "code", "execution_count": 1, "id": "927cb731", "metadata": {}, "outputs": [], "source": ["# Applying the  function for preprocessing for all features\ndf_final,X,Y = clean_data(df)"]}, {"cell_type": "code", "execution_count": 1, "id": "ebf6f8c9", "metadata": {}, "outputs": [], "source": ["# New Cleaned Dataframe\ndf_final.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "34bed26c", "metadata": {}, "outputs": [], "source": ["# Cleaned DF Overview\nshow_me_details(X)"]}, {"cell_type": "markdown", "id": "1c42d4b2", "metadata": {}, "source": ["<center><br><img src = \"https://i.pinimg.com/originals/75/05/dc/7505dccdc71025d0db695e07703b83c3.gif\"><br></center>\n<center><font size = 6 color = \"red\">\u2728 Data Cleaning completed successfully \u2728</font></center>"]}, {"cell_type": "markdown", "id": "e3ce3f2b", "metadata": {}, "source": ["### Target Feature Analysis"]}, {"cell_type": "code", "execution_count": 1, "id": "f8cbedd4", "metadata": {}, "outputs": [], "source": ["# 0 \u2013 Not looking for job change,\n# 1 \u2013 Looking for a job change\nmnj = df['target'].value_counts()  \nplt.figure(figsize=(6,4))\nsns.barplot(mnj.index, mnj.values, alpha=0.8)\nplt.ylabel('Number of Data', fontsize=12)\nplt.xlabel('target', fontsize=9)\nplt.xticks(rotation=90)\nprint(mnj)\nplt.show();"]}, {"cell_type": "markdown", "id": "6f0d8382", "metadata": {}, "source": ["<center><img src = \"https://media3.giphy.com/media/S3gzCODk6Yz72/giphy.gif\">"]}, {"cell_type": "markdown", "id": "cb5d040b", "metadata": {}, "source": ["<font size = 3>As you can see, here we have imbalanced data, the number of 1 ( Looking for a job change) is around 20% of the whole data < 0 (Not looking for job change) is around 80% of the whole data, we have to upscale them accordinly to maintain a balanced state and to overcome the bias of model</font>"]}, {"cell_type": "markdown", "id": "17829892", "metadata": {}, "source": ["# Data Upscaling"]}, {"cell_type": "markdown", "id": "f0f1d3d1", "metadata": {}, "source": ["<center><img src = \"https://www.researchgate.net/profile/Abedalrhman-Alkhateeb/publication/339907291/figure/fig4/AS:962470881554444@1606482348377/Synthetic-Minority-Oversampling-Technique-SMOTE-works-by-adding-new-synthetic-sample.png\" width = 600></center>\n           <br><font size = 3>SMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line."]}, {"cell_type": "code", "execution_count": 1, "id": "96485306", "metadata": {}, "outputs": [], "source": ["# Upsmapling\nsmote = SMOTE()\nX, Y = smote.fit_resample(X, Y)"]}, {"cell_type": "code", "execution_count": 1, "id": "7f1c44eb", "metadata": {}, "outputs": [], "source": ["# Syntethetics rows generated\nX.tail()"]}, {"cell_type": "code", "execution_count": 1, "id": "108fb82e", "metadata": {}, "outputs": [], "source": ["# Class Balanced\n# 0 \u2013 Not looking for job change,\n# 1 \u2013 Looking for a job change\nmnj = Y['target'].value_counts()  \nplt.figure(figsize=(6,4))\nsns.barplot(mnj.index, mnj.values, alpha=0.8)\nplt.ylabel('Number of Data', fontsize=12)\nplt.xlabel('target', fontsize=9)\nplt.xticks(rotation=90)\nprint(mnj)\nplt.show();"]}, {"cell_type": "code", "execution_count": 1, "id": "2f0bbd3f", "metadata": {}, "outputs": [], "source": ["# Cleaned DF Overview after applying SMOTE\nshow_me_details(X)"]}, {"cell_type": "markdown", "id": "44a2fe04", "metadata": {}, "source": ["### Exporting cleaned Data"]}, {"cell_type": "code", "execution_count": 1, "id": "23d2f55a", "metadata": {}, "outputs": [], "source": ["df_final = X.copy()\ndf_final['target'] = Y\n\n# Exporting Data for preventing any data loss\n# df_final.to_csv(\"../Output/Cleaned.csv\",index = False)\ndf_final.to_csv(\"Cleaned.csv\",index = False)"]}, {"cell_type": "markdown", "id": "32f1e8ff", "metadata": {}, "source": ["## Feature Scaling"]}, {"cell_type": "code", "execution_count": 1, "id": "4996a3a5", "metadata": {}, "outputs": [], "source": ["# Normalizing the independent features\nscaler_x = MinMaxScaler()\nX = scaler_x.fit_transform(X)"]}, {"cell_type": "markdown", "id": "bbb33598", "metadata": {}, "source": ["## CO1 - Model Selection and Performance Comparison"]}, {"cell_type": "markdown", "id": "ff67fd74", "metadata": {}, "source": ["## Train Test Split"]}, {"cell_type": "code", "execution_count": 1, "id": "9dc5cbff", "metadata": {}, "outputs": [], "source": ["# Train Test Split\nX_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.20,stratify=Y)\n\n# Taking copy to prevent data loss\nX_test_copy = X_test.copy()"]}, {"cell_type": "code", "execution_count": 1, "id": "f5888397", "metadata": {}, "outputs": [], "source": ["def metrics(Y_test,Y_pred):\n    print(\"\\n\")\n    print(\"Accuracy :\",accuracy_score(Y_pred,Y_test))\n    print(\"\\n\")\n    print(\"Precision Score : \",precision_score(Y_pred,Y_test))\n    print(\"\\n\")\n    print(\"Recall Score : \",recall_score(Y_pred,Y_test))\n    print(\"\\n\")\n    print(\"ROC AUC Score : \",roc_auc_score(Y_pred,Y_test))\n    print(\"\\n\")\n    print(\"Classification Report\\n\\n\",classification_report(Y_pred,Y_test))\n    print(\"\\n\")\n    cf_matrix = confusion_matrix(Y_pred,Y_test)\n    group_names = ['True Neg','False Pos','False Neg','True Pos']\n    group_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\n    group_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\n    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    print(\"Confusion Matrix\")\n    sns.heatmap(cf_matrix, annot=labels, fmt=\"\", cmap='Blues')\n    plt.plot()"]}, {"cell_type": "markdown", "id": "ca4d9461", "metadata": {}, "source": ["### 1.1 Investigation on Base Learners"]}, {"cell_type": "code", "execution_count": 1, "id": "d2ac7d29", "metadata": {}, "outputs": [], "source": ["def train_model(model,params):\n    print(\"For\",str(model).split(\"(\")[0])\n    gsc = GridSearchCV(model, params, cv=5)\n    gsc.fit(X_train,y_train)\n    gsc_best = gsc.best_estimator_\n    y_pred=gsc_best.predict(X_test)\n    y_prob = gsc_best.predict_proba(X_test)\n    print(\"\\n\")\n    print(\"Best Parameters : \", gsc.best_estimator_)\n    metrics(y_pred,y_test)\n    print(\"\\n\")"]}, {"cell_type": "markdown", "id": "47a62232", "metadata": {}, "source": ["### 1.2 Metrics & Measures"]}, {"cell_type": "code", "execution_count": 1, "id": "cb15d94e", "metadata": {}, "outputs": [], "source": ["train_model(LogisticRegression(random_state=123,solver='liblinear', penalty='l2', max_iter=5000),dict(C=np.logspace(1, 4, 10)))"]}, {"cell_type": "code", "execution_count": 1, "id": "749f45a7", "metadata": {}, "outputs": [], "source": ["train_model(KNeighborsClassifier(),{'n_neighbors': np.arange(1, 5)})"]}, {"cell_type": "code", "execution_count": 1, "id": "620e25aa", "metadata": {}, "outputs": [], "source": ["train_model(RandomForestClassifier(random_state=0),{'n_estimators': [50, 100, 200]})"]}, {"cell_type": "code", "execution_count": 1, "id": "51596ada", "metadata": {}, "outputs": [], "source": ["train_model(GaussianNB(),{'var_smoothing': np.logspace(0,-9, num=100)})"]}, {"cell_type": "code", "execution_count": 1, "id": "c714b646", "metadata": {}, "outputs": [], "source": ["train_model(DecisionTreeClassifier(),{'max_depth': np.arange(1, 25)})"]}, {"cell_type": "markdown", "id": "73d5e536", "metadata": {}, "source": ["# CO2 - Ensembling & Tree based Learning Methods"]}, {"cell_type": "markdown", "id": "fdb7ca7b", "metadata": {}, "source": ["## 2.1 Identification of Meta Learners"]}, {"cell_type": "code", "execution_count": 1, "id": "1094ec32", "metadata": {}, "outputs": [], "source": ["n=int(input(\"Number of models ensembles :\"))\nprint(\"Models You can select \ud83d\ude0e:\")\nprint(\"\"\"\n1. knn : KnearestNeighbours Classifier\n2. DT  : DecisionTrees Classifier\n3. LOR : LogisticRegression Classifier\n4. NB  : NaiveBayes Classifier\n5. RF  : RandomForest Classifier\n\n\"\"\")\nmodels=[]\nfor i in range(n):\n  models.append(input('Model {}: '.format(i+1)))"]}, {"cell_type": "code", "execution_count": 1, "id": "834da5bf", "metadata": {}, "outputs": [], "source": ["# Hyper parameters for the Mixed_models\nHyper_parameters={\n     \"knn\" : 'n_neighbors',\n     \"DT\"  : 'max_depth',\n     \"LOR\" : 'C',\n     \"NB\"  : 'var_smoothing',\n     \"RF\"  : 'n_estimators'\n}"]}, {"cell_type": "code", "execution_count": 1, "id": "14be0fc5", "metadata": {}, "outputs": [], "source": ["# Mixed modelling\ndef Ensembling_MixedModelling(learners,hyper_parameters,X_train,y_train):\n  estimators=[]\n  for model in learners:\n    if(model=='knn' ):\n      knn = KNeighborsClassifier()\n      params_knn = {hyper_parameters[model]: np.arange(1, 25)}\n      knn_gs = GridSearchCV(knn, params_knn, cv=5)\n      knn_gs.fit(X_train, y_train)\n      knn_best = knn_gs.best_estimator_\n      estimators.append((model,knn_best))\n    if(model=='DT'):\n      DT = DecisionTreeClassifier()\n      params_DT = {hyper_parameters[model]: np.arange(1, 25)}\n      DT_gs = GridSearchCV(DT, params_DT, cv=5)\n      DT_gs.fit(X_train, y_train)\n      DT_best = DT_gs.best_estimator_\n      estimators.append((model,DT_best))\n    if(model=='LOR'):\n      log_reg = LogisticRegression(random_state=123,\n      solver='liblinear', penalty='l2', max_iter=5000)\n      params_LOR = {hyper_parameters[model]:np.logspace(1, 4, 10)}\n      LOR_gs = GridSearchCV(log_reg, params_LOR, cv=5)\n      LOR_gs.fit(X_train, y_train)\n      LOR_best = LOR_gs.best_estimator_\n      estimators.append((model,LOR_best))\n    if(model=='RF'):\n      RF = RandomForestClassifier(random_state=0)\n      params_RF = {hyper_parameters[model]: [50, 100, 200]}\n      RF_gs = GridSearchCV(RF, params_RF, cv=5)\n      RF_gs.fit(X_train, y_train)\n      RF_best = RF_gs.best_estimator_\n      estimators.append((model,RF_best))\n    if(model=='NB'):\n      NB=GaussianNB()\n      params_NB = {hyper_parameters[model]: np.logspace(0,-9, num=100)}\n      NB_gs = GridSearchCV(NB, params_NB, cv=5)\n      NB_gs.fit(X_train, y_train)\n      NB_best = NB_gs.best_estimator_\n      estimators.append((model,NB_best))\n  ensembles = VotingClassifier(estimators, voting='soft')\n  ensembles.fit(X_train, y_train)\n  metrics(y_test,ensembles.predict(X_test))\n#   sns.heatmap(confusion_matrix(y_test,ensembles.predict(X_test)),annot=True)\n  return ensembles,estimators"]}, {"cell_type": "code", "execution_count": 1, "id": "43cef140", "metadata": {}, "outputs": [], "source": ["Ensemble_model,estimators=Ensembling_MixedModelling(models,Hyper_parameters,X_train,y_train)"]}, {"cell_type": "markdown", "id": "2c2c0dc1", "metadata": {}, "source": ["## 2.2 Ensemble Design Techniques"]}, {"cell_type": "markdown", "id": "4cd0fba8", "metadata": {}, "source": ["### Bagging"]}, {"cell_type": "code", "execution_count": 1, "id": "84e2a38c", "metadata": {}, "outputs": [], "source": ["def Ensembling_bagging(base_estimator) :\n  if(base_estimator==\"log_reg\"):\n    model=BaggingClassifier(base_estimator=log_reg,random_state=1)\n    params_Bagging = {'n_estimators':np.arange(1,15)}\n    Bagging_gs = GridSearchCV(model, params_Bagging, cv=5)\n    Bagging_gs.fit(X_train, y_train)\n    Bagging_best = Bagging_gs.best_estimator_\n    model_score=Bagging_best.fit(X_train,y_train)\n    metrics(y_test,Bagging_best.predict(X_test))\n    \n  elif(base_estimator==\"Ensembled_model\"):\n    model= VotingClassifier(estimators)\n    params_Bagging = {'voting':['soft','hard']}\n    Bagging_gs = GridSearchCV(model, params_Bagging, cv=5)\n    Bagging_gs.fit(X_train, y_train)\n    Bagging_best = Bagging_gs.best_estimator_\n    model_score=Bagging_best.fit(X_train,y_train)\n    metrics(y_test,Bagging_best.predict(X_test))\n\n  elif(base_estimator==\"DT\"):\n    DT = DecisionTreeClassifier()\n    params_DT = {'max_depth': np.arange(1, 25)}\n    DT_gs = GridSearchCV(DT, params_DT, cv=5)\n    DT_gs.fit(X_train, y_train)\n    DT_best = DT_gs.best_estimator_\n    model=BaggingClassifier(base_estimator=DT_best,n_estimators=20,random_state=1)\n    model.fit(X_train, y_train)\n    metrics(y_test,DT_best.predict(X_test))\n\n      \n  elif(base_estimator==\"RF\"):\n    RF = RandomForestClassifier(random_state=0)\n    params_RF = {'n_estimators': [50, 100, 200]}\n    RF_gs = GridSearchCV(RF, params_RF, cv=5)\n    RF_gs.fit(X_train, y_train)\n    RF_best = RF_gs.best_estimator_\n    model=BaggingClassifier(base_estimator=RF_best,n_estimators=20,random_state=1)\n    model.fit(X_train, y_train)\n    metrics(y_test,RF_best.predict(X_test))\n"]}, {"cell_type": "code", "execution_count": 1, "id": "236e78cc", "metadata": {}, "outputs": [], "source": ["print(\"Base estimators You can select for ensembling using bagging \ud83d\ude0e:\")\nprint(\"\"\"\n1. Ensembled_model : Mixed_Modelling Classifier\n2. DT  : DecisionTrees Classifier\n3. log_reg : LogisticRegression Classifier\n4. RF  : RandomForest Classifier\n\n\"\"\")\nEnsembling_bagging(input('base_estimator : '))"]}, {"cell_type": "markdown", "id": "3bb57a28", "metadata": {}, "source": ["### Boosting"]}, {"cell_type": "code", "execution_count": 1, "id": "724eca4c", "metadata": {}, "outputs": [], "source": ["def Ensembling_boosting(Type,X_train,y_train):\n  if(Type==\"AdaBoost\"):\n    model = AdaBoostClassifier()\n    model.fit(X_train,y_train)\n    print(\"Applying cross validation\")\n    # evaluate the model\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    n_scores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n    # report performance\n    metrics(y_test,model.predict(X_test))\n\n  if(Type==\"Xgboost\"):\n    xg_model = XGBClassifier()\n    xg_model.fit(X_train, y_train)\n    xg_model.fit(X_train,y_train)\n    plot_tree(xg_model, num_trees=5, rankdir='LR')\n    fig = plt.gcf()\n    fig.set_size_inches(40, 40)\n    # evaluate the model\n    print(\"Applying cross validation\")\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    n_scores = cross_val_score(xg_model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n    # report performance\n    metrics(y_test,xg_model.predict(X_test))"]}, {"cell_type": "code", "execution_count": 1, "id": "ed7ec777", "metadata": {}, "outputs": [], "source": ["print(\n    \"\"\"\n    1.AdaBoost-Adaptive boosting algorithum\n    2.XgBoost-Extreme gradient boosting algorithum\n    \"\"\"\n)\nEnsembling_boosting(input(\"Which type of boosting you need \ud83e\udd17 : \"),X_train,y_train)"]}, {"cell_type": "markdown", "id": "2706ba94", "metadata": {}, "source": ["# CO3 - Dimensionality Reduction"]}, {"cell_type": "markdown", "id": "5579406f", "metadata": {}, "source": ["## 3.1 Feature Selection & Extraction"]}, {"cell_type": "code", "execution_count": 1, "id": "73f519ce", "metadata": {}, "outputs": [], "source": ["# Based on CHI Square Value, it interrupts the dependency of feature with dependent feature\nbestfeatures = SelectKBest(score_func=chi2, k=10)\nX_new = df_final.drop(\"target\",axis = 1)\nY_new = df_final[\"target\"]\nfit = bestfeatures.fit(X_new,Y_new)\ndfscores = pd.DataFrame(fit.scores_,columns = [\"Score\"])\ndfcolumns = pd.DataFrame(X_new.columns)\nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.nlargest(10,'Score')"]}, {"cell_type": "code", "execution_count": 1, "id": "a1be6884", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize = (12,10))\nsns.heatmap(df_final.corr())"]}, {"cell_type": "markdown", "id": "d2ac758c", "metadata": {}, "source": ["## 3.2 Dimensionality Reduction"]}, {"cell_type": "markdown", "id": "06cbec92", "metadata": {}, "source": ["## What is PCA\nPrincipal Component Analysis popularly known as PCA is a traditional statistical dimensionality reduction technique that helps us identify as the name suggests, most important (key) features/factors that define the variability in the data. Often in our datasets we have correlations between the columns, either small or large. PCA finds linear combinations of these columns to create uncorrelated columns which are independent of each other.\n\nPCA converts the data into a transformed dataset whose columns are a linear combination of the columns of the original dataset such that they are independent (correlation between columns is zero).\n\nThe principal component transformation consists of three parts \n\n- translation (the center of the data comes to the origin (0,0)),\n- rotation (the principal components are perpendicular to each other) and\n- scaling (principal components are not in the same range as is the original data)"]}, {"cell_type": "code", "execution_count": 1, "id": "95a14f00", "metadata": {}, "outputs": [], "source": ["# With N Components = 2\npca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(X)\nprincipalDf = pd.DataFrame(data = principalComponents, columns = [f'principal component {i}' for i in range(2)])\n# Explained Variance Ratio\npca.explained_variance_ratio_*100"]}, {"cell_type": "code", "execution_count": 1, "id": "27b69bb5", "metadata": {}, "outputs": [], "source": ["# Visualising the features extracted from PCA\ndef draw_vector(v0, v1, ax=None):\n    ax = ax or plt.gca()\n    arrowprops=dict(arrowstyle='->',\n                    linewidth=2,\n                    shrinkA=0, shrinkB=0)\n    # ax.annotate('', v1, v0, arrowprops=arrowprops)\n\n# plot data\nplt.scatter(X[:, 0], X[:, 1], alpha=0.2)\nfor length, vector in zip(pca.explained_variance_, pca.components_):\n    v = vector * 3 * np.sqrt(length)\n    draw_vector(pca.mean_, pca.mean_ + v,None)\nplt.axis('equal');"]}, {"cell_type": "code", "execution_count": 1, "id": "b3f2dbdd", "metadata": {}, "outputs": [], "source": ["# With N Components = 9\npca = PCA(n_components=9)\nprincipalComponents = pca.fit_transform(X)\nprincipalDf = pd.DataFrame(data = principalComponents\n             , columns = [f'principal component {i}' for i in range(9)])\n# Explained Variance Ratio\npca.explained_variance_ratio_*100"]}, {"cell_type": "code", "execution_count": 1, "id": "551965e3", "metadata": {}, "outputs": [], "source": ["# Train Test Split on PCA feature extracted data\nxtrain,xtest,ytrain,ytest = train_test_split(principalDf,Y,test_size=0.2,random_state=0)"]}, {"cell_type": "code", "execution_count": 1, "id": "610414e8", "metadata": {}, "outputs": [], "source": ["# Function to train models after applying PCA\ndef train_model_red(model,xtrain,ytrain,xtest,ytest,n):\n    pca = PCA(n_components=n)\n    principalComponents = pca.fit_transform(X)\n    principalDf = pd.DataFrame(data = principalComponents, columns = [f'principal component {i}' for i in range(n)])\n    xtrain,xtest,ytrain,ytest = train_test_split(principalDf,Y,test_size=0.2,random_state=0)\n    model.fit(xtrain,ytrain)\n    return model.score(xtest,ytest)"]}, {"cell_type": "code", "execution_count": 1, "id": "6e0ecbe0", "metadata": {}, "outputs": [], "source": ["# Training Logistic Regression model with different PCA components\nscores = []\nfor i in range(1,36):\n    scores.append(train_model_red(LogisticRegression(),xtrain,ytrain,xtest,ytest,i))\nplt.figure(figsize = (25,6))\nplt.xlabel(\"PCA N Components\")\nplt.xlabel(\"Accuracy\")\nplt.title(\"For Logistic Regression\")\nplt.plot(scores)"]}, {"cell_type": "code", "execution_count": 1, "id": "ae403145", "metadata": {}, "outputs": [], "source": ["# Training Decision Tree Classifier model with different PCA components\nscores = []\nfor i in range(1,36):\n    scores.append(train_model_red(DecisionTreeClassifier(),xtrain,ytrain,xtest,ytest,i))\n\nplt.figure(figsize = (25,6))\nplt.xlabel(\"PCA N Components\")\nplt.xlabel(\"Accuracy\")\nplt.title(\"For Decision Tree Classifier\")\nplt.plot(scores)"]}, {"cell_type": "markdown", "id": "514acafd", "metadata": {}, "source": ["## Inference\nPCA is not really required for this data since the variance is too low to make a significant difference by reducing the dimension of the data since the data is not linearly separable."]}, {"cell_type": "markdown", "id": "b70a04bb", "metadata": {}, "source": ["## LDA\n\nLinear discriminant analysis (LDA) is a statistical method for categorizing data; it assumes that the features come from populations with different means (called groups), and uses this assumption to identify which features differentiate among these groups. Unlike other methods of classification, LDA may be able to handle classes where some members have similar attributes."]}, {"cell_type": "code", "execution_count": 1, "id": "66c2e2d6", "metadata": {}, "outputs": [], "source": ["# Initilaizing LDA with n_compoenents = 1\nlda = LDA(n_components=1)\nX_train_lda = lda.fit_transform(X_train, y_train)\nX_test_lda = lda.transform(X_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "13a5b251", "metadata": {}, "outputs": [], "source": ["# Variance Ratio \nlda.explained_variance_ratio_"]}, {"cell_type": "markdown", "id": "6480dac3", "metadata": {}, "source": ["Since the variance ratio is 1.0 or if it is even close to 1.0"]}, {"cell_type": "markdown", "id": "1e14aee3", "metadata": {}, "source": ["## 3.3 Discussion on Results"]}, {"cell_type": "code", "execution_count": 1, "id": "d5bb7ba4", "metadata": {}, "outputs": [], "source": ["# Modelling the data from LDA into Logistic Regresion\nlr = LogisticRegression()\nlr.fit(X_train_lda,y_train)\ny_pred_lda = lr.predict(X_test_lda)\ny_prob_lda = lr.predict_proba(X_test_lda)\nmetrics(y_test,y_pred_lda)"]}, {"cell_type": "code", "execution_count": 1, "id": "b88093ab", "metadata": {}, "outputs": [], "source": ["# Modelling the data from LDA into Decision Tree Classifier\nlr = DecisionTreeClassifier()\nlr.fit(X_train_lda,y_train)\ny_pred_lda = lr.predict(X_test_lda)\ny_prob_lda = lr.predict_proba(X_test_lda)\nmetrics(y_test,y_pred_lda)"]}, {"cell_type": "markdown", "id": "67150db7", "metadata": {}, "source": ["# CO4 - Artificial Neural Network"]}, {"cell_type": "markdown", "id": "c7b601b0", "metadata": {}, "source": ["## 4.1 Development of ANN"]}, {"cell_type": "code", "execution_count": 1, "id": "59f3e09e", "metadata": {}, "outputs": [], "source": ["# Base ANN network with Dense & Dropout layers \n\n# Params\nfactor=0.0001\nrate=0.4\n\n# Model Structure\nmodel=tf.keras.models.Sequential([\n                                  tf.keras.layers.Dense(160,input_shape=(36,),activation=\"relu\",kernel_regularizer=l2(factor)),\n                                  tf.keras.layers.Dropout(rate),\n                                  tf.keras.layers.Dense(120,activation=\"relu\",kernel_regularizer=l2(factor)),\n                                  tf.keras.layers.Dropout(rate),\n                                  tf.keras.layers.Dense(80,activation='relu',kernel_regularizer=l2(factor)),\n                                  tf.keras.layers.Dropout(rate),\n                                  tf.keras.layers.Dense(40,activation='relu',kernel_regularizer=l2(factor)),\n                                  tf.keras.layers.Dropout(rate),\n                                  tf.keras.layers.Dense(20,activation='relu',kernel_regularizer=l2(factor)),\n                                  tf.keras.layers.Dropout(rate),\n                                  tf.keras.layers.Dense(10,activation='relu',kernel_regularizer=l2(factor)),\n                                  tf.keras.layers.Dropout(rate),\n                                  tf.keras.layers.Dense(units=1, activation='sigmoid')])\n"]}, {"cell_type": "code", "execution_count": 1, "id": "1c62b9f2", "metadata": {}, "outputs": [], "source": ["# callback function for accuracy\nclass myCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self,epoch,logs={}):\n    if(logs.get(\"val_accuracy\")>0.95):\n      print(\"Reached the accuracy required (ie) 90%\", logs)\n      self.model.stop_training=True\ncallback=myCallback()"]}, {"cell_type": "code", "execution_count": 1, "id": "0c3e206f", "metadata": {}, "outputs": [], "source": ["# Function for plotting metrics\ndef plot_metric(history, metric):\n    train_metrics = history.history[metric]\n    val_metrics = history.history['val_'+metric]\n    epochs = range(1, len(train_metrics) + 1)\n    plt.plot(epochs, train_metrics)\n    plt.plot(epochs, val_metrics)\n    plt.title('Training and validation '+ metric)\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(metric)\n    plt.legend([\"train_\"+metric, 'val_'+metric])\n    plt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "793f77a2", "metadata": {}, "outputs": [], "source": ["# Compiling the ANN model\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"]}, {"cell_type": "code", "execution_count": 1, "id": "f7307916", "metadata": {}, "outputs": [], "source": ["# Fitting the model with callbacks & validation split\nhistory=model.fit(X_train, y_train, batch_size = 256,verbose=2, epochs = 100,callbacks=[callback],validation_split=0.25)"]}, {"cell_type": "code", "execution_count": 1, "id": "cf643fc5", "metadata": {}, "outputs": [], "source": ["# Accuracy PLot on validation & Train Data\nplot_metric(history, 'accuracy')"]}, {"cell_type": "code", "execution_count": 1, "id": "d584611f", "metadata": {}, "outputs": [], "source": ["# Metrics of base ANN model\ny_pred=model.predict(X_test).reshape(5753)>0.5\nmetrics(y_pred.astype(int),y_test)"]}, {"cell_type": "markdown", "id": "cf5b847f", "metadata": {}, "source": ["## 4.2 Ensembling of ANN\n<font size = 3>Homogeneous Ensembles is  bagging work by applying the same algorithm on all the estimators. These algorithms should not be fine-tuned -> They should be weak ! In contrast to Heterogenous ensembles we will use a large amount of estimators. Note that the datsets for this model should be separately sampled in order to guarantee independence. Furthermore, the datasets should be different for each model. This will allow us to be more precise when aggregating the results of each model. Bagging reduces variances as the sampling is truly random. Through using the ensemble itself, we can reduce the risk of over-fitting and we create a robust model. Unfortunately bagging is computationally expensive."]}, {"cell_type": "markdown", "id": "556c5424", "metadata": {}, "source": ["## Homogenous Ensembling with ANN"]}, {"cell_type": "code", "execution_count": 1, "id": "34bf9680", "metadata": {}, "outputs": [], "source": ["# Params\nlearning_rate=0.001\nensemble = 3\nfrac = 0.9\n\n# Train Test split\ndf_traindata, df_testdata = train_test_split(df_final, test_size=0.3,random_state=1,stratify=df_final['target'])\nX_test = df_testdata.iloc[:,:36]\nY_test =df_testdata.iloc[:,36]\npredictions_total = []\n\n# Iterating n times\nfor i in range(ensemble):\n  print(\"Iteration Number : \", i)\n\n  #sample randomly the train data\n  traindata = df_traindata.sample(frac=frac)\n  X_train=traindata.iloc[:,:36]\n  y_train=traindata.iloc[:,36]\n\n  # Apply Standardization\n  sc = StandardScaler()\n  X_train = sc.fit_transform(X_train)\n  X_test = sc.transform(X_test)\n\n  # Model Structure  \n  model=tf.keras.models.Sequential([\n                                  tf.keras.layers.Dense(160,input_shape=(36,),activation=\"relu\",kernel_regularizer=l2(factor)),\n                                  tf.keras.layers.Dropout(rate),\n                                  tf.keras.layers.Dense(120,activation=\"relu\",kernel_regularizer=l2(factor)),\n                                  tf.keras.layers.Dropout(rate),\n                                  tf.keras.layers.Dense(80,activation='relu',kernel_regularizer=l2(factor)),\n                                  tf.keras.layers.Dropout(rate),\n                                  tf.keras.layers.Dense(40,activation='relu',kernel_regularizer=l2(factor)),\n                                  tf.keras.layers.Dense(80,activation='relu',kernel_regularizer=l2(factor)),\n                                  tf.keras.layers.Dropout(rate),\n                                  tf.keras.layers.Dense(40,activation='relu',kernel_regularizer=l2(factor)),\n                                  tf.keras.layers.Dropout(rate),\n                                  tf.keras.layers.Dense(20,activation='relu',kernel_regularizer=l2(factor)),\n                                  tf.keras.layers.Dropout(rate),\n                                  tf.keras.layers.Dense(10,activation='relu',kernel_regularizer=l2(factor)),\n                                  tf.keras.layers.Dropout(rate),\n                                  tf.keras.layers.Dense(units=1, activation='sigmoid')])\n\n  # Creating Optimizer  \n  adam = tf.keras.optimizers.Adam(learning_rate=0.01)\n\n  # Compiling the model  \n  model.compile(optimizer = adam, loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n  # Fitting the model  \n  history=model.fit(X_train, y_train, batch_size = 256,verbose=2, epochs = 100,validation_split=0.2,)\n\n  # Loss Plot on both training & testing data  \n  plot_metric(history, 'loss')\n\n  # Predictions for the test data\n  model_predictions = model.predict(X_test).reshape(X_test.shape[0])>0.5\n\n  # Appending them at every iteration  \n  predictions_total.append(model_predictions)\n\n  # Log loss for each iteration\n  print(\"\\nTEST log_loss for individual Models\",log_loss(Y_test, model_predictions.astype(int)))"]}, {"cell_type": "markdown", "id": "9285395e", "metadata": {}, "source": ["## 4.3 Discussion on Results"]}, {"cell_type": "code", "execution_count": 1, "id": "36e9f7ad", "metadata": {}, "outputs": [], "source": ["# Converting the prediction array to integer datatype\npredictions_total=np.array(predictions_total).astype(int)\n\n# Based on hard voting, we find output and compare it with ground truth\nmetrics(Y_test,pd.DataFrame(predictions_total).mode().T.values)"]}, {"cell_type": "markdown", "id": "db8f6657", "metadata": {}, "source": ["## Heterogenous Ensembling\n<font size = 3>Heterogeneous Ensembles usually work well if we have a small amount of estimators. Note that the number of algorithms should always be odd (3+) in order to avoid ties. For example, we could combine a decision tree, a SVM and a logistic regression using a voting mechanism to improve the results. Then use combined wisdom through majority vote in order to classify a given sample. Besides voting, we can also use averaging or stacking to aggregate the results of the models.The data for each model is the same.</font>"]}, {"cell_type": "code", "execution_count": 1, "id": "becef91d", "metadata": {}, "outputs": [], "source": ["# Stacking with Randomforest classifier, SVC, DecisionTree Classifier\nestimators = [(\"rfc\", RandomForestClassifier(n_estimators=10, random_state=42)),\n              (\"svc\", make_pipeline(StandardScaler(), SVC(random_state=42))),\n              (\"dtc\", DecisionTreeClassifier(random_state=42))]\n\nstac_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\nstac_clf.fit(X_train, y_train)\n()\n# Predictions\ny_pred = stac_clf.predict(X_test_copy)\n\n# Metrics\nmetrics(y_test,y_pred)"]}, {"cell_type": "markdown", "id": "6207ce5a", "metadata": {}, "source": ["# CO5 - Model Selection and Performance comparison"]}, {"cell_type": "markdown", "id": "6b59c09e", "metadata": {}, "source": ["## 5.1 Report of Results & Discussions"]}, {"cell_type": "markdown", "id": "41e8bc42", "metadata": {}, "source": ["<font size = 5>For Base models<br><br>\n<img src  = \"https://res.cloudinary.com/qna/image/upload/v1642369806/basemodels_cyozra.png\"><br><br>\n    \n<font size = 5>For Ensemble models<br><br>\n<img src  = \"https://res.cloudinary.com/qna/image/upload/v1642369806/ensemble_clfsv3.png\"><br><br> \n    \n<font size = 5>For Base models after Dimensionality Reduction<br><br>\n<img src  = \"https://res.cloudinary.com/qna/image/upload/v1642369806/dimensionalityreduction_lwdhj0.png\"><br><br> \n   \n<font size = 5>For ANN Models<br><br>\n<img src  = \"https://res.cloudinary.com/qna/image/upload/v1642369806/ann_k5tpyf.png\"><br><br> \n    "]}, {"cell_type": "markdown", "id": "951af929", "metadata": {}, "source": ["## Auto ML - Model Selection"]}, {"cell_type": "code", "execution_count": 1, "id": "8fd529e5", "metadata": {}, "outputs": [], "source": ["experiment = setup(df_final, target = \"target\")"]}, {"cell_type": "code", "execution_count": 1, "id": "99725d5f", "metadata": {}, "outputs": [], "source": ["compare_models()"]}, {"cell_type": "code", "execution_count": 1, "id": "f0d47158", "metadata": {}, "outputs": [], "source": ["catboost = create_model('catboost')"]}, {"cell_type": "markdown", "id": "30452055", "metadata": {}, "source": ["## 5.2 Project outcome\n\n<font size = 3>This project demonstrates several techniques and methods by which the data can be transformed & used to predict with various features whether a person will join the company after training given by the company.</font>\n\n<font size = 3>While exporing various algorithms, we observed that Random Forest produces the best result as it can automatically balance data sets when a class is more infrequent than other classes in the data.\n</font>\n"]}, {"cell_type": "markdown", "id": "ac6d44f0", "metadata": {}, "source": ["<center><font size = 7>\ud83e\udd17THANK YOU\ud83e\udd17</font></center>"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}