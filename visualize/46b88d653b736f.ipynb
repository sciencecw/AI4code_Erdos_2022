{"cells": [{"cell_type": "code", "execution_count": 1, "id": "c41a2bf5", "metadata": {}, "outputs": [], "source": ["import numpy as np\ndef bandit(A, q):\n    R = np.random.normal(q[A], 1)\n    \n    return R\ndef selectBandit(epsilon, excludeCurrentMax=False):\n    if(np.random.random()>epsilon):\n        return np.argmax(Q)\n    else:\n        return np.random.randint(0, len(Q))    "]}, {"cell_type": "code", "execution_count": 1, "id": "cfb121b6", "metadata": {}, "outputs": [], "source": ["num_exps = 2000\nnum_timesteps = 1000\nepsilon = 0.0\nrhist = np.zeros(num_timesteps)\nahist = np.zeros(num_timesteps)\n\nfor k in range(num_exps):\n    Q = np.zeros(10)\n    N = np.zeros(10)\n    q = np.array([np.random.standard_normal() for i in range(10)])\n    optimal_action=np.argmax(q)\n    reward_history = []\n    optimal_action_history = []\n    for i in range(num_timesteps):\n        # choose an action\n        A = selectBandit(epsilon)\n        R = bandit(A, q)\n        reward_history.append(R)\n        if A == optimal_action:\n            optimal_action_history.append(1)\n        else:\n            optimal_action_history.append(0)\n        N[A] += 1\n        Q[A] += 1/N[A]*(R - Q[A])\n    \n    rhist += np.array(reward_history)\n    ahist += np.array(optimal_action_history)"]}, {"cell_type": "code", "execution_count": 1, "id": "ba304477", "metadata": {}, "outputs": [], "source": ["rhist /= np.float(num_exps)\nahist /= np.float(num_exps)"]}, {"cell_type": "code", "execution_count": 1, "id": "eb23f5e9", "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n"]}, {"cell_type": "code", "execution_count": 1, "id": "94c5156d", "metadata": {}, "outputs": [], "source": ["plt.plot(rhist, label='average reward, eps = ' + str(epsilon))\nplt.legend()\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "fe061262", "metadata": {}, "outputs": [], "source": ["plt.plot(ahist, label='average optimal action, eps = ' + str(epsilon))\nplt.legend()\nplt.show()"]}, {"cell_type": "markdown", "id": "ce55fa56", "metadata": {}, "source": ["# Variances\n### Optimistic\n### UCB (Upper confidence boundary) => actually works better.\n# Convergence conditions\nTwo conditions to have convergence for this\n- Stationary\n- If non-stationary, should not change rapidly\n- E.g., Texas State Lottery (change the air + change the probability distribution of the balls) => make it non-stationary.\n\n\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}