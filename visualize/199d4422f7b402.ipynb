{"cells": [{"cell_type": "markdown", "id": "c3d14193", "metadata": {}, "source": ["Within this notebook, there is a basic exploratory data analysis and a comparison of classification models in predicting stroke in patients.\nFeedback, criticism and comments are much appreciated."]}, {"cell_type": "markdown", "id": "ed101383", "metadata": {}, "source": ["Attribute Info:\n1. id: unique identifier\n2. gender: \"Male\", \"Female\" or \"Other\"\n3. age: age of the patient\n4. hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n5. heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n6. ever_married: \"No\" or \"Yes\"\n7. work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n8. Residence_type: \"Rural\" or \"Urban\"\n9. avg_glucose_level: average glucose level in blood\n10. bmi: body mass index\n11. smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n12. stroke: 1 if the patient had a stroke or 0 if not\n*Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient"]}, {"cell_type": "code", "execution_count": 1, "id": "2604322f", "metadata": {}, "outputs": [], "source": ["import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# data prep\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# under/over sampling\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE\n\n# modelling\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# classification metrics\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_recall_curve\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")"]}, {"cell_type": "code", "execution_count": 1, "id": "caa9fe90", "metadata": {}, "outputs": [], "source": ["df = pd.read_csv(\"/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv\")\ndf.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "2f596043", "metadata": {}, "outputs": [], "source": ["df.nunique()"]}, {"cell_type": "code", "execution_count": 1, "id": "e8f7a7d1", "metadata": {}, "outputs": [], "source": ["df.drop(['id'], inplace=True, axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "90a17633", "metadata": {}, "outputs": [], "source": ["df[df['smoking_status'] == 'Unknown'].smoking_status.count()"]}, {"cell_type": "markdown", "id": "91f7174c", "metadata": {}, "source": ["Since 'Unknown' smoking status is a large portion of the data, we will not drop it."]}, {"cell_type": "code", "execution_count": 1, "id": "1b34fcab", "metadata": {}, "outputs": [], "source": ["df.describe()"]}, {"cell_type": "markdown", "id": "3bdcf7f0", "metadata": {}, "source": ["The minimum age is 0.08. Lets investigate this"]}, {"cell_type": "code", "execution_count": 1, "id": "ad8a4a78", "metadata": {}, "outputs": [], "source": ["df[df['age'] < 1].head(5)"]}, {"cell_type": "markdown", "id": "98e9f004", "metadata": {}, "source": ["Are these patients truly younger than 1 year when this data was recorded? I don't know too much about how the data was collected and if these are misinputs, but these values could be true because their work type is 'children'"]}, {"cell_type": "markdown", "id": "169cdb57", "metadata": {}, "source": ["# Exploratory Data Analysis"]}, {"cell_type": "markdown", "id": "44d9fab0", "metadata": {}, "source": ["# Target Variable"]}, {"cell_type": "code", "execution_count": 1, "id": "9bf4a0bd", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(1, 2, figsize=(12,8))\n\ndf['stroke'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%', ax=ax[0])\nsns.countplot(x='stroke', data=df, ax=ax[1])\n\nax[0].set_ylabel('')\nax[0].set_title('Stroke')\nax[1].set_title('Stroke')\n\nplt.show()"]}, {"cell_type": "markdown", "id": "7e8ec801", "metadata": {}, "source": ["Note that we are dealing with an imbalanced data set. About 5:95 or 1:19 ratio"]}, {"cell_type": "markdown", "id": "016dd5bc", "metadata": {}, "source": ["# Correlation"]}, {"cell_type": "code", "execution_count": 1, "id": "de558345", "metadata": {}, "outputs": [], "source": ["# convert discrete variables with string values to numeric as label encoding\ndfcorr = df.copy()\ndfcorr[['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']] = dfcorr[['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']].astype('category')\ndfcorr['gender'] = dfcorr['gender'].cat.codes\ndfcorr['ever_married'] = dfcorr['ever_married'].cat.codes\ndfcorr['work_type'] = dfcorr['work_type'].cat.codes\ndfcorr['Residence_type'] = dfcorr['Residence_type'].cat.codes\ndfcorr['smoking_status'] = dfcorr['smoking_status'].cat.codes\n\ncorr = dfcorr.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\nfig, ax = plt.subplots(figsize=(10,8))\n\ncmap= sns.diverging_palette(230, 20, as_cmap=True)\n\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\":.5})\n\nplt.title('Heatmap of all variables')\n\nplt.show()"]}, {"cell_type": "markdown", "id": "c3f69455", "metadata": {}, "source": ["The lifestyle variables, residence_type, work_type and smoking_status don't correlate well with stroke. However, smoking_status has a lot of unknown values. Smoking_status will be investigated further in the notebook."]}, {"cell_type": "code", "execution_count": 1, "id": "5b5cfa88", "metadata": {}, "outputs": [], "source": ["corr2 = df.corr()\nmask = np.triu(np.ones_like(corr2, dtype=bool))\n\nfig, ax = plt.subplots(figsize=(10,8))\n\ncmap= sns.diverging_palette(230, 20, as_cmap=True)\n\nsns.heatmap(corr2, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\":.5})\n\nplt.title('Heatmap with only health-related variables')\n\nplt.show()"]}, {"cell_type": "markdown", "id": "04f007fa", "metadata": {}, "source": ["seems like age is correlated with everything"]}, {"cell_type": "markdown", "id": "2d42ec55", "metadata": {}, "source": ["# Gender"]}, {"cell_type": "code", "execution_count": 1, "id": "581d35ff", "metadata": {}, "outputs": [], "source": ["df[['gender','stroke']].groupby(['gender']).count()"]}, {"cell_type": "markdown", "id": "38aff69d", "metadata": {}, "source": ["lets remove 'Other' since there is only 1 entry"]}, {"cell_type": "code", "execution_count": 1, "id": "b3ff8e7a", "metadata": {}, "outputs": [], "source": ["df = df[df['gender'] != 'Other']"]}, {"cell_type": "code", "execution_count": 1, "id": "51cbc482", "metadata": {}, "outputs": [], "source": ["# male\ndfmale = df[df['gender'] == 'Male']\ndfmale = dfmale['stroke'].value_counts(normalize=True)\ndfmale = dfmale.mul(100)\ndfmale = dfmale.rename('percent').reset_index()\n\n# female\ndffemale = df[df['gender'] == 'Female']\ndffemale = dffemale['stroke'].value_counts(normalize=True)\ndffemale = dffemale.mul(100)\ndffemale = dffemale.rename('percent').reset_index()\n\n\n\nfig, ax = plt.subplots(2, 2, figsize=(14,10))\n\ndf[['gender','stroke']].groupby(['gender']).count().plot.bar(ax=ax[0,0])\nsns.countplot(x='gender', hue='stroke', data=df, ax=ax[0,1])\nsns.barplot(x='index', y='percent', data=dfmale, ax=ax[1,0])\nsns.barplot(x='index', y='percent', data=dffemale, ax=ax[1,1])\n\nax[0,0].set_title('Gender Count')\nax[0,1].set_title('Gender and Stroke')\nax[1,0].set_title('Male + Stroke')\nax[1,1].set_title('Female + Stroke')\n\nax[1,0].set_xlabel('Stroke')\nax[1,1].set_xlabel('Stroke')\n\nax[0,0].set_xticklabels(ax[0,0].get_xticklabels(), rotation=0) # labels were vertical\nax[0,0].invert_xaxis() # male and female labels in wrong order\nax[0,0].get_legend().remove()\n\nax[1,0].text(0, 50, \"95.58%\", va='center', ha='center', fontsize=20)\nax[1,0].text(1, 4.7, \"4.43%\", ha='center', fontsize=20)\n\nax[1,1].text(0, 50, \"95.86%\", va='center', ha='center', fontsize=20)\nax[1,1].text(1, 4.3, \"4.15%\", ha='center', fontsize=20)\n\nfig.tight_layout()\nplt.show()"]}, {"cell_type": "markdown", "id": "89c43b41", "metadata": {}, "source": ["Even though there are more females than males recorded in the data, the relative % of males and females having experienced a stroke is roughly equal."]}, {"cell_type": "markdown", "id": "9f462481", "metadata": {}, "source": ["# Age"]}, {"cell_type": "code", "execution_count": 1, "id": "03191e02", "metadata": {}, "outputs": [], "source": ["fig = plt.figure(figsize=(20,10))\n\nax = [None for _ in range(5)] # List to save ax for setting parameter\n\nax[0] = plt.subplot2grid((3,4), (0,0), colspan = 2)\nax[1] = plt.subplot2grid((3,4), (1,0), colspan = 1)\nax[2] = plt.subplot2grid((3,4), (1,1), colspan = 1)\nax[3] = plt.subplot2grid((3,4), (2,0), colspan = 1)\nax[4] = plt.subplot2grid((3,4), (2,1), colspan = 1)\n\nsns.histplot(x='age', hue='stroke', multiple='stack', binwidth=5, data=df, ax=ax[0])\nsns.histplot(x='age', binwidth=5, data=df[df['stroke'] == 0], ax=ax[1])\nsns.histplot(x='age', color= '#FF8C00', binwidth=5, data=df[df['stroke'] == 1], ax=ax[2])\nsns.histplot(x='age', hue='stroke', multiple='stack', binwidth=5, data=df[df['gender'] == 'Male'], ax=ax[3])\nsns.histplot(x='age', hue='stroke', multiple='stack', binwidth=5, data=df[df['gender'] == 'Female'], ax=ax[4])\n\nax[0].set_title('Age and Stroke/No Stroke')\nax[1].set_title('Age and No Stroke')\nax[2].set_title('Age and Stroke')\nax[3].set_title('Age, Male, Stroke')\nax[4].set_title('Age, Female, Stroke')\n\nfig.tight_layout()\nplt.show()"]}, {"cell_type": "markdown", "id": "fcbf88d8", "metadata": {}, "source": ["This data suggests that the older the patient is, the more likely that they will have a stroke. There are a few cases where a patient had a stroke before the age of 20."]}, {"cell_type": "code", "execution_count": 1, "id": "904191a4", "metadata": {}, "outputs": [], "source": ["df[(df['age'] < 20) & (df['stroke'] == 1)]"]}, {"cell_type": "markdown", "id": "57b805a1", "metadata": {}, "source": ["Now a child at the age of about 1 and 14 having a stroke is surprising. Not sure whether this is a misinput or not. Maybe the stroke was due to a rare disease/condition? This makes me question what was considered a stroke when this data was collected, but this is out of our scope for now."]}, {"cell_type": "markdown", "id": "d8ac067a", "metadata": {}, "source": ["# Hypertension"]}, {"cell_type": "code", "execution_count": 1, "id": "e5066cdf", "metadata": {}, "outputs": [], "source": ["# no hypertension\ndfnohyper = df[df['hypertension'] !=1]\ndfnohyper = dfnohyper['stroke'].value_counts(normalize=True)\ndfnohyper = dfnohyper.mul(100)\ndfnohyper = dfnohyper.rename('percent').reset_index()\n\n# with hypertension\ndfhyper = df[df['hypertension'] !=0]\ndfhyper = dfhyper['stroke'].value_counts(normalize=True)\ndfhyper = dfhyper.mul(100)\ndfhyper = dfhyper.rename('percent').reset_index()\n\nfig, ax = plt.subplots(1,3, figsize=(16,6))\n\nsns.countplot(x='hypertension', hue='stroke', data=df, ax=ax[0])\nsns.barplot(x='index', y='percent', data=dfnohyper, ax=ax[1])\nsns.barplot(x='index', y='percent', data=dfhyper, ax=ax[2])\n\nax[2].set(ylim=(0, 100))\n\nax[0].set_title('Count of Stroke and Hypertension')\nax[1].set_title('No Hypertension with Stroke')\nax[2].set_title('Hypertension with Stroke')\n\nax[1].set_xlabel('Stroke')\nax[2].set_xlabel('Stroke')\n\nax[1].text(0, 48, \"96.66%\", va='center', ha='center', fontsize=20)\nax[1].text(1, 3.5, \"3.34%\", ha='center', fontsize=20)\nax[2].text(0, 43, \"86.70%\", ha='center', fontsize=20)\nax[2].text(1, 4.7, \"13.30%\", ha='center', fontsize=20)\n\nplt.show()"]}, {"cell_type": "markdown", "id": "ce253b27", "metadata": {}, "source": ["patients with hypertension have a higher occurrance of stroke"]}, {"cell_type": "code", "execution_count": 1, "id": "b7a4d9ca", "metadata": {}, "outputs": [], "source": ["fig, ax= plt.subplots(1, 2, figsize=(16,8))\n\nsns.violinplot(x='hypertension', y='age', hue='stroke', data=df, split=True, ax=ax[0])\nsns.boxplot(x='hypertension', y='age', hue='stroke', data=df, ax=ax[1])\n\nax[0].set_title('Hypertension, age, stroke')\nax[1].set_title('Hypertension, age, stroke')\n\nplt.show()"]}, {"cell_type": "markdown", "id": "7290d065", "metadata": {}, "source": ["the larger distribution of having hypertension is located at older ages"]}, {"cell_type": "markdown", "id": "e6abe076", "metadata": {}, "source": ["# Heart Disease"]}, {"cell_type": "code", "execution_count": 1, "id": "2293693d", "metadata": {}, "outputs": [], "source": ["# no heart disease\ndfnohd = df[df['heart_disease'] !=1]\ndfnohd = dfnohd['stroke'].value_counts(normalize=True)\ndfnohd = dfnohd.mul(100)\ndfnohd = dfnohd.rename('percent').reset_index()\n\n# with heart disease\ndfhd = df[df['heart_disease'] !=0]\ndfhd = dfhd['stroke'].value_counts(normalize=True)\ndfhd = dfhd.mul(100)\ndfhd = dfhd.rename('percent').reset_index()\n\nfig, ax = plt.subplots(1,3, figsize=(16,6))\n\nsns.countplot(x='heart_disease', hue='stroke', data=df, ax=ax[0])\nsns.barplot(x='index', y='percent', data=dfnohd, ax=ax[1])\nsns.barplot(x='index', y='percent', data=dfhd, ax=ax[2])\n\nax[2].set(ylim=(0, 100))\n\nax[0].set_title('Count of Stroke and Heart Disease')\nax[1].set_title('No Heart Disease with Stroke')\nax[2].set_title('Heart Disease with Stroke')\n\nax[1].set_xlabel('Stroke')\nax[2].set_xlabel('Stroke')\n\nax[1].text(0, 48, \"96.38%\", va='center', ha='center', fontsize=20)\nax[1].text(1, 3.9, \"3.62%\", ha='center', fontsize=20)\nax[2].text(0, 43, \"83.54%\", ha='center', fontsize=20)\nax[2].text(1, 8, \"16.46%\", ha='center', fontsize=20)\n\nplt.show()"]}, {"cell_type": "markdown", "id": "6e8bd552", "metadata": {}, "source": ["Although the count of patients with heart disease is very low, there is relatively a 12.84% increased occurance of strokes with patients that have a heart disease"]}, {"cell_type": "markdown", "id": "6e2249cb", "metadata": {}, "source": ["# Average Glucose Level"]}, {"cell_type": "code", "execution_count": 1, "id": "7627b91c", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(1,2, figsize=(16, 6))\n\nsns.histplot(x='avg_glucose_level', hue='stroke', multiple='stack', data=df, binwidth=10, ax=ax[0])\nsns.histplot(x='avg_glucose_level', color= '#FF8C00', binwidth=10, data=df[df['stroke'] == 1], ax=ax[1])\n\nax[0].set_title('Average Glucose Level')\nax[1].set_title('Average Glucose Level with Stroke')\nplt.show()"]}, {"cell_type": "markdown", "id": "31645b0e", "metadata": {}, "source": ["The shape of the second peak (avg_glucose_level above 150) for patients with stroke looks relatively higher than the second peak of the left plot, suggesting that there is a higher occurance of stroke in patients with a higher average glucose level "]}, {"cell_type": "code", "execution_count": 1, "id": "3004ca1c", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(1,2, figsize=(16,6))\n\nsns.violinplot(x='stroke', y='avg_glucose_level', data=df, ax=ax[0])\nsns.boxplot(x='stroke', y='avg_glucose_level', data=df, ax=ax[1])\n\nax[0].set_title('stroke and avg glucose level')\nax[1].set_title('stroke and avg glucose level')\n\n\nplt.show()"]}, {"cell_type": "markdown", "id": "e3220997", "metadata": {}, "source": ["This plot better describes that with higher avg_glucose level, the higher risk of having a stroke. We can see more clearly than the histogram that there is a portionally larger distribution of patients with a higher average glucose level having experienced a stroke."]}, {"cell_type": "markdown", "id": "4f9fd9bf", "metadata": {}, "source": ["# BMI"]}, {"cell_type": "code", "execution_count": 1, "id": "a3d75cbd", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(1,2, figsize=(16, 6))\n\nsns.histplot(x='bmi', hue='stroke', multiple='stack', data=df, binwidth=5, ax=ax[0])\nsns.histplot(x='bmi', color= '#FF8C00', binwidth=5, data=df[df['stroke'] == 1], ax=ax[1])\n\nax[0].set_title('bmi')\nax[1].set_title('bmi with stroke')\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "40b4ee34", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(20,12))\n\nax = [None for _ in range(3)] # create # of axes\n\nax[0] = plt.subplot2grid((4,4), (0,0), colspan=2)\nax[1] = plt.subplot2grid((4,4), (1,0), colspan=1)\nax[2] = plt.subplot2grid((4,4), (1,1), colspan=1)\n    \n    \nsns.regplot(x='avg_glucose_level', y='bmi', data=df, ax=ax[0], scatter_kws={'s':2}, line_kws={\"color\": \"black\", 'linewidth':1})\nsns.regplot(x='avg_glucose_level', y='bmi', data=df[df['stroke'] == 0], ax=ax[1], scatter_kws={'s':2}, line_kws={\"color\": \"black\", 'linewidth':1})\nsns.regplot(x='avg_glucose_level', y='bmi', color= '#FF8C00', data=df[df['stroke'] == 1], ax=ax[2], scatter_kws={'s':2}, line_kws={\"color\": \"black\", 'linewidth':1})\n\nax[0].set_title('avg glucose level and BMI')\nax[1].set_title('No Stroke')\nax[2].set_title('Stroke')\n\nax[2].set_ylim(0, 100)\n\nplt.tight_layout()\nplt.show()"]}, {"cell_type": "markdown", "id": "b6e230a0", "metadata": {}, "source": ["Slight positive correlation from all graphs"]}, {"cell_type": "markdown", "id": "b9df80b2", "metadata": {}, "source": ["# Smoking Status"]}, {"cell_type": "code", "execution_count": 1, "id": "39048355", "metadata": {}, "outputs": [], "source": ["df.groupby(['smoking_status'])['smoking_status'].describe()"]}, {"cell_type": "code", "execution_count": 1, "id": "96cef01e", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(1,2, figsize=(16,8))\n\nsns.violinplot(x='smoking_status', y='age', data=df, ax=ax[0])\nsns.boxplot(x='smoking_status', y='age', data=df, ax=ax[1])\n\nax[0].set_title('Age and smoking status')\nax[1].set_title('Age and smoking status')\n\nplt.show()"]}, {"cell_type": "markdown", "id": "f5590703", "metadata": {}, "source": ["Seems that there is a lot of unknown values for ages below 20. Let's trying dividing up this group"]}, {"cell_type": "code", "execution_count": 1, "id": "d9873a0c", "metadata": {}, "outputs": [], "source": ["# split the unknown smoking status into two groups: Unknown >20 age, and Unknown <20 age\ndf[(df['smoking_status'] == 'Unknown') & (df['age'] > 20)] = df[(df['smoking_status'] == 'Unknown') & (df['age'] > 20)].replace('Unknown','Unknown over 20')\ndf['smoking_status'] = df['smoking_status'].replace('Unknown', 'Unknown under 20')"]}, {"cell_type": "code", "execution_count": 1, "id": "6f9cee4a", "metadata": {}, "outputs": [], "source": ["pd.crosstab(df.smoking_status, df.stroke,\n           rownames=['smoking_status'], colnames=['stroke'])"]}, {"cell_type": "code", "execution_count": 1, "id": "34c53339", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(16,6))\n\nsns.countplot(x='smoking_status', hue='stroke', data=df)\n\nax.text(-0.2, 400, \"92.21%\", va='center', ha='center', fontsize=15)\nax.text(0.2, 100, \"7.79%\", va='center', ha='center', fontsize=15)\nax.text(0.8, 400, \"95.24%\", va='center', ha='center', fontsize=15)\nax.text(1.2, 120, \"4.76%\", va='center', ha='center', fontsize=15)\nax.text(1.8, 400, \"94.68%\", va='center', ha='center', fontsize=15)\nax.text(2.2, 80, \"5.32%\", va='center', ha='center', fontsize=15)\nax.text(2.8, 400, \"94.37%\", va='center', ha='center', fontsize=15)\nax.text(3.2, 80, \"5.63%\", va='center', ha='center', fontsize=15)\nax.text(3.8, 400, \"99.97%\", va='center', ha='center', fontsize=15)\nax.text(4.2, 40, \"0.30%\", va='center', ha='center', fontsize=15)\n\nax.text(3.7, 1800, \"Note: local % for each group\", va='center', ha='center', fontsize=15)\n\nplt.title('Smoking Status and Stroke')\n\nplt.tight_layout()\nplt.show()"]}, {"cell_type": "markdown", "id": "6ddd2536", "metadata": {}, "source": ["Patients who formerly smoked or smokes had a slightly higher occurance of stroke than patients who never smoked."]}, {"cell_type": "markdown", "id": "44d87e13", "metadata": {}, "source": ["# Preparing the Data"]}, {"cell_type": "code", "execution_count": 1, "id": "5aabe821", "metadata": {}, "outputs": [], "source": ["df.isnull().sum()\n# will simple impute the bmi null values later on"]}, {"cell_type": "code", "execution_count": 1, "id": "b01ea6e7", "metadata": {}, "outputs": [], "source": ["# separate features and target\nX = df.drop(['stroke'], axis=1)\ny = df['stroke']\n\n# split the data first before doing any\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n\n# Validation set\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.3, random_state=1)\n\n# separate numerical and categorical columns\nX_num = X[['age', 'avg_glucose_level', 'bmi']]\nX_cat = X[['gender', 'hypertension', 'heart_disease', 'ever_married', \n           'work_type', 'smoking_status', 'Residence_type']]"]}, {"cell_type": "code", "execution_count": 1, "id": "1b41e7e7", "metadata": {}, "outputs": [], "source": ["# pipeline\n\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='mean')),\n     ('scaler', StandardScaler())\n])\n    \ncat_pipeline = Pipeline([\n    ('onehot', OneHotEncoder(sparse=False, drop='first'))\n])\n\nnum_colnames = list(X_num)\ncat_colnames = list(X_cat)\n\n\nfull_pipeline = ColumnTransformer([\n    ('num', num_pipeline, num_colnames),\n    ('cat', cat_pipeline, cat_colnames)\n])\n\n# SMOTE (resampling)\npipelinebsmote = Pipeline([\n    ('smote', SMOTE(random_state=1))\n])\n\n# training set\nX_train = full_pipeline.fit_transform(X_train)\nX_train, y_train = pipelinebsmote.fit_resample(X_train, y_train)\n\n# validation set\nX_valid = full_pipeline.transform(X_valid)\nX_valid, y_valid = pipelinebsmote.fit_resample(X_valid, y_valid)\n\n# test set \nX_test = full_pipeline.transform(X_test)\n\n# Make sure test set is not resampled "]}, {"cell_type": "markdown", "id": "cb732642", "metadata": {}, "source": ["# Modelling"]}, {"cell_type": "code", "execution_count": 1, "id": "f7509de2", "metadata": {}, "outputs": [], "source": ["# create an empty df with formatted index to be filled later on\niterables = [['LOG', 'KNN', 'SVC', 'RF', 'DT', 'GB'], ['ROC AUC', 'Precision', 'Recall', 'F1']]\niterabletuples = pd.MultiIndex.from_product(iterables, names=[\"Model\", \"Metric\"])\nresults = pd.DataFrame(index = iterabletuples) \n\n\ndef evalmodel(model, X, y):\n    modelpred = model.predict(X)\n    modelroc = roc_auc_score(y, modelpred)\n    modelprec = precision_score(y, modelpred)\n    modelrecall = recall_score(y, modelpred)\n    modelf1 = f1_score(y, modelpred)\n    ROC = round(modelroc, 3)\n    prec = round(modelprec, 3)\n    recall = round(modelrecall, 3)\n    F1 = round(modelf1, 3)\n    return ROC, prec, recall, F1\n\n# models\n\nlogclf = LogisticRegression(random_state=1)\nknnclf = KNeighborsClassifier()\nsvcclf = SVC(random_state=1)\nrfclf = RandomForestClassifier(random_state=1)\ndtclf = DecisionTreeClassifier(random_state=1)\ngbclf = GradientBoostingClassifier(random_state=1)\n\nlistofmodels = [logclf, knnclf, svcclf, rfclf, dtclf, gbclf]\ntrainresults= [] # empty list to be converted to a column for results df\n\nfor i in listofmodels:\n    i.fit(X_train, y_train)\n\nfor a in listofmodels:\n    ROC, prec, recall, F1 = evalmodel(a, X_valid, y_valid)\n    trainresults.extend([ROC, prec, recall, F1])\n\nresults['Training Set'] = trainresults\nresults"]}, {"cell_type": "markdown", "id": "3851e59d", "metadata": {}, "source": ["The GradientBoostingClassifier model seems to be doing the best with the validation data"]}, {"cell_type": "markdown", "id": "29b508d4", "metadata": {}, "source": ["# Randomized Search"]}, {"cell_type": "code", "execution_count": 1, "id": "3db691e9", "metadata": {}, "outputs": [], "source": ["# KNN\nKNNparam = {\n    'n_neighbors': range(3,15),\n    'weights': ['uniform', 'distance'],\n    'leaf_size': np.linspace(10,50,4)\n    \n}\n\nKNNgrid = RandomizedSearchCV(KNeighborsClassifier(), KNNparam, cv=3, scoring='roc_auc')\nKNNgrid.fit(X_train, y_train)\n\n\n# SVC\nSVCparam = {\n    'C':[0.01, 0.1, 0.9, 1, 1.1, 10, 100],\n    'kernel':['linear', 'poly', 'rbf', 'sigmoid'],\n    'degree': range(1,10)\n    \n}\n\nSVCgrid = RandomizedSearchCV(SVC(random_state=1), SVCparam, cv=3, scoring='roc_auc')\nSVCgrid.fit(X_train, y_train)\n\n\n# LOG\nLOGparam = {\n    'C':[0.01, 0.1, 0.9, 1, 1.1, 10, 100],\n    'class_weight':['dict', 'balanced']\n}\n\nLOGgrid = RandomizedSearchCV(LogisticRegression(random_state=1), LOGparam, cv=3, scoring='roc_auc')\nLOGgrid.fit(X_train, y_train)\n\n\n\n# RF\nRFparam = {\n    'max_depth': np.linspace(2,32,2),\n    'n_estimators': range(100,1100,100),\n    'min_samples_split': range(2,22,2)\n}\n\nRFgrid = RandomizedSearchCV(RandomForestClassifier(random_state=1), RFparam, cv=3, scoring='roc_auc')\nRFgrid.fit(X_train, y_train)\n\n# DT\nDTparam = {\n    'splitter': ['best','random'],\n    'max_depth': np.linspace(2,32,2),\n    'min_samples_split': range(2,22,2)\n}\n\nDTgrid = RandomizedSearchCV(DecisionTreeClassifier(random_state=1), DTparam, cv=3, scoring='roc_auc')\nDTgrid.fit(X_train, y_train)\n\n# GB\nGBparam = {\n    'max_depth': np.linspace(2,32,2),\n    'n_estimators': range(100,1100,100),\n    'learning_rate': [0.01,0.05,0.1,0.5, 1]\n}\n\nGBgrid = RandomizedSearchCV(GradientBoostingClassifier(random_state=1), GBparam, cv=3, scoring='roc_auc')\nGBgrid.fit(X_train, y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "ece3bd3d", "metadata": {}, "outputs": [], "source": ["listofgridmodels = [LOGgrid.best_estimator_, KNNgrid.best_estimator_, SVCgrid.best_estimator_, \n                RFgrid.best_estimator_, DTgrid.best_estimator_, GBgrid.best_estimator_]\n\ngridresults = [] # empty list to be converted to a column for results df\n\nfor a in listofgridmodels:\n    ROC, prec, recall, F1 = evalmodel(a, X_valid, y_valid)\n    gridresults.extend([ROC, prec, recall, F1])\n\nresults['Grid Set'] = gridresults\nresults"]}, {"cell_type": "markdown", "id": "8d3829b5", "metadata": {}, "source": ["# Evaluate model on Test Set"]}, {"cell_type": "code", "execution_count": 1, "id": "f978e174", "metadata": {}, "outputs": [], "source": ["listofgridmodels = [LOGgrid.best_estimator_, KNNgrid.best_estimator_, SVCgrid.best_estimator_, \n                RFgrid.best_estimator_, DTgrid.best_estimator_, GBgrid.best_estimator_]\n\ntestresults = [] # empty list to be converted to a column for results df\n\nfor a in listofgridmodels:\n    ROC, prec, recall, F1 = evalmodel(a, X_test, y_test)\n    testresults.extend([ROC, prec, recall, F1])\n\nresults['Test Set'] = testresults\nresults"]}, {"cell_type": "markdown", "id": "2a95e688", "metadata": {}, "source": ["looks like LogisticRegression generalized the best since it has the best ROC AUC score.\n\n- Note all models seem to overfit the training data and not generalize well with the test data. However, the training set has been resampled with SMOTE while the test data has not. As a result, the models are predicting on data that is severely imbalanced (few cases of patients with stroke)\n- Interestingly enough for the LogisticRegression and the KNN model, the precision has definitely decreased with the test set, but the recall scores stayed consistent."]}, {"cell_type": "markdown", "id": "07d77a1a", "metadata": {}, "source": ["### Precision/Recall Plots for Logistic Regression"]}, {"cell_type": "code", "execution_count": 1, "id": "49e6d5cd", "metadata": {}, "outputs": [], "source": ["y_predict_proba = LOGgrid.best_estimator_.predict_proba(X_test)[:,1]\nprecision, recall, thresholds = precision_recall_curve(y_test, y_predict_proba)\nrecall = recall[:-1]\nprecision = precision[:-1]\n# :-1 slice because threshold does not exist for last value in array for precision and recall"]}, {"cell_type": "code", "execution_count": 1, "id": "7b938c40", "metadata": {}, "outputs": [], "source": ["# Plot precision/recall curve with and w/o threshold values\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(16,6))\n\nsns.lineplot(recall, precision, ax=ax1) # w/o threshold values\n\nsns.lineplot(x=thresholds, y=recall, color=\"g\", ax=ax2) # with threshold values\n\nax12 = plt.twinx()\nsns.lineplot(x=thresholds, y=precision, color=\"b\", ax=ax12) \n# axis labels\nax1.set_xlabel('Recall')\nax1.set_ylabel('Precision')\nax12.set_ylabel('Precision')\nax2.set_ylabel('Recall')\nax2.set_xlabel('Thresholds')\n\nax2.yaxis.label.set_color('g')\nax2.tick_params(axis='y', colors='g')\n\nax12.yaxis.label.set_color('b')\nax12.tick_params(axis='y', colors='b')\n\nax1.set_title('Precision vs Recall Curve for LogisticRegression')\nax2.set_title('Precision/Recall curves with Threshold values for LogisticRegression')\n# show the plot\nplt.show()"]}, {"cell_type": "markdown", "id": "70e4658e", "metadata": {}, "source": ["Again with the imbalanced class issue, we see irregular shaped curves from both plots and so it's difficult to determine the best threshold value depending on if you want higher precision or recall score while optimally balancing between the two."]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}