{"cells": [{"cell_type": "code", "execution_count": 1, "id": "7d8e6b33", "metadata": {}, "outputs": [], "source": ["import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt"]}, {"cell_type": "code", "execution_count": 1, "id": "7f26759d", "metadata": {}, "outputs": [], "source": ["train = pd.read_csv('/kaggle/input/titanic/train.csv')\ntrain.head()"]}, {"cell_type": "markdown", "id": "e725401d", "metadata": {}, "source": ["# **Explore Data Analysis**"]}, {"cell_type": "code", "execution_count": 1, "id": "3484af21", "metadata": {}, "outputs": [], "source": ["train.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "7710cd86", "metadata": {}, "outputs": [], "source": ["train.info()"]}, {"cell_type": "code", "execution_count": 1, "id": "922cc67b", "metadata": {}, "outputs": [], "source": ["train.dtypes"]}, {"cell_type": "code", "execution_count": 1, "id": "cc3e0113", "metadata": {}, "outputs": [], "source": ["def show_missing(data):\n  ''' This function is used to show percentage of missing data. '''\n  missing_values = data.isnull().sum()\n\n  percent_missing = missing_values / data.shape[0] * 100\n  percent_missing = percent_missing.round(2) \n\n  show_missing = pd.concat([percent_missing, data.nunique(), data.dtypes], keys=['PercentageMissing', 'Nunique values', 'Dtype'], axis = 1)\n\n  return show_missing\n\nshow_missing(train)"]}, {"cell_type": "markdown", "id": "a014262f", "metadata": {}, "source": ["\n\n*   3/12 columns have missing values. Age - 19.87% and Cabin - 77.10%\n*    5/12 columns have object dtype converting\n\n"]}, {"cell_type": "markdown", "id": "38691ddd", "metadata": {}, "source": ["# Data Visualization"]}, {"cell_type": "code", "execution_count": 1, "id": "856c2909", "metadata": {}, "outputs": [], "source": ["survivors = train[train['Survived'] == 1].shape[0]\ndeads = train.shape[0] - survivors\n\nprint(f'Survivor: {round(survivors / train.shape[0], 2) * 100} % \\nDead people: {round(deads / train.shape[0], 2) * 100}%')\n\nsns.countplot(data=train, x='Survived')"]}, {"cell_type": "markdown", "id": "bfb588b8", "metadata": {}, "source": ["### Sex Column"]}, {"cell_type": "code", "execution_count": 1, "id": "58ca4433", "metadata": {}, "outputs": [], "source": ["sns.countplot(data=train, hue='Survived', x='Sex')"]}, {"cell_type": "markdown", "id": "74b548e5", "metadata": {}, "source": ["\n\n*   Man have trending to not rescued\n\n\n"]}, {"cell_type": "markdown", "id": "180a8b8e", "metadata": {}, "source": ["### Pclass Column"]}, {"cell_type": "code", "execution_count": 1, "id": "2f2fcf75", "metadata": {}, "outputs": [], "source": ["# Survivors rate by class\ntrain.groupby('Pclass')['Survived'].mean().to_frame()"]}, {"cell_type": "code", "execution_count": 1, "id": "15b1403a", "metadata": {}, "outputs": [], "source": ["print(train['Pclass'].value_counts())"]}, {"cell_type": "code", "execution_count": 1, "id": "40884087", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(1, 2, figsize=(12, 6))\nsns.barplot(data=train, x='Pclass', y='Survived', palette=['green'], ci=None, ax=ax[0])\nax[0].set_title('Survived Rate by Pclass')\nsns.countplot(data=train, x='Pclass', hue='Survived', palette=['red', 'blue'], ax=ax[1])\nax[1].set_title('Survived or Dead by Pclass')"]}, {"cell_type": "markdown", "id": "9f7b6097", "metadata": {}, "source": ["Pclass\n*   There were three classes on the ship and from the plot we see that the number of passengers in the third class was higher than the number of passengers in the first and second classes combined.\n*   However, the survival rate by class is not the same, more than 60% of first-class passengers and around half of the second class passengers were rescued, whereas 75% of third class passengers were not able to survive the disaster.\n*   For this reason, this is definitely an important aspect to consider.\n\n"]}, {"cell_type": "markdown", "id": "0b95bea6", "metadata": {}, "source": ["### Pclass & Sex Columns"]}, {"cell_type": "code", "execution_count": 1, "id": "b1a4643b", "metadata": {}, "outputs": [], "source": ["train.groupby(['Pclass', 'Sex']).Survived.mean().to_frame()"]}, {"cell_type": "code", "execution_count": 1, "id": "ce3864b5", "metadata": {}, "outputs": [], "source": ["sns.barplot(data=train, x='Pclass', y='Survived', hue='Sex', palette=['red', 'blue'], ci=None)\nplt.title('Survival rate by Pclass and Sex')"]}, {"cell_type": "markdown", "id": "762d4a13", "metadata": {}, "source": ["\n\n*  We can also see the survival rate by Sex and Pclass, which is quite impressive. First class and second class women who were rescued were respectively 97% and 92%, while the percentage drops to 50% for third-class women.\n*  Despite that, this is still more than the 37% survival rate for first-class men.\n\n"]}, {"cell_type": "markdown", "id": "3700d76e", "metadata": {}, "source": ["### Age Column"]}, {"cell_type": "code", "execution_count": 1, "id": "1d2420a1", "metadata": {}, "outputs": [], "source": ["train['Age'].value_counts().head(10)"]}, {"cell_type": "code", "execution_count": 1, "id": "231887b8", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n\nsns.distplot(x=train['Age'], bins=40, kde=True, ax=ax[0], color='g')\nax[0].set_title('Age Distribution')\n\nax[1].set_title('Age distribution for the two subpopulations')\nsns.kdeplot(train['Age'].loc[train['Survived'] == 1], color='green', ax=ax[1], shade=True, label='Survived')\nsns.kdeplot(train['Age'].loc[train['Survived'] == 0], color='red', ax=ax[1], shade=True, label='Not Survived')\n# ax[1].set_legends()"]}, {"cell_type": "markdown", "id": "314dc6cb", "metadata": {}, "source": ["### Age & Sex Columns"]}, {"cell_type": "code", "execution_count": 1, "id": "197eb4f6", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(10, 6))\nsns.swarmplot(y='Sex', x='Age', hue='Survived', palette=('#C52219', '#23C552'), data=train)\nplt.title('Survived by age and sex')"]}, {"cell_type": "markdown", "id": "5e8e38a1", "metadata": {}, "source": ["* At a first look, the relationship between Age and Survived appears not to be very clear, we notice for sure that there is a peak corresponding to young passengers for those who survived, but apart from that the rest is not very informative.\n* We can appreciate this feature more if we consider Sex too: now it is clearer that a good number of male survivors had less than 12 years, while the female group has no particular properties."]}, {"cell_type": "markdown", "id": "aee187a0", "metadata": {}, "source": ["### Fare Column"]}, {"cell_type": "code", "execution_count": 1, "id": "f4473edd", "metadata": {}, "outputs": [], "source": ["train['Fare'].value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "e4d67c34", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(1,2,figsize=(12,6))\n\nsns.distplot(train.Fare, color='g', ax=ax[0])\nax[0].set_title('Fare distribution')\n\nfare_range = pd.qcut(train.Fare, 4, labels = ['Low', 'Mid', 'High', 'Very high'])\nsns.barplot(x=fare_range, y=train.Survived, palette='mako', ci=None, ax=ax[1])\nax[1].set_ylabel('Survival rate')"]}, {"cell_type": "markdown", "id": "8a2eb6b6", "metadata": {}, "source": ["### Fare & Sex"]}, {"cell_type": "code", "execution_count": 1, "id": "d5fd2e72", "metadata": {}, "outputs": [], "source": ["sns.swarmplot(x='Sex', y='Fare', hue='Survived', palette=('#C52219', '#23C552'), data=train)\nplt.title('Survived by fare and sex')"]}, {"cell_type": "markdown", "id": "92251bfc", "metadata": {}, "source": ["* Looking at the more detailed plot, we also see for example that all males with fare between 200 and 300 died.\n* For this reason, we can left the Fare feature as it is in order to prevent losing too much information; at deeper levels of a tree, a more discriminant relationship might open up and it could become a good group detector."]}, {"cell_type": "markdown", "id": "b15234d4", "metadata": {}, "source": ["### SibSp and Parch Columns"]}, {"cell_type": "code", "execution_count": 1, "id": "7814f8a6", "metadata": {}, "outputs": [], "source": ["# Create new feature with +1 is by passenger's self\ntrain['Nmember'] = train['SibSp'] + train['Parch'] + 1\nprint(train['Nmember'].value_counts())\n\nsns.countplot(data=train, x='Nmember', hue='Survived', palette=['red', 'blue'])"]}, {"cell_type": "markdown", "id": "7f955119", "metadata": {}, "source": ["### Ticket Column"]}, {"cell_type": "code", "execution_count": 1, "id": "75a97759", "metadata": {}, "outputs": [], "source": ["train['Ticket'].value_counts().head(10)"]}, {"cell_type": "code", "execution_count": 1, "id": "56d83620", "metadata": {}, "outputs": [], "source": ["# Calculate length of ticket\ntrain['Ticket_len'] = train.Ticket.apply(lambda x: len(x))\ntrain['Ticket_len'].value_counts()"]}, {"cell_type": "markdown", "id": "b8e0a3ad", "metadata": {}, "source": ["### Cabin Column"]}, {"cell_type": "code", "execution_count": 1, "id": "9e1bc7d4", "metadata": {}, "outputs": [], "source": ["print(train['Cabin'].unique())\n\n# Extrac to carbin models\ntrain['Cabin'] = train['Cabin'].str.get(0)\n\nsns.countplot(data=train, x='Cabin', hue='Survived')"]}, {"cell_type": "code", "execution_count": 1, "id": "8e02f473", "metadata": {}, "outputs": [], "source": ["train['Cabin'].value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "4ec1468c", "metadata": {}, "outputs": [], "source": ["print(train['Embarked'].value_counts())\nsns.countplot(data=train, x='Embarked', hue='Survived')"]}, {"cell_type": "markdown", "id": "1ece87ae", "metadata": {}, "source": ["# Data Processing"]}, {"cell_type": "code", "execution_count": 1, "id": "98db7a03", "metadata": {}, "outputs": [], "source": ["train.drop(columns=['SibSp', 'Parch'], inplace=True)\ntrain.head()"]}, {"cell_type": "markdown", "id": "6586413f", "metadata": {}, "source": ["### Function for transform data"]}, {"cell_type": "code", "execution_count": 1, "id": "746877c3", "metadata": {}, "outputs": [], "source": ["def remove_zero_fares(row): # Function for processing Fare column\n    if row.Fare == 0:\n        row.Fare = np.NaN\n    return row\n\ndef age_transform(row): # Function for classifying to age groups\n  if row['Age'] < 7:\n    return 0\n  elif (row['Age'] >= 8) & (row['Age'] < 19):\n    return 1\n  elif (row['Age'] >=19) & (row['Age'] < 30):\n    return 2\n  elif (row['Age'] >=30) & (row['Age'] < 60):\n    return  3\n  else:\n    return 4\n\ndef fare_sex(row): # Function for relationship between Fare column and Sex column\n  special_arrange = (row.Fare >= 200.0) & (row.Fare <=300.0)\n  not_special_arrange = (row.Fare > 300.0) | (row.Fare < 200.0)\n  if (row.Sex == 'male') & special_arrange:\n    return 0\n  elif (row.Sex == 'female') & special_arrange:\n    return 1\n  elif (row.Sex == 'male') & not_special_arrange:\n    return 0\n  else:\n    return 1\n\ndef age_sex(row): # Function for relationship between Age column and Sex column\n  special = (row.Age >=8) & (row.Age <= 12)\n  not_special = (row.Age >12) | (row.Age < 8)\n  if (row.Sex == 'female') & special:\n    return 0\n  else:\n    return 1\n    \ndef ticket_len_cat(row):  # Function for classifying to ticket's length groups\n  if row.Ticket_len <= 5:\n    return 0\n  elif (row.Ticket_len > 5) & (row.Ticket_len <= 10):\n    return 1\n  else:\n    return 2"]}, {"cell_type": "code", "execution_count": 1, "id": "3945ce03", "metadata": {}, "outputs": [], "source": ["def transform_data(data):\n  data.drop(columns=['Name'])\n  try:\n    # Sex column\n    data['nSex'] = data['Sex'].replace({'male': 0, 'female': 1})\n\n    # Age column\n    data['AgeCa'] = data.apply(age_transform, axis=1)\n    AgeCa_dummies = pd.get_dummies(data['AgeCa'], prefix='AgeCa')\n\n    # Fare column\n    data = data.apply(remove_zero_fares, axis=1)\n    data['Fare'].fillna(value=data['Fare'].median())\n    data['FareCat'] = pd.qcut(data['Fare'], 4, labels = [ 0, 1, 2, 3])\n    FareCat_dummies = pd.get_dummies(data['FareCat'], prefix='FareCat')\n\n    # Cabin column\n    data['Cabin'] = data['Cabin'].fillna(value='C')\n    cabin_dummies = pd.get_dummies(data['Cabin'], prefix='Cabin')\n\n    # Ticket's column\n    data['TicketLen'] = data.apply(ticket_len_cat, axis=1)\n    TicketLen_dummies = pd.get_dummies(data['TicketLen'], prefix='TicketLen')\n\n    # Embarked column\n    data['Embarked'] = data['Embarked'].dropna()\n    Embarked_dummies = pd.get_dummies(data['Embarked'], prefix='Embarked')\n\n    # Nnumber column\n    Nmember_dummies = pd.get_dummies(data['Nmember'], prefix='Nmember')\n\n    #Pclass\n    Pclass_dummies = pd.get_dummies(data['Pclass'], prefix='Pclass')\n\n    # New feature\n    data['FareSex'] = data.apply(fare_sex, axis=1)\n\n    data['AgeSex'] = data.apply(age_sex, axis=1)\n\n    # New data\n    new_data = pd.concat([data[['Survived', 'FareSex', 'AgeSex', 'nSex']],\n                          AgeCa_dummies, FareCat_dummies, cabin_dummies,\n                          TicketLen_dummies, Embarked_dummies, Nmember_dummies,\n                          Pclass_dummies], axis=1)\n  except: \n    new_data = pd.concat([data[['FareSex', 'AgeSex', 'nSex']],\n                          AgeCa_dummies, FareCat_dummies, cabin_dummies,\n                          TicketLen_dummies, Embarked_dummies, Nmember_dummies,\n                          Pclass_dummies], axis=1)\n\n  return new_data"]}, {"cell_type": "code", "execution_count": 1, "id": "b00ba51a", "metadata": {}, "outputs": [], "source": ["train_data = transform_data(train)\ntrain_data.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "61816266", "metadata": {}, "outputs": [], "source": ["train_data.drop(columns=['Cabin_T'], inplace=True)\ntrain_data.shape"]}, {"cell_type": "markdown", "id": "91bb12f3", "metadata": {}, "source": ["# Train Modeling"]}, {"cell_type": "code", "execution_count": 1, "id": "171da240", "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import MinMaxScaler\ndef split_data(data):\n  '''Function for splitting data.\n  Input: A dataframe \n  Output: X_train, X_test, y_train, y_test from dataframe input.'''\n\n  scale = MinMaxScaler()\n\n  X = data.drop(['Survived'], axis=1)\n  X = scale.fit_transform(X)\n  y = data['Survived']\n\n  # Train_test_split of data 70% - 30%\n  X_train, X_test, y_train, y_test = train_test_split(X, y.values, test_size=0.3, random_state=365)\n\n  return (X_train, X_test, y_train, y_test)\n\ndef base_learners_evaluation(data, base_classifiers):\n  '''Function for showing different score from base classifier models.\n  Input: A dataframe and a list of classifier model\n  Output: A dataframe score such as accuracy score, f1 score, precision score and recall score. '''\n\n\n  X_train, X_test, y_train, y_test = split_data(data)\n\n  idx = []\n  scores = {'Accuracy': [], 'F1_score': [], 'Precision': [], 'Recall': []}\n  for bc in base_classifiers:\n    lm = bc[1]\n    lm.fit(X_train, y_train)\n\n    prediction = lm.predict(X_test)\n\n    idx.append(bc[0])\n\n    scores['Accuracy'].append(accuracy_score(y_test, prediction))\n    scores['F1_score'].append(f1_score(y_test, prediction))\n    scores['Precision'].append(precision_score(y_test, prediction))\n    scores['Recall'].append(recall_score(y_test, prediction))\n\n  return pd.DataFrame(data=scores, index=idx)\n"]}, {"cell_type": "code", "execution_count": 1, "id": "2db717c4", "metadata": {}, "outputs": [], "source": ["from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n\nbase_classifiers = [('Decision Tree 3', DecisionTreeClassifier(max_depth=3)),\n                    ('Decision Tree 5', DecisionTreeClassifier(max_depth=5)),\n                    ('Decision Tree 8', DecisionTreeClassifier(max_depth=8)),\n                    ('Naive Bayes', GaussianNB()),\n                    ('SVC', SVC()),\n                    ('Logistic Regression', LogisticRegression(max_iter=500))]\n\nbase_learners_evaluation(train_data, base_classifiers)"]}, {"cell_type": "markdown", "id": "b301bff9", "metadata": {}, "source": ["# Feature Selection\n\n- We will use the following for this purpose :\n    - Pearson correlation factor pearson\n    - chi square test\n    - f_regression\n    - f_classif "]}, {"cell_type": "markdown", "id": "13e73de1", "metadata": {}, "source": ["## Using Pearson Correlation factor for feature selection"]}, {"cell_type": "code", "execution_count": 1, "id": "1da76f64", "metadata": {}, "outputs": [], "source": ["correlations = train_data.corr(method='pearson')['Survived'].drop('Survived')\ncorrelations.sort_values().plot(kind='barh')"]}, {"cell_type": "code", "execution_count": 1, "id": "36c9d908", "metadata": {}, "outputs": [], "source": ["# Filtering features with lower absolute value than a threshold\n\nthreshold = 0.1\n\npearson_feature = list(correlations[abs(correlations) > threshold].index.values)\npearson_feature"]}, {"cell_type": "code", "execution_count": 1, "id": "39d11262", "metadata": {}, "outputs": [], "source": ["data_corr = pd.concat([train_data[pearson_feature], train_data['Survived']], axis=1)\n\nbase_learners_evaluation(data_corr, base_classifiers)"]}, {"cell_type": "markdown", "id": "5a1226dc", "metadata": {}, "source": ["## Using chi2 test for feature selection"]}, {"cell_type": "code", "execution_count": 1, "id": "651d0fee", "metadata": {}, "outputs": [], "source": ["from sklearn.feature_selection import SelectKBest, chi2\n\n# Finding the best 20 features using chi2 test\ndata_chi2 = pd.DataFrame(SelectKBest(chi2, k=27).fit_transform(train_data.drop([\"Survived\"],axis = 1),train_data[\"Survived\"]))\ndata_chi2.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "7d330d72", "metadata": {}, "outputs": [], "source": ["data_chi2 = pd.concat([data_chi2, train_data['Survived']], axis=1)\nbase_learners_evaluation(data_chi2, base_classifiers)"]}, {"cell_type": "markdown", "id": "17b33d41", "metadata": {}, "source": ["## Using f_classif for feature selection"]}, {"cell_type": "code", "execution_count": 1, "id": "7f9890ce", "metadata": {}, "outputs": [], "source": ["from sklearn.feature_selection import SelectKBest, f_classif\n\n# Find the best 20 feature by f_classif test\ndata_classif = pd.DataFrame(SelectKBest(f_classif, 27).fit_transform(train_data.drop(['Survived'], axis=1), train_data['Survived']))\ndata_classif.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "c0cd1eb7", "metadata": {}, "outputs": [], "source": ["data_classif = pd.concat([data_classif, train_data['Survived']], axis=1)\nbase_learners_evaluation(data_classif, base_classifiers)"]}, {"cell_type": "markdown", "id": "8f02a6f0", "metadata": {}, "source": ["## Using f_regression for feature selection"]}, {"cell_type": "code", "execution_count": 1, "id": "5124d7a1", "metadata": {}, "outputs": [], "source": ["from sklearn.feature_selection import SelectKBest, f_regression\n\n# Find the best 20 feature by f_regression test\ndata_regression = pd.DataFrame(SelectKBest(f_regression, 27).fit_transform(train_data.drop(['Survived'], axis=1), train_data['Survived']))\ndata_regression.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "4b72d13e", "metadata": {}, "outputs": [], "source": ["data_regression = pd.concat([data_regression, train_data['Survived']], axis=1)\nbase_learners_evaluation(data_regression, base_classifiers)"]}, {"cell_type": "code", "execution_count": 1, "id": "0b1d4ab3", "metadata": {}, "outputs": [], "source": ["public_data = pd.read_csv('/kaggle/input/titanic/test.csv')\nPassengerId = public_data['PassengerId']\npublic_data.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "c83b81e4", "metadata": {}, "outputs": [], "source": ["public_data['Ticket_len'] = public_data.Ticket.apply(lambda x: len(x))\npublic_data['Nmember'] = public_data['SibSp'] + public_data['Parch'] + 1\npublic_data['Cabin'] = public_data['Cabin'].str.get(0)"]}, {"cell_type": "code", "execution_count": 1, "id": "2702623d", "metadata": {}, "outputs": [], "source": ["public_data.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "11681089", "metadata": {}, "outputs": [], "source": ["public_data.drop(columns=[ 'PassengerId', 'SibSp', 'Parch'], inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "0d6468b1", "metadata": {}, "outputs": [], "source": ["X = transform_data(public_data)\nX.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "9f5e6808", "metadata": {}, "outputs": [], "source": ["X.shape"]}, {"cell_type": "markdown", "id": "8d6e75a3", "metadata": {}, "source": ["# Training by ensemble model"]}, {"cell_type": "code", "execution_count": 1, "id": "49615049", "metadata": {}, "outputs": [], "source": ["def ensemble_evaluation(data, model, label='Original'):\n  '''This function show score with Original data or Filtered data.'''\n  X_train, X_test, y_train, y_test = split_data(data)\n  model.fit(X_train, y_train)\n  prediction = model.predict(X_test)\n\n  return pd.DataFrame({'Accuracy' : [accuracy_score(y_test, prediction)],\n                       'F1_score' : [f1_score(y_test, prediction)],\n                       'precision' : [precision_score(y_test, prediction)],\n                       'Recall' : [recall_score(y_test, prediction)]}, index=[label])"]}, {"cell_type": "code", "execution_count": 1, "id": "8ed03c03", "metadata": {}, "outputs": [], "source": ["from sklearn.ensemble import VotingClassifier\n\nmodels_comparison = {}\n\nensemble = VotingClassifier(base_classifiers)     \n\nensemble_data_origin = ensemble_evaluation(train_data, ensemble, label='Original')\nensemble_data_filtered = ensemble_evaluation(data_corr, ensemble, label='Filtered')\n\nmodels_comparison['Voting'] = pd.concat([ensemble_data_origin, ensemble_data_filtered], axis=0)"]}, {"cell_type": "code", "execution_count": 1, "id": "613ce00e", "metadata": {}, "outputs": [], "source": ["models_comparison['Voting']"]}, {"cell_type": "code", "execution_count": 1, "id": "8dd14638", "metadata": {}, "outputs": [], "source": ["from sklearn.ensemble import BaggingClassifier\n\nensemble = BaggingClassifier(n_estimators=10,\n                             base_estimator=DecisionTreeClassifier(max_depth=5))\n\nensemble_data_origin = ensemble_evaluation(train_data, ensemble, label='Original')\nensemble_data_filtered = ensemble_evaluation(data_corr, ensemble, label='Filtered')\nmodels_comparison['Bagging'] = pd.concat([ensemble_data_origin, ensemble_data_filtered], axis=0)"]}, {"cell_type": "code", "execution_count": 1, "id": "3f9a6071", "metadata": {}, "outputs": [], "source": ["models_comparison['Bagging']"]}, {"cell_type": "code", "execution_count": 1, "id": "2f8c1a05", "metadata": {}, "outputs": [], "source": ["from sklearn.ensemble import AdaBoostClassifier\n\nensemble = AdaBoostClassifier(n_estimators=365)\n\nensemble_data_origin = ensemble_evaluation(train_data, ensemble, label='Original')\nensemble_data_filtered = ensemble_evaluation(data_chi2, ensemble, label='Filtered')\nmodels_comparison['AdaBoost'] = pd.concat([ensemble_data_origin, ensemble_data_filtered], axis=0)"]}, {"cell_type": "code", "execution_count": 1, "id": "f195c1f8", "metadata": {}, "outputs": [], "source": ["models_comparison['AdaBoost']"]}, {"cell_type": "code", "execution_count": 1, "id": "afdbf6c2", "metadata": {}, "outputs": [], "source": ["from sklearn.ensemble import RandomForestClassifier\n\nensemble = RandomForestClassifier(n_estimators=500, max_depth=5, criterion=\"entropy\", n_jobs=-1)\n\nensemble_data_origin = ensemble_evaluation(train_data, ensemble, label='Original')\nensemble_data_filtered = ensemble_evaluation(data_corr, ensemble, label='Filtered')\nmodels_comparison['RandomForest'] = pd.concat([ensemble_data_origin, ensemble_data_filtered], axis=0)"]}, {"cell_type": "code", "execution_count": 1, "id": "0dc42d14", "metadata": {}, "outputs": [], "source": ["models_comparison['RandomForest']"]}, {"cell_type": "code", "execution_count": 1, "id": "921f9899", "metadata": {}, "outputs": [], "source": ["from xgboost import XGBClassifier\n\nensemble = XGBClassifier()\n\nensemble_data_origin = ensemble_evaluation(train_data, ensemble, label='Original')\nensemble_data_filtered = ensemble_evaluation(data_corr, ensemble, label='Filtered')\nmodels_comparison['XGBClassifier'] = pd.concat([ensemble_data_origin, ensemble_data_filtered], axis=0)"]}, {"cell_type": "code", "execution_count": 1, "id": "4b281664", "metadata": {}, "outputs": [], "source": ["models_comparison['XGBClassifier']"]}, {"cell_type": "code", "execution_count": 1, "id": "b8f0445b", "metadata": {}, "outputs": [], "source": ["from lightgbm import LGBMClassifier\n\nensemble = LGBMClassifier()\n\nensemble_data_origin = ensemble_evaluation(train_data, ensemble, label='Original')\nensemble_data_filtered = ensemble_evaluation(data_corr, ensemble, label='Filtered')\nmodels_comparison['LGBMClassifier'] = pd.concat([ensemble_data_origin, ensemble_data_filtered], axis=0)"]}, {"cell_type": "code", "execution_count": 1, "id": "73e48a95", "metadata": {}, "outputs": [], "source": ["models_comparison['LGBMClassifier']"]}, {"cell_type": "code", "execution_count": 1, "id": "26dac0f8", "metadata": {}, "outputs": [], "source": ["# !pip install catboost\n# import catboost \n# from catboost import CatBoostClassifier\n# np.random.seed(42)\n\n# ensemble = CatBoostClassifier()\n# ensemble_data_origin = ensemble_evaluation(train_data, ensemble, label='Original')\n# ensemble_data_filtered = ensemble_evaluation(data_corr, ensemble, label='Filtered')\n# models_comparison['CatBoostClf'] = pd.concat([ensemble_data_origin, ensemble_data_filtered], axis=0)\n# models_comparison['CatBoostClf']"]}, {"cell_type": "markdown", "id": "be9d10ed", "metadata": {}, "source": ["***In general, models above is not good model for public data. Finally, i will use pipeline model for this problem***"]}, {"cell_type": "markdown", "id": "afd4db3c", "metadata": {}, "source": ["# Use Pipeline for prediction"]}, {"cell_type": "code", "execution_count": 1, "id": "6cbd501b", "metadata": {}, "outputs": [], "source": ["train.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "b7747695", "metadata": {}, "outputs": [], "source": ["public_data.head()"]}, {"cell_type": "markdown", "id": "72ca5076", "metadata": {}, "source": ["## Data preprocessing for pipeline model"]}, {"cell_type": "code", "execution_count": 1, "id": "23be9dc7", "metadata": {}, "outputs": [], "source": ["# Because null value of Cabin column is so much, we will remove it from model training\npublic_data['Cabin'].isnull().sum()"]}, {"cell_type": "code", "execution_count": 1, "id": "eb290f0d", "metadata": {}, "outputs": [], "source": ["# Creation of four groups\ntrain['Nmember'] = pd.cut(train.Nmember, [0,1,4,7,11], labels=['Solo', 'Small', 'Big', 'Very big'])\npublic_data['Nmember'] = pd.cut(public_data.Nmember, [0,1,4,7,11], labels=['Solo', 'Small', 'Big', 'Very big'])"]}, {"cell_type": "code", "execution_count": 1, "id": "16217e4c", "metadata": {}, "outputs": [], "source": ["train['Title'] = train['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\n\npublic_data['Title'] = public_data['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())"]}, {"cell_type": "code", "execution_count": 1, "id": "c15db132", "metadata": {}, "outputs": [], "source": ["train['Title'].replace(['Mme', 'Ms', 'Lady', 'Mlle', 'the Countess', 'Dona'], 'Miss', inplace=True)\npublic_data['Title'].replace(['Mme', 'Ms', 'Lady', 'Mlle', 'the Countess', 'Dona'], 'Miss', inplace=True)\n\ntrain['Title'].replace(['Major', 'Col', 'Capt', 'Don', 'Sir', 'Jonkheer'], 'Mr', inplace=True)\npublic_data['Title'].replace(['Major', 'Col', 'Capt', 'Don', 'Sir', 'Jonkheer'], 'Mr', inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "16f7b470", "metadata": {}, "outputs": [], "source": ["train = train.apply(remove_zero_fares, axis=1)\ntrain['Fare'].fillna(value=train['Fare'].median())\n\npublic_data = public_data.apply(remove_zero_fares, axis=1)\npublic_data['Fare'].fillna(value=public_data['Fare'].median())"]}, {"cell_type": "code", "execution_count": 1, "id": "7dcf040b", "metadata": {}, "outputs": [], "source": ["train['Ticket_lett'] = train.Ticket.apply(lambda x: x[:2])\n\npublic_data['Ticket_lett'] = public_data.Ticket.apply(lambda x: x[:2])"]}, {"cell_type": "code", "execution_count": 1, "id": "fc4ef1ec", "metadata": {}, "outputs": [], "source": ["# train['Ticket_len'] = train.apply(ticket_len_cat, axis=1)\n# public_data['Ticket_len'] = public_data.apply(ticket_len_cat, axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "82db9c17", "metadata": {}, "outputs": [], "source": ["# Create group for fare ticket\n# train['FareCat'] = pd.qcut(train['Fare'], 4, labels = [ 0, 1, 2, 3])\n# public_data['FareCat'] = pd.qcut(public_data['Fare'], 4, labels = [ 0, 1, 2, 3])"]}, {"cell_type": "code", "execution_count": 1, "id": "f8b78beb", "metadata": {}, "outputs": [], "source": ["# train['FareSex'] = train.apply(fare_sex, axis=1)\n# public_data['FareSex'] = public_data.apply(fare_sex, axis=1) "]}, {"cell_type": "code", "execution_count": 1, "id": "5a9f6fe4", "metadata": {}, "outputs": [], "source": ["# train['AgeSex'] = train.apply(age_sex, axis=1)\n# public_data['AgeSex'] = public_data.apply(age_sex, axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "ff652746", "metadata": {}, "outputs": [], "source": ["y_train = train['Survived']\nfeatures = ['Pclass', 'Fare', 'Title', 'Embarked', 'Nmember', 'Ticket_len', 'Ticket_lett']\nX_train = train[features]\nX_train.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "b2c037bb", "metadata": {}, "outputs": [], "source": ["from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\nnumerical_cols = ['Fare']\ncategorical_cols = ['Pclass', 'Title', 'Embarked', 'Nmember', 'Ticket_len', 'Ticket_lett']\n\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='median')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Bundle preprocessing and modeling code \ntitanic_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('model', RandomForestClassifier(random_state=0, n_estimators=500, max_depth=5))\n])\n\n# Preprocessing of training data, fit model \ntitanic_pipeline.fit(X_train,y_train)\n\nprint('Cross validation score: {:.3f}'.format(cross_val_score(titanic_pipeline, X_train, y_train, cv=10).mean()))"]}, {"cell_type": "code", "execution_count": 1, "id": "09fb6730", "metadata": {}, "outputs": [], "source": ["X_test = public_data[features]\nX_test.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "7a389e98", "metadata": {}, "outputs": [], "source": ["# Preprocessing of test data, get predictions\npredictions = titanic_pipeline.predict(X_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "3eee520a", "metadata": {}, "outputs": [], "source": ["submission = pd.DataFrame({\n        \"PassengerId\": PassengerId,\n        \"Survived\": predictions\n    })\n\nsubmission.to_csv('submission_rd.csv', index=False)"]}, {"cell_type": "markdown", "id": "efd537f8", "metadata": {}, "source": ["# Conclusion\n  Cabin and Sex columns are not valuable for model although illustrating insight is very good. Over-reliance on these two attributes will cause the model score to decrease. So next time I will redo this predictive model in a different way. Let's look forward to it.\n\nNote: This article has references and improvements from other notebooks on Kaggle.\n\n## Thank you!!"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}