{"cells": [{"cell_type": "markdown", "id": "d9eae103", "metadata": {}, "source": ["The reference is taken from following:<br>\n[Deep Learning based Object Detection using YOLOv3 with OpenCV ( Python / C++ )](https://www.learnopencv.com/deep-learning-based-object-detection-using-yolov3-with-opencv-python-c/)<br>\n[YOLO object detection using Opencv with Python](https://pysource.com/2019/06/27/yolo-object-detection-using-opencv-with-python/)"]}, {"cell_type": "code", "execution_count": 1, "id": "ff4a7372", "metadata": {}, "outputs": [], "source": ["import numpy as np \nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt"]}, {"cell_type": "markdown", "id": "e18a5261", "metadata": {}, "source": ["First need to configure yolov3 model with opencv.\nThe *readNet* function from dnn module detects an original framwork of train model and calls automatically the function *readNetFromDarknet*.\nAnd *readNetFromDarknet* function returns the object that is ready to do forward, throw an exception in failure cases.\n\nFor *readNet* function order for passing the weights and cfg files doesn't matter.\n> So here *readNetFromDarknet* also can be used instead of *readNet*.\nBut the reason for considering readNet is to make it generic. If thr trained model belongs to tensorflow *readNet* automatcally calls *readNetFromTensorflow*.\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "3a4d0c08", "metadata": {}, "outputs": [], "source": ["net = cv2.dnn.readNet(\"/kaggle/input/yolov3-weight/yolov3.weights\", \"/kaggle/input/yolov3-weight/yolov3.cfg\")"]}, {"cell_type": "markdown", "id": "85749ed6", "metadata": {}, "source": ["Let's have a look at different layers using *getLayerNames*. "]}, {"cell_type": "code", "execution_count": 1, "id": "e01c105c", "metadata": {}, "outputs": [], "source": ["layer_names = net.getLayerNames()\nprint(\"layers names:\")\nprint(layer_names)"]}, {"cell_type": "markdown", "id": "84b679c3", "metadata": {}, "source": ["* conv - convolution layer<br>\n  Convolution layer applies a filter to an input to create a feature map\n* bn - batch normalization layer<br>\n This normalize the input for the hidden layer and also helps to reduce the training time, to reduce the effect of covariate shift and also add regularization effect.\n* relu - relu activation layer\n* shortcut - skip connection or residual connection<br>\nThis helps to improve the accuracy for a large neural network which tends to reduce the accuracy because of vanishing gradients as the network grows.\n* Permute - Permute layer<br>\nThis is used to re-order the dimention of the input according to the given pattern.\n* identity - This layer maps the output of unconnected layer to next input layer. [yolo_84(unconnected layer) --> conv_84]\n* upsample - Convolution layer performs downsampling by filtering input genarate the output of a smaller shape compare to input. Upsample layer performs the reverse opration by repeating rows and columns of input.\n* concat - This merges s list of inputs.\n* yolo - This is an output layer which a list of bounding boxes along with the recognised classes."]}, {"cell_type": "markdown", "id": "7501c3ca", "metadata": {}, "source": ["Let's idetify output layers using a function *getUnconnectedOutLayersNames*."]}, {"cell_type": "code", "execution_count": 1, "id": "008f68a1", "metadata": {}, "outputs": [], "source": ["output_layers = net.getUnconnectedOutLayersNames()\nprint(\"output layers:\")\nprint(output_layers)"]}, {"cell_type": "markdown", "id": "17b73b0b", "metadata": {}, "source": ["YoloV3 is trained to indetify 80 different types of objects.\nLet's fetch this detail from coco names."]}, {"cell_type": "code", "execution_count": 1, "id": "27f88403", "metadata": {}, "outputs": [], "source": ["classes = []\nwith open(\"/kaggle/input/coconames/coco.names\", \"r\") as f:\n    classes = [line.strip() for line in f.readlines()]\n    \ncolors = np.random.uniform(0, 255, size=(len(classes), 3)) #This will be used later to assign colors for the bounding box for the detected objects"]}, {"cell_type": "markdown", "id": "1bad9bdc", "metadata": {}, "source": ["The network requires the image is blob format.<br>\nBlob - Binary Large Objects.<br>\nBlob represents the group of pixels having simmilar values and different from surrounding pixels.<br>\nThe function blobFromImage convets the image in blob.<br>\nWe can scale, resize , subtract the mean from each pixels, change the order of the channels from BGR to RGB using swapRB argument and also crop the image.<br>\nWith the method setInput, the blob of an image is set as input for the network.<br>\nThe forward method propragate the blob of an image through the network and return the predictions."]}, {"cell_type": "code", "execution_count": 1, "id": "54c5f711", "metadata": {}, "outputs": [], "source": ["def get_objects_predictions(img):\n    height, width = img.shape[:2]\n    blob = cv2.dnn.blobFromImage(img, scalefactor = 1/255, size = (416, 416), mean= (0, 0, 0), swapRB = True, crop=False)\n    net.setInput(blob)\n    predictions = net.forward(output_layers)\n    return predictions,height, width"]}, {"cell_type": "markdown", "id": "ca194bc0", "metadata": {}, "source": ["The first 4 elements represent the center_x, center_y, width and height. The fifth element represents the confidence that the bounding box encloses an object.<br>\nThe rest of the elements are the confidence associated with each class (i.e. object type). The box is assigned to the class corresponding to the highest score for the box.<br>\nThe highest score for a box is also called its confidence.(here the confidence is set as 0.5). If the confidence of a box is less than the given threshold, the bounding box is dropped and not considered for further processing."]}, {"cell_type": "code", "execution_count": 1, "id": "dea8571e", "metadata": {}, "outputs": [], "source": ["def get_box_dimentions(predictions,height, width, confThreshold = 0.5):\n    class_ids = []\n    confidences = []\n    boxes = []\n    for out in predictions:\n        for detection in out:\n            scores = detection[5:]\n            class_id = np.argmax(scores)#Identifing the class type of the detected object by checking maximum confidence\n            confidence = scores[class_id]\n            if confidence > confThreshold:\n                # Object detected\n                center_x = int(detection[0] * width) #converting center_x with respect to original image size\n                center_y = int(detection[1] * height)#converting center_y with respect to original image size\n                w = int(detection[2] * width)#converting width with respect to original image size\n                h = int(detection[3] * height)#converting height with respect to original image size\n                # Rectangle coordinates\n                x = int(center_x - w / 2)\n                y = int(center_y - h / 2)\n                boxes.append([x, y, w, h])\n                confidences.append(float(confidence))\n                class_ids.append(class_id)\n    return boxes,confidences,class_ids"]}, {"cell_type": "markdown", "id": "96fcfcbc", "metadata": {}, "source": ["The Non max suppression technique is used to ensure that the obeject is detected only once.<br>\nIn this the bounding box with probability more nmsThresold is considered, other bounding boxes will be dropped out."]}, {"cell_type": "code", "execution_count": 1, "id": "dc1e2292", "metadata": {}, "outputs": [], "source": ["def non_max_suppression(boxes,confidences,confThreshold = 0.5, nmsThreshold = 0.4):\n    return cv2.dnn.NMSBoxes(boxes, confidences, confThreshold, nmsThreshold)"]}, {"cell_type": "markdown", "id": "86698898", "metadata": {}, "source": ["Let's draw the bounding boxes."]}, {"cell_type": "code", "execution_count": 1, "id": "6517ac98", "metadata": {}, "outputs": [], "source": ["def draw_bouding_boxes(img,boxes,confidences,class_ids,nms_indexes,colors):\n    for i in range(len(boxes)):\n        if i in nms_indexes:\n            x, y, w, h = boxes[i]\n            label = str(classes[class_ids[i]]) + ' :' + str(int(confidences[i]*100)) + '%'\n            color = colors[i]\n            cv2.rectangle(img, (x, y), (x + w, y + h), color, 3)\n            cv2.putText(img, label, (x, y - 15),cv2.FONT_HERSHEY_PLAIN ,2, color, 3)\n    return img"]}, {"cell_type": "code", "execution_count": 1, "id": "11927833", "metadata": {}, "outputs": [], "source": ["def detect_objects(img_path):\n    predictions,height, width = get_objects_predictions(img_path)\n    boxes,confidences,class_ids = get_box_dimentions(predictions,height, width)\n    nms_indexes = non_max_suppression(boxes,confidences)\n    img = draw_bouding_boxes(img_path,boxes,confidences,class_ids,nms_indexes,colors)\n    return img"]}, {"cell_type": "code", "execution_count": 1, "id": "0b1eec04", "metadata": {}, "outputs": [], "source": ["files = ['/kaggle/input/open-images-2019-object-detection/test/' + i for i in os.listdir('/kaggle/input/open-images-2019-object-detection/test')]"]}, {"cell_type": "markdown", "id": "1d70060c", "metadata": {}, "source": ["Let's have a look at some images with object detection."]}, {"cell_type": "code", "execution_count": 1, "id": "2693b11b", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(25,30))\n\nfor i in range(1,13):\n    index = np.random.randint(len(files))\n    plt.subplot(6, 2, i)\n    plt.imshow(detect_objects(cv2.imread(files[index])), cmap='cool')\nplt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}