{"cells": [{"cell_type": "markdown", "id": "dd009417", "metadata": {}, "source": ["# Datavidia 7.0\n## By: Tim VWXYZ - Universitas Trunojoyo\n### 1. Mohamad Zaelani (Leader) - mohamadzaelani09@gmail.com\n### 2. Yusuf Sugiono - ysf.sugiono@gmail.com\n### 3. Viki Wahyudi - vikiwahyudi1982@gmail.com"]}, {"cell_type": "markdown", "id": "33cd248c", "metadata": {}, "source": ["# 1. Understand the Problem (Business and Data)"]}, {"cell_type": "markdown", "id": "6e2b8d93", "metadata": {}, "source": ["## A. Understand the Business"]}, {"cell_type": "markdown", "id": "f38c1ae4", "metadata": {}, "source": ["Pada tahap ini kami berusaha memahami permasalahan yang sedang dihadapi. Pada kompetisi ini kami diminta untuk membuat model yang dapat memprediksi apakah sebuah review terhadap hotel adalah review buruk atau review positif.\n\nProblem Statement : Prediksi apakah suatu review termasuk review positif atau negatif.\n\nDengan membuat model yang dapat memprediksi sebuah review apakah termasuk review positif atau negatif maka perusahaan dapat menangani permasalahan dengan segera."]}, {"cell_type": "markdown", "id": "20f3fd41", "metadata": {}, "source": ["## B. Understand the Data"]}, {"cell_type": "markdown", "id": "c054583f", "metadata": {}, "source": ["Pada tahap ini kami akan berusaha memahami dataset yang diberikan. Tahapan ini sangat penting karena pada kasus dilapangan terkadang memiliki kualitas yang buruk sehingga perlu penanganan lebih lanjut.\n\nBerikut data-data yang akan digunakan:\n- train.csv - Data training yang berisi review beserta kategori sentimen review tersebut\n- test.csv - Data uji / test yang berisi review yang ingin diketahui kategori sentimennya\n- sample_submission.csv - File yang berisi contoh format submisi pada platform kaggle"]}, {"cell_type": "markdown", "id": "2b10ddf8", "metadata": {}, "source": ["### Import Library"]}, {"cell_type": "code", "execution_count": 1, "id": "0c0c7696", "metadata": {}, "outputs": [], "source": ["import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))"]}, {"cell_type": "code", "execution_count": 1, "id": "b900c928", "metadata": {}, "outputs": [], "source": ["import string \nimport re\nfrom collections import Counter\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.tokenize import word_tokenize \nfrom nltk.probability import FreqDist\nfrom wordcloud import WordCloud, ImageColorGenerator\n\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences"]}, {"cell_type": "markdown", "id": "73527fe4", "metadata": {}, "source": ["### Import Files"]}, {"cell_type": "code", "execution_count": 1, "id": "a6142e0e", "metadata": {}, "outputs": [], "source": ["train_df = pd.read_csv('/kaggle/input/penyisihan-datavidia-7-0/train.csv')\ntest_df = pd.read_csv('/kaggle/input/penyisihan-datavidia-7-0/test.csv')\nsubmission = pd.read_csv('/kaggle/input/penyisihan-datavidia-7-0/sample_submission.csv')"]}, {"cell_type": "markdown", "id": "3f3f6ab3", "metadata": {}, "source": ["### Info Pada Dataset"]}, {"cell_type": "code", "execution_count": 1, "id": "dc8bb19d", "metadata": {}, "outputs": [], "source": ["train_df.info()"]}, {"cell_type": "code", "execution_count": 1, "id": "a2ea3320", "metadata": {}, "outputs": [], "source": ["test_df.info()"]}, {"cell_type": "markdown", "id": "50eae685", "metadata": {}, "source": ["### Cek 5 Data Teratas"]}, {"cell_type": "code", "execution_count": 1, "id": "65a5abac", "metadata": {}, "outputs": [], "source": ["train_df.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "c906e98e", "metadata": {}, "outputs": [], "source": ["test_df.head()"]}, {"cell_type": "markdown", "id": "33507ff8", "metadata": {}, "source": ["Pada tahap bussines understanding dan data undersanding, terdapat beberapa hal yang dapat kami simpulkan:\n\n- Target Variable: `category`\n- Problem Type:  Klasifikasi Binary dengan class '1' (review positif) dan '0' (review negatif)\n- Metric:  Macro F1-Score\n- Tidak ada kolom yang memiliki missing value untuk setiap dataset\n- Kolom `review_id` tidak akan digunakan dalam proses training model sehingga akan di hapus\n- Kolom `review_text` berisi kalimat-kalimat review dari pelanggan hotel sehingga kami harus melakukan Pemrosesan Bahasa Alami (Natural Language Processing)."]}, {"cell_type": "markdown", "id": "1aa17ecc", "metadata": {}, "source": ["# 2. Data Preprocessing"]}, {"cell_type": "markdown", "id": "1a541a18", "metadata": {}, "source": ["Pada kasus pemrosesan bahasa alami, data yang akan di proses merupakan data yang tidak terstruktur dimana data-data tersebut berupa teks atau kalimat. Oleh karena itu diperlukan pengolahan terhadap data tersebut sehingga menjadi data yang terstruktur agar dapat digunakan pada tahap berikutnya.\n\nPada tahap ini, kami manggunakan *library* `nltk` atau *Natural Language Tolkit*. Library `nltk` lazim digunakan untuk pemodelan dan preprosesing teks. `nltk` menyediakan metode-metode yang baik untuk mempersiapkan teks sebelum digunakan pada tahap selanjutnya.\n\nTahapan preprosesing data yang akan kami lakukan sebagai berikut:\n1. Case Folding\n2. Tokenizing\n3. Normalisasi\n4. Filtering"]}, {"cell_type": "markdown", "id": "f6503f20", "metadata": {}, "source": ["Sebelum melakukan teknik preprosesing terhadap data yang diberikan, kami akan menghapus kolom `review_id` karena kolom tersebut tidak akan digunakan dalam proses training."]}, {"cell_type": "code", "execution_count": 1, "id": "10d964bc", "metadata": {}, "outputs": [], "source": ["train_df = train_df.drop(['review_id'], axis=1)\ntest_df = test_df.drop(['review_id'], axis=1)"]}, {"cell_type": "markdown", "id": "f5d8cc85", "metadata": {}, "source": ["## Case Folding"]}, {"cell_type": "markdown", "id": "3dd158ed", "metadata": {}, "source": ["*Case folding* merupakan teknik preprosesing yang paling sederhana. Tujuan dari case folding adalah untuk mengubah semua huruf dalam dataset menjadi huruf kecil. Selain itu, karakter-karakter selain huruf juga akan dihapus. Pada tahapan ini kami akan menggunakan modul yang sudah tersedia.\n"]}, {"cell_type": "markdown", "id": "c0c6c559", "metadata": {}, "source": ["### Mengubah Text Menjadi Lowercase"]}, {"cell_type": "markdown", "id": "474bbb56", "metadata": {}, "source": ["Huruf besar dan kecil akan dianggap berbeda sehingga perlu ditangani lebih lanjut. Proses *case folding* pada data yang kami miliki akan menggunakan fungsi `lower()` pada class `Pandas.Series.str`. "]}, {"cell_type": "code", "execution_count": 1, "id": "5246369f", "metadata": {}, "outputs": [], "source": ["train_df['review_text'] = train_df['review_text'].str.lower()\ntest_df['review_text'] = test_df['review_text'].str.lower()"]}, {"cell_type": "markdown", "id": "f4223283", "metadata": {}, "source": ["### Menghapus New Line"]}, {"cell_type": "markdown", "id": "c25f240e", "metadata": {}, "source": ["Untuk menghapus karekter *new line* kami menggunakan fungsi `replace()` dan regular expression/regex."]}, {"cell_type": "code", "execution_count": 1, "id": "c9c61c09", "metadata": {}, "outputs": [], "source": ["train_df['review_text'] = train_df['review_text'].replace('\\n',' ', regex=True)\ntest_df['review_text'] = test_df['review_text'].replace('\\n',' ', regex=True)"]}, {"cell_type": "markdown", "id": "73e79ba8", "metadata": {}, "source": ["### Menghapus Karakter Angka"]}, {"cell_type": "markdown", "id": "52bc6ada", "metadata": {}, "source": ["Untuk menghapus angka dalam data, kami menggunakan *Regular Expression (Regex)*. Kami juga akan menggunakan fungsi `replace()` untuk proses ini."]}, {"cell_type": "code", "execution_count": 1, "id": "0dd51953", "metadata": {}, "outputs": [], "source": ["train_df['review_text'] = train_df['review_text'].str.replace('\\d+', ' ')\ntest_df['review_text'] = test_df['review_text'].str.replace('\\d+', ' ')"]}, {"cell_type": "markdown", "id": "b26c855f", "metadata": {}, "source": ["### Menghapus Tanda Baca dan Whitespace"]}, {"cell_type": "markdown", "id": "9ea74985", "metadata": {}, "source": ["Kami juga akan menghapus tanda baca dan whitespace yang berlebihan. Untuk melakukan itu, kami menggunakan modul `nltk` untuk menghapus tanda baca *(punctuation)* dan menggunakan modul `re` atau *regular expression*."]}, {"cell_type": "code", "execution_count": 1, "id": "5cd99ed9", "metadata": {}, "outputs": [], "source": ["#remove punctuation\ndef remove_punctuation(text):\n    return text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n\ntest_df['review_text'] = test_df['review_text'].apply(remove_punctuation)\ntrain_df['review_text'] = train_df['review_text'].apply(remove_punctuation)\n\n#remove whitespace leading & trailing\ndef remove_whitespace_LT(text):\n    return text.strip()\n\ntest_df['review_text'] = test_df['review_text'].apply(remove_whitespace_LT)\ntrain_df['review_text'] = train_df['review_text'].apply(remove_whitespace_LT)\n\n#remove multiple whitespace into single whitespace\ndef remove_whitespace_multiple(text):\n    return re.sub('\\s+',' ',text)\n\ntest_df['review_text'] = test_df['review_text'].apply(remove_whitespace_multiple)\ntrain_df['review_text'] = train_df['review_text'].apply(remove_whitespace_multiple)\n\n# remove single char\ndef remove_singl_char(text):\n    return re.sub(r\"\\b[a-zA-Z]\\b\", \" \", text)\n\ntest_df['review_text'] = test_df['review_text'].apply(remove_singl_char)\ntrain_df['review_text'] = train_df['review_text'].apply(remove_singl_char)"]}, {"cell_type": "markdown", "id": "9bab8ef1", "metadata": {}, "source": ["## Tokenizing"]}, {"cell_type": "markdown", "id": "4acd1ed6", "metadata": {}, "source": ["*Tokenizing* merupakan proses untuk memisahkan teks menjadi potongan-potongan yang disebut sebagai token untuk kemudian dilakukan analisa. Dalam NLP, token merupakan \"kata\" yang diekstrak pada sebuah kalimat. Untuk melakukan *tokenizing* kami menggunakan fungsi `word_tokenize()` yang tersedia pada class `nltk.tokenize` pada modul `nltk`."]}, {"cell_type": "code", "execution_count": 1, "id": "04687de2", "metadata": {}, "outputs": [], "source": ["# NLTK word rokenize \ndef word_tokenize_wrapper(text):\n    return word_tokenize(text)\n\ntest_df['review_text_token'] = test_df['review_text'].apply(word_tokenize_wrapper)\ntrain_df['review_text_token'] = train_df['review_text'].apply(word_tokenize_wrapper)"]}, {"cell_type": "markdown", "id": "4c841a98", "metadata": {}, "source": ["## Normalisasi"]}, {"cell_type": "markdown", "id": "8b7ae1e2", "metadata": {}, "source": ["Tahap normalisasi bertujuan untuk menyeragamkan kata yang memiliki makna sama namun dengan penulisan yang berbeda. Hal ini bisa diakibatkan karena kesalahan penulisan, penyingkatan kata, ataupun penggunaan \"bahasa gaul\".\n\nUntuk itu kami harus menyiapkan dataset untuk mapping kata yang ingin diseragamkan. Kami sudah menyiapkan dataset tersebut yang tersedia pada link github dibawah. Selanjutnya kami telah membuat fungsi yang dapat digunakan untuk proses normalisai."]}, {"cell_type": "code", "execution_count": 1, "id": "310d3bef", "metadata": {}, "outputs": [], "source": ["def normalisasi(token):\n  kbba=[kamus.strip('\\n').strip('\\r') for kamus in open('/kaggle/input/normalisasitxt/normalisasi.txt')]\n  dic={}\n  for i in kbba:\n    (key,val)=i.split('\\t')\n    dic[str(key)]=val\n  final_string = ' '.join(str(dic.get(word, word)) for word in token).split()\n  return final_string\ntrain_df['normalisasi'] = train_df['review_text_token'].apply(normalisasi)\ntest_df['normalisasi'] = test_df['review_text_token'].apply(normalisasi)"]}, {"cell_type": "markdown", "id": "d36b8925", "metadata": {}, "source": ["## Filtering (Menghapus Stopwords)"]}, {"cell_type": "markdown", "id": "69a12299", "metadata": {}, "source": ["Stopwords merupakan kata-kata yang sering muncul dan dianggap tidak memiliki makna. Dalam bahasa Indonesia, beberapa contoh stopwords yang sering muncul seperti \"yang\", \"dan\", \"di\", \"dari\", dan lain lain. Menghapus stopwords bertujuan untuk menghapus kata yang memiliki makna rendah, sehingga kita dapat lebih fokus terhadap kata yang memiliki makna lebih tinggi dalam sebuah kalimat.\n\nUntuk melakukan filtering stopwords, kami sudah membuat beberapa list kata yang termasuk stopwords. Selanjutnya kami membuat sebuah fungsi `stopwords_removal` yang akan mengecek apakah sebuah token ada di dalam list stopwords, jika 'Ya' maka kate tersebut akan dihapus dari token."]}, {"cell_type": "code", "execution_count": 1, "id": "2f301a68", "metadata": {}, "outputs": [], "source": ["list_stopwords = (['yg', 'dg', 'dgn', 'dengan', 'ny', 'd', 'klo',\n                   'kalo', 'amp', 'biar', 'bikin', 'bilang', 'jadi',\n                   'krn', 'nya', 'nih', 'sih', 'untuk', 'juga', 'the',\n                   'si', 'tau', 'tuh', 'utk', 'ya', 'dari', 'and',\n                   'jd', 'jgn', 'sdh', 'aja', 'n', 't', 'karena',\n                   'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n                   '&amp', 'yah', 'yng', 'yang', 'di', 'ada', 'dan',\n                   'saya', 'ke', 'dll', 'gw', 'gwe', 'gua', 'itu', 'saja'])\n\nlist_stopwords = set(list_stopwords)\n\n\n#remove stopword pada list token\ndef stopwords_removal(words):\n    return [word for word in words if word not in list_stopwords]\n\ntrain_df['normalisasi'] = train_df['normalisasi'].apply(stopwords_removal)\ntest_df['normalisasi'] = test_df['normalisasi'].apply(stopwords_removal)"]}, {"cell_type": "markdown", "id": "c76471bd", "metadata": {}, "source": ["## 3. Exploratory Data Analysis (EDA)"]}, {"cell_type": "markdown", "id": "195878d7", "metadata": {}, "source": ["*Exploratory Data Analysis (EDA)* adalah proses memeriksa dataset untuk menemukan fakta tentang data dan mengkomunikasikan fakta tersebut, seringkali melalui visualisasi."]}, {"cell_type": "markdown", "id": "418f485c", "metadata": {}, "source": ["## Distribusi Kategori Review"]}, {"cell_type": "code", "execution_count": 1, "id": "58473ff7", "metadata": {}, "outputs": [], "source": ["cat = train_df['category'].value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "0bdd7d71", "metadata": {}, "outputs": [], "source": ["print('Jumlah review negatif', cat[0])\nprint('Jumlah review positif', cat[1])"]}, {"cell_type": "code", "execution_count": 1, "id": "e144871b", "metadata": {}, "outputs": [], "source": ["flavors = ('Negative', 'Positive')\nexplode = (0, 0.1)\n\nplt.title('Persentase Review Positif dan Negatif')\nplt.pie(\n    cat,\n    labels=flavors,\n    autopct='%1.1f%%',\n    explode=explode,\n    shadow=True\n    )\nplt.show()"]}, {"cell_type": "markdown", "id": "c49fe1b3", "metadata": {}, "source": ["Dari hasil diatas kita dapat ketahui bahwa dalam dataset yang kita miliki terdapat 86.4% review negatif dan 13.6% review positif. Total dari keseluruhan sampel yang kita miliki sebanyak 14856. Kemudian dari hasil tersebut dapat kita simpulkan bahwa data yang kita miliki adalah data yang *imbalance*."]}, {"cell_type": "markdown", "id": "1c73b6cb", "metadata": {}, "source": ["## Text length"]}, {"cell_type": "markdown", "id": "5e6c8eb5", "metadata": {}, "source": ["### Distribusi Jumlah Kata Seluruh Review"]}, {"cell_type": "code", "execution_count": 1, "id": "599f1168", "metadata": {}, "outputs": [], "source": ["lens = train_df['normalisasi'].apply(lambda x: len(x))\nprint(lens.describe())\nlens.hist()"]}, {"cell_type": "markdown", "id": "44f2b309", "metadata": {}, "source": ["Hasil diatas menunjukkan bahwa panjang kata rata-rata yang terdapat pada dataset adalah 14 kata. Kemudian terdapat review dengan panjang 0 kata, maka kami akan menghapus data tersebut."]}, {"cell_type": "code", "execution_count": 1, "id": "5dc6e04e", "metadata": {}, "outputs": [], "source": ["train_df.drop(train_df[lens < 1].index, inplace=True)"]}, {"cell_type": "markdown", "id": "6c58db96", "metadata": {}, "source": ["### Distribusi Jumlah Kata Review Positif"]}, {"cell_type": "code", "execution_count": 1, "id": "9782caf4", "metadata": {}, "outputs": [], "source": ["lens_positive = train_df.loc[(train_df.category == 1), 'normalisasi'].apply(lambda x: len(x))\nprint(lens_positive.describe())\nlens_positive.hist()"]}, {"cell_type": "markdown", "id": "7f34b8bc", "metadata": {}, "source": ["### Distribusi Jumlah Kata Review Negatif"]}, {"cell_type": "code", "execution_count": 1, "id": "d4da25df", "metadata": {}, "outputs": [], "source": ["negative = train_df.loc[(train_df.category == 0), 'normalisasi'].apply(lambda x: len(x))\nprint(negative.describe())\nnegative.hist()"]}, {"cell_type": "markdown", "id": "95046c1d", "metadata": {}, "source": ["## Term Frequency Analysis"]}, {"cell_type": "code", "execution_count": 1, "id": "23f84fb9", "metadata": {}, "outputs": [], "source": ["words = train_df['normalisasi']\nallwords = []\nfor wordlist in words:\n  allwords += wordlist"]}, {"cell_type": "code", "execution_count": 1, "id": "8feda2e5", "metadata": {}, "outputs": [], "source": ["mostcommon = FreqDist(allwords).most_common(100)\n\nwordcloud = WordCloud(width=1600, \n                      height=800,\n                      background_color='white').generate(str(mostcommon))\nfig = plt.figure(figsize=(30,10),\n                 facecolor='white')\nplt.imshow(wordcloud,\n           interpolation='bilinear')\nplt.axis('off')\nplt.title('Top Most 100 Common Words', fontsize=100)\n\nplt.tight_layout(pad=0)\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "35c3c644", "metadata": {}, "outputs": [], "source": ["mostcommon_small = FreqDist(allwords).most_common(25)\nx, y = zip(*mostcommon_small)\n\nplt.figure(figsize=(50,30))\nplt.margins(0.02)\nplt.bar(x, y)\nplt.xlabel('Words', fontsize=50)\nplt.ylabel('Frequency of Words', fontsize=50)\nplt.yticks(fontsize=40)\nplt.xticks(rotation=60,\n           fontsize=40)\nplt.title('Frequency of 25 Most Common Words',\n          fontsize=60)\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "68dccec5", "metadata": {}, "outputs": [], "source": ["def gabung(sentence):\n  return ' '.join(sentence)\ntrain_df['normalisasi_text'] = train_df['normalisasi'].apply(gabung)\n\ngroup_by = train_df.groupby('category')['normalisasi_text'].apply(lambda x: Counter(' '.join(x).split()).most_common(25))\ngroup_by_0 = group_by.iloc[0]\nwords0 = list(zip(*group_by_0))[0]\nfreq0 = list(zip(*group_by_0))[1]\nplt.figure(figsize=(50,30))\nplt.bar(words0, freq0)\nplt.xlabel('Words', fontsize=50)\nplt.ylabel('Frequency of Words', fontsize=50)\nplt.yticks(fontsize=40)\nplt.xticks(rotation=60, fontsize=40)\nplt.title('Frequency of 25 Most Common Words for Negative Review', fontsize=60)\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "acb1c16b", "metadata": {}, "outputs": [], "source": ["group_by = train_df.groupby('category')['normalisasi_text'].apply(lambda x: Counter(' '.join(x).split()).most_common(25))\ngroup_by_1 = group_by.iloc[1]\nwords1 = list(zip(*group_by_1))[0]\nfreq1 = list(zip(*group_by_1))[1]\nplt.figure(figsize=(50,30))\nplt.bar(words1, freq1)\nplt.xlabel('Words', fontsize=50)\nplt.ylabel('Frequency of Words', fontsize=50)\nplt.yticks(fontsize=40)\nplt.xticks(rotation=60, fontsize=40)\nplt.title('Frequency of 25 Most Common Words for Positive Review', fontsize=60)\nplt.show()"]}, {"cell_type": "markdown", "id": "51230978", "metadata": {}, "source": ["# 4. Feature Enginering"]}, {"cell_type": "markdown", "id": "d45c3023", "metadata": {}, "source": ["Machine learning hanya bisa memproses data dalam bentuk angka, oleh karena itu diperlukan proses rekayasa fitur *(Feature Enginering)* untuk mengubah data dalam bentuk teks menjadi data dalam bentuk angka. Pada tahap ini kami menggunakan *class* `tf.keras.preprocessing.text.Tokenizer`. \n\n*Class* ini memungkinkan untuk membuat vektor korpus teks, dengan mengubah setiap teks menjadi urutan bilangan bulat `sequence` (setiap bilangan bulat menjadi indeks token dalam `dictionary`) atau menjadi vektor di mana koefisien untuk setiap token dapat berupa biner, berdasarkan jumlah kata dan/atau berdasarkan tf-idf. Argumen yang digunakan dalam *class* ini adalah sebagai berikut:\n- **num_words** : jumlah kata maksimum yang harus disimpan, berdasarkan frekuensi kata. Hanya kata-kata `num_words-1` paling umum yang akan disimpan.\n- **filter** : string di mana setiap elemen adalah karakter yang akan difilter dari teks. Default dari argumen ini adalah semua tanda baca, ditambah tab dan jeda baris, tanpa karakter `'`.\n- **lower** : boolean. Apakah akan mengonversi teks menjadi huruf kecil. Default `True`\n- **split**: str. Pemisah untuk memisahkan kata.\n- **char_level** : jika `True`, setiap karakter akan diperlakukan sebagai token.\n- **oov_token** : jika diberikan, maka akan ditambahkan ke `word_index` dan digunakan untuk mengganti kata-kata yang tidak ada selama pemanggilan fungsi `text_to_sequence`\n\nSelanjutnya kami akan menambahkan *padding* yang bertujuan untuk menyeragamkan panjang dari `sequence`.\n"]}, {"cell_type": "code", "execution_count": 1, "id": "992cd76c", "metadata": {}, "outputs": [], "source": ["# Memisahkan Review dengan Labelnya\nreview = train_df['normalisasi']\nlabel = train_df['category']"]}, {"cell_type": "code", "execution_count": 1, "id": "494c15db", "metadata": {}, "outputs": [], "source": ["# Memecah dataset menjadi training set dan validation set\nreview_latih, review_test, label_latih, label_test = train_test_split(review, label, test_size=0.2, random_state=42)"]}, {"cell_type": "code", "execution_count": 1, "id": "26be8b2d", "metadata": {}, "outputs": [], "source": ["pad_type = 'pre'\ntrunc_type = 'pre'\n\n# Tokenize our training data\ntokenizer = Tokenizer(num_words=1000, oov_token='x')\ntokenizer.fit_on_texts(review_latih)\ntokenizer.fit_on_texts(review_test)\n\n# Encode training data sentences into sequences\nsekuens_latih = tokenizer.texts_to_sequences(review_latih)\nsekuens_test = tokenizer.texts_to_sequences(review_test)\n\n# Get max training sequence length\nmaxlen = max([len(x) for x in sekuens_latih])\n\n# Pad the training sequences\npadded_latih = pad_sequences(sekuens_latih, padding=pad_type, truncating=trunc_type, maxlen=maxlen) \npadded_test = pad_sequences(sekuens_test, padding=pad_type, truncating=trunc_type, maxlen=maxlen)"]}, {"cell_type": "markdown", "id": "42221e50", "metadata": {}, "source": ["# 5. Modeling"]}, {"cell_type": "markdown", "id": "cd8da9d0", "metadata": {}, "source": ["Tahap modeling bertujuan untuk melatih model menggunakan dataset yang tersedia yang kemudian setelah melalui proses training, model tersebut dapat digunakan untuk memprediksi label dari data baru yang belum pernah dijumpai. Pada tahap ini kami menggunakan arsitektur model *Recurrent Neural Networks (RNNs)*. Dalam arsitektur *RNNs* yang kami buat, kami menggunakan 4 layer utama dan menggunakan layer 'dropout' pada masing-masing layer yang bertujuan untuk mengurangi 'overfiting' pada data training. 4 Layer yang kami gunakan adalah sebagai berikut:\n1. Embedding <br>\nLayer Embedding diinisialisasi dengan bobot acak dan akan mempelajari embedding untuk semua kata dalam set data pelatihan. \n2. LSTM (Long Short-Term Memory) <br>\nLSTM adalah jenis arsitektur RNN yang sangat bagus dalam menangani urutan informasi yang panjang. Layer ini mengambil input dimensi (batch size, maxlen, dimensi embedding) dan mengembalikan output dimensi (batch size, 32). Ukuran output yang lebih besar berarti model yang lebih kompleks; kami telah memilih 32 setelah melakukan *tuning* berdasarkan kinerja model.\n3. Dense <br>\nSebuah hidden layer dengan fungsi aktifasi 'relu\"\n4. Dense <br>\nLayer terakhir untuk mengembalikan prediksi review positif atau negatif."]}, {"cell_type": "code", "execution_count": 1, "id": "6c301d46", "metadata": {}, "outputs": [], "source": ["model = tf.keras.Sequential([\n  tf.keras.layers.Embedding(input_dim=1000, output_dim=16),\n  tf.keras.layers.Dropout(0.8),\n  tf.keras.layers.LSTM(32),\n  tf.keras.layers.Dropout(0.8),\n  tf.keras.layers.Dense(32, activation='relu'),\n  tf.keras.layers.Dropout(0.8),\n  tf.keras.layers.Dense(1, activation='sigmoid')\n  ])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy', tfa.metrics.F1Score(num_classes=1, average='macro', threshold=0.5)])"]}, {"cell_type": "markdown", "id": "35b73207", "metadata": {}, "source": ["Berikut *summary* dari model yang kami buat"]}, {"cell_type": "code", "execution_count": 1, "id": "97fe5057", "metadata": {}, "outputs": [], "source": ["model.summary()"]}, {"cell_type": "markdown", "id": "5a9a5e84", "metadata": {}, "source": ["Setelah menyusun arsitektur model RNN, kami melakukan fitting dataset pada model dengan *epochs* 30."]}, {"cell_type": "code", "execution_count": 1, "id": "3811fd6a", "metadata": {}, "outputs": [], "source": ["num_epochs = 30\nhist = model.fit(padded_latih, label_latih,\n                 epochs=num_epochs,\n                 validation_data=(padded_test, label_test),\n                 verbose=2)"]}, {"cell_type": "markdown", "id": "60a75da3", "metadata": {}, "source": ["# 6. Validasi"]}, {"cell_type": "code", "execution_count": 1, "id": "1a6fc32b", "metadata": {}, "outputs": [], "source": ["# summarize history for accuracy\nplt.plot(hist.history['accuracy'])\nplt.plot(hist.history['val_accuracy'])\nplt.title('Training Accuracy vs Validation Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='lower right')\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "000f8fbd", "metadata": {}, "outputs": [], "source": ["# summarize history for F1 Score\nplt.plot(hist.history['f1_score'])\nplt.plot(hist.history['val_f1_score'])\nplt.title('Training F1 Score vs Validation F1 Score')\nplt.ylabel('F1 Score')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='lower right')\nplt.show()"]}, {"cell_type": "markdown", "id": "64bd7a1d", "metadata": {}, "source": ["Dari hasil grafik diatas, skor validasi lebih tinggi daripada skor training disebabkan karena penggunaan layer dropout pada arsitektur model yang kami buat."]}, {"cell_type": "code", "execution_count": 1, "id": "533ddf5c", "metadata": {}, "outputs": [], "source": ["# summarize history for Loss\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title('Training Loss vs Validation Loss')\nplt.ylabel('Loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper right')\nplt.show()"]}, {"cell_type": "markdown", "id": "5aaa2adb", "metadata": {}, "source": ["# 7. Kesimpulan"]}, {"cell_type": "markdown", "id": "ac7e21ee", "metadata": {}, "source": ["Hasil dari model yang kami buat memiliki skor Macro F1 0.89835 pada public leaderboard. skor dari model masih dapat ditingkatkan dengan teknik preprocessing data yang tepat"]}, {"cell_type": "markdown", "id": "4d38e767", "metadata": {}, "source": ["# Referensi"]}, {"cell_type": "markdown", "id": "5f0c5c07", "metadata": {}, "source": ["- https://medium.com/@ksnugroho/dasar-text-preprocessing-dengan-python-a4fa52608ffe\n- https://yunusmuhammad007.medium.com/text-preprocessing-menggunakan-pandas-nltk-dan-sastrawi-untuk-large-dataset-5fb3c0a88571\n- https://dair.ai/Exploratory_Data_Analysis_for_Text_Data/\n- https://keras.io/api/preprocessing/text/\n- https://github.com/google/applied-machine-learning-intensive"]}, {"cell_type": "code", "execution_count": 1, "id": "47d85d60", "metadata": {}, "outputs": [], "source": ["sekuens_submit = tokenizer.texts_to_sequences(test_df['normalisasi'])\npadded_submit = pad_sequences(sekuens_submit, padding=pad_type, truncating=trunc_type, maxlen=maxlen)"]}, {"cell_type": "code", "execution_count": 1, "id": "d2a81169", "metadata": {}, "outputs": [], "source": ["s = model.predict_classes(padded_submit)"]}, {"cell_type": "code", "execution_count": 1, "id": "cba2f151", "metadata": {}, "outputs": [], "source": ["submission['category'] = s\nsubmission.to_csv('submission.csv', index=False)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}