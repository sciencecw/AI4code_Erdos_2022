{"cells": [{"cell_type": "markdown", "id": "9ff5f16b", "metadata": {}, "source": ["# Keras Deep Learning Model hyperparameter optimization using talos "]}, {"cell_type": "markdown", "id": "0fdd3576", "metadata": {}, "source": ["As machine learning progresses throughout the years, Deep learning is becoming more popular machine learning method not only when applying it in image classification tasks but also normal tabular data. Despite all the advantages and hype of deep learning,\nit shares one limitation with traditional machine learning models, hyperparameter optimization. The selection of values for the \nhyperparameters of models like logistical regression, k-means, e.t.c are usually based on trial and error till we get a\nsuitable value for our evaluation metric of our choosing. Deep learning models are no different, In the case of keras, when\nwe build models, we have to fine-tune hyperparameters like batch size, number of epochs, optimizer used in order to get a acceptable evaluation metric value.Of course, one of the more effcient ways we can go about hyperparameter optimization would be to\nuse tools like gridsearchCV and randomsearchCV but in the case of deep learning models using keras to build either Recurrent Neural Networks (RNNs) or Convulusion Neural Networks (CNNs),\nwe can use a hyperparameter optimization library called talos to fine tune these hyperparameters. In a demonstration below, the Heart Disease UCI dataset by from kaggle will be used to demonstrate how talos can be used to make hyperparameter optimization more efficient than compared to the standard trial and error"]}, {"cell_type": "markdown", "id": "c28fcbd1", "metadata": {}, "source": ["# Loading the Heart_diease dataset"]}, {"cell_type": "markdown", "id": "094e3993", "metadata": {}, "source": ["Importing the neccessary modules for preprocessing"]}, {"cell_type": "code", "execution_count": 1, "id": "5eb88181", "metadata": {}, "outputs": [], "source": ["import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split"]}, {"cell_type": "markdown", "id": "656fd9cb", "metadata": {}, "source": ["Loading up the dataset"]}, {"cell_type": "code", "execution_count": 1, "id": "1f686450", "metadata": {}, "outputs": [], "source": ["df = pd.read_csv(r\"../input/heart.csv\")"]}, {"cell_type": "markdown", "id": "30419f44", "metadata": {}, "source": ["# Some exploratory data analysis"]}, {"cell_type": "markdown", "id": "4975a4da", "metadata": {}, "source": ["So as we can see below, the data contains some factors such as age, sex, e.t.c that might affect the 'target' value to be either 0 or 1. "]}, {"cell_type": "code", "execution_count": 1, "id": "0527455a", "metadata": {}, "outputs": [], "source": ["df.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "bd38155a", "metadata": {}, "outputs": [], "source": ["# Some distribution plots\n\nf, axes = plt.subplots(2, 2, figsize=(10, 10)) #sharex=True)\nsns.distplot(df[\"age\"][df[\"target\"]==1], color=\"red\", label=\"heart diease\", ax=axes[0, 0])\nsns.distplot(df[\"age\"][df[\"target\"]==0] , color=\"skyblue\", label=\"No heart diease\", ax=axes[0, 0])\n\nsns.distplot(df[\"chol\"][df[\"target\"]==1], color=\"red\", label=\"heart diease\", ax=axes[0, 1])\nsns.distplot(df[\"chol\"][df[\"target\"]==0] , color=\"skyblue\", label=\"No heart diease\", ax=axes[0, 1])\n\nsns.distplot(df[\"thalach\"][df[\"target\"]==1], color=\"red\", label=\"heart diease\", ax=axes[1, 0])\nsns.distplot(df[\"thalach\"][df[\"target\"]==0] , color=\"skyblue\", label=\"No heart diease\", ax=axes[1, 0])\n\nsns.distplot(df[\"trestbps\"][df[\"target\"]==1], color=\"red\", label=\"heart diease\", ax=axes[1, 1])\nsns.distplot(df[\"trestbps\"][df[\"target\"]==0] , color=\"skyblue\", label=\"No heart diease\", ax=axes[1, 1])\nplt.legend()\n\nplt.show()\n"]}, {"cell_type": "code", "execution_count": 1, "id": "9d4583b2", "metadata": {}, "outputs": [], "source": ["sns.heatmap(df.corr(),annot=True,cmap='RdYlGn') \nfig=plt.gcf()\nfig.set_size_inches(10,8)\nplt.show\n\n# Well, there is nothing too striking in the correlation heatmap"]}, {"cell_type": "markdown", "id": "ef75e916", "metadata": {}, "source": ["# Some data preprocessing"]}, {"cell_type": "markdown", "id": "9e2f71db", "metadata": {}, "source": ["Importing Keras modules"]}, {"cell_type": "code", "execution_count": 1, "id": "3122fffb", "metadata": {}, "outputs": [], "source": ["from keras.optimizers import SGD \n\nfrom keras.utils import np_utils\nfrom keras.models import Sequential # The common deep learning network\nfrom keras.layers.core import Dense, Activation\nfrom keras.activations import relu, elu, selu, sigmoid, exponential, tanh"]}, {"cell_type": "markdown", "id": "1fa335bc", "metadata": {}, "source": ["Y will be the targets 0 or 1 under the targets col while X will be the data in the rest of the cols which we will use as factors that we assume will affect the outcome of the target "]}, {"cell_type": "code", "execution_count": 1, "id": "1d0f831e", "metadata": {}, "outputs": [], "source": ["X = df.drop(\"target\",axis=1)\ny = df[\"target\"].values"]}, {"cell_type": "markdown", "id": "2244756b", "metadata": {}, "source": ["Like all modeling practises, we need to split the data up into the training set (the data that we will use to train the model with) and the testing set (the data that we will use to test the accuracy of the model). In this case, I will be spliting it into a 75% (training set), 25% (testing set) ratio"]}, {"cell_type": "code", "execution_count": 1, "id": "8e6178e5", "metadata": {}, "outputs": [], "source": ["X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=42)"]}, {"cell_type": "markdown", "id": "c09c0848", "metadata": {}, "source": ["Lets inspect the shape of our training data"]}, {"cell_type": "code", "execution_count": 1, "id": "c635dd48", "metadata": {}, "outputs": [], "source": ["X_train.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "5f2b9d76", "metadata": {}, "outputs": [], "source": ["y_train.shape"]}, {"cell_type": "markdown", "id": "f75527ae", "metadata": {}, "source": ["# Scalling the data"]}, {"cell_type": "markdown", "id": "8813ada8", "metadata": {}, "source": ["As for most datasets out there, it is necessary to scale the data before fitting it with our model"]}, {"cell_type": "code", "execution_count": 1, "id": "0f94d14e", "metadata": {}, "outputs": [], "source": ["scaler = StandardScaler()"]}, {"cell_type": "code", "execution_count": 1, "id": "d3288505", "metadata": {}, "outputs": [], "source": ["X_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled  = scaler.fit_transform(X_test)"]}, {"cell_type": "markdown", "id": "ed88c78b", "metadata": {}, "source": ["For my keras model, there is one parameter that I do not want to change, that is the number of classes that my model has to predict, hence, I will set a variable to it"]}, {"cell_type": "code", "execution_count": 1, "id": "b838976d", "metadata": {}, "outputs": [], "source": ["NB_CLASSES = 2 # Number of classes to predict"]}, {"cell_type": "markdown", "id": "f110aa33", "metadata": {}, "source": ["Binarising the target variables \n"]}, {"cell_type": "code", "execution_count": 1, "id": "ec01179c", "metadata": {}, "outputs": [], "source": ["y_train_new = np_utils.to_categorical(y_train,NB_CLASSES)\ny_test_new = np_utils.to_categorical(y_test,NB_CLASSES)"]}, {"cell_type": "markdown", "id": "3d340082", "metadata": {}, "source": ["# Import talos"]}, {"cell_type": "markdown", "id": "ca28b697", "metadata": {}, "source": ["Importing talos which can be use as hyperparameter optimization for our keras model"]}, {"cell_type": "code", "execution_count": 1, "id": "47e44b99", "metadata": {}, "outputs": [], "source": ["import talos as ts"]}, {"cell_type": "markdown", "id": "3155d038", "metadata": {}, "source": ["Now, I will be setting the values that I want to test out for the hyperparameters of my model. In all I will be setting 6 different hyperparameters with a set of values that I want to test out "]}, {"cell_type": "code", "execution_count": 1, "id": "4c911eba", "metadata": {}, "outputs": [], "source": ["from keras.activations import relu, elu, selu, sigmoid, exponential, tanh\n\np = {\n    'activation': [relu, elu, selu, sigmoid, exponential, tanh],\n    'batch_size': [64,128,256],\n    'First_Neron' : [64,128,256],\n    'Second_Neron' : [64,128,256],\n    'Third_Neron' : [64,128,256],\n    'epochs': [50,100,150,200]\n}"]}, {"cell_type": "markdown", "id": "b1fb01ad", "metadata": {}, "source": ["# Building our keras model for hyperparameter optimization"]}, {"cell_type": "markdown", "id": "cd6b32e2", "metadata": {}, "source": ["For our model, I've a function that will build, compile and fit the model with the training data, The model have has to be of a standard structure before testing in terms of the number of hidden layers. In this case, I've decided to use 3 dense layers only. However, If you were to look closer, this model is a bit different from how a keras model is normally builded as the hyperparameters that I've decided to finetune are fitted with the variable 'params' which will then be scanned by talos."]}, {"cell_type": "code", "execution_count": 1, "id": "67434415", "metadata": {}, "outputs": [], "source": ["def get_model_talos(X_train,y_train,X_test,y_test,params):\n    model = Sequential() # load the sequential model\n    model.add(Dense(NB_CLASSES, input_shape=(13,), activation=params['activation'])) # add a dense layer\n    model.add(Dense(params['First_Neron'], input_shape=(13,), activation=params['activation']))\n    model.add(Dense(params['Second_Neron'], input_shape=(13,), activation=params['activation']))\n    model.add(Dense(params['Third_Neron'], input_shape=(13,), activation=params['activation']))\n    model.add(Dense(NB_CLASSES, input_shape=(13,))) # add a dense layer\n    model.add(Activation('softmax'))\n    \n    model.compile(loss='binary_crossentropy', optimizer=SGD(), metrics=['accuracy'])\n    out = model.fit(X_train, y_train,\n                    epochs=params['epochs'],\n                    batch_size=params['batch_size'],\n                    validation_data=(X_test,y_test),\n                    verbose=0)\n            \n    return out, model"]}, {"cell_type": "markdown", "id": "a1158297", "metadata": {}, "source": ["Once we have created our model, we use Scan() to perform hyper parameter optimization on the 6 hyperparameters that I've selected and the values that I want to test it out on."]}, {"cell_type": "code", "execution_count": 1, "id": "33fb2feb", "metadata": {}, "outputs": [], "source": ["t = ts.Scan(x=X_train_scaled, y=y_train_new, x_val=X_test_scaled, y_val=y_test_new, params=p, model=get_model_talos)"]}, {"cell_type": "markdown", "id": "726495bf", "metadata": {}, "source": ["As you can see, the process will take quite long so, if you were to try this out, I would recommend going for the a coffee break or 2 while we wait for the tunning to complete."]}, {"cell_type": "markdown", "id": "49dbbdcc", "metadata": {}, "source": ["Talos kind of works the same as gridsearchCV or RandomSearchCV, the more hyperparameter values you want to test out, the more time it will take to finetune those parameters"]}, {"cell_type": "markdown", "id": "86826ddf", "metadata": {}, "source": ["Once the tunning is done, we want to see the results of the tunning, To do this, I will be using Reporting() from talos"]}, {"cell_type": "code", "execution_count": 1, "id": "f6f3a652", "metadata": {}, "outputs": [], "source": ["from talos import Reporting\nr = Reporting(t)"]}, {"cell_type": "markdown", "id": "df5b6323", "metadata": {}, "source": ["The high() returns the highest evaluation metric in your hyperparameter optimization, In my case, it would be the validation accuracy "]}, {"cell_type": "code", "execution_count": 1, "id": "30b59216", "metadata": {}, "outputs": [], "source": ["r.high()"]}, {"cell_type": "markdown", "id": "dcc30221", "metadata": {}, "source": ["Hmmm, not too bad, I think I'm going to stick with this result"]}, {"cell_type": "markdown", "id": "43124b43", "metadata": {}, "source": ["Lets see what round of optimization produced that result"]}, {"cell_type": "code", "execution_count": 1, "id": "75f8db2f", "metadata": {}, "outputs": [], "source": ["r.rounds2high()"]}, {"cell_type": "markdown", "id": "0757fa0d", "metadata": {}, "source": ["I would also like to see remaining model parameters that came close to the highest validation accuracy as well"]}, {"cell_type": "code", "execution_count": 1, "id": "20a379ae", "metadata": {}, "outputs": [], "source": ["frame = t.data"]}, {"cell_type": "code", "execution_count": 1, "id": "02197425", "metadata": {}, "outputs": [], "source": ["frame[frame[\"val_acc\"] >= '0.89']"]}, {"cell_type": "markdown", "id": "e88c75fb", "metadata": {}, "source": ["test number 1864 list of parameters"]}, {"cell_type": "markdown", "id": "5940e2c5", "metadata": {}, "source": ["# Deploying the model using talos"]}, {"cell_type": "markdown", "id": "1edeac53", "metadata": {}, "source": ["I'm statisfied with the validation accuracy result above so I'm going to save my model details so that I can use it again in the future.\nTo do this, I'm going to use Deploy() and Restore() to save and load up my model using talos\nNote that the Deploy() will automatically take in the best model parameters for your model based on your specified evaluation metric when tunning the hyperparameters earlier."]}, {"cell_type": "code", "execution_count": 1, "id": "bd34bf84", "metadata": {}, "outputs": [], "source": ["ts.Deploy(t, 'heart_diease_predmodel');"]}, {"cell_type": "code", "execution_count": 1, "id": "3765d697", "metadata": {}, "outputs": [], "source": ["heart_diease_predmodel = ts.Restore('heart_diease_predmodel.zip')"]}, {"cell_type": "markdown", "id": "4bbad310", "metadata": {}, "source": ["Once that is done, I can use the model to predict my target values by inputing my own X_values using heart_diease_predmodel.model.predict_classes(X_values)"]}, {"cell_type": "markdown", "id": "9043723a", "metadata": {}, "source": ["Of course, I can try and continue to raise the accuracy of the model by trying out more possible hyperparameter choices, doing some feature engineering or changing the structure of my model but all in all, I think talos demonstrated the efficiency in which a keras model can be fine tunned and builded."]}, {"cell_type": "markdown", "id": "2e0ef6ea", "metadata": {}, "source": ["In conclusion, I just want to thank the creators of talos for such an awesome library. It really makes hyperparameter optimization much more efficient for deep learning models. This is my first kernel published on hyperparameter optimization on deep learning models as well as my first kernel on Kaggle and any form of constructive feedbacks are appreciated. Thank You"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}