{"cells": [{"cell_type": "markdown", "id": "53b9eefb", "metadata": {}, "source": ["![](https://marychin.org/download/kaggle/tabmar.png)"]}, {"cell_type": "markdown", "id": "d5925551", "metadata": {}, "source": ["Update 10th March:\n* Plot ROC curves: \n* - manual back-of-envelop calculation (excellent refresher);\n* - sklearn.metrics.plot_roc_curve.\n* ```roc_auc_score``` with and without ```average='micro'``` option: no difference found.\n\nThis is the first walkthrough of the March Playground:\n* identifying troublesome features such as ```cat10```, ```cat5```, ```cat8```, ```cat7``` and others, which require handling;\n* running quick-and-dirty baselines (without parameters tweaking) using LightGBM, XGBoost, CatBoost and Random Forests;\n* looking at gains and feature rankings from LightGBM, XGBoost, CatBoost and Random Forests;\n* running BorutaShap, which reports each feature as either confirmed important, unimportant or tentative."]}, {"cell_type": "code", "execution_count": 1, "id": "362bb0f0", "metadata": {}, "outputs": [], "source": ["import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"whitegrid\")\nsns.set_palette('hot')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, plot_roc_curve\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nimport sys, glob, copy, warnings, time\nwarnings.simplefilter('ignore') # once | error | always | default | module\n\ninp = '/kaggle/input/tabular-playground-series-mar-2021/'"]}, {"cell_type": "code", "execution_count": 1, "id": "f0ef3106", "metadata": {}, "outputs": [], "source": ["df, features = {}, {}\nprint('{:18s}{:>10s}{:>5s}{:>5s}'.format('FILE', 'ROWS', 'COLS', 'NULL'))\nfor file in glob.glob(f'{inp}/*.csv'):\n    label = file.split('/')[-1].split('.')[0]\n    df[label] = pd.read_csv(file, index_col='id')\n    features[label] = set(df[label].columns.to_list())\n    print('{:18s}{:10,d}{:5d}{:5d}'.format(label, *df[label].shape, df[label].isna().any().sum()))"]}, {"cell_type": "code", "execution_count": 1, "id": "695952d5", "metadata": {}, "outputs": [], "source": ["(df['sample_submission'].index == df['test'].index).all()\n# Straightforward if True."]}, {"cell_type": "code", "execution_count": 1, "id": "f55773f0", "metadata": {}, "outputs": [], "source": ["features['train'] == features['test'].union(features['sample_submission'])\n# Straightforward if True."]}, {"cell_type": "code", "execution_count": 1, "id": "9d3ef0ad", "metadata": {}, "outputs": [], "source": ["df['train'].sample(5)"]}, {"cell_type": "markdown", "id": "d0fff669", "metadata": {}, "source": ["## Datatypes"]}, {"cell_type": "code", "execution_count": 1, "id": "9f451507", "metadata": {}, "outputs": [], "source": ["sr = pd.DataFrame(df['train'].dtypes, columns=['dtype'])\nfor dtype, dtype_data in sr.groupby('dtype'):\n    print('{:2d} columns of dtype {}\\n{}'.format(len(dtype_data), dtype, '='*10))\n    print(dtype_data.index.to_list(), '\\n')"]}, {"cell_type": "code", "execution_count": 1, "id": "7b9ffef0", "metadata": {}, "outputs": [], "source": ["df['train']['target'].unique()\n# target is in fact categorical, not continuous."]}, {"cell_type": "code", "execution_count": 1, "id": "b34442fc", "metadata": {}, "outputs": [], "source": ["df['train'].describe(include='float')"]}, {"cell_type": "code", "execution_count": 1, "id": "dd6fdd30", "metadata": {}, "outputs": [], "source": ["df['train'].describe(include='object')"]}, {"cell_type": "code", "execution_count": 1, "id": "6de42597", "metadata": {}, "outputs": [], "source": ["features = {'cat': df['train'].columns[ df['train'].columns.str.startswith('cat') ].to_list(),\n            'con': df['train'].columns[ df['train'].columns.str.startswith('con') ].to_list(),\n            'num': df['train'].select_dtypes(include=[float, int]).columns.to_list()}\n# We are going to use these over and over; save us from having to do dot-columns again and again.\nfeatures"]}, {"cell_type": "markdown", "id": "158c7527", "metadata": {}, "source": ["## Categories & encoding"]}, {"cell_type": "code", "execution_count": 1, "id": "c872ed12", "metadata": {}, "outputs": [], "source": ["unik = {'train'    : {}, # to hold unique categorical values from train\n        'test'     : {}} # to hold unique categorical values from test\nprint('{:<8s}{} {}'.format('FEATURE', 'NUNIQUE', 'UNIQUE VALUES IN TRAIN'))\n# Print list of unique values starting from the lowest nunique, in that order. Features near the bottom are the troublesome ones.\nfor feature in df['train'][features['cat']].nunique().sort_values().index:\n    unik['train'].update({feature: set(sorted(df['train'][feature].unique()))})\n    unik['test'].update({feature: set(sorted(df['test'][feature].unique()))})\n    print('{:<8s}{:7d} {}'.format(feature, len(unik['train'][feature]), str(unik['train'][feature])))"]}, {"cell_type": "code", "execution_count": 1, "id": "4a174210", "metadata": {}, "outputs": [], "source": ["print('{:<8s}{:76s}'.format('FEATURE', 'UNIQUE VALUES IN TRAIN'))\nfor feature in features['cat']:\n    if unik['train'][feature]!=unik['test'][feature]:\n        print('in train but not in test:', feature, unik['train'][feature].difference(unik['test'][feature]))\n        print('in test but not in train:', feature, unik['test'][feature].difference(unik['train'][feature]))"]}, {"cell_type": "code", "execution_count": 1, "id": "c8189562", "metadata": {}, "outputs": [], "source": ["ncoda = OrdinalEncoder().fit(pd.concat([ df['train'][features['cat']], \n                             df['test'][features['cat']] ]))\n# For sanity check only; will be deleted real soon:\norig = copy.deepcopy(df)\nfor dataset in ['train', 'test']:\n    df[dataset][features['cat']] = ncoda.transform(df[dataset][features['cat']])\n    df[dataset][features['cat']] = df[dataset][features['cat']].astype(int)# .astype('category')\nncoda.categories_"]}, {"cell_type": "code", "execution_count": 1, "id": "4998c725", "metadata": {}, "outputs": [], "source": ["# Just a pedantic sanity check.\nassert (ncoda.inverse_transform(df['train'][features['cat']]) == orig['train'][features['cat']]).all().all()\nassert (ncoda.inverse_transform(df['test'][features['cat']]) == orig['test'][features['cat']]).all().all()\ndel orig   # Deleted as promised."]}, {"cell_type": "code", "execution_count": 1, "id": "061cd870", "metadata": {}, "outputs": [], "source": ["df['train'].info()"]}, {"cell_type": "markdown", "id": "2e5ec51f", "metadata": {}, "source": ["## Distribution: categorical features by target"]}, {"cell_type": "code", "execution_count": 1, "id": "089f577e", "metadata": {}, "outputs": [], "source": ["valuecount2D = pd.DataFrame()\nfor nfeature, feature in enumerate(features['cat']):\n    tis = {'feature': feature}\n    for group, group_data in df['train'].groupby(feature):\n        tis['feature_category'] = group\n        if group_data['target'].value_counts().nunique()==1:\n            print(feature, group)\n        for tis['target'], tis['count'] in group_data['target'].value_counts().iteritems():\n            valuecount2D = pd.concat([valuecount2D, pd.DataFrame(tis, index=[f'{feature}_{group}_{tis[\"target\"]}'])])\n# valuecount2D.reset_index(drop=True, inplace=True)\nvaluecount2D.rename(columns={0: 'target=0', 1: 'target=1'}, inplace=True)\nvaluecount2D"]}, {"cell_type": "code", "execution_count": 1, "id": "0774a40c", "metadata": {}, "outputs": [], "source": ["# sanity\nauto = valuecount2D.loc['cat18_3_1', 'count']\nmanual = len(df['train'].query('target==1 and cat18==3'))\nif auto==manual:\n    print('sane')\nelse:\n    print('insane')"]}, {"cell_type": "code", "execution_count": 1, "id": "11e8bd1a", "metadata": {}, "outputs": [], "source": ["cols = 3\nrows = int(np.ceil(len(features['cat'])/cols))\nfig, ax = plt.subplots(rows, cols, figsize=(15, 7*rows), sharex=True)\n# As before, start with well-behaved features, with the problematic ones at the end, in that order.\nfor nfeature, feature in enumerate(df['train'][features['cat']].nunique().sort_values().index):\n    tis_ax = ax[nfeature//cols][nfeature%cols]\n    sns.barplot(data=valuecount2D.loc[valuecount2D['feature']==feature], \n                x='count', y='feature_category', hue='target', orient='h', ax=tis_ax, palette='hot')\n    tis_ax.set_title(feature)\n# As warned by earlier text output we find cat5 and cat10 screaming for attention."]}, {"cell_type": "markdown", "id": "f90ca266", "metadata": {}, "source": ["## Distribution: continuous features"]}, {"cell_type": "code", "execution_count": 1, "id": "86093bf7", "metadata": {}, "outputs": [], "source": ["%%time\nplt.figure(figsize=(15, 5))\nsns.violinplot(data=df['train'][ features['con'] ])"]}, {"cell_type": "code", "execution_count": 1, "id": "2ada478d", "metadata": {}, "outputs": [], "source": ["tmp = df['train'][features['con']]\nxx = tmp.mean()\nyy = tmp.median()\nplt.figure(figsize=(10, 10))\nplt.plot([xx.min(), xx.max()], [yy.min(), yy.max()], 'y-.')\nplt.plot(xx, yy, '.r')\nfor x, y, z in zip(xx, yy, tmp):\n    plt.text(x+.005, y, z)\n_ = plt.axis('equal'); plt.xlabel('feature mean'); plt.ylabel('feature median')"]}, {"cell_type": "markdown", "id": "7bf0dfa3", "metadata": {}, "source": ["## Distribution: continuous features by target"]}, {"cell_type": "code", "execution_count": 1, "id": "7f577ca4", "metadata": {}, "outputs": [], "source": ["cols = 2\nrows = int(np.ceil(len(features['con'])/cols))\nfig, ax = plt.subplots(rows, cols, figsize= (15, 5*rows))\nfor nfeature, feature in enumerate(features['con']):\n    sns.histplot(data=df['train'], y=feature, hue='target', stat='density', ax=ax[nfeature//cols, nfeature%cols], palette='hot')"]}, {"cell_type": "markdown", "id": "520b545b", "metadata": {}, "source": ["## 2D flood maps: how features pair cross-talk\nSeaborn has one-liners for this; but runs till eternity without returning. Here is therefore a dirty hack."]}, {"cell_type": "code", "execution_count": 1, "id": "60bd9c39", "metadata": {}, "outputs": [], "source": ["traintest = pd.concat([df['train'], df['test']])\nbinned = traintest[features['con']].apply(lambda x: pd.cut(x, bins=32, labels=False))\nplt.figure(figsize=(15, 15))\nnfeatures = len(features['con'])\nfor aa in range(1, nfeatures):\n    for bb in range(aa):\n        plt.subplot(nfeatures, nfeatures, aa*nfeatures + bb + 1)\n        sns.heatmap(binned.groupby(features['con'][aa]).apply(lambda x: x[features['con'][bb]].value_counts()).unstack(), \n                    square=True, cmap='hot', cbar=False, xticklabels=False, yticklabels=False)\n        plt.axis('off')\nfor tmp in range(1, nfeatures):\n    plt.subplot(nfeatures, nfeatures, nfeatures*tmp+1)\n    plt.axis('on'); plt.ylabel(features['con'][tmp])\nfor tmp in range(nfeatures-1):\n    plt.subplot(nfeatures, nfeatures, nfeatures*(nfeatures-1)+tmp+1)\n    plt.axis('on'); plt.xlabel(features['con'][tmp])\nfor tmp in range(1, nfeatures-1):\n    plt.subplot(nfeatures, nfeatures, nfeatures*(nfeatures-1)+tmp+1)\n    plt.ylabel('')"]}, {"cell_type": "code", "execution_count": 1, "id": "75db6c41", "metadata": {}, "outputs": [], "source": ["%%time\ncorr = traintest.corr()\ncorr.to_csv('corr.csv')\nplt.figure(figsize=(15, 15))\nsns.heatmap(corr, mask=np.triu(np.ones_like(corr, dtype=bool)), annot=True, fmt='.1f', linewidths=.5, square=True, cmap='hot', annot_kws={'size': 10}, cbar_kws={\"shrink\": .5})"]}, {"cell_type": "code", "execution_count": 1, "id": "022b7863", "metadata": {}, "outputs": [], "source": ["slimcorr = pd.Series(dtype=float)\nfor feature in corr.columns:\n    slimcorr.loc[feature] = corr[feature].sort_values()[-2]\nslimcorr.sort_values(ascending=False)\n# output reports no correlation too high; therefore too premature to drop any feature"]}, {"cell_type": "markdown", "id": "acb825fd", "metadata": {}, "source": ["## 4 baselines before tuning"]}, {"cell_type": "code", "execution_count": 1, "id": "ac623df3", "metadata": {}, "outputs": [], "source": ["dataX = df['train'].copy()\ndatay = dataX.pop('target')\ntrainX, validX, trainy, validy = train_test_split(dataX, datay)\n\ndef trainNpredict(model):\n    tic = time.time()\n    pred = model.fit(trainX, trainy).predict_proba(validX)[:, 1]\n    roc_auc = roc_auc_score(validy, pred, average='micro')\n    print(\"roc_auc_score(validy, pred, average='micro') =\", roc_auc)\n    print(\"roc_auc_score(validy, pred) =\", roc_auc_score(validy, pred))\n#   plot ROC curve\n    return model, time.time()-tic, roc_auc\n\nmodel, tictoc, roc_auc = {}, pd.Series(dtype=float), pd.Series(dtype=float)"]}, {"cell_type": "code", "execution_count": 1, "id": "827eccc3", "metadata": {}, "outputs": [], "source": ["label = 'rf'\nmodel[label], tictoc[label], roc_auc.loc[label] = trainNpredict(RandomForestClassifier(n_estimators=200, max_depth=7))"]}, {"cell_type": "code", "execution_count": 1, "id": "a2c20cd8", "metadata": {}, "outputs": [], "source": ["label = 'lgb'\nmodel[label], tictoc[label], roc_auc.loc[label] = trainNpredict(LGBMClassifier(**{'is_unbalance': True}))"]}, {"cell_type": "code", "execution_count": 1, "id": "ac7bfed4", "metadata": {}, "outputs": [], "source": ["label = 'xgb'\nscale_pos_weight = (df['train']['target']==0).sum() / (df['train']['target']==1).sum()\nmodel[label], tictoc[label], roc_auc.loc[label] = trainNpredict(XGBClassifier(**{'scale_pos_weight': scale_pos_weight}))"]}, {"cell_type": "code", "execution_count": 1, "id": "2c8e5e9c", "metadata": {}, "outputs": [], "source": ["label = 'cat'\nmodel[label], tictoc[label], roc_auc.loc[label] = trainNpredict(CatBoostClassifier(**{'scale_pos_weight': scale_pos_weight}))"]}, {"cell_type": "code", "execution_count": 1, "id": "b615f37d", "metadata": {}, "outputs": [], "source": ["roc_auc.sort_values(ascending=False)"]}, {"cell_type": "code", "execution_count": 1, "id": "deb22cb5", "metadata": {}, "outputs": [], "source": ["tictoc.sort_values()"]}, {"cell_type": "markdown", "id": "d2968be3", "metadata": {}, "source": ["## Plot ROC curves"]}, {"cell_type": "code", "execution_count": 1, "id": "ddeb1ba8", "metadata": {}, "outputs": [], "source": ["# manual back-of-envelop calculation\nplt.figure(figsize=(7, 5))\nkolor = {'rf': 'r',\n         'lgb': 'g',\n         'xgb': 'b',\n         'cat': 'k'}\nfor k, v in model.items():\n    df['train'][k] = v.predict_proba(dataX)[:, 1]\n    for threshold in np.linspace(df['train'][k].min(), df['train'][k].max(), 100):\n        positive = df['train'][k]>threshold\n        true_positive = positive & (datay==1)\n        false_positive = positive & (datay==0)\n        plt.plot(false_positive.sum()/len(df['train']), true_positive.sum()/len(df['train']), '.', color=kolor[k])\n        plt.xlabel('false positives'); plt.ylabel('true ppsitives')"]}, {"cell_type": "code", "execution_count": 1, "id": "31909615", "metadata": {}, "outputs": [], "source": ["# auto: sklearn.metrics.plot_roc_curve\nfor k in model.keys():\n    plot_roc_curve(model[k], dataX, datay, color=kolor[k])"]}, {"cell_type": "markdown", "id": "4fd789bc", "metadata": {}, "source": ["## Pick the best baseline, submit and see"]}, {"cell_type": "code", "execution_count": 1, "id": "866f2b74", "metadata": {}, "outputs": [], "source": ["df['sample_submission']['target'] = model[roc_auc.idxmax()].predict_proba(df['test'])[:, 1]\ndf['sample_submission'].to_csv('submission.csv')"]}, {"cell_type": "markdown", "id": "03a08530", "metadata": {}, "source": ["## Gain"]}, {"cell_type": "code", "execution_count": 1, "id": "1636a1ed", "metadata": {}, "outputs": [], "source": ["gain = pd.DataFrame(index=trainX.columns)\nfor treetype in model.keys():\n    gain[treetype] = model[treetype].feature_importances_\ngain.rank().astype(int).sort_values(by='lgb')"]}, {"cell_type": "markdown", "id": "d9734787", "metadata": {}, "source": ["## BorutaShap"]}, {"cell_type": "code", "execution_count": 1, "id": "e818d93f", "metadata": {}, "outputs": [], "source": ["if 'BorutaShap' not in sys.modules:\n    !pip install BorutaShap\nfrom BorutaShap import BorutaShap"]}, {"cell_type": "code", "execution_count": 1, "id": "a80e353a", "metadata": {}, "outputs": [], "source": ["Feature_Selector = BorutaShap(model=XGBClassifier(**{'tree_method':'gpu_hist'}), importance_measure='shap')   # importance_measure='gini'\n# Feature_Selector = BorutaShap(model=LGBMClassifier(), importance_measure='shap')\nFeature_Selector.fit(X=dataX, y=datay, n_trials=1000, verbose=False) # sample=False, train_or_test = 'test', normalize=True, verbose=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "0662cd7b", "metadata": {}, "outputs": [], "source": ["Feature_Selector.results_to_csv(filename='borutashap.csv')\nFeature_Selector.plot(which_features='all')  # X_size=15, figsize=(12,8), y_scale='log'"]}, {"cell_type": "code", "execution_count": 1, "id": "7dd39e60", "metadata": {}, "outputs": [], "source": ["Feature_Selector.accepted"]}, {"cell_type": "code", "execution_count": 1, "id": "474f25b6", "metadata": {}, "outputs": [], "source": ["Feature_Selector.features_to_remove"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}