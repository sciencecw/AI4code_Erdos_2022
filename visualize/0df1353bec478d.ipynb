{"cells": [{"cell_type": "markdown", "id": "c3fae2ea", "metadata": {}, "source": ["![image.png](attachment:8ec6e554-0d95-4868-ba79-ddab755b449c.png)\n# Use a Community QLattice to solve the rain prediction in very little time\n\nThe QLattice is a supervised machine learning tool for symbolic regression developed by [Abzu](https://www.abzu.ai) . It is inspired by Richard Feynman's path integral formulation. That's why the python module to use it is called *Feyn*, and the *Q* in QLattice is for Quantum.\n\nAbzu provides free QLattices for non-commercial use to anyone. These free community QLattices gets allocated for you automatically if you use Feyn without an active subscription, as we will do in this notebook. Read more about how it works here: https://docs.abzu.ai/docs/guides/getting_started/community.html\n\nThe feyn Python module is not installed on Kaggle by default so we have to pip install it first. \n\n__Note__: the pip install will fail unless you enable *Internet* in the *settings* to the right --->"]}, {"cell_type": "code", "execution_count": 1, "id": "b3e3f2be", "metadata": {}, "outputs": [], "source": ["!pip install feyn"]}, {"cell_type": "markdown", "id": "1b1af83d", "metadata": {}, "source": ["# Python imports\nIn this notebook we will use only two python modules: the `feyn` module to access the QLattice, and the `pandas` module to access the data"]}, {"cell_type": "code", "execution_count": 1, "id": "b731b5d7", "metadata": {}, "outputs": [], "source": ["import feyn\nimport pandas as pd"]}, {"cell_type": "markdown", "id": "b7bdcced", "metadata": {}, "source": ["# Data\nRead in the data and have a quick look at it:"]}, {"cell_type": "code", "execution_count": 1, "id": "d9879f6d", "metadata": {}, "outputs": [], "source": ["data = '/kaggle/input/weather-dataset-rattle-package/weatherAUS.csv'\ndf = pd.read_csv(data)\ndf"]}, {"cell_type": "markdown", "id": "05ad1010", "metadata": {}, "source": ["# First impressions:\nWe notice that:\n- The target variable is `RainingTomorrow`.\n- All the other variables are possible predictors\n\n## The date column\nWe have the `Date` of each observation. This should probably not be used as a predictor as it could lead to overfitting. However, the time of year of the observation is perhaps a good predictor. So we will create a `week` of year column to allow the model to capture seasonality."]}, {"cell_type": "code", "execution_count": 1, "id": "13b1312a", "metadata": {}, "outputs": [], "source": ["df[\"week\"] = pd.to_datetime(df.Date).dt.isocalendar().week.astype(int)\ndf = df.drop(\"Date\", axis=1)"]}, {"cell_type": "markdown", "id": "8bf09c30", "metadata": {}, "source": ["# Data types\nWe observe that we have categorical (dtype `object`) and numerical columns. The QLattice will work fine with the categorical and numerical data. We just need to tell it which is which. We will create a `dict` names `stypes` with this information which we will pass to the QLattice later. Read more about categorical data in the QLattice here: https://docs.abzu.ai/docs/guides/essentials/stypes.html"]}, {"cell_type": "code", "execution_count": 1, "id": "5e0dc11a", "metadata": {}, "outputs": [], "source": ["df.dtypes"]}, {"cell_type": "code", "execution_count": 1, "id": "453738bc", "metadata": {}, "outputs": [], "source": ["stypes = {\n    \"Location\": \"cat\",\n    \"WindGustDir\": \"cat\",\n    \"WindDir9am\": \"cat\",\n    \"WindDir3pm\": \"cat\",\n    \"RainToday\": \"cat\",\n    \"week\": \"cat\",\n}"]}, {"cell_type": "markdown", "id": "25fbd6fb", "metadata": {}, "source": ["# Deal with missing data.\nWe have 145,460 observations. Let's check for missing data"]}, {"cell_type": "code", "execution_count": 1, "id": "bc02f20e", "metadata": {}, "outputs": [], "source": ["df.isna().sum()"]}, {"cell_type": "markdown", "id": "f79e8ddf", "metadata": {}, "source": ["# The target variable\nThere are over 3000 observations with no data for the target variable. We drop those observations up front\n\nWe also notice that the target variable is boolean but expressed as a string. Let us convert it to a proper boolean (0/1)"]}, {"cell_type": "code", "execution_count": 1, "id": "a0fb1544", "metadata": {}, "outputs": [], "source": ["df = df.dropna(subset=[\"RainTomorrow\"])\ndf[\"RainTomorrow\"]=df[\"RainTomorrow\"]==\"Yes\""]}, {"cell_type": "markdown", "id": "108f4f16", "metadata": {}, "source": ["# More missing data\nThere is a lot of missing data. Some columns just have too much to keep around: \"Evaporation\", \"Sunshine\", \"Cloud9am\" and \"Cloud3pm\". We will drop those without further ado."]}, {"cell_type": "code", "execution_count": 1, "id": "04c52caf", "metadata": {}, "outputs": [], "source": ["df = df.drop([\"Evaporation\", \"Sunshine\", \"Cloud9am\", \"Cloud3pm\"], axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "e678745b", "metadata": {}, "outputs": [], "source": ["df.isna().sum()"]}, {"cell_type": "markdown", "id": "2883394e", "metadata": {}, "source": ["# Dropping the remainder\nThere is still a lot of missing values. We will set aside the data with missing values (because we might use this in later extensions of this notebook where we demonstrate how to use the QLattice to impute data)."]}, {"cell_type": "code", "execution_count": 1, "id": "c1333e9b", "metadata": {}, "outputs": [], "source": ["na_data = df[df.isna().any(axis=1)].copy() # This dataframe holds observations where any of the values are missing\nfull_data = df.dropna() # This dataframe holds observations where *none* of the values are missing"]}, {"cell_type": "markdown", "id": "c45a4249", "metadata": {}, "source": ["# What do we have left?\n\nWe still have 112925 observations left. That should be plenty to find a good model "]}, {"cell_type": "code", "execution_count": 1, "id": "12866422", "metadata": {}, "outputs": [], "source": ["full_data"]}, {"cell_type": "markdown", "id": "d033cf1b", "metadata": {}, "source": ["# Split the data in train and test.\nWe split the data in two equally sized buckets. We will use one set to find the model and the other to test that the model is not overfitted."]}, {"cell_type": "code", "execution_count": 1, "id": "bbade4ae", "metadata": {}, "outputs": [], "source": ["train, test = feyn.tools.split(full_data, ratio=(1,1), random_state=42)"]}, {"cell_type": "markdown", "id": "b0b5f5dd", "metadata": {}, "source": ["# Community QLattice\nWe are now ready to connect to the QLattice. The feyn module will look in you local configuration file to see if we have a commercial QLattice. If not, it will allocate a community QLattice for us on the Abzu compute cluster."]}, {"cell_type": "code", "execution_count": 1, "id": "23cbca65", "metadata": {}, "outputs": [], "source": ["ql = feyn.connect_qlattice()"]}, {"cell_type": "markdown", "id": "32d7e61d", "metadata": {}, "source": ["# Reproducability\nThe qlattice will be reset when we get it, but to ensure that we get exactly the same result every time we run the notebook we need to seed the QLattice. This is done with the `reset` method"]}, {"cell_type": "code", "execution_count": 1, "id": "e1d474a1", "metadata": {}, "outputs": [], "source": ["ql.reset(random_seed=42)"]}, {"cell_type": "markdown", "id": "db611d0b", "metadata": {}, "source": ["# Search for the best model\nWe are now ready to instruct the QLattice to search for the best mathematical model to explain the data. Here we use the high-level convenience function that does everything with sensible defaults: https://docs.abzu.ai/docs/guides/essentials/auto_run.html. \n\nFor more detailed control, we could use the primitives: https://docs.abzu.ai/docs/guides/primitives/using_primitives.html\n\nNotice that the `stypes` dictionary we created earlier gets passed to the QLattice here. \n\n__NOTE:__ This will take several minutes to complete. It invoves work done on the QLattice machine remotely as well as in the local notebook. The part that runs locally is slowing things down because of the limited CPU resources on Kaggle. Running the same on my machine locally only takes 20 seconds!"]}, {"cell_type": "code", "execution_count": 1, "id": "2da67456", "metadata": {}, "outputs": [], "source": ["models = ql.auto_run(train, output_name=\"RainTomorrow\", kind=\"classification\", stypes = stypes)"]}, {"cell_type": "markdown", "id": "ba9e16b8", "metadata": {}, "source": ["# Evaluate\nThe QLattice has found a mathematical relationship tha can relate the predictors to the output. The final step is to evaluate the model on the test and the train set. To do that we plot the *ROC* curve of the classifier on both the test and the training data. You can read more about ROC curves here: https://docs.abzu.ai/docs/guides/plotting/roc_curve.html\n\nIn this case they overlap almost perfectly which indicates that the model generalizes to unseen data very well."]}, {"cell_type": "code", "execution_count": 1, "id": "3a5d01a7", "metadata": {}, "outputs": [], "source": ["models[0].plot_roc_curve(train)\nmodels[0].plot_roc_curve(test)"]}, {"cell_type": "markdown", "id": "d5ecfed8", "metadata": {}, "source": ["# Confusion matrix\nA simpler and less powerfull way to evaluate classifiers is a confusion matrix: https://docs.abzu.ai/docs/guides/plotting/confusion_matrix.html\nLet us see how that looks at various thresholds"]}, {"cell_type": "code", "execution_count": 1, "id": "5d3248fa", "metadata": {}, "outputs": [], "source": ["models[0].plot_confusion_matrix(test, threshold=.3)"]}, {"cell_type": "code", "execution_count": 1, "id": "140ef2f4", "metadata": {}, "outputs": [], "source": ["models[0].plot_confusion_matrix(test, threshold=.5)"]}, {"cell_type": "markdown", "id": "260f254d", "metadata": {}, "source": ["# The model is math!\nWhat the QLattice actually finds is an equation that relates the input to the output. The user can control the compexity and structure of the equation in various ways. Here we just went with the defaults. Let's see the actual mathematical expression:"]}, {"cell_type": "code", "execution_count": 1, "id": "936dfdae", "metadata": {}, "outputs": [], "source": ["models[0].sympify(2)"]}, {"cell_type": "markdown", "id": "93e54115", "metadata": {}, "source": ["# Feature interaction\nFinally we can see how the features interact by plotting the equation with pearson correlations of each node. See more here: https://docs.abzu.ai/docs/guides/plotting/model_plot.html\n"]}, {"cell_type": "code", "execution_count": 1, "id": "25c46add", "metadata": {}, "outputs": [], "source": ["models[0].plot(test)"]}, {"cell_type": "markdown", "id": "8dcb069e", "metadata": {}, "source": ["# Conclusion\nIn a fex simple steps we were able to:\n- Find a mathematical model that predicts rain\n- Show that it gereralizes very well to new data.\n- Understand which features interacts to predict rain\n- Visualize the performance of the model in various ways.\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}