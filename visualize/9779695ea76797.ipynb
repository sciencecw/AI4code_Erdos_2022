{"cells": [{"cell_type": "markdown", "id": "82645fdc", "metadata": {}, "source": ["*Python Machine Learning 2nd Edition* by [Sebastian Raschka](https://sebastianraschka.com), Packt Publishing Ltd. 2017\n\nCode Repository: https://github.com/rasbt/python-machine-learning-book-2nd-edition\n\nCode License: [MIT License](https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/LICENSE.txt)"]}, {"cell_type": "markdown", "id": "ec545555", "metadata": {}, "source": ["# Python Machine Learning - Code Examples"]}, {"cell_type": "markdown", "id": "551227b5", "metadata": {}, "source": ["# Chapter 3 - A Tour of Machine Learning Classifiers Using Scikit-Learn"]}, {"cell_type": "markdown", "id": "2407b18e", "metadata": {}, "source": ["Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s)."]}, {"cell_type": "code", "execution_count": 1, "id": "631330d4", "metadata": {}, "outputs": [], "source": ["from sklearn import __version__ as sklearn_version\nfrom distutils.version import LooseVersion\n\nif LooseVersion(sklearn_version) < LooseVersion('0.18'):\n    raise ValueError('Please use scikit-learn 0.18 or newer')"]}, {"cell_type": "markdown", "id": "f923d938", "metadata": {}, "source": ["*The use of `watermark` is optional. You can install this IPython extension via \"`pip install watermark`\". For more information, please see: https://github.com/rasbt/watermark.*"]}, {"cell_type": "markdown", "id": "e87d23c8", "metadata": {}, "source": ["### Overview"]}, {"cell_type": "markdown", "id": "2b5e9c08", "metadata": {}, "source": ["- [Choosing a classification algorithm](#Choosing-a-classification-algorithm)\n- [Modeling class probabilities via logistic regression](#Modeling-class-probabilities-via-logistic-regression)\n    - [Logistic regression intuition and conditional probabilities](#Logistic-regression-intuition-and-conditional-probabilities)\n    - [Learning the weights of the logistic cost function](#Learning-the-weights-of-the-logistic-cost-function)\n    - [Training a logistic regression model with scikit-learn](#Training-a-logistic-regression-model-with-scikit-learn)\n    - [Tackling overfitting via regularization](#Tackling-overfitting-via-regularization)\n- [Maximum margin classification with support vector machines](#Maximum-margin-classification-with-support-vector-machines)\n    - [Maximum margin intuition](#Maximum-margin-intuition)\n    - [Dealing with the nonlinearly separable case using slack variables](#Dealing-with-the-nonlinearly-separable-case-using-slack-variables)\n    - [Alternative implementations in scikit-learn](#Alternative-implementations-in-scikit-learn)\n- [Solving nonlinear problems using a kernel SVM](#Solving-nonlinear-problems-using-a-kernel-SVM)\n    - [Using the kernel trick to find separating hyperplanes in higher dimensional space](#Using-the\n- [Summary](#Summary)"]}, {"cell_type": "markdown", "id": "659afffa", "metadata": {}, "source": ["<br>\n<br>"]}, {"cell_type": "markdown", "id": "8dd95d99", "metadata": {}, "source": ["<br>\n<br>"]}, {"cell_type": "code", "execution_count": 1, "id": "9d7fe200", "metadata": {}, "outputs": [], "source": ["from IPython.display import Image\n%matplotlib inline\n"]}, {"cell_type": "markdown", "id": "a617fc8f", "metadata": {}, "source": ["# Choosing a classification algorithm"]}, {"cell_type": "markdown", "id": "1ebf5228", "metadata": {}, "source": ["..."]}, {"cell_type": "markdown", "id": "a291910c", "metadata": {}, "source": ["# First steps with scikit-learn"]}, {"cell_type": "markdown", "id": "6b26de1a", "metadata": {}, "source": ["Loading the Iris dataset from scikit-learn. Here, the third column represents the petal length, and the fourth column the petal width of the flower samples. The classes are already converted to integer labels where 0=Iris-Setosa, 1=Iris-Versicolor, 2=Iris-Virginica."]}, {"cell_type": "code", "execution_count": 1, "id": "04018d59", "metadata": {}, "outputs": [], "source": ["from sklearn import datasets\nimport numpy as np\n\niris = datasets.load_iris()\nX = iris.data[:, [2, 3]]\ny = iris.target\n\nprint('Class labels:', np.unique(y))"]}, {"cell_type": "markdown", "id": "a2ae7721", "metadata": {}, "source": ["Looking at the shape of the arrays and the data."]}, {"cell_type": "code", "execution_count": 1, "id": "09ebd6f9", "metadata": {}, "outputs": [], "source": ["X.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "2281bb4e", "metadata": {}, "outputs": [], "source": ["X[0:10,:]"]}, {"cell_type": "code", "execution_count": 1, "id": "bce65125", "metadata": {}, "outputs": [], "source": ["y[0:10]"]}, {"cell_type": "markdown", "id": "8b54904f", "metadata": {}, "source": ["Checking the distribution of classes in the dataset."]}, {"cell_type": "code", "execution_count": 1, "id": "571fd845", "metadata": {}, "outputs": [], "source": ["print(np.sum(y==0))\nprint(np.sum(y==1))\nprint(np.sum(y==2))"]}, {"cell_type": "markdown", "id": "79447e2c", "metadata": {}, "source": ["There are equal number of flowers in each class. As an exercise, it will be interesting to check if the first 50 values of y are in class 0, the next 50 in class 1 and the last 50 in class 2."]}, {"cell_type": "code", "execution_count": 1, "id": "5a951f1a", "metadata": {}, "outputs": [], "source": ["# Check if the first 50 values in y are from Class 0 (setosa), the next 50 from Class 1 (Versicolor) and the last 50 from Class 2 (Virginica)"]}, {"cell_type": "markdown", "id": "24e0a9df", "metadata": {}, "source": ["Splitting data into 70% training and 30% test data:\nNote that the train_test_split function already shuffles the training sets\ninternally before splitting; otherwise, all class 0 and class 1 samples would have\nended up in the training set, and the test set would consist of 45 samples from\nclass 2. Via the random_state parameter, we provided a fixed random seed\n( random_state=1 ) for the internal pseudo-random number generator that is used\nfor shuffling the datasets prior to splitting. Using such a fixed random_state ensures\nthat our results are reproducible.\nLastly, we took advantage of the built-in support for stratification via stratify=y . In\nthis context, stratification means that the train_test_split method returns training\nand test subsets that have the same proportions of class labels as the input dataset."]}, {"cell_type": "code", "execution_count": 1, "id": "28bfc69a", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=1, stratify=y)"]}, {"cell_type": "code", "execution_count": 1, "id": "eb86b42f", "metadata": {}, "outputs": [], "source": ["print('Labels counts in y:', np.bincount(y))\nprint('Labels counts in y_train:', np.bincount(y_train))\nprint('Labels counts in y_test:', np.bincount(y_test))"]}, {"cell_type": "markdown", "id": "34af034e", "metadata": {}, "source": ["![](http://)Standardizing the features:\nUsing the fit method, StandardScaler estimated the\nparameters \u03bc (sample mean) and \u03c3 (standard deviation) for each feature dimension\nfrom the training data. By calling the transform method, we then standardized the\ntraining data using those estimated parameters \u03bc and \u03c3 ."]}, {"cell_type": "code", "execution_count": 1, "id": "810b5302", "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nsc.fit(X_train)\nX_train_std = sc.transform(X_train)\nX_test_std = sc.transform(X_test)"]}, {"cell_type": "markdown", "id": "b5dfae6c", "metadata": {}, "source": ["Redefining the `plot_decision_region` function from chapter 2:"]}, {"cell_type": "code", "execution_count": 1, "id": "d240b5ce", "metadata": {}, "outputs": [], "source": ["from matplotlib.colors import ListedColormap\nimport matplotlib.pyplot as plt\n\n\ndef plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n\n    # setup marker generator and color map\n    markers = ('s', 'x', 'o', '^', 'v')\n    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n\n    # plot the decision surface\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n                           np.arange(x2_min, x2_max, resolution))\n    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n\n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x=X[y == cl, 0], \n                    y=X[y == cl, 1],\n                    alpha=0.8, \n                    c=colors[idx],\n                    marker=markers[idx], \n                    label=cl, \n                    edgecolor='black')\n\n    # highlight test samples\n    if test_idx:\n        # plot all samples\n        X_test, y_test = X[test_idx, :], y[test_idx]\n\n        plt.scatter(X_test[:, 0],\n                    X_test[:, 1],\n                    c='',\n                    edgecolor='black',\n                    alpha=1.0,\n                    linewidth=1,\n                    marker='o',\n                    s=100, \n                    label='test set')"]}, {"cell_type": "code", "execution_count": 1, "id": "22566536", "metadata": {}, "outputs": [], "source": ["X_combined_std = np.vstack((X_train_std, X_test_std))\ny_combined = np.hstack((y_train, y_test))"]}, {"cell_type": "markdown", "id": "217214c1", "metadata": {}, "source": ["<br>\n<br>"]}, {"cell_type": "markdown", "id": "739a6b5f", "metadata": {}, "source": ["# Modeling class probabilities via logistic regression"]}, {"cell_type": "markdown", "id": "e7fcde00", "metadata": {}, "source": ["..."]}, {"cell_type": "markdown", "id": "e757852e", "metadata": {}, "source": ["Logistic Regression is a classificaton method that predicts the probability that an input X belongs to Class y. The probability P(y =1 / X) is calculated and converted to 0 or 1 for classifying y. \n\nIt is named for the logistic (sigmoid) function that is an S curve that can map any real number to a value between 0 and 1, but not at those limits. \n\ny = e^(b0 + b1 * x) / (1 + e^(b0 + b1 * x))\nwhere b0 and b1 are the weights. \n\nP(y = 1 / X) = P(X) = e^(b0 + b1 * X) / (1 + e^(b0 + b1 * X))\n\nThis can be written as\nln(p(X) / 1 \u2013 p(X)) = b0 + b1 * X\n\nThe log odds of the default class is a linear combination of the input X. The coefficients are estimated using Maximum Likelihood Estimation.\nP(X) >= 0.5 => y = 1\n\nP(X) < 0.5 => y = 0\n\nhttps://machinelearningmastery.com/logistic-regression-for-machine-learning/"]}, {"cell_type": "markdown", "id": "9e0a4f76", "metadata": {}, "source": ["### Logistic regression intuition and conditional probabilities"]}, {"cell_type": "code", "execution_count": 1, "id": "1c962c0c", "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef sigmoid(z):\n    return 1.0 / (1.0 + np.exp(-z))\n\nz = np.arange(-7, 7, 0.1)\nphi_z = sigmoid(z)\n\nplt.plot(z, phi_z)\nplt.axvline(0.0, color='k')\nplt.ylim(-0.1, 1.1)\nplt.xlabel('z')\nplt.ylabel('$\\phi (z)$')\n\n# y axis ticks and gridline\nplt.yticks([0.0, 0.5, 1.0])\nax = plt.gca()\nax.yaxis.grid(True)\n\nplt.tight_layout()\n#plt.savefig('images/03_02.png', dpi=300)\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "cd0a8ff5", "metadata": {}, "outputs": [], "source": ["Image('../input/python-ml-ch03-images/03_03.png',width=700)"]}, {"cell_type": "markdown", "id": "9cb7b18c", "metadata": {}, "source": ["<br>\n<br>"]}, {"cell_type": "markdown", "id": "e10b6d93", "metadata": {}, "source": ["### Learning the weights of the logistic cost function\nCost function\n\nhttps://sebastianraschka.com/faq/docs/probablistic-logistic-regression.html"]}, {"cell_type": "code", "execution_count": 1, "id": "c81eab5a", "metadata": {}, "outputs": [], "source": ["Image(filename='../input/regularization/LR-cost.png')"]}, {"cell_type": "markdown", "id": "4e7b47cd", "metadata": {}, "source": ["where z = b0 + b1 * x"]}, {"cell_type": "code", "execution_count": 1, "id": "841ec7e2", "metadata": {}, "outputs": [], "source": ["def cost_1(z):\n    return - np.log(sigmoid(z))\n\n\ndef cost_0(z):\n    return - np.log(1 - sigmoid(z))\n\nz = np.arange(-10, 10, 0.1)\nphi_z = sigmoid(z)\n\nc1 = [cost_1(x) for x in z]\nplt.plot(phi_z, c1, label='J(w) if y=1')\n\nc0 = [cost_0(x) for x in z]\nplt.plot(phi_z, c0, linestyle='--', label='J(w) if y=0')\n\nplt.ylim(0.0, 5.1)\nplt.xlim([0, 1])\nplt.xlabel('$\\phi$(z)')\nplt.ylabel('J(w)')\nplt.legend(loc='best')\nplt.tight_layout()\n#plt.savefig('images/03_04.png', dpi=300)\nplt.show()"]}, {"cell_type": "markdown", "id": "89022647", "metadata": {}, "source": ["<br>\n<br>"]}, {"cell_type": "markdown", "id": "46ae3a82", "metadata": {}, "source": ["### Training a logistic regression model with scikit-learn"]}, {"cell_type": "markdown", "id": "9e7104ce", "metadata": {}, "source": ["Parameters\n\nclass sklearn.linear_model.LogisticRegression(penalty=\u2019l2\u2019, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=\u2019warn\u2019, max_iter=100, multi_class=\u2019warn\u2019, verbose=0, warm_start=False, n_jobs=None)\n\nC : float, default: 1.0\nInverse of regularization strength; must be a positive float. Smaller values specify stronger regularization."]}, {"cell_type": "code", "execution_count": 1, "id": "b58ae9d0", "metadata": {}, "outputs": [], "source": ["from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(C=100,penalty='l2', random_state=1)\nlr.fit(X_train_std, y_train)\n\nplot_decision_regions(X_combined_std, y_combined,\n                      classifier=lr, test_idx=range(105, 150))\nplt.xlabel('petal length [standardized]')\nplt.ylabel('petal width [standardized]')\nplt.legend(loc='upper left')\nplt.tight_layout()\n#plt.savefig('images/03_06.png', dpi=300)\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "a3e902d8", "metadata": {}, "outputs": [], "source": ["#Probability estimate\nlr.predict_proba(X_test_std[:3, :])"]}, {"cell_type": "code", "execution_count": 1, "id": "196cda03", "metadata": {}, "outputs": [], "source": ["lr.predict_proba(X_test_std[:3, :]).sum(axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "341294c2", "metadata": {}, "outputs": [], "source": ["lr.predict_proba(X_test_std[:3, :]).argmax(axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "fac5fb30", "metadata": {}, "outputs": [], "source": ["lr.predict(X_test_std[:3, :])"]}, {"cell_type": "code", "execution_count": 1, "id": "87eb29ec", "metadata": {}, "outputs": [], "source": ["lr.predict(X_test_std[0, :].reshape(1, -1))"]}, {"cell_type": "code", "execution_count": 1, "id": "6a3c7138", "metadata": {}, "outputs": [], "source": ["#Returns the mean accuracy on the given test data and labels.\nlr.score(X_test_std, y_test)\n"]}, {"cell_type": "code", "execution_count": 1, "id": "5c8d55aa", "metadata": {}, "outputs": [], "source": ["lr.score(X_train_std,y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "dd59dfda", "metadata": {}, "outputs": [], "source": ["# Find the weights b0 and b1. \nprint(lr.coef_)\nprint(lr.intercept_)"]}, {"cell_type": "markdown", "id": "7fd6d934", "metadata": {}, "source": ["Exercise: Recalculate scores by changing the value of C. C = 0.1, 1, 10, 100\n"]}, {"cell_type": "markdown", "id": "c1aad640", "metadata": {}, "source": ["Exercise: Change penalty from default \"L2\" to \"L1\". Note change in the weights. Try C = 0.1 and see the difference in scores."]}, {"cell_type": "markdown", "id": "fca00c9d", "metadata": {}, "source": ["<br>\n<br>"]}, {"cell_type": "markdown", "id": "628ad3d5", "metadata": {}, "source": ["### Tackling overfitting via regularization"]}, {"cell_type": "markdown", "id": "a14bd485", "metadata": {}, "source": ["What is overfitting?\nModel performs well on training data but not on unseen data (test data). Has high variance that can be due to too many parameters yielding a model with high complexity. Variance measures the stability or consistency of the model if we rebuild model multiple times with different subsets of training data.\n\nWhat is underfitting?\nModel does not capture the pattern in the training data and hence performs poorly on both training and test data. Low complexity model with low performance. High bias model.\nBias measures how far off the predictions are from the true values if we train the model multiple times with different subsets of training data.\n\nSolution: Tune the complexity of the model using regularization. It handles high collinearity, filters out noise from data and prevents overfitting. Add a bias term to penalize extreme values for weights. Feature scaling such as standardization are very important for regularization to work properly."]}, {"cell_type": "markdown", "id": "606159df", "metadata": {}, "source": ["The original solution to minimize the cost function is below."]}, {"cell_type": "code", "execution_count": 1, "id": "b7bc51e9", "metadata": {}, "outputs": [], "source": ["#Image(filename='../input/regularization/04_04.png',width=700)"]}, {"cell_type": "markdown", "id": "949cb900", "metadata": {}, "source": ["https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c\n\nhttps://www.knime.com/blog/regularization-for-logistic-regression-l1-l2-gauss-or-laplace\n\nhttp://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/\n\nL1 regularization or LASSO adds the absolute value of the weights as the penalty term to the loss/cost function modulated by the lambda. It is computationally inefficient, yields sparse outputs (shrinks less important features' coefficients to zero) and hence has feature selection built in. "]}, {"cell_type": "code", "execution_count": 1, "id": "bb62a5f8", "metadata": {}, "outputs": [], "source": ["#Image(filename='../input/regularization/04_06.png',width=700)"]}, {"cell_type": "markdown", "id": "40e90bfe", "metadata": {}, "source": ["L2 regularization adds the squared magnitude of the coefficients to the cost function. It is computationally efficient, yields nonsparse outputs and does not help in feature selection.  "]}, {"cell_type": "code", "execution_count": 1, "id": "e2b4d9d1", "metadata": {}, "outputs": [], "source": ["Image(filename='../input/regularization/l2-term.png', width=700)"]}, {"cell_type": "code", "execution_count": 1, "id": "f80e2d8c", "metadata": {}, "outputs": [], "source": ["#Image(filename='../input/regularization/04_05.png',width=300)"]}, {"cell_type": "code", "execution_count": 1, "id": "55d4c1dc", "metadata": {}, "outputs": [], "source": ["weights, params = [], []\nfor c in np.arange(-5, 5):\n    lr = LogisticRegression(C=10.**c, random_state=1)\n    lr.fit(X_train_std, y_train)\n    weights.append(lr.coef_[1])\n    params.append(10.**c)\n\nweights = np.array(weights)\nplt.plot(params, weights[:, 0],\n         label='petal length')\nplt.plot(params, weights[:, 1], linestyle='--',\n         label='petal width')\nplt.ylabel('weight coefficient')\nplt.xlabel('C')\nplt.legend(loc='upper left')\nplt.xscale('log')\n#plt.savefig('images/03_08.png', dpi=300)\nplt.show()"]}, {"cell_type": "markdown", "id": "9ac2c750", "metadata": {}, "source": ["<br>\n<br>"]}, {"cell_type": "markdown", "id": "a9f4b946", "metadata": {}, "source": ["# Maximum margin classification with support vector machines\nMaximize the margin. Margin is the distance between the decision boundary and the training samples closest to the decision boundary, the support vectors."]}, {"cell_type": "code", "execution_count": 1, "id": "f704c606", "metadata": {}, "outputs": [], "source": ["Image(filename='../input/python-ml-ch03-images/03_09.png', width=700) "]}, {"cell_type": "markdown", "id": "954c8013", "metadata": {}, "source": ["## Maximum margin intuition"]}, {"cell_type": "markdown", "id": "b0133e18", "metadata": {}, "source": ["..."]}, {"cell_type": "markdown", "id": "9b555fe2", "metadata": {}, "source": ["## Dealing with the nonlinearly separable case using slack variables"]}, {"cell_type": "code", "execution_count": 1, "id": "8fe6cae2", "metadata": {}, "outputs": [], "source": ["#Image(filename='../input/python-ml-ch03-images/03_10.png', width=600) "]}, {"cell_type": "code", "execution_count": 1, "id": "c73b389e", "metadata": {}, "outputs": [], "source": ["from sklearn.svm import SVC\n\nsvm = SVC(kernel='linear', C=0.1, random_state=1)\nsvm.fit(X_train_std, y_train)\n\nplot_decision_regions(X_combined_std, \n                      y_combined,\n                      classifier=svm, \n                      test_idx=range(105, 150))\nplt.xlabel('petal length [standardized]')\nplt.ylabel('petal width [standardized]')\nplt.legend(loc='upper left')\nplt.tight_layout()\n#plt.savefig('images/03_11.png', dpi=300)\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "4b5aabbf", "metadata": {}, "outputs": [], "source": ["svm.score(X_test_std,y_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "08829110", "metadata": {}, "outputs": [], "source": ["svm.score(X_train_std,y_train)"]}, {"cell_type": "markdown", "id": "cf2adf70", "metadata": {}, "source": ["## Alternative implementations in scikit-learn"]}, {"cell_type": "code", "execution_count": 1, "id": "dc2598d0", "metadata": {}, "outputs": [], "source": ["from sklearn.linear_model import SGDClassifier\n\nppn = SGDClassifier(loss='perceptron', n_iter=1000)\nlr = SGDClassifier(loss='log', n_iter=1000)\nsvm = SGDClassifier(loss='hinge', n_iter=1000)"]}, {"cell_type": "markdown", "id": "efa4afa9", "metadata": {}, "source": ["**Note**\n\n- You can replace `Perceptron(n_iter, ...)` by `Perceptron(max_iter, ...)` in scikit-learn >= 0.19. The `n_iter` parameter is used here deriberately, because some people still use scikit-learn 0.18."]}, {"cell_type": "markdown", "id": "c95a06d8", "metadata": {}, "source": ["<br>\n<br>"]}, {"cell_type": "markdown", "id": "ef207b8a", "metadata": {}, "source": ["# Solving non-linear problems using a kernel SVM"]}, {"cell_type": "code", "execution_count": 1, "id": "38fb0fc3", "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\nimport numpy as np\n\nnp.random.seed(1)\nX_xor = np.random.randn(200, 2)\ny_xor = np.logical_xor(X_xor[:, 0] > 0,\n                       X_xor[:, 1] > 0)\ny_xor = np.where(y_xor, 1, -1)\n\nplt.scatter(X_xor[y_xor == 1, 0],\n            X_xor[y_xor == 1, 1],\n            c='b', marker='x',\n            label='1')\nplt.scatter(X_xor[y_xor == -1, 0],\n            X_xor[y_xor == -1, 1],\n            c='r',\n            marker='s',\n            label='-1')\n\nplt.xlim([-3, 3])\nplt.ylim([-3, 3])\nplt.legend(loc='best')\nplt.tight_layout()\n#plt.savefig('images/03_12.png', dpi=300)\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "f9487674", "metadata": {}, "outputs": [], "source": ["Image(filename='../input/python-ml-ch03-images/03_13.png', width=700) "]}, {"cell_type": "markdown", "id": "af2d398d", "metadata": {}, "source": ["<br>\n<br>"]}, {"cell_type": "markdown", "id": "bec08573", "metadata": {}, "source": ["## Using the kernel trick to find separating hyperplanes in higher dimensional space"]}, {"cell_type": "code", "execution_count": 1, "id": "9a38c096", "metadata": {}, "outputs": [], "source": ["svm = SVC(kernel='rbf', random_state=1, gamma=0.10, C=10.0)\nsvm.fit(X_xor, y_xor)\nplot_decision_regions(X_xor, y_xor,\n                      classifier=svm)\n\nplt.legend(loc='upper left')\nplt.tight_layout()\n#plt.savefig('images/03_14.png', dpi=300)\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "c2f9bd3f", "metadata": {}, "outputs": [], "source": ["svm = SVC(kernel='rbf', random_state=1, gamma=100.0, C=1.0)\nsvm.fit(X_train_std, y_train)\n\nplot_decision_regions(X_combined_std, y_combined, \n                      classifier=svm, test_idx=range(105, 150))\nplt.xlabel('petal length [standardized]')\nplt.ylabel('petal width [standardized]')\nplt.legend(loc='upper left')\nplt.tight_layout()\n#plt.savefig('images/03_16.png', dpi=300)\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "8d7edc9f", "metadata": {}, "outputs": [], "source": ["from sklearn.svm import SVC\n\nsvm = SVC(kernel='rbf', random_state=1, gamma=0.2, C=1.0)\nsvm.fit(X_train_std, y_train)\n\nplot_decision_regions(X_combined_std, y_combined,\n                      classifier=svm, test_idx=range(105, 150))\nplt.xlabel('petal length [standardized]')\nplt.ylabel('petal width [standardized]')\nplt.legend(loc='upper left')\nplt.tight_layout()\n#plt.savefig('images/03_15.png', dpi=300)\nplt.show()"]}, {"cell_type": "markdown", "id": "c4f8c581", "metadata": {}, "source": ["<br>\n<br>"]}, {"cell_type": "markdown", "id": "fcbebb38", "metadata": {}, "source": ["<br>\n<br>"]}, {"cell_type": "markdown", "id": "6d26a685", "metadata": {}, "source": ["<br>\n<br>"]}, {"cell_type": "markdown", "id": "cf9dc59a", "metadata": {}, "source": ["# Summary"]}, {"cell_type": "markdown", "id": "75c31f4b", "metadata": {}, "source": ["..."]}, {"cell_type": "markdown", "id": "65a47af4", "metadata": {}, "source": ["---\n\nReaders may ignore the next cell."]}, {"cell_type": "code", "execution_count": 1, "id": "a3a225d4", "metadata": {}, "outputs": [], "source": ["! python ../.convert_notebook_to_script.py --input ch03.ipynb --output ch03.py"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}