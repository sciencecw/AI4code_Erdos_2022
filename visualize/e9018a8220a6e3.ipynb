{"cells": [{"cell_type": "markdown", "id": "bc6844f7", "metadata": {}, "source": ["## Topic Modeling with Latent Dirichlet Allocation"]}, {"cell_type": "code", "execution_count": 1, "id": "49b43ff9", "metadata": {}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nfrom pathlib import Path\nimport os\nimport glob\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pandas.io.json import json_normalize\nimport json\n\nimport pprint\nimport string\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport re\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# for dirname, _, filenames in os.walk('C:/Users/trivikram.cheedella/OneDrive - JD Power/Data Science Data/CORD-19-research-challenge'):\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        # print(os.path.join(dirname, filename))\n        pass\n\n# Any results you write to the current directory are saved as output."]}, {"cell_type": "code", "execution_count": 1, "id": "31cf43b9", "metadata": {}, "outputs": [], "source": ["input = Path('/kaggle/input/CORD-19-research-challenge')\noutput = Path('/kaggle/output')\nbiorxiv_medrxiv = Path('/kaggle/input/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv')"]}, {"cell_type": "markdown", "id": "a70369f0", "metadata": {}, "source": ["## Load the meta data file"]}, {"cell_type": "code", "execution_count": 1, "id": "f18d2119", "metadata": {}, "outputs": [], "source": ["df_all_sources_metadata = pd.read_csv(input / 'metadata.csv')"]}, {"cell_type": "code", "execution_count": 1, "id": "be766084", "metadata": {}, "outputs": [], "source": ["print(df_all_sources_metadata.shape)\ndf_all_sources_metadata.info()"]}, {"cell_type": "code", "execution_count": 1, "id": "c163e463", "metadata": {}, "outputs": [], "source": ["df_all_sources_metadata.head(3)"]}, {"cell_type": "code", "execution_count": 1, "id": "c1e2921b", "metadata": {}, "outputs": [], "source": ["pd.pivot_table(df_all_sources_metadata, \n               index='full_text_file', \n               values=['cord_uid','sha', 'source_x', 'has_pdf_parse', 'has_pmc_xml_parse'], \n               aggfunc={'cord_uid': 'count','sha': 'count', 'source_x': 'count', 'has_pdf_parse': np.sum, 'has_pmc_xml_parse': np.sum}, \n               margins=True)"]}, {"cell_type": "markdown", "id": "d5a441c3", "metadata": {}, "source": ["## ReadIn the JSON Files"]}, {"cell_type": "markdown", "id": "af91c9a1", "metadata": {}, "source": ["The following code for reading in the JSON file is taken from the notebook <b>COVID EDA: Initial Exploration Tool</b> by <i><b>Ivan Ega Pratama</b></i>\n\nhttps://www.kaggle.com/ivanegapratama/covid-eda-initial-exploration-tool"]}, {"cell_type": "code", "execution_count": 1, "id": "b456aa14", "metadata": {}, "outputs": [], "source": ["%%time\nall_json = glob.glob(f'{biorxiv_medrxiv}/**/*.json', recursive=True)\nlen(all_json)"]}, {"cell_type": "code", "execution_count": 1, "id": "b480c6a8", "metadata": {}, "outputs": [], "source": ["class FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.abstract = []\n            self.body_text = []\n            # Abstract\n            for entry in content['abstract']:\n                self.abstract.append(entry['text'])\n            # Body text\n            for entry in content['body_text']:\n                self.body_text.append(entry['text'])\n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n            # Extend Here\n            #\n            #\n    def __repr__(self):\n        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\nfirst_row = FileReader(all_json[0])\nprint(first_row)"]}, {"cell_type": "code", "execution_count": 1, "id": "c68be372", "metadata": {}, "outputs": [], "source": ["%%time\ndict_ = {'paper_id': [], 'abstract': [], 'body_text': []}\nfor idx, entry in enumerate(all_json):\n    if idx % (len(all_json) // 10) == 0:\n        print(f'Processing index: {idx} of {len(all_json)}')\n    content = FileReader(entry)\n    dict_['paper_id'].append(content.paper_id)\n    dict_['abstract'].append(content.abstract)\n    dict_['body_text'].append(content.body_text)\ndf_covid = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text'])\ndf_covid.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "b45aa909", "metadata": {}, "outputs": [], "source": ["dict_ = None"]}, {"cell_type": "code", "execution_count": 1, "id": "1c7a70de", "metadata": {}, "outputs": [], "source": ["%%time\ndf_covid['abstract_word_count'] = df_covid['abstract'].apply(lambda x: len(x.strip().split()))\ndf_covid['body_word_count'] = df_covid['body_text'].apply(lambda x: len(x.strip().split()))\ndf_covid.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "55a8be07", "metadata": {}, "outputs": [], "source": ["df_covid.describe(include='all').T"]}, {"cell_type": "code", "execution_count": 1, "id": "d33a3590", "metadata": {}, "outputs": [], "source": ["df_covid.drop_duplicates(['body_text'], inplace=True)\ndf_covid.describe(include='all').T"]}, {"cell_type": "markdown", "id": "bc5e2fdb", "metadata": {}, "source": ["## Clean up the text"]}, {"cell_type": "code", "execution_count": 1, "id": "afbe807c", "metadata": {}, "outputs": [], "source": ["import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.probability import FreqDist\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import STOPWORDS"]}, {"cell_type": "code", "execution_count": 1, "id": "9253a51d", "metadata": {}, "outputs": [], "source": ["lemmatizer = WordNetLemmatizer()\n\nprint(\"Number of stopwrods from STOPWORDS: \", len(STOPWORDS))\nprint(\"Number of stopwrods from stopwords.words('english'): \", len(stopwords.words('english')))\nother_stopwords = ['q', 'license', 'preprint', 'copyright', 'http', 'doi', 'preprint', 'copyright', \n                   'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', 'rights', \n                   'reserved', 'permission', 'used', 'using', 'biorxiv', 'fig', 'fig.', 'al.', \n                   'di', 'la', 'il', 'del', 'le', 'della', 'dei', 'delle', 'una', 'da',  'dell',  \n                   'non', 'si', 'cc']\n\ncustom_stopwords = []\ncustom_stopwords = list(set(stopwords.words('english') + list(STOPWORDS))) + other_stopwords\n\nprint(\"Number of stopwrods from custom_stopwords: \", len(custom_stopwords))\nprint(custom_stopwords[-25:])"]}, {"cell_type": "code", "execution_count": 1, "id": "a5221a18", "metadata": {}, "outputs": [], "source": ["def clean_the_text(text):\n        text = re.sub('[^a-zA-Z0-9-]', ' ', text)\n        tokens = word_tokenize(text)\n        # remove_punc = [word for word in tokens if word not in string.punctuation]\n        remove_stopwords = [word.lower() for word in tokens if word.lower() not in custom_stopwords]\n        more_than_three = [w for w in remove_stopwords if len(w)>3]\n        lem = [lemmatizer.lemmatize(w) for w in more_than_three]\n        return ' '.join(lem)\n"]}, {"cell_type": "code", "execution_count": 1, "id": "2475aa4c", "metadata": {}, "outputs": [], "source": ["df_covid_for_nlp = df_covid.copy()\ndf_covid = None"]}, {"cell_type": "code", "execution_count": 1, "id": "3c8a3fbc", "metadata": {}, "outputs": [], "source": ["%%time\ndf_covid_for_nlp['cleaned_text'] = df_covid_for_nlp['body_text'].apply(lambda x: clean_the_text(x))"]}, {"cell_type": "code", "execution_count": 1, "id": "451cc5a5", "metadata": {}, "outputs": [], "source": ["df_covid_for_nlp['cleaned_text'].head()"]}, {"cell_type": "markdown", "id": "b6b6b514", "metadata": {}, "source": ["# Topic Modeling using Latent Dirichlet Allocation (LDA)"]}, {"cell_type": "code", "execution_count": 1, "id": "1b5d5fb1", "metadata": {}, "outputs": [], "source": ["from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.decomposition import LatentDirichletAllocation"]}, {"cell_type": "markdown", "id": "a75d1ba0", "metadata": {}, "source": ["### Convert the text data to Term frequency - Inverse Document frequency"]}, {"cell_type": "code", "execution_count": 1, "id": "4dfc3dec", "metadata": {}, "outputs": [], "source": ["%%time \n\ntfidf_vect = TfidfVectorizer(max_df=0.8, min_df=2, stop_words='english')\ndoc_term_matrix_Tfidf = tfidf_vect.fit_transform(df_covid_for_nlp['cleaned_text'].values.astype('U'))"]}, {"cell_type": "code", "execution_count": 1, "id": "592b9880", "metadata": {}, "outputs": [], "source": ["doc_term_matrix_Tfidf"]}, {"cell_type": "markdown", "id": "f0a8af5e", "metadata": {}, "source": ["### Use GridSearchCV to find the best parameters for LDA"]}, {"cell_type": "code", "execution_count": 1, "id": "3e1fab26", "metadata": {}, "outputs": [], "source": ["%%time\n# Define Search Param\nsearch_params = {'n_components': [10, 15, 20, 25, 30, 50], 'learning_decay': [.5, .7, .9]}\n\n# Init the Model\nlda = LatentDirichletAllocation()\n\n# Init Grid Search Class\ngrid_search_model = GridSearchCV(lda, param_grid=search_params, n_jobs=-1)\n\n# Do the Grid Search\ngrid_search_model.fit(doc_term_matrix_Tfidf)"]}, {"cell_type": "code", "execution_count": 1, "id": "82173b4c", "metadata": {}, "outputs": [], "source": ["# Best Model\nbest_lda_model = grid_search_model.best_estimator_\n\n# Model Parameters\nprint(\"Best Model's Params: \", grid_search_model.best_params_)\n\n# Log Likelihood Score\nprint(\"Best Log Likelihood Score: \", grid_search_model.best_score_)\n\n# Perplexity\nprint(\"Model Perplexity: \", best_lda_model.perplexity(doc_term_matrix_Tfidf))"]}, {"cell_type": "code", "execution_count": 1, "id": "6d83e053", "metadata": {}, "outputs": [], "source": ["df_results = pd.DataFrame(grid_search_model.cv_results_)\n\ncurrent_palette = sns.color_palette(\"Set2\", 3)\n\nplt.figure(figsize=(12,8))\n\nsns.lineplot(data=df_results,\n             x='param_n_components',\n             y='mean_test_score',\n             hue='param_learning_decay',\n             palette=current_palette,\n             marker='o')\n\nplt.show()"]}, {"cell_type": "markdown", "id": "3a21e86f", "metadata": {}, "source": ["### Getting 10 random features"]}, {"cell_type": "code", "execution_count": 1, "id": "fdbbc4b1", "metadata": {}, "outputs": [], "source": ["import random\n\nfor i in range(10):\n    random_id = random.randint(0,len(tfidf_vect.get_feature_names()))\n    print(tfidf_vect.get_feature_names()[random_id])"]}, {"cell_type": "markdown", "id": "dc2d08fa", "metadata": {}, "source": ["### Print the top 10 words from the first topic"]}, {"cell_type": "code", "execution_count": 1, "id": "aee97b1e", "metadata": {}, "outputs": [], "source": ["first_topic = best_lda_model.components_[0]"]}, {"cell_type": "code", "execution_count": 1, "id": "e8f29075", "metadata": {}, "outputs": [], "source": ["top_topic_words = first_topic.argsort()[-10:]\ntop_topic_words"]}, {"cell_type": "code", "execution_count": 1, "id": "890ee09e", "metadata": {}, "outputs": [], "source": ["for i in top_topic_words:\n    print(tfidf_vect.get_feature_names()[i])"]}, {"cell_type": "markdown", "id": "9e5583ff", "metadata": {}, "source": ["### Print the top 10 words topic wise"]}, {"cell_type": "code", "execution_count": 1, "id": "9b558604", "metadata": {}, "outputs": [], "source": ["for i,topic in enumerate(best_lda_model.components_):\n    print(f'Top 10 words for topic #{i}:')\n    print([tfidf_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n    print('\\n')"]}, {"cell_type": "markdown", "id": "5faa0181", "metadata": {}, "source": ["### Find the most dominant topic in each document"]}, {"cell_type": "code", "execution_count": 1, "id": "b97e9ede", "metadata": {}, "outputs": [], "source": ["topic_values_tfidf = best_lda_model.transform(doc_term_matrix_Tfidf)\ntopic_values_tfidf.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "5d98188b", "metadata": {}, "outputs": [], "source": ["topicnames = [\"Topic\" + str(i) for i in range(best_lda_model.n_components)]\n\nrows = df_covid_for_nlp['paper_id']"]}, {"cell_type": "code", "execution_count": 1, "id": "514ee3c7", "metadata": {}, "outputs": [], "source": ["df_topic_values_tfidf = pd.DataFrame(topic_values_tfidf, columns=topicnames, index=rows)"]}, {"cell_type": "code", "execution_count": 1, "id": "efd9a69d", "metadata": {}, "outputs": [], "source": ["df_topic_values_tfidf['topic_number_tfidf'] = topic_values_tfidf.argmax(axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "832f741e", "metadata": {}, "outputs": [], "source": ["# Styling\ndef color_green(val):\n    color = 'green' if val > .1 else 'black'\n    return 'color: {col}'.format(col=color)\n\ndef make_bold(val):\n    weight = 700 if val > .1 else 400\n    return 'font-weight: {weight}'.format(weight=weight)"]}, {"cell_type": "code", "execution_count": 1, "id": "35211502", "metadata": {}, "outputs": [], "source": ["df_topic_values_tfidf.head(10).style.applymap(color_green).applymap(make_bold)"]}, {"cell_type": "markdown", "id": "1d5905e8", "metadata": {}, "source": ["### Create a dataframe for the 10 topics"]}, {"cell_type": "code", "execution_count": 1, "id": "23a3889a", "metadata": {}, "outputs": [], "source": ["dict_topic = {'topic_number_tfidf': [], 'topic_words_tfidf': []}\n\nfor i,topic in enumerate(best_lda_model.components_):\n    dict_topic['topic_number_tfidf'].append(i)\n    dict_topic['topic_words_tfidf'].append([tfidf_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n\ndf_covid_topics_tfidf = pd.DataFrame(dict_topic, columns=['topic_number_tfidf', 'topic_words_tfidf'])\ndf_covid_topics_tfidf.head(10)"]}, {"cell_type": "markdown", "id": "307a1bc2", "metadata": {}, "source": ["## Visualization using pyLDAvis"]}, {"cell_type": "markdown", "id": "391d6e65", "metadata": {}, "source": ["The visualization is a learning from the following source\n\nhttps://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/"]}, {"cell_type": "code", "execution_count": 1, "id": "8ed947a2", "metadata": {}, "outputs": [], "source": ["import pyLDAvis\nimport pyLDAvis.sklearn"]}, {"cell_type": "code", "execution_count": 1, "id": "68340ad5", "metadata": {}, "outputs": [], "source": ["%%time\n\npyLDAvis.enable_notebook()\npanel = pyLDAvis.sklearn.prepare(best_lda_model, doc_term_matrix_Tfidf, tfidf_vect, mds='tsne', sort_topics=False)\npanel"]}, {"cell_type": "markdown", "id": "015cb2ae", "metadata": {}, "source": ["In the above visualization one has to make a note that the topic numbering starts from 1 while the sklearn LatentDirichletAllocation generates the topic starting with 0"]}, {"cell_type": "markdown", "id": "186ab933", "metadata": {}, "source": ["### Merge the Topic numbers and top 10 words of each topic to the Meta Data for further analysis"]}, {"cell_type": "code", "execution_count": 1, "id": "1a914ff6", "metadata": {}, "outputs": [], "source": ["df_covid_for_nlp['topic_number_tfidf'] = topic_values_tfidf.argmax(axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "43193ad9", "metadata": {}, "outputs": [], "source": ["df_covid_for_nlp.head(3)"]}, {"cell_type": "code", "execution_count": 1, "id": "bb0a66f2", "metadata": {}, "outputs": [], "source": ["df_covid_for_nlp['topic_number_tfidf'].value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "641a285b", "metadata": {}, "outputs": [], "source": ["df_covid_for_nlp = df_covid_for_nlp.merge(df_covid_topics_tfidf,\n                                          how='left', \n                                          left_on='topic_number_tfidf', \n                                          right_on='topic_number_tfidf')"]}, {"cell_type": "code", "execution_count": 1, "id": "4524d6d3", "metadata": {}, "outputs": [], "source": ["df_covid_for_nlp.head(3)"]}, {"cell_type": "code", "execution_count": 1, "id": "07add914", "metadata": {}, "outputs": [], "source": ["df_covid_for_nlp.columns"]}, {"cell_type": "code", "execution_count": 1, "id": "5c19051b", "metadata": {}, "outputs": [], "source": ["df_all_sources_metadata_with_topics = df_all_sources_metadata.copy()\ndf_all_sources_metadata_with_topics.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "2a998c22", "metadata": {}, "outputs": [], "source": ["df_all_sources_metadata_with_topics = df_all_sources_metadata.merge(\n    df_covid_for_nlp[['paper_id', 'abstract_word_count', 'body_word_count', 'cleaned_text', 'topic_number_tfidf', 'topic_words_tfidf']], \n    how='left', \n    left_on='sha', \n    right_on='paper_id')"]}, {"cell_type": "code", "execution_count": 1, "id": "665ebdd9", "metadata": {}, "outputs": [], "source": ["print(df_all_sources_metadata_with_topics.columns)\nprint(df_all_sources_metadata_with_topics.shape)\ndf_all_sources_metadata_with_topics.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "8290a3b3", "metadata": {}, "outputs": [], "source": ["# We can export the data for further analysis by executing the following code.\n# df_all_sources_metadata_with_topics.to_csv(output / 'df_all_sources_metadata_with_topics_biorxiv.csv', index = False)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}