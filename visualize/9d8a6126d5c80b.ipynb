{"cells": [{"cell_type": "markdown", "id": "165d7dc6", "metadata": {}, "source": ["I want to convey two things in this notebook.\n## 1. Don't have to be hesitant about using Loop.\nThey say \"avoid loops!'.\nBut I think It's not bad idea to use loops for this competition.\nBecause:\n* We have to use small batch inference using Time-series API.\n* Loops have very small overhead for each batch.\n* Loops are more flexible.\n* Even loops are not so slow. 3 features are extracted within 10 minits for 100M train data, as you can see blow.\n\n## 2. Future information should not be used.\nTime-series API doesn't allow us to use information from the future.\nSo we should not use it, especially user statistics from future make things very bad."]}, {"cell_type": "code", "execution_count": 1, "id": "b0f26f25", "metadata": {}, "outputs": [], "source": ["import pandas as pd\nimport numpy as np\nimport gc\nfrom sklearn.metrics import roc_auc_score\nfrom collections import defaultdict\nfrom tqdm.notebook import tqdm\nimport lightgbm as lgb"]}, {"cell_type": "markdown", "id": "5c4f60f8", "metadata": {}, "source": ["## setting\nCV files are generated by [this notebook](https://www.kaggle.com/its7171/cv-strategy)"]}, {"cell_type": "code", "execution_count": 1, "id": "e9ecd9b9", "metadata": {}, "outputs": [], "source": ["train_pickle = '../input/pickle1/cv1_train.pickle'\nvalid_pickle = '../input/pickle1/cv1_valid.pickle'\nquestion_file = '../input/riiid-test-answer-prediction/questions.csv'\ndebug = False\nvalidaten_flg = False"]}, {"cell_type": "code", "execution_count": 1, "id": "3e04e21e", "metadata": {}, "outputs": [], "source": ["train = pd.read_pickle(train_pickle)\nvalid = pd.read_pickle(valid_pickle)"]}, {"cell_type": "code", "execution_count": 1, "id": "8a0a8d35", "metadata": {}, "outputs": [], "source": ["question_df = pd.read_pickle('../input/questionspickle/question.pickle')"]}, {"cell_type": "code", "execution_count": 1, "id": "ea88dcb0", "metadata": {}, "outputs": [], "source": ["# def cal_method(type_of):\n#     return len(str(type_of).split(' '))\n# questions=pd.read_csv('../input/riiid-test-answer-prediction/questions.csv')\n# questions['enhence'] = questions['tags'].apply(cal_method)\n# questions['enhence'] = questions['enhence'].astype(np.int8)"]}, {"cell_type": "code", "execution_count": 1, "id": "8a2affd8", "metadata": {}, "outputs": [], "source": ["question_df_avg =  question_df.question_average.mean()"]}, {"cell_type": "code", "execution_count": 1, "id": "317bfdfd", "metadata": {}, "outputs": [], "source": ["# train = train.join(questions['enhence'],on=['content_id'],rsuffix='_question_average')\n# valid = valid.join(questions['enhence'],on=['content_id'],rsuffix='_question_average')\n"]}, {"cell_type": "markdown", "id": "33589b60", "metadata": {}, "source": ["## modeling"]}, {"cell_type": "code", "execution_count": 1, "id": "5ab850c8", "metadata": {}, "outputs": [], "source": ["TARGET = 'answered_correctly'\nFEATS = ['answered_correctly_avg_u','content_id', 'answered_correctly_sum_u', 'count_u', 'answered_correctly_avg_c', 'prior_question_had_explanation', 'prior_question_elapsed_time']\ndro_cols = list(set(train.columns) - set(FEATS))\ny_tr = train[TARGET]\ny_va = valid[TARGET]\ntrain.drop(dro_cols, axis=1, inplace=True)\nvalid.drop(dro_cols, axis=1, inplace=True)\n_=gc.collect()"]}, {"cell_type": "code", "execution_count": 1, "id": "0da558ed", "metadata": {}, "outputs": [], "source": ["train"]}, {"cell_type": "code", "execution_count": 1, "id": "ee944525", "metadata": {}, "outputs": [], "source": ["prior_question_elapsed_time_mean = train.prior_question_elapsed_time.dropna().values.mean()\n"]}, {"cell_type": "code", "execution_count": 1, "id": "12a93fb8", "metadata": {}, "outputs": [], "source": ["lgb_train = lgb.Dataset(train[FEATS], y_tr)\nlgb_valid = lgb.Dataset(valid[FEATS], y_va)\ndel train, y_tr,valid,y_va\n_=gc.collect()"]}, {"cell_type": "code", "execution_count": 1, "id": "6be3adef", "metadata": {}, "outputs": [], "source": ["model = lgb.train(\n                    {'objective': 'binary',#,#,\n        # 'num_iterations' : 10},#50\n\n                    lgb_train,\n                    valid_sets=[lgb_train, lgb_valid],\n                    verbose_eval=10,\n                    num_boost_round=10000,\n                    early_stopping_rounds=10\n                )\n_ = lgb.plot_importance(model)"]}, {"cell_type": "code", "execution_count": 1, "id": "c506925f", "metadata": {}, "outputs": [], "source": ["answered_correctly_sum_u_dict = defaultdict(int)\ncount_u_dict = defaultdict(int)\nvalid = pd.read_pickle(valid_pickle)\ny_va = valid[TARGET]\nprint('auc:', roc_auc_score(y_va, model.predict(valid[FEATS])))\n_ = lgb.plot_importance(model)\ndel valid,y_va"]}, {"cell_type": "markdown", "id": "a3a955ee", "metadata": {}, "source": ["## inference"]}, {"cell_type": "code", "execution_count": 1, "id": "8fc5f852", "metadata": {}, "outputs": [], "source": ["def add_user_feats_without_update(df, answered_correctly_sum_u_dict, count_u_dict):\n    acsu = np.zeros(len(df), dtype=np.int32)\n    cu = np.zeros(len(df), dtype=np.int32)\n    for cnt,row in enumerate(df[['user_id']].values):\n        acsu[cnt] = answered_correctly_sum_u_dict[row[0]]\n        cu[cnt] = count_u_dict[row[0]]\n    user_feats_df = pd.DataFrame({'answered_correctly_sum_u':acsu, 'count_u':cu})\n    user_feats_df['answered_correctly_avg_u'] = user_feats_df['answered_correctly_sum_u'] / user_feats_df['count_u']\n    df = pd.concat([df, user_feats_df], axis=1)\n    return df"]}, {"cell_type": "code", "execution_count": 1, "id": "657b6ce3", "metadata": {}, "outputs": [], "source": ["content_df = pd.read_pickle('../input/pickle1/content.pickle')"]}, {"cell_type": "code", "execution_count": 1, "id": "111ebbd0", "metadata": {}, "outputs": [], "source": ["# You can debug your inference code to reduce \"Submission Scoring Error\" with `validaten_flg = True`.\n# Please refer https://www.kaggle.com/its7171/time-series-api-iter-test-emulator about Time-series API (iter_test) Emulator.\nimport riiideducation\nenv = riiideducation.make_env()\niter_test = env.iter_test()\nset_predict = env.predict\nfor (test_df, sample_prediction_df) in iter_test:\n    previous_test_df = test_df.copy()\n    test_df = test_df[test_df['content_type_id'] == 0].reset_index(drop=True)\n    test_df = add_user_feats_without_update(test_df, answered_correctly_sum_u_dict, count_u_dict)\n    test_df = pd.merge(test_df, content_df, on='content_id',  how=\"left\")\n    test_df = pd.merge(test_df, questions_df, left_on='content_id', right_on='question_id', how='left')\n    #test_df['prior_question_had_explanation'] = test_df.prior_question_had_explanation.fillna(False).astype('int8')\n    test_df['prior_question_elapsed_time_mean'] = test_df.prior_question_elapsed_time.fillna(prior_question_elapsed_time_mean)\n    test_df[TARGET] =  model.predict(test_df[FEATS])\n    set_predict(test_df[['row_id', TARGET]])"]}, {"cell_type": "markdown", "id": "4dc93ae6", "metadata": {}, "source": ["Have a fun with loops! :)"]}, {"cell_type": "code", "execution_count": 1, "id": "5aefe804", "metadata": {}, "outputs": [], "source": ["answered_correctly_sum_u_dict.to_csv('answered_correctly_sum_u_dict.csv')"]}, {"cell_type": "code", "execution_count": 1, "id": "ad69fab3", "metadata": {}, "outputs": [], "source": ["count_u_dict.to_csv('count_u_dict.csv')"]}, {"cell_type": "code", "execution_count": 1, "id": "21b5d519", "metadata": {}, "outputs": [], "source": ["model.save_model('modelfeats7.txt')"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}