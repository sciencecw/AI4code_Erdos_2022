{"cells": [{"cell_type": "code", "execution_count": 1, "id": "e960fbc5", "metadata": {}, "outputs": [], "source": ["import nltk\n\nimport warnings\nwarnings.filterwarnings('ignore')"]}, {"cell_type": "markdown", "id": "88525a4d", "metadata": {}, "source": ["# Setting up train/test data"]}, {"cell_type": "code", "execution_count": 1, "id": "35df170a", "metadata": {}, "outputs": [], "source": ["from nltk.corpus import reuters\ntrain_documents, train_categories = zip(*[(reuters.raw(i), reuters.categories(i)) for i in reuters.fileids() if i.startswith('training/')])\ntest_documents, test_categories = zip(*[(reuters.raw(i), reuters.categories(i)) for i in reuters.fileids() if i.startswith('test/')])"]}, {"cell_type": "markdown", "id": "1be6f765", "metadata": {}, "source": ["# TF-IDF"]}, {"cell_type": "markdown", "id": "c54e7b25", "metadata": {}, "source": ["The following cell defines a function **tokenize** that performs following actions:\n- Receive a document as an argument to the function\n- Tokenize the document using `nltk.word_tokenize()`\n- Use `PorterStemmer` provided by the `nltk` to remove morphological affixes from each token\n- Append stemmed token to an already defined list `stems`\n- Return the list `stems`"]}, {"cell_type": "code", "execution_count": 1, "id": "4a152896", "metadata": {}, "outputs": [], "source": ["from nltk.stem.porter import PorterStemmer\ndef tokenize(text):\n    tokens = nltk.word_tokenize(text)\n    stems = []\n    for item in tokens:\n        stems.append(PorterStemmer().stem(item))\n    return stems"]}, {"cell_type": "markdown", "id": "7bb5cf6f", "metadata": {}, "source": ["To begin, I first used TF-IDF for feature selection on both train as well as test data using `TfidfVectorizer`.\n\nBut first, What `TfidfVectorizer` actually does?\n- `TfidfVectorizer` converts a collection of raw documents to a matrix of **TF-IDF** features.\n\n**TF-IDF**?\n- TFIDF (abbreviation of the term *frequency\u2013inverse document frequency*) is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. [tf\u2013idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n\n**Why `TfidfVectorizer`**?\n- `TfidfVectorizer` scale down the impact of tokens that occur very frequently (e.g., \u201ca\u201d, \u201cthe\u201d, and \u201cof\u201d) in a given corpus. [Feature Extraction and Transformation](https://spark.apache.org/docs/latest/mllib-feature-extraction.html#tf-idf)\n\nI gave following two arguments to `TfidfVectorizer`:\n- tokenizer: `tokenize` function\n- stop_words\n\nThen I used `fit_transform` and `transform` on the train and test documents repectively.\n\n**Why `fit_transform` for training data while `transform` for test data**?\n\nTo avoid data leakage during cross-validation, imputer computes the statistic on the train data during the `fit`, **stores it** and uses the same on the test data, during the `transform`. This also prevents the test data from appearing in `fit` operation."]}, {"cell_type": "code", "execution_count": 1, "id": "7edb7ad4", "metadata": {}, "outputs": [], "source": ["%%time\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(tokenizer = tokenize, stop_words = 'english')\n\nvectorised_train_documents = vectorizer.fit_transform(train_documents)\nvectorised_test_documents = vectorizer.transform(test_documents)"]}, {"cell_type": "markdown", "id": "6dba8e36", "metadata": {}, "source": ["# MultiLabelBinarizer"]}, {"cell_type": "markdown", "id": "3ea198d1", "metadata": {}, "source": ["For the **efficient implementation** of machine learning algorithms, many machine learning algorithms **requires all input variables and output variables to be numeric**. This means that categorical data must be converted to a numerical form.\n\nFor this purpose, I used `MultiLabelBinarizer` from `sklearn.preprocessing`."]}, {"cell_type": "code", "execution_count": 1, "id": "996af3f4", "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import MultiLabelBinarizer\n\nmlb = MultiLabelBinarizer()\ntrain_labels = mlb.fit_transform(train_categories)\ntest_labels = mlb.transform(test_categories)"]}, {"cell_type": "markdown", "id": "928c7d38", "metadata": {}, "source": ["# Training the model"]}, {"cell_type": "markdown", "id": "136e3fda", "metadata": {}, "source": ["Now, To **train** the classifier, I used `LinearSVC` in combination with the `OneVsRestClassifier` function in the scikit-learn package.\n\nThe strategy of `OneVsRestClassifier` is of **fitting one classifier per label** and the `OneVsRestClassifier` can efficiently do this task and also outputs are easy to interpret. Since each label is represented by **one and only one classifier**, it is possible to gain knowledge about the label by inspecting its corresponding classifier. [OneVsRestClassifier](http://scikit-learn.org/stable/modules/multiclass.html#one-vs-the-rest)\n\nThe reason I combined `LinearSVC` with `OneVsRestClassifier` is because `LinearSVC` supports **Multi-class**, while we want to perform **Multi-label** classification."]}, {"cell_type": "code", "execution_count": 1, "id": "fafc99e0", "metadata": {}, "outputs": [], "source": ["%%time\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\n\nclassifier = OneVsRestClassifier(LinearSVC())\nclassifier.fit(vectorised_train_documents, train_labels)"]}, {"cell_type": "markdown", "id": "f08317a6", "metadata": {}, "source": ["After fitting the classifier, I decided to use `cross_val_score` to **measure score** of the classifier by **cross validation** on the training data. But the only problem was, I wanted to **shuffle** data to use with `cross_val_score`, but it does not support shuffle argument.\n\nSo, I decided to use `KFold` with `cross_val_score` as `KFold` supports shuffling the data.\n\nI also enabled `random_state`, because `random_state` will guarantee the same output in each run. By setting the `random_state`, it is guaranteed that the pseudorandom number generator will generate the same sequence of random integers each time, which in turn will affect the split.\n\nWhy **42**?\n- [Why '42' is the preferred number when indicating something random?](https://softwareengineering.stackexchange.com/questions/507/why-42-is-the-preferred-number-when-indicating-something-random)"]}, {"cell_type": "markdown", "id": "3d83715b", "metadata": {}, "source": ["# Cross-Validation"]}, {"cell_type": "code", "execution_count": 1, "id": "96d4664e", "metadata": {}, "outputs": [], "source": ["%%time\nfrom sklearn.model_selection import KFold, cross_val_score\n\nkf = KFold(n_splits=10, random_state = 42, shuffle = True)\nscores = cross_val_score(classifier, vectorised_train_documents, train_labels, cv = kf)"]}, {"cell_type": "code", "execution_count": 1, "id": "451f1ab8", "metadata": {}, "outputs": [], "source": ["print('Cross-validation scores:', scores)\nprint('Cross-validation accuracy: {:.4f} (+/- {:.4f})'.format(scores.mean(), scores.std() * 2))"]}, {"cell_type": "markdown", "id": "e206c965", "metadata": {}, "source": ["In the end, I used different methods (`accuracy_score`, `precision_score`, `recall_score`, `f1_score` and `confusion_matrix`) provided by scikit-learn **to evaluate** the classifier. (both *Macro-* and *Micro-averages*)"]}, {"cell_type": "markdown", "id": "4990e606", "metadata": {}, "source": ["# Evaluation"]}, {"cell_type": "code", "execution_count": 1, "id": "580078d7", "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\npredictions = classifier.predict(vectorised_test_documents)\n\naccuracy = accuracy_score(test_labels, predictions)\n\nmacro_precision = precision_score(test_labels, predictions, average='macro')\nmacro_recall = recall_score(test_labels, predictions, average='macro')\nmacro_f1 = f1_score(test_labels, predictions, average='macro')\n\nmicro_precision = precision_score(test_labels, predictions, average='micro')\nmicro_recall = recall_score(test_labels, predictions, average='micro')\nmicro_f1 = f1_score(test_labels, predictions, average='micro')\n\ncm = confusion_matrix(test_labels.argmax(axis = 1), predictions.argmax(axis = 1))"]}, {"cell_type": "code", "execution_count": 1, "id": "4830dd82", "metadata": {}, "outputs": [], "source": ["print(\"Accuracy: {:.4f}\\nPrecision:\\n- Macro: {:.4f}\\n- Micro: {:.4f}\\nRecall:\\n- Macro: {:.4f}\\n- Micro: {:.4f}\\nF1-measure:\\n- Macro: {:.4f}\\n- Micro: {:.4f}\".format(accuracy, macro_precision, micro_precision, macro_recall, micro_recall, macro_f1, micro_f1))"]}, {"cell_type": "markdown", "id": "912f6666", "metadata": {}, "source": ["# Confusion Matrix"]}, {"cell_type": "markdown", "id": "e094303d", "metadata": {}, "source": ["In below cell, I used `matplotlib.pyplot` to **plot the confusion matrix** (of first *few results only* to keep the readings readable) using `heatmap` of `seaborn`."]}, {"cell_type": "code", "execution_count": 1, "id": "38644014", "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\nimport seaborn as sb\nimport pandas as pd\n\ncm_plt = pd.DataFrame(cm[:73])\n\nplt.figure(figsize = (25, 25))\nax = plt.axes()\n\nsb.heatmap(cm_plt, annot=True)\n\nax.xaxis.set_ticks_position('top')\n\nplt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}