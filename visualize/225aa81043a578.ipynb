{"cells": [{"cell_type": "markdown", "id": "c3f5285c", "metadata": {}, "source": ["# **Methodology**"]}, {"cell_type": "markdown", "id": "888324c9", "metadata": {}, "source": ["\n*   Importing Some Basic Libraries\n*   Importing Data\n*   Performing Descriptive Analysis on the dataset to know data better before Pre-processing\n*   Checking null values\n*   Doing Pre-processing\n*   Handling missing values\n*   Processing Categorical Values by Performing Label Encoding on it\n*   Checking Data Description After Pre-Processing\n*   Plotting the Histogram\n*   Analysis of Target Variable using different count plot w.r.t to different independent features. Also, by using Strip and Violin Plot in b/w Age and Survived and in b/w Fare and Survived\n*   Plotting Correlation Matrix and Heat Map\n*   Splitting train_df into 70% and 30% to construct training data and validation data respectively\n*   Implements 5 models which are Logistic Regression, GBM, SVM, DT, Naive Bayes\n*   Performing Prediction on Validation Data\n*   Evaluating Model based on Confusion Matrix and Classification Report for each model\n*   Save predictions on Testing data in .csv format\n\n\n\n\n\n\n\n\n\n\n\n"]}, {"cell_type": "markdown", "id": "2c4ce1f5", "metadata": {}, "source": ["# **Importing Some Basic Libraries**"]}, {"cell_type": "code", "execution_count": 1, "id": "45579a09", "metadata": {}, "outputs": [], "source": ["import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport sys, os\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nimport random\nfrom math import exp\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.metrics import accuracy_score \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix"]}, {"cell_type": "markdown", "id": "874e6872", "metadata": {}, "source": ["# **Importing Data**"]}, {"cell_type": "code", "execution_count": 1, "id": "fc27783a", "metadata": {}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]}, {"cell_type": "code", "execution_count": 1, "id": "013c4211", "metadata": {}, "outputs": [], "source": ["input_data_dir = \"../input/titanic/\"\ntrain_df = pd.read_csv(os.path.join(input_data_dir, \"train.csv\"))\ntest_df = pd.read_csv(os.path.join(input_data_dir, \"test.csv\"))"]}, {"cell_type": "markdown", "id": "8edfe798", "metadata": {}, "source": ["# **Descriptive Analysis of the dataset**"]}, {"cell_type": "code", "execution_count": 1, "id": "1a2c8878", "metadata": {}, "outputs": [], "source": ["print(\"Size of training dataset       : {}\".format(train_df.shape))\nprint(\"Size of test dataset           : {}\".format(test_df.shape))"]}, {"cell_type": "markdown", "id": "4c77f264", "metadata": {}, "source": ["## **Data Description**"]}, {"cell_type": "markdown", "id": "eee26750", "metadata": {}, "source": ["### **Training Data**"]}, {"cell_type": "code", "execution_count": 1, "id": "e677b5e0", "metadata": {}, "outputs": [], "source": ["train_df.info()"]}, {"cell_type": "code", "execution_count": 1, "id": "b9fcce65", "metadata": {}, "outputs": [], "source": ["train_df.describe().T"]}, {"cell_type": "markdown", "id": "1c667ecb", "metadata": {}, "source": ["### **Testing Data**"]}, {"cell_type": "code", "execution_count": 1, "id": "124228b0", "metadata": {}, "outputs": [], "source": ["test_df.info()"]}, {"cell_type": "code", "execution_count": 1, "id": "8bcc99a4", "metadata": {}, "outputs": [], "source": ["test_df.describe().T"]}, {"cell_type": "code", "execution_count": 1, "id": "1193cb1d", "metadata": {}, "outputs": [], "source": ["test_df_PassengerId = test_df.PassengerId"]}, {"cell_type": "markdown", "id": "85056397", "metadata": {}, "source": ["## **NULL VALUES**"]}, {"cell_type": "code", "execution_count": 1, "id": "6e827074", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(15, 20))\nsns.heatmap(train_df.isnull(), cbar=False)        #plotting heatmap using sns library to find missing values in train_df\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "8a60eae2", "metadata": {}, "outputs": [], "source": ["train_df.isna().sum()                        # Printing a count of missing value w.r.t each feature in train_df"]}, {"cell_type": "code", "execution_count": 1, "id": "5517cf6d", "metadata": {}, "outputs": [], "source": ["test_df.isna().sum()                        # Printing a count of missing value w.r.t each feature in test_df"]}, {"cell_type": "markdown", "id": "a93a2958", "metadata": {}, "source": ["# **Pre-Processing**"]}, {"cell_type": "markdown", "id": "7dd36806", "metadata": {}, "source": ["**As seen above, there is one Independent Feature(i.e. Cabin) having more than 75%  of the total values are missing values. So it is illogical to fill Missing Values for this feature.**\n"]}, {"cell_type": "markdown", "id": "12a5ac3c", "metadata": {}, "source": ["**Hence, We are going to drop this feature from our training dataset as well as testing data**"]}, {"cell_type": "code", "execution_count": 1, "id": "90f8c320", "metadata": {}, "outputs": [], "source": ["train_df = train_df.drop(['Cabin'], axis=1)\ntest_df = test_df.drop(['Cabin'], axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "deafb814", "metadata": {}, "outputs": [], "source": ["train_df = train_df.drop(['PassengerId','Name','Ticket'], axis=1)   # Dropping unuseful features for prediction.\ntest_df = test_df.drop(['PassengerId','Name','Ticket'], axis=1)"]}, {"cell_type": "markdown", "id": "0c522d8b", "metadata": {}, "source": ["## **Handling Missing Values:**"]}, {"cell_type": "code", "execution_count": 1, "id": "f3967577", "metadata": {}, "outputs": [], "source": ["train_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace=True)   # Since it contains discrete values.\ntrain_df['Embarked'].isna().sum()                                   # Prints the remaining missing values in 'Embarked' feature."]}, {"cell_type": "code", "execution_count": 1, "id": "baee3e7e", "metadata": {}, "outputs": [], "source": ["train_df['Age'].fillna(train_df['Age'].mean(),inplace=True)   # Since it contains continuous values.\ntrain_df['Age'].isna().sum()                                   # Prints the remaining missing values in 'Age' feature."]}, {"cell_type": "code", "execution_count": 1, "id": "e1d4877d", "metadata": {}, "outputs": [], "source": ["test_df['Age'].fillna(test_df['Age'].mean(),inplace=True)   # Since it contains continuous values.\ntest_df['Age'].isna().sum()                                   # Prints the remaining missing values in 'Age' feature."]}, {"cell_type": "code", "execution_count": 1, "id": "a2f8ba83", "metadata": {}, "outputs": [], "source": ["test_df['Fare'].fillna(test_df['Fare'].mean(),inplace=True)   # Since it contains continuous values.\ntest_df['Fare'].isna().sum()                                   # Prints the remaining missing values in 'Fare' feature."]}, {"cell_type": "markdown", "id": "a163a645", "metadata": {}, "source": ["## **Label Encoding On Categorical Features:**"]}, {"cell_type": "code", "execution_count": 1, "id": "c7523370", "metadata": {}, "outputs": [], "source": ["# There are two Features in our data which we are going to encode.\ntrain_df_encode = train_df[['Sex','Embarked']]\ntest_df_encode = test_df[['Sex','Embarked']]\ntrain_df_encode.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "923d8ae3", "metadata": {}, "outputs": [], "source": ["# Features which we are not going to encode.\ntrain_df_not_encode = train_df.drop(['Sex','Embarked'], axis=1)\ntest_df_not_encode = test_df.drop(['Sex','Embarked'], axis=1)\ntrain_df_not_encode.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "90413dbb", "metadata": {}, "outputs": [], "source": ["le = LabelEncoder()            # Using Label Encoder to encode features that are having data type as object in training data.\nfor i in train_df_encode:\n    train_df_encode[i]=le.fit_transform(train_df_encode[i])"]}, {"cell_type": "code", "execution_count": 1, "id": "aa7df230", "metadata": {}, "outputs": [], "source": ["for j in test_df_encode:        # Using Label Encoder to encode features that are having data type as object in testing data.\n    test_df_encode[j]=le.fit_transform(test_df_encode[j])"]}, {"cell_type": "code", "execution_count": 1, "id": "e22ddce3", "metadata": {}, "outputs": [], "source": ["train_df_encode.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "68256d46", "metadata": {}, "outputs": [], "source": ["train_df = pd.concat([train_df_encode, train_df_not_encode], axis=1)\ntest_df = pd.concat([test_df_encode, test_df_not_encode], axis=1)"]}, {"cell_type": "markdown", "id": "e3618d35", "metadata": {}, "source": ["# **Data Description After Pre-Processing**"]}, {"cell_type": "markdown", "id": "e1baa1fc", "metadata": {}, "source": ["### **Training Data:**"]}, {"cell_type": "code", "execution_count": 1, "id": "006a0c12", "metadata": {}, "outputs": [], "source": ["train_df.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "8ba9e8e1", "metadata": {}, "outputs": [], "source": ["train_df.info()"]}, {"cell_type": "code", "execution_count": 1, "id": "631870f3", "metadata": {}, "outputs": [], "source": ["train_df.describe().T"]}, {"cell_type": "markdown", "id": "df6a3ba3", "metadata": {}, "source": ["### **Testing Data:**"]}, {"cell_type": "code", "execution_count": 1, "id": "c0dfaa1d", "metadata": {}, "outputs": [], "source": ["test_df.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "bd1f81c2", "metadata": {}, "outputs": [], "source": ["test_df.info()"]}, {"cell_type": "code", "execution_count": 1, "id": "8d50f97c", "metadata": {}, "outputs": [], "source": ["test_df.describe().T"]}, {"cell_type": "code", "execution_count": 1, "id": "714d5b4d", "metadata": {}, "outputs": [], "source": ["train_df.hist(bins = 60, figsize = (20,17), color='magenta')"]}, {"cell_type": "markdown", "id": "3eb1abd2", "metadata": {}, "source": ["**I plotted the histogram to check the distribution of a sample of Training data.**"]}, {"cell_type": "markdown", "id": "53e565cd", "metadata": {}, "source": ["# **Analysis of Target Variable**"]}, {"cell_type": "code", "execution_count": 1, "id": "968c90b2", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(8,5))\nsns.countplot(x='Survived', data=train_df, order=[0, 1] )"]}, {"cell_type": "code", "execution_count": 1, "id": "a432e01b", "metadata": {}, "outputs": [], "source": ["a = sns.countplot(y='Survived',hue='Sex', data=train_df, order=[0,1])\n\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., title='Sex')\nplt.ylabel('Survived')\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "5197428c", "metadata": {}, "outputs": [], "source": ["b = sns.countplot(y='Survived',hue='Embarked', data=train_df, order=[0,1])\n\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., title='Embarked')\nplt.ylabel('Survived')\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "503a2f35", "metadata": {}, "outputs": [], "source": ["c = sns.countplot(y='Survived',hue='Pclass', data=train_df, order=[0,1])\n\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., title='Pclass')\nplt.ylabel('Survived')\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "5aba2541", "metadata": {}, "outputs": [], "source": ["d = sns.countplot(y='Survived',hue='SibSp', data=train_df, order=[0,1])\n\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., title='SibSp')\nplt.ylabel('Survived')\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "c5790393", "metadata": {}, "outputs": [], "source": ["e = sns.countplot(y='Survived',hue='Parch', data=train_df, order=[0,1])\n\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., title='Parch')\nplt.ylabel('Survived')\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "371536a5", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(10, 8))  \nsns.violinplot(x='Survived', y='Age', data=train_df, ax=ax)  \nax.set_title('Age Vs Survived')  \nplt.show()  "]}, {"cell_type": "code", "execution_count": 1, "id": "3ef758bd", "metadata": {}, "outputs": [], "source": ["# Using Strip plot to visualize the Age feature impact on Survived.  \nfig, ax= plt.subplots(figsize=(10, 8))  \nsns.stripplot(train_df['Survived'], train_df['Age'], jitter=True, ax=ax)  \nax.set_title('Age Vs Survived')  \nplt.show() "]}, {"cell_type": "markdown", "id": "f8fcb0e0", "metadata": {}, "source": ["**Using Above Strip Plot and Violin Plot for Age Vs Survived , we can easily seen that:**\n1.   **Those People which having a Age value in between 60 to 80 are mostly died.**\n2.   **Youngsters Which having a Age value in the range of 20 to 40 are mostly died.**\n"]}, {"cell_type": "code", "execution_count": 1, "id": "f2ec52a7", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(10, 8))  \nsns.violinplot(x='Survived', y='Fare', data=train_df, ax=ax)  \nax.set_title('Fare Vs Survived')  \nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "2d08230a", "metadata": {}, "outputs": [], "source": ["# Using Strip plot to visualize the Fare feature impact on Survived.\nfig, ax= plt.subplots(figsize=(10, 8))  \nsns.stripplot(train_df['Survived'], train_df['Fare'], jitter=True, ax=ax)  \nax.set_title('Fare Vs Survived')  \nplt.show() "]}, {"cell_type": "markdown", "id": "7745d17e", "metadata": {}, "source": ["**Using Above Strip Plot and Violin Plot for Fare Vs Survived , we can easily seen that:**\n\n\n1.   **Those People which having a very costlier ticket(around 500) are mostly survived.**\n2.   **Those People which having a very cheaper ticket(around 0) are mostly died.**\n\n"]}, {"cell_type": "markdown", "id": "9669810a", "metadata": {}, "source": ["## **`Correlation Matrix and Heat Map`**\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "9a5bf667", "metadata": {}, "outputs": [], "source": ["corr_data = train_df.corr()                       # calculating correlation data between features\nplt.figure(figsize=(19, 17))                      # setting figure size\nsns.set_style('ticks')                            # setting plot style\nsns.heatmap(corr_data, cmap='viridis',annot=True)                # plotting heatmap using sns library\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "eb2c9d77", "metadata": {}, "outputs": [], "source": ["corr_data.Survived.apply(lambda x: abs(x)).sort_values(ascending=False).iloc[1:8][::-1].plot(kind='barh',color='purple')  # calculating top correlated faetures\n                                                                                                                           # with respect to target variable i.e. \"Survived\"\nplt.title(\"Top Correlated Features\", size=20, pad=26)\nplt.xlabel(\"Correlation coefficient\")\nplt.ylabel(\"Features\")"]}, {"cell_type": "code", "execution_count": 1, "id": "687b46fe", "metadata": {}, "outputs": [], "source": ["train_df_X = train_df[['Sex',\t'Embarked',\t'Pclass',\t'Age',\t'SibSp',\t'Parch',\t'Fare']]\ntrain_df_X.head(n=5)"]}, {"cell_type": "code", "execution_count": 1, "id": "7584e16b", "metadata": {}, "outputs": [], "source": ["train_df_y = train_df.Survived\ntrain_df_y.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "2b0bb2af", "metadata": {}, "outputs": [], "source": ["# Splitting selected_train_df into 70% and 30% to construct training data and Validation data respectively.\ntrainX, valX, trainy, valy = train_test_split(train_df_X, train_df_y,test_size=0.3, random_state=12) "]}, {"cell_type": "code", "execution_count": 1, "id": "00255b97", "metadata": {}, "outputs": [], "source": ["trainX.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "b8c79686", "metadata": {}, "outputs": [], "source": ["trainy.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "e8295101", "metadata": {}, "outputs": [], "source": ["valX.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "507c7d32", "metadata": {}, "outputs": [], "source": ["valy.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "3c25f586", "metadata": {}, "outputs": [], "source": ["#Creating a Logistic Regression Classifier\nLogisticRegression_Model = LogisticRegression(penalty='l2',solver='newton-cg')\n#Train the model using the training sets\nLogisticRegression_Model.fit(trainX, trainy)"]}, {"cell_type": "code", "execution_count": 1, "id": "1ef4376a", "metadata": {}, "outputs": [], "source": ["#Creating a svm Classifier\nSVM_Model = svm.SVC(kernel='linear', probability=True) # Linear Kernel\n#Train the model using the training sets\nSVM_Model.fit(trainX, trainy)"]}, {"cell_type": "code", "execution_count": 1, "id": "16acc0c0", "metadata": {}, "outputs": [], "source": ["lr_list = [0.5, 0.6, 0.7, 0.71, 0.75, 0.9, 1]\n\nfor learning_rate in lr_list:\n    GBM_Model = GradientBoostingClassifier(n_estimators=20, learning_rate=learning_rate, max_features=2, max_depth=2, random_state=0)\n    GBM_Model.fit(trainX, trainy)\n\n    print(\"Learning rate: \", learning_rate)\n    print(\"Accuracy score (Training): {0:.3f}\".format(GBM_Model.score(trainX, trainy)))\n    print(\"Accuracy score (Validation): {0:.3f}\".format(GBM_Model.score(valX, valy)))   "]}, {"cell_type": "code", "execution_count": 1, "id": "8ad4a527", "metadata": {}, "outputs": [], "source": ["#Creating a GBM Classifier\nGBM_Model = GradientBoostingClassifier(n_estimators=20, learning_rate=0.71, max_features=2, max_depth=2, random_state=0)\n#Train the model using the training sets\nGBM_Model.fit(trainX, trainy)"]}, {"cell_type": "code", "execution_count": 1, "id": "9ced8c68", "metadata": {}, "outputs": [], "source": ["#Creating a Gaussian Classifier\nNB_Model = GaussianNB()\n# Train the model using the training sets\nNB_Model.fit(trainX, trainy)"]}, {"cell_type": "code", "execution_count": 1, "id": "947d41ba", "metadata": {}, "outputs": [], "source": ["#Creating a Decision Tree Classifier\nDT_Model = DecisionTreeClassifier(criterion = \"entropy\",splitter = \"best\", random_state = 100, max_depth=3, min_samples_leaf=5)  \n# Train the model using the training sets\nDT_Model.fit(trainX, trainy) "]}, {"cell_type": "markdown", "id": "81a66587", "metadata": {}, "source": ["**Perform Prediction on Validation Data:**"]}, {"cell_type": "code", "execution_count": 1, "id": "665e2df6", "metadata": {}, "outputs": [], "source": ["LogisticRegression_predictions = LogisticRegression_Model.predict(valX)\nSVM_predictions = SVM_Model.predict(valX)\nNB_predictions = NB_Model.predict(valX)\nDT_predictions = DT_Model.predict(valX)\nGBM_predictions = GBM_Model.predict(valX)"]}, {"cell_type": "markdown", "id": "3f6fe8fc", "metadata": {}, "source": ["# **Evaluation**"]}, {"cell_type": "code", "execution_count": 1, "id": "81b4927d", "metadata": {}, "outputs": [], "source": ["print(\"Logistic Regression_Confusion Matrix:\")\nprint(confusion_matrix(valy, LogisticRegression_predictions))\n\nprint(\"Logistic Regression_predictions_Classification Report\")\nprint(classification_report(valy, LogisticRegression_predictions))"]}, {"cell_type": "code", "execution_count": 1, "id": "01df2b25", "metadata": {}, "outputs": [], "source": ["print(\"SVM_Confusion Matrix:\")\nprint(confusion_matrix(valy, SVM_predictions))\n\nprint(\"SVM_Classification Report\")\nprint(classification_report(valy, SVM_predictions))"]}, {"cell_type": "code", "execution_count": 1, "id": "8f9a9a18", "metadata": {}, "outputs": [], "source": ["print(\"Naive Bayes Confusion Matrix:\")\nprint(confusion_matrix(valy, NB_predictions))\nprint(\"Naive Bayes Classification Report\")\nprint(classification_report(valy, NB_predictions))"]}, {"cell_type": "code", "execution_count": 1, "id": "f59d3775", "metadata": {}, "outputs": [], "source": ["print(\"DT_Confusion Matrix:\")\nprint(confusion_matrix(valy, DT_predictions))\n\nprint(\"DT_Classification Report\")\nprint(classification_report(valy, DT_predictions))"]}, {"cell_type": "code", "execution_count": 1, "id": "b48b5404", "metadata": {}, "outputs": [], "source": ["print(\"GBM_Confusion Matrix:\")\nprint(confusion_matrix(valy, GBM_predictions))\n\nprint(\"GBM_Classification Report\")\nprint(classification_report(valy, GBM_predictions))"]}, {"cell_type": "markdown", "id": "6a319d15", "metadata": {}, "source": ["**Since, out of 5 Different Models GBM_Model(i.e. Gradient Boosting Machine) provides the best Accuracy(i.e. 82%). Therefore, we perform prediction on Test Data Using GBM_Model.**"]}, {"cell_type": "code", "execution_count": 1, "id": "ee23a9ba", "metadata": {}, "outputs": [], "source": ["GBM_predictions_On_Test_Data = GBM_Model.predict(test_df)"]}, {"cell_type": "markdown", "id": "2bf8ad12", "metadata": {}, "source": ["\n\n# **Predictions on Test Data:**"]}, {"cell_type": "code", "execution_count": 1, "id": "97d72a8f", "metadata": {}, "outputs": [], "source": ["Output_DF = pd.DataFrame({'PassengerId':test_df_PassengerId,'Survived':GBM_predictions_On_Test_Data})"]}, {"cell_type": "code", "execution_count": 1, "id": "a779231c", "metadata": {}, "outputs": [], "source": ["#Save to csv\nOutput_DF.to_csv('Titanic_pred.csv',index=False)\nOutput_DF.head()"]}, {"cell_type": "markdown", "id": "6ee579e2", "metadata": {}, "source": ["Colab Link For same Notebook:\nhttps://colab.research.google.com/drive/16PzOiBXX5ay89N_nq9jT7ofbIbiOIJHd?usp=sharing"]}, {"cell_type": "markdown", "id": "ea25827e", "metadata": {}, "source": ["**Thank you**,<br>\nNikunj Bansal,<br>\nR177218063,<br>\nB2 Batch<br>"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}