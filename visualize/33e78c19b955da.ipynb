{"cells": [{"cell_type": "markdown", "id": "52f652ca", "metadata": {}, "source": ["In this kernel I'll make a simple logistic regression model to predict questions as sincere or insincere."]}, {"cell_type": "code", "execution_count": 1, "id": "e051d1ca", "metadata": {}, "outputs": [], "source": ["import pandas as pd\nimport numpy as np\nimport re\nfrom textblob.classifiers import NaiveBayesClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split"]}, {"cell_type": "code", "execution_count": 1, "id": "acf6416d", "metadata": {}, "outputs": [], "source": ["df = pd.read_csv('../input/train.csv')\ndf.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "7f83b85c", "metadata": {}, "outputs": [], "source": ["print(df['target'].value_counts(), end = '\\n\\n')\nprint(sum(df['target'] == 1) / sum(df['target'] == 0) * 100, 'percent of questions are insincere.')"]}, {"cell_type": "markdown", "id": "efd637f2", "metadata": {}, "source": ["The vast majority of questions are considered sincere, and any model we train to predict the sincerity of reviews should do significantly better than simply predicting every question as sincere (based on the above, that would be correct nearly 94% of the time)."]}, {"cell_type": "markdown", "id": "ba779535", "metadata": {}, "source": ["### Training a Model"]}, {"cell_type": "markdown", "id": "73bd7dc3", "metadata": {}, "source": ["I'll make a train and test set, with 80% of the data being the train set and 20% being the test set. The model will be trained on the training data and evaluated on the test data."]}, {"cell_type": "code", "execution_count": 1, "id": "0a849800", "metadata": {}, "outputs": [], "source": ["msk = np.random.rand(len(df)) < 0.8\ntrain = df[msk]\ntest = df[~msk]"]}, {"cell_type": "markdown", "id": "72ab798f", "metadata": {}, "source": ["Before feeding reviews into a model, they should be cleaned to remove punctuation, as that willl be useless from the computers point of view in trying to work out the sentiment of a text - for this case we're only interested in the words in a review. The code below (from https://towardsdatascience.com/sentiment-analysis-with-python-part-1-5ce197074184) gets rid of punctuation so a review is turned into words only."]}, {"cell_type": "code", "execution_count": 1, "id": "887763c5", "metadata": {}, "outputs": [], "source": ["REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\|)|(\\()|(\\))|(\\[)|(\\])\")\nREPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n\ndef preprocess_reviews(reviews):\n    reviews = [REPLACE_NO_SPACE.sub(\"\", line.lower()) for line in reviews]\n    reviews = [REPLACE_WITH_SPACE.sub(\" \", line) for line in reviews]\n    \n    return reviews\n\ntrain_clean = preprocess_reviews(train['question_text'])\ntest_clean = preprocess_reviews(test['question_text'])"]}, {"cell_type": "markdown", "id": "a057b88b", "metadata": {}, "source": ["The next step is one hot encoding, where we turn each review into a very large matrix of 0's and 1's. A 0 would represent a certain word isn't included, whereas a 1 means that word is included. A short review of a few words would have a matrix of almost entirely 0's (ie. the vast majority of unique words across all reviews aren't present in the review), with just a small number of 1's. This is necessary for the logistic regression algorithm used below."]}, {"cell_type": "code", "execution_count": 1, "id": "6198addf", "metadata": {}, "outputs": [], "source": ["cv = CountVectorizer(binary=True)\ncv.fit(train_clean)\nX_train = cv.transform(train_clean)\nX_test = cv.transform(test_clean)"]}, {"cell_type": "markdown", "id": "0c6bd0dc", "metadata": {}, "source": ["Now we can finally train a model on the train data with model.fit(). After that, model.predict() can be called on the test data to evaluate how good the model does."]}, {"cell_type": "code", "execution_count": 1, "id": "6acc81f7", "metadata": {}, "outputs": [], "source": ["target_train = train['target']\ntarget_test = test['target']\n\nmodel = LogisticRegression()\nmodel.fit(X_train, target_train)\nprint(\"Accuracy: %s\" % accuracy_score(target_test, model.predict(X_test)))"]}, {"cell_type": "markdown", "id": "d0afa971", "metadata": {}, "source": ["With over 95% accuracy, this model clearly beats the naive baseline, but could clearly be improved. Finally to load up the Kaggle test.csv file and make predictions on the questions there, then save them into a submissions.csv file that will be evaluated by kaggle."]}, {"cell_type": "code", "execution_count": 1, "id": "ec2c9f3c", "metadata": {}, "outputs": [], "source": ["kaggle_test = pd.read_csv('../input/test.csv')\nkaggle_test_clean = preprocess_reviews(kaggle_test['question_text'])\nX_kaggle_test = cv.transform(kaggle_test_clean)\nresults = model.predict(X_kaggle_test)\nsubmission = pd.DataFrame({\"qid\" : kaggle_test['qid'], \"prediction\" : results})\nsubmission.to_csv(\"submission.csv\", index=False)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}