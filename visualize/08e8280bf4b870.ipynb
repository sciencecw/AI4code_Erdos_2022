{"cells": [{"cell_type": "code", "execution_count": 1, "id": "cbc1190d", "metadata": {}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns # plots\nimport matplotlib.pyplot as plt # plots\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]}, {"cell_type": "markdown", "id": "7e9a2c0e", "metadata": {}, "source": ["# \ud83d\udcda Beginner Logistic Regression - Breast Cancer"]}, {"cell_type": "markdown", "id": "1c9c8c4e", "metadata": {}, "source": ["![beginner](https://img.shields.io/badge/Level-Beginner-darkgreen.svg)"]}, {"cell_type": "markdown", "id": "864460cf", "metadata": {}, "source": ["**Details**:\n> **Author**: Itokiana RAFIDINARIVO\n\n> **Last update**: Fri. 4 Dec 2020\n\n> **Topic**: Logistic regression"]}, {"cell_type": "markdown", "id": "2a5081d9", "metadata": {}, "source": ["## \ud83d\udc4b Introduction\n\nHi readers! Are you new in datascience and you want to learn how to create a model which can do a binary classification, well you are at the right place! This notebook will be a kind of a tutorial for anyone who want to create a binary classification model with **python**."]}, {"cell_type": "markdown", "id": "49844a49", "metadata": {}, "source": ["___"]}, {"cell_type": "markdown", "id": "792db778", "metadata": {}, "source": ["## \ud83d\udcdc Load the data\nAs you might guess if you want to create a model, data is needed so the data we will be using for this notebook is the ***Breast Cancer Wisconsin (Diagnostic) Data Set*** a \"*classic and very easy binary classification dataset*\" from [here](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)).\n\n> **Description**\n: Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. A few of the images can be found at [Web Link](http://pages.cs.wisc.edu/~street/images/)\n\nWe will use the ***scikit-learn*** packages in order to load the data in the notebook!\n\n> [**scikit-learn**](https://scikit-learn.org/stable/index.html) package\n: Simple and efficient tools for predictive data analysis;  Accessible to everybody, and reusable in various contexts;  Built on NumPy, SciPy, and matplotlib;  Open source, commercially usable - BSD license."]}, {"cell_type": "code", "execution_count": 1, "id": "078cd7e9", "metadata": {}, "outputs": [], "source": ["from sklearn.datasets import load_breast_cancer  # Necessary function that loads the data\n\nbunch = load_breast_cancer(as_frame=True)  #\u00a0Load the Bunch in a variable\n\ndata = bunch[\"frame\"]  # Access the dataframe in the Bunch\n\ndata.head()  # Display the 5 first rows in the DataFrame"]}, {"cell_type": "markdown", "id": "711be0a1", "metadata": {}, "source": ["The code blocks above did the following :\n1. Load the Bunch from the package\n2. Extract the ***DataFrame*** from the data bunch\n3. Display the 5 first rows in the ***DataFrame***\n\n##### Vocabulary\n> [**Bunch**](https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html#sklearn.utils.Bunch)\n: Container object exposing keys as attributes. Bunch objects are sometimes used as an output for functions and methods. They extend dictionaries by enabling values to be accessed by key, `bunch[\"value_key\"]`, or by an attribute, `bunch.value_key`.\n\n> [**DataFrame**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)\n: Two-dimensional, size-mutable, potentially heterogeneous tabular data. Data structure also contains labeled axes (rows and columns). Arithmetic operations align on both row and column labels. Can be thought of as a dict-like container for Series objects. The primary pandas data structure."]}, {"cell_type": "code", "execution_count": 1, "id": "36007eb8", "metadata": {}, "outputs": [], "source": ["names  = [\"rows\", \"columns\"]\nmaxstr = 10\nmaxval = 3\n\nprint(\"No. of {} : {}\".format(\"dimensions\".rjust(maxstr), str(len(data.shape)).rjust(maxval)))\n\nfor name, value in zip(names, data.shape):\n    print(\"No. of {} : {}\".format(name.rjust(maxstr), str(value).rjust(maxval)))"]}, {"cell_type": "markdown", "id": "34574e6e", "metadata": {}, "source": ["#### Dimensions\n\nThe ***DataFrame*** is **bi-dimensional** and has :\n- **568 rows**\n- **31 columns**\n\n#### Code explained\n\n> **`data.shape`**\n: `data`, our DataFrame, has an attribute named \"shape\". This attribute `shape` contains a tuple of the length of every dimension. Here `data.shape` returns `(568, 31)`."]}, {"cell_type": "code", "execution_count": 1, "id": "2b667933", "metadata": {}, "outputs": [], "source": ["n_missing_values = data.isna().sum().sum()\n\nprint(f\"There are {n_missing_values} missing value(s).\")"]}, {"cell_type": "markdown", "id": "1947576d", "metadata": {}, "source": ["#### Missing values\nA good practice before implementing a prediction model is to look at the quantity of missing values in the data and choose the right method to impute them. Hopefully the data does not have any missing values here so we don't have to do that!\n\n#### Code explained\n> **`data.isna().sum().sum()`**\n: `data.isna()` returns a DataFrame of boolean values where it contains `False` if the values is not missing otherwise `True`. Then `data.isna().sum()` returns a Series where each values represent the quantity of missing values in the corresponding column, computed by adding 0 if the values is `False` otherwise 1. Finally `data.isna().sum().sum()` is just the sum of the values in `data.isna().sum()`.\n\n#### Vocabulary\n> [**Series**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html)\n: One-dimensional ndarray with axis labels (including time series). Labels need not be unique but must be a hashable type. The object supports both integer- and label-based indexing and provides a host of methods for performing operations involving the index. Statistical methods from ndarray have been overridden to automatically exclude missing data (currently represented as NaN). Operations between Series (+, -, /, , *) align values based on their associated index values\u2013 they need not be the same length. The result index will be the sorted union of the two indexes."]}, {"cell_type": "code", "execution_count": 1, "id": "d7bc9c56", "metadata": {}, "outputs": [], "source": ["corr = data.corr().round(2)  # Returns a correlation table between variables\n\nmask = np.triu(np.ones_like(corr, dtype=bool))  # Generate a mask for the upper triangle\n\nplt.figure(figsize=(18,10))  # Define the figure size\nsns.heatmap(corr, mask=mask, vmin=-1, vmax=1, annot=True)  # Create the correlation heatmap\nplt.show()  # Show the plot"]}, {"cell_type": "markdown", "id": "ac383df2", "metadata": {}, "source": ["#### Correlation\nAnother good practice is to see if among the variables there are some that are linear combinations of others. If some variables are highly correlated it might negatively affect our model! So the solution in this case is delete them of the features for the model.\n\n#### Code explained\n> `data.corr()`\n: Our DataFrame `data` has a method `corr` which returns a DataFrame representing the correlations between variables.\n\n#### Vocabulary\n\n> **variable** | **feature**\n: Most of the time you might see the words *variable* and *feature* being used in datascience. Those words are representing the same entity, that is for instance the column of a DataFrame. "]}, {"cell_type": "code", "execution_count": 1, "id": "6221c08a", "metadata": {}, "outputs": [], "source": ["data.describe()"]}, {"cell_type": "markdown", "id": "5ee9bafb", "metadata": {}, "source": ["The code block above shows some basics descriptive statistics about our features, the method `describe` of DataFrame is used to do that.\n\n#### Code explained\n> `data.describe()`\n:\nGenerate descriptive statistics.\nDescriptive statistics include those that summarize the central tendency, dispersion and shape of a dataset\u2019s distribution, excluding NaN values.\nAnalyzes both numeric and object series, as well as DataFrame column sets of mixed data types. The output will vary depending on what is provided. Refer to the notes below for more detail."]}, {"cell_type": "markdown", "id": "bd7fc357", "metadata": {}, "source": ["#### Normalization or Standardization?\n\nAnother common practice before creating a model is to standardize or normalize the numerical variables. But what is the difference between those two terms?\n\n> **Normalization**\n: Scale a variable to have a values between 0 and 1.\n\n> **Standardization**\n: Have a mean of 0 and a standard deviation of 1. \n"]}, {"cell_type": "code", "execution_count": 1, "id": "d0b98a2b", "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import Normalizer, StandardScaler  #\u00a0Import the functions from the sklearn package\n\ndef process_data(dataframe=None, preprocessing_method=None):\n    scaled = preprocessing_method.fit_transform(dataframe)  # Apply the preprocessing function to the DataFrame\n    return pd.DataFrame(scaled)\n\nnormalized = process_data(dataframe=data, preprocessing_method=Normalizer())\nscaled     = process_data(dataframe=data, preprocessing_method=StandardScaler())"]}, {"cell_type": "code", "execution_count": 1, "id": "19d366ea", "metadata": {}, "outputs": [], "source": ["max_values = normalized.max()  # Maximum values among columns\nmin_values = normalized.min()  # Minimum values among columns\n\nprint(\"Normalized data :\")\nprint(\"  > max =\", min(min_values))\nprint(\"  > min =\", max(max_values))"]}, {"cell_type": "markdown", "id": "27d3bc33", "metadata": {}, "source": ["> The maximum and the minimum are between 0 and 1 so the data has been normalized."]}, {"cell_type": "code", "execution_count": 1, "id": "00af8ac9", "metadata": {}, "outputs": [], "source": ["described = scaled.describe()\nmeans     = described.loc[\"mean\"]\nstds      = described.loc[\"std\"]\n\nprint(\"Standardization data :\")\nprint(\"  > mean between [ {}; {} ]\".format(min(means), max(means)))\nprint(\"  > std  between [ {} ; {} ]\".format(min(stds), max(stds)))"]}, {"cell_type": "markdown", "id": "9741cdc0", "metadata": {}, "source": ["> The mean is approximately 0 and standard deviation is approximately 1 so the data has been standardized."]}, {"cell_type": "markdown", "id": "029a3edc", "metadata": {}, "source": ["Well now let's get into the real deal! If I missed a few things please let me know down in the comments."]}, {"cell_type": "markdown", "id": "13f0c80e", "metadata": {}, "source": ["___"]}, {"cell_type": "markdown", "id": "13dcb6ed", "metadata": {}, "source": ["##\u00a0\ud83e\uddf0 Create the model\n\nBefore getting into it we have to know do we have to do and how? Well here are some facts first...\n\n#### Facts\n\n- **Type of problem** : Binary classification\n- **Target variable** : 'target' column\n- **Features types**  : Mostly **float64** and an **int64**\n\n#### Basic process\n\n1. Prepare the data\n3. Fit the model\n4. Evaluate the model"]}, {"cell_type": "code", "execution_count": 1, "id": "7faad9f2", "metadata": {}, "outputs": [], "source": ["from sklearn.linear_model import LogisticRegression  # class Logistic regression model\nfrom sklearn.model_selection import train_test_split  #\u00a0function that separates 'train' and 'test' sets\nfrom sklearn.metrics import roc_auc_score  # function calculating the area under the ROC curve\nfrom sklearn.metrics import plot_roc_curve  # function plotting the ROC curve\nfrom sklearn.preprocessing import scale  # function that scales the data\n\nRANDOM_STATE = 42\n\ndef basic_logistic_regression(data, target, test_size=0.3, threshold=0.75, plot_roc=False):\n    # Separate 'features' and 'target' such that : y = f(X)\n    X = scale(data.drop(columns=[target]))\n    y = data[target]\n    \n    # Instanciate the logistic regression model\n    mLR = LogisticRegression(random_state=RANDOM_STATE)\n    \n    # Separate the data into 'train' and 'test' sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=RANDOM_STATE)\n    \n    # Fit the model to the 'train' set\n    mLR.fit(X_train, y_train)\n    \n    #\u00a0Predict the probabilities\n    y_train_pred = mLR.predict(X_train)\n    y_test_pred  = mLR.predict(X_test)\n    \n    # Compute the scores\n    score_train = roc_auc_score(y_train, y_train_pred)\n    score_test  = roc_auc_score(y_test, y_test_pred)\n    \n    # Printing\n    print(\"Score (train) :\", score_train)\n    print(\"Score (test ) :\", score_test)\n    \n    if plot_roc:  # Display the ROC curve\n        plt.figure(figsize=(10, 8))\n        plot_roc_curve(mLR, X_test, y_test)\n        line = np.linspace(0, 1, 20)\n        plt.plot(line, line)\n        plt.show()\n\n    return mLR"]}, {"cell_type": "markdown", "id": "5567ae9a", "metadata": {}, "source": ["#### Code explained\n\n> `sklearn.metrics.roc_auc_score`\n: Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n\n> `sklearn.metrics.plot_roc_curve`\n: Plot Receiver operating characteristic (ROC) curve.\n\n> `sklearn.linear_model.LogisticRegression`\n: Logistic Regression (aka logit, MaxEnt) classifier. This class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019, \u2018saga\u2019 and \u2018lbfgs\u2019 solvers.\n\n> `sklearn.preprocessing.scale`\n: Standardize a dataset along any axis. Center to the mean and component wise scale to unit variance.\n\n> `sklearn.model_selection.train_test_split`\n: Split arrays or matrices into random train and test subsets."]}, {"cell_type": "code", "execution_count": 1, "id": "9042ae5d", "metadata": {}, "outputs": [], "source": ["model = basic_logistic_regression(data, target=\"target\", plot_roc=True)"]}, {"cell_type": "markdown", "id": "9044eef2", "metadata": {}, "source": ["####\u00a0Observations\nIt seems like both scores for training and testing set are very high! Despite the lack of relevance of the training score, the testing one is quite good because it approches 1.\n\n#### Conclusion\n\nThat's it for a basic process in order to create a logistic regression with ***scikit-learn***."]}, {"cell_type": "markdown", "id": "788e043a", "metadata": {}, "source": ["___"]}, {"cell_type": "markdown", "id": "9f07441d", "metadata": {}, "source": ["## \u2692\ufe0f *Pipeline* and *K-Fold cross-validation*\n\nIn addition to basic process of creating a predictive model and evaluating it with the ROC curve, there is another way to evaluate the model, the ***K-Fold cross-validation***. Here is an illustration:\n\n![kfold_crossvalidation](https://ethen8181.github.io/machine-learning/model_selection/img/kfolds.png)\n> source: http://ethen8181.github.io/machine-learning/model_selection/model_selection.html"]}, {"cell_type": "code", "execution_count": 1, "id": "8ce64502", "metadata": {}, "outputs": [], "source": ["from sklearn.pipeline import make_pipeline  # function that create a pipeline\nfrom sklearn.model_selection import StratifiedKFold  #\u00a0class for k-fold cross-validation\n\nscores = list()  #\u00a0scores computed by every fold\n\n#\u00a0Create a Pipeline that will: scale and create the model\npipeline = make_pipeline(\n    StandardScaler(),  # scale the data\n    LogisticRegression(),  # model \n)\n\n# Create a 3-Fold cross-validation\nk   = 4\nskf = StratifiedKFold(\n    n_splits=k,\n    random_state=RANDOM_STATE,\n    shuffle=True\n)\n\n#\u00a0Separate 'target' and 'features'\ntarget = \"target\"\nX      = data.drop(columns=[target])\ny      = data[target]\n\n# Loop over the 5 folds\nfor train_index, test_index in skf.split(X, y):\n    # Get the features\n    X_train, X_test = X.values[train_index], X.values[test_index]\n    \n    # Get the target\n    y_train, y_test = y.values[train_index], y.values[test_index]\n    \n    # Scale the data & Fit to the model\n    pipeline.fit(X_train, y_train)\n    \n    #\u00a0Get the score\n    score = roc_auc_score(pipeline.predict(X_test), y_test)\n    \n    #\u00a0Add the score\n    scores.append(score)"]}, {"cell_type": "markdown", "id": "24520e83", "metadata": {}, "source": ["#### Code explained\n\n> `sklearn.model_selection.StratifiedKFold` \n: This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class.\n\n> `sklearn.pipeline.make_pipeline`\n: Construct a `Pipeline` from the given estimators. This is a shorthand for the Pipeline constructor; it does not require, and does not permit, naming the estimators. Instead, their names will be set to the lowercase of their types automatically."]}, {"cell_type": "code", "execution_count": 1, "id": "ed2b0ccb", "metadata": {}, "outputs": [], "source": ["print(\"Scores  :\",[round(value, 4) for value in scores])\naverage = np.mean(scores)\nvmin    = np.min(scores)\nvmax    = np.max(scores)\nstd     = np.std(scores)\nprint(\"Average :\", round(average, 4))\nprint(\"Minimum :\", round(vmin, 4))\nprint(\"Maximum :\", round(vmax, 4))\nprint(\"STD     :\", round(std, 4))"]}, {"cell_type": "markdown", "id": "cf5f4b9a", "metadata": {}, "source": ["___\n\n##\u00a0\ud83c\udfc1 Conclusion\n\nWell this is all for now, I hope that you learnt something from this notebook. This notebook covered the following principles :\n- Do some basic check-up of the data\n- Normalization / Standardization\n- Create a logistic regression with the ***scikit-learn*** package\n- Evaluate with the ROC curves\n- Pipelines & K-Fold cross validation"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}