{"cells": [{"cell_type": "markdown", "id": "b0f2291e", "metadata": {}, "source": ["# <font color='orange'>My First Kaggle Kernel and how I got first rank in competition </font>"]}, {"cell_type": "markdown", "id": "eeb8dc98", "metadata": {}, "source": ["![title](https://datahack-prod.s3.ap-south-1.amazonaws.com/__sized__/contest_cover/jantahack_-thumbnail-1200x1200-90.jpg)"]}, {"cell_type": "markdown", "id": "17bfe702", "metadata": {}, "source": ["* From the research on all the Time Series Competitons on Kaggle ,it has been found that boosting models perform better as compared to the traditional approach using Statistical models like Holt Winters, Arima.\n* Research PDF Link: https://www.researchgate.net/publication/339362837_Learnings_from_Kaggle's_Forecasting_Competitions\n* Here, I am using the data from the Analytics Vidhya-JanataHack-IOT hackathon where we won the hackathon with Regression approach using mighty XGBoost.\n* You can check the problem statement here:https://datahack.analyticsvidhya.com/contest/janatahack-machine-learning-for-iot/ "]}, {"cell_type": "markdown", "id": "ad8c267b", "metadata": {}, "source": ["* Don't forget to check the last part of the solution which is the main secret sauce :-)."]}, {"cell_type": "markdown", "id": "e8b925eb", "metadata": {}, "source": ["#### <font color='red'>Import libraries</font>"]}, {"cell_type": "code", "execution_count": 1, "id": "c63fcf65", "metadata": {}, "outputs": [], "source": ["# import libraries\nimport numpy as np\nimport pandas as pd\nimport time\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\n\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\n# Function to plot feature importance\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\nimport matplotlib.pyplot as plt\n"]}, {"cell_type": "markdown", "id": "47e87f84", "metadata": {}, "source": ["#### <font color='red'>Loading the Data</font>"]}, {"cell_type": "code", "execution_count": 1, "id": "2925edf0", "metadata": {}, "outputs": [], "source": ["# Read train , test and submission csv in pandas dataframe\ntrain=pd.read_csv('/kaggle/input/train.csv',parse_dates=['DateTime'])\ntest=pd.read_csv('/kaggle/input/test.csv',parse_dates=['DateTime'])\nsub=pd.read_csv('/kaggle/input/sub.csv')"]}, {"cell_type": "code", "execution_count": 1, "id": "181d2641", "metadata": {}, "outputs": [], "source": ["# let's check first 5 rows from train \ntrain.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "b5c4c21a", "metadata": {}, "outputs": [], "source": ["# let's check last 5 rows from train \ntrain.tail()"]}, {"cell_type": "markdown", "id": "ea5a0763", "metadata": {}, "source": ["#### <font color='red'>Few observations by looking at training data</font>\n* We have 1 record for each hour and has details like vehicle count , junction number and unique id.\n* Train data is from 01Nov2015 to 30Jun2017\n"]}, {"cell_type": "code", "execution_count": 1, "id": "1458879f", "metadata": {}, "outputs": [], "source": ["# let's explore test data's first 5 and last 5 row\ntest.head().append(test.tail())"]}, {"cell_type": "markdown", "id": "ddde4014", "metadata": {}, "source": ["* Here, Vehicles which is the vehicle count at a particular hour is the target Feature which we need to predict for the timeperiod (2017-07-01 - 2017-10-01) using Train data from(2015-11-01 - 2017-06-01)"]}, {"cell_type": "markdown", "id": "05d77b0f", "metadata": {}, "source": ["#### <font color='red'>Vehicle Trends wrt Time Period. </font>"]}, {"cell_type": "code", "execution_count": 1, "id": "1f0e3770", "metadata": {}, "outputs": [], "source": ["train.loc[:,['DateTime','Vehicles']].plot(x='DateTime',y='Vehicles',title='Vehicle Trend',figsize=(16,4))"]}, {"cell_type": "markdown", "id": "757aaa59", "metadata": {}, "source": ["#### <font color='red'>Few observations.</font>\n\n* By Analysing our data we have found out that 2015 data has very low vehicle trend compared to the timeperiod 2017 which we are going to predict and also 2015 has only data for the month- 11&12 which has different trend compared to the month (7,8,9&10) for which we need to predict.So we decided to ignore 2015 data .\n* [Tip].The winner of https://www.kaggle.com/c/favorita-grocery-sales-forecasting has only used very recent data in the models, electing to drop older observations based on validation dataset performance.  \n* Selecting the right timeperiod data is very important in Time Series forecasting.\n"]}, {"cell_type": "code", "execution_count": 1, "id": "afbe0e70", "metadata": {}, "outputs": [], "source": ["# filtering data greater than or equal to 01 Jan 2016\ntrain=train[train['DateTime']>='2016-01-01']"]}, {"cell_type": "markdown", "id": "a650ce2d", "metadata": {}, "source": ["#### <font color='red'>Concating train and test data for preprocessing</font>"]}, {"cell_type": "code", "execution_count": 1, "id": "53a157ad", "metadata": {}, "outputs": [], "source": ["# concat train, test data and mark where it is test , train \ntrain['train_or_test']='train'\ntest['train_or_test']='test'\ndf=pd.concat([train,test])"]}, {"cell_type": "markdown", "id": "f5e9abb3", "metadata": {}, "source": ["#### <font color='red'>Creating Time Based Feature.This helps regression models to understand the trend in the data.</red>"]}, {"cell_type": "code", "execution_count": 1, "id": "239bbed3", "metadata": {}, "outputs": [], "source": ["# Below function extracts date related features from datetime\ndef create_date_featues(df):\n\n    df['Year'] = pd.to_datetime(df['DateTime']).dt.year\n\n    df['Month'] = pd.to_datetime(df['DateTime']).dt.month\n\n    df['Day'] = pd.to_datetime(df['DateTime']).dt.day\n\n    df['Dayofweek'] = pd.to_datetime(df['DateTime']).dt.dayofweek\n\n    df['DayOfyear'] = pd.to_datetime(df['DateTime']).dt.dayofyear\n\n    df['Week'] = pd.to_datetime(df['DateTime']).dt.week\n\n    df['Quarter'] = pd.to_datetime(df['DateTime']).dt.quarter \n\n    df['Is_month_start'] = pd.to_datetime(df['DateTime']).dt.is_month_start\n\n    df['Is_month_end'] = pd.to_datetime(df['DateTime']).dt.is_month_end\n\n    df['Is_quarter_start'] = pd.to_datetime(df['DateTime']).dt.is_quarter_start\n\n    df['Is_quarter_end'] = pd.to_datetime(df['DateTime']).dt.is_quarter_end\n\n    df['Is_year_start'] = pd.to_datetime(df['DateTime']).dt.is_year_start\n\n    df['Is_year_end'] = pd.to_datetime(df['DateTime']).dt.is_year_end\n\n    df['Semester'] = np.where(df['Quarter'].isin([1,2]),1,2)\n\n    df['Is_weekend'] = np.where(df['Dayofweek'].isin([5,6]),1,0)\n\n    df['Is_weekday'] = np.where(df['Dayofweek'].isin([0,1,2,3,4]),1,0)\n    \n    df['Hour'] = pd.to_datetime(df['DateTime']).dt.hour\n    \n    return df"]}, {"cell_type": "code", "execution_count": 1, "id": "bcc08fd6", "metadata": {}, "outputs": [], "source": ["# extracting time related \ndf=create_date_featues(df)"]}, {"cell_type": "markdown", "id": "0d825477", "metadata": {}, "source": ["#### <font color='red'>one hot encoding Junction</font>"]}, {"cell_type": "code", "execution_count": 1, "id": "35352024", "metadata": {}, "outputs": [], "source": ["for col in ['Junction']:\n    df = pd.get_dummies(df, columns=[col])"]}, {"cell_type": "markdown", "id": "605619e3", "metadata": {}, "source": ["#### <font color='red'>Getting back train and test</font>"]}, {"cell_type": "code", "execution_count": 1, "id": "e627bb98", "metadata": {}, "outputs": [], "source": ["train=df.loc[df.train_or_test.isin(['train'])]\ntest=df.loc[df.train_or_test.isin(['test'])]\ntrain.drop(columns={'train_or_test'},axis=1,inplace=True)\ntest.drop(columns={'train_or_test'},axis=1,inplace=True)"]}, {"cell_type": "markdown", "id": "3dbce32e", "metadata": {}, "source": [" #### <font color='red'>Log transforming Vehicle to have normal distribution.</red>"]}, {"cell_type": "code", "execution_count": 1, "id": "03b583c6", "metadata": {}, "outputs": [], "source": ["train['Vehicles']=np.log1p(train['Vehicles'])"]}, {"cell_type": "markdown", "id": "9d0acba9", "metadata": {}, "source": ["#### <font color='red'>       Here comes the most important step in solving timeseries.</font>"]}, {"cell_type": "markdown", "id": "d9947e08", "metadata": {}, "source": ["* Timeseries problems requires **time based validation** instead of generaly used kfold validation in regression problem. Kfold splits the data randomly and checking the model accuracy by predicting on timeperiod 2016 by using 2017 data makes no sense. \n* Here we used time based validation for the time period (2017-01-01 to 2017-04-01) of 4 months, since the test set contains 4 months data to predict."]}, {"cell_type": "code", "execution_count": 1, "id": "7cdce1ca", "metadata": {}, "outputs": [], "source": ["train1=train[train['DateTime']<'2017-03-01']#Train period from 2016-01-01 to 2017-02-31\nval1=train[train['DateTime']>='2017-03-01'] #Month 3,4,5,6 as validtaion period"]}, {"cell_type": "markdown", "id": "8a3fe70e", "metadata": {}, "source": ["#### <font color='red'>Why drop date feature when we can make use out of it.</font>"]}, {"cell_type": "code", "execution_count": 1, "id": "e6805ecd", "metadata": {}, "outputs": [], "source": ["def datetounix(df):\n    # Initialising unixtime list\n    unixtime = []\n    \n    # Running a loop for converting Date to seconds\n    for date in df['DateTime']:\n        unixtime.append(time.mktime(date.timetuple()))\n    \n    # Replacing Date with unixtime list\n    df['DateTime'] = unixtime\n    return(df)\ntrain1=datetounix(train1)\nval1=datetounix(val1)\n\ntrain=datetounix(train)\ntest=datetounix(test)"]}, {"cell_type": "code", "execution_count": 1, "id": "97b2ddcb", "metadata": {}, "outputs": [], "source": ["x_train1=train1.drop(columns={'ID','Vehicles'},axis=1)\ny_train1=train1.loc[:,['Vehicles']]\n\nx_val1=val1.drop(columns={'ID','Vehicles'},axis=1)\ny_val1=val1.loc[:,['Vehicles']]"]}, {"cell_type": "markdown", "id": "0f6d462c", "metadata": {}, "source": ["#### <font color='red'>Validating the performance.</font>"]}, {"cell_type": "code", "execution_count": 1, "id": "f0abdc5d", "metadata": {}, "outputs": [], "source": ["ts = time.time()\n\nmodel = XGBRegressor(\n    max_depth=8,\n    booster = \"gbtree\",\n    n_estimators=100000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,\n    seed=42,\n    objective='reg:linear')\n\nmodel.fit(\n    x_train1, \n    y_train1, \n    eval_metric=\"rmse\", \n    eval_set=[(x_train1, y_train1), (x_val1, y_val1)], \n    verbose=True, \n    early_stopping_rounds = 100)\n\ntime.time() - ts"]}, {"cell_type": "code", "execution_count": 1, "id": "d0aa8c36", "metadata": {}, "outputs": [], "source": ["#predicting validation data.\npred=model.predict(x_val1)"]}, {"cell_type": "code", "execution_count": 1, "id": "45ad770a", "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import mean_squared_error\nfrom math import sqrt\nnp.sqrt(mean_squared_error(np.expm1(y_val1), np.expm1(pred)))"]}, {"cell_type": "markdown", "id": "9ee8a134", "metadata": {}, "source": ["#### <font color='red'>Feature Importance</font>"]}, {"cell_type": "code", "execution_count": 1, "id": "0377e843", "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n%matplotlib inline\nplot_features(model, (10,14))"]}, {"cell_type": "markdown", "id": "8261b280", "metadata": {}, "source": [" <font color='red'>Here comes the secret sauce which pushed us from Rank2 to Rank 1.</font>\n* Reference:https://www.kaggle.com/xwxw2929/rossmann-sales-top1\n* This technique is also used by the Winner of Rossmann store sales prediction.\n* Here you can see the documentation from the winner of Rossman sales prediction:https://www.kaggle.com/c/rossmann-store-sales/discussion/18024       \n* This approach calculates the error in the predicted value and chooses the best  weight to mutiply with the prediction ."]}, {"cell_type": "code", "execution_count": 1, "id": "6be00b97", "metadata": {}, "outputs": [], "source": ["#checks error in prediction\nres = pd.DataFrame(data = pd.concat([x_val1,y_val1],axis=1))\nres['Prediction']= np.expm1(model.predict(x_val1))\nres['Ratio'] = res.Prediction/np.expm1(res.Vehicles)\nres['Error'] =abs(res.Ratio-1)\nres['Weight'] = np.expm1(res.Vehicles)/res.Prediction\nres.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "7baa2fa7", "metadata": {}, "outputs": [], "source": ["#calculates best weight\npred1  = model.predict(x_val1)\nprint(\"weight correction\")\nW=[(0.990+(i/1000)) for i in range(20)]\nS =[]\nfor w in W:\n    error = sqrt(mean_squared_error(np.expm1(y_val1), np.expm1(pred1*w)))\n    print('RMSE for {:.3f}:{:.6f}'.format(w,error))\n    S.append(error)\nScore = pd.Series(S,index=W)\nScore.plot()\nBS = Score[Score.values == Score.values.min()]\nprint ('Best weight for Score:{}'.format(BS))"]}, {"cell_type": "code", "execution_count": 1, "id": "6c6a4b37", "metadata": {}, "outputs": [], "source": ["pred=model.predict(x_val1)*1.009\nnp.sqrt(mean_squared_error(np.expm1(y_val1), np.expm1(pred)))"]}, {"cell_type": "markdown", "id": "359bace1", "metadata": {}, "source": ["* Validation accuracy RMSE dropped from **8.015 to 7.46**  by multiplying with error weight . This helped us to top the leaderboard.\n\n* We have validated this particular weight by creating another validation for period(2016-7,8,9,10). It worked well there and also LB score increased.\n* Don't forget to upvote if you find this useful."]}, {"cell_type": "markdown", "id": "18f287d3", "metadata": {}, "source": ["####  <font color='red'>Model using all train data (except 2015)</font>"]}, {"cell_type": "code", "execution_count": 1, "id": "3c851b35", "metadata": {}, "outputs": [], "source": ["x=train.drop(columns={'ID','Vehicles'},axis=1)\ny=train.loc[:,['Vehicles']]\ntest=test.drop(columns={'ID','Vehicles'},axis=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "c9d5c658", "metadata": {}, "outputs": [], "source": ["model = XGBRegressor(\n    max_depth=8,\n    n_estimators=220,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,\n    \n    seed=42)\n\nmodel.fit(x, y)\n"]}, {"cell_type": "code", "execution_count": 1, "id": "199fb069", "metadata": {}, "outputs": [], "source": ["pred=model.predict(test)*1.009\nsub['Vehicles']=np.expm1(pred)\nsub.to_csv('finalsub.csv',index=False)"]}, {"cell_type": "markdown", "id": "b60abdaa", "metadata": {}, "source": ["#### <font color='red'>Other usefull kaggle kernels on this topic .</font>"]}, {"cell_type": "markdown", "id": "6e8e1367", "metadata": {}, "source": ["* https://www.kaggle.com/dlarionov/feature-engineering-xgboost\n* https://www.kaggle.com/abhilashawasthi/feature-engineering-lgb-model\n* https://www.kaggle.com/xwxw2929/rossmann-sales-top1"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}