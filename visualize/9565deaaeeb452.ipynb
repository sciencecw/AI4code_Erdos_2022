{"cells": [{"cell_type": "code", "execution_count": 1, "id": "6d4422ae", "metadata": {}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]}, {"cell_type": "markdown", "id": "99069c0c", "metadata": {}, "source": ["### Hi! My name is Alex and this is my first notebook in order to show you my approach to this problem. As this is my first notebook I will try to state clearly my hypothesis so we can discuss in the comments. I hope you can enjoy it as much as I enjoyed while I was writting this! :D"]}, {"cell_type": "markdown", "id": "5bc9e5d1", "metadata": {}, "source": ["# Import data\n"]}, {"cell_type": "code", "execution_count": 1, "id": "15e46a3f", "metadata": {}, "outputs": [], "source": ["heart = pd.read_csv(\"/kaggle/input/heart-failure-prediction/heart.csv\")"]}, {"cell_type": "markdown", "id": "7cc4c939", "metadata": {}, "source": ["### See the first five rows of the dataset to see if the data has been loaded well"]}, {"cell_type": "code", "execution_count": 1, "id": "7472af1c", "metadata": {}, "outputs": [], "source": ["heart.head()"]}, {"cell_type": "markdown", "id": "190dfde2", "metadata": {}, "source": ["### At first sight we can conclude there are 5 categorical data. Which are Sex, ChestPainType, RestingECG, ExerciseAngina and ST_Slope\n### Let's see if the target variable is skewed. If so we have to use other techniques such as undersampling or oversampling."]}, {"cell_type": "code", "execution_count": 1, "id": "cebd1319", "metadata": {}, "outputs": [], "source": ["heart[\"HeartDisease\"].value_counts()"]}, {"cell_type": "markdown", "id": "280cf6df", "metadata": {}, "source": ["### Due to the fact the results are balanced (nearly 50% each one) we do not need to use those techniques :)\n\n### Let's see info of the columns in order to get some insights of the dataset such as **null entries** or which columns have **categorical data**"]}, {"cell_type": "code", "execution_count": 1, "id": "1deac689", "metadata": {}, "outputs": [], "source": ["heart.info()"]}, {"cell_type": "markdown", "id": "1b2887e7", "metadata": {}, "source": ["### At this point we can conclude there are no null entries on the dataset. This doesn't mean that there is not null data because sometimes people can use different values to represent missing data, and Pandas will not recognize that :\\ . But not everything is lost! We can use different methods to differentiate those points. Here I would like to start the EDA, and plot different graphs to have a wider view of the dataset\n\n## *Exploratory Data Analysis*\n\n### First let's import Seaborn to make the plots"]}, {"cell_type": "code", "execution_count": 1, "id": "66fce2de", "metadata": {}, "outputs": [], "source": ["import seaborn as sns\nsns.set(rc={'figure.figsize':(11.7,8.27)}) #Bigger images"]}, {"cell_type": "markdown", "id": "51de4e8b", "metadata": {}, "source": ["# Correlation matrix\n\n### With the correlation matrix we can see the relationship between the different variables. The number has a range between 1 and -1 and represents if there is a **linear** relationship between two variables. It is important to remember that shows only **linear** relationships becuase a correlation of zero doesn't mean there is no relationship, it just shows there is no **linear** relationship.\n\n### It is important to remember that this matrix is simetric so the info of the upper diagonal is the same as the one in the lower diagonal."]}, {"cell_type": "code", "execution_count": 1, "id": "69190603", "metadata": {}, "outputs": [], "source": ["sns.heatmap(heart.corr(),cmap=\"YlGnBu\")"]}, {"cell_type": "markdown", "id": "7671d792", "metadata": {}, "source": ["### In order to see the relation between the different variables I will use a pairplot."]}, {"cell_type": "code", "execution_count": 1, "id": "d46bda08", "metadata": {}, "outputs": [], "source": ["sns.pairplot(heart,hue=\"HeartDisease\")"]}, {"cell_type": "markdown", "id": "0b1920aa", "metadata": {}, "source": ["### With hue we can change the color regarding heart disease. With this plots we can see that oldpeak and restingBP has very long tails. This sometimes can be a symptom of data that has been loaded incorrectly or extreme values used to represent missing data.\n### Let's see the box plots to see more clearly the outliers."]}, {"cell_type": "markdown", "id": "cc62a599", "metadata": {}, "source": ["## Outliers"]}, {"cell_type": "markdown", "id": "16386cbc", "metadata": {}, "source": ["### Using boxplots it's easy and fast to see the outliers as it represents those points clearly on the plot."]}, {"cell_type": "code", "execution_count": 1, "id": "fbabb32e", "metadata": {}, "outputs": [], "source": ["sns.boxplot(data=heart)"]}, {"cell_type": "markdown", "id": "41e23c60", "metadata": {}, "source": ["### With a quick research we can see some values with RestingBP near zero, which is not possible. Also there are some values with\n### Cholesterol near zero which is not possible also. Let's analizy how many of them are and how we can [impute](https://en.wikipedia.org/wiki/Imputation_(statistics)) them.\n### Here I will replace the RestingBP equal zero with the median as this is more resistant to outliers that the mean\n### Let's search the values with RestingBP equal zero."]}, {"cell_type": "code", "execution_count": 1, "id": "808f8d2f", "metadata": {}, "outputs": [], "source": ["sns.boxplot(y=\"RestingBP\",data=heart)"]}, {"cell_type": "markdown", "id": "8eba75c9", "metadata": {}, "source": ["### Let's search tthe outlier"]}, {"cell_type": "code", "execution_count": 1, "id": "dc73e575", "metadata": {}, "outputs": [], "source": ["heart[heart[\"RestingBP\"]<50]"]}, {"cell_type": "markdown", "id": "ee24f856", "metadata": {}, "source": ["### Here we see the entry do not has RestingBP Cholesterol and FastingBS. I will drop it because it is one entry in one thousand. If \n### we had more data to represent we should use a method to imputate the values.\n\n### Now let's search the values with zero value of Cholesterol."]}, {"cell_type": "code", "execution_count": 1, "id": "a929861c", "metadata": {}, "outputs": [], "source": ["sns.boxplot(y=\"Cholesterol\",data=heart)"]}, {"cell_type": "code", "execution_count": 1, "id": "2c644409", "metadata": {}, "outputs": [], "source": ["heart[heart[\"Cholesterol\"]==0]"]}, {"cell_type": "markdown", "id": "ea83b8f5", "metadata": {}, "source": ["### Almost 10% of the dataset has this value missing. Before making assumptions I will see which is the distribution of HeartDisease within these points. Then I will justify a method to impute the values."]}, {"cell_type": "code", "execution_count": 1, "id": "7d25a416", "metadata": {}, "outputs": [], "source": ["heart[heart[\"Cholesterol\"]==0][\"HeartDisease\"].value_counts()"]}, {"cell_type": "markdown", "id": "9ead8a85", "metadata": {}, "source": ["### Here we can see that the 88% of the entries here has a heart disease so this is highly skewed. In order to impute skewed data I will use the median of the values of Cholesterol with heart disease. I will do this because nearly all the entries have heart disease and the median of this values is resistant to outliers."]}, {"cell_type": "markdown", "id": "33eae0eb", "metadata": {}, "source": ["### This line of code gives me the non zero values of cholesterol with heart disease."]}, {"cell_type": "code", "execution_count": 1, "id": "5727aa98", "metadata": {}, "outputs": [], "source": ["mask = (heart[\"Cholesterol\" ]!= 0) & (heart[\"HeartDisease\"] == 1)\nheart.loc[mask][\"Cholesterol\"].median()"]}, {"cell_type": "markdown", "id": "bd9b3c24", "metadata": {}, "source": ["### I will impute with this value."]}, {"cell_type": "code", "execution_count": 1, "id": "f2232f55", "metadata": {}, "outputs": [], "source": ["heart.loc[heart[\"Cholesterol\"]==0,\"Cholesterol\"] = heart.loc[mask][\"Cholesterol\"].median()\n"]}, {"cell_type": "markdown", "id": "da46eae3", "metadata": {}, "source": ["### Let's see how the distribution is modified and if the correlation between heart disease and cholesterol has changed"]}, {"cell_type": "code", "execution_count": 1, "id": "c71ed934", "metadata": {}, "outputs": [], "source": ["sns.pairplot(data=heart,hue=\"HeartDisease\")"]}, {"cell_type": "markdown", "id": "de0a3a60", "metadata": {}, "source": ["### Here we can see that the distribution of Cholesterol has changed and also the correlation. From having a negative correlation with heart disease now it is positive. This makes sense as this is one of the first estimators a doctor uses to suggest further analysis."]}, {"cell_type": "code", "execution_count": 1, "id": "4c7aeb07", "metadata": {}, "outputs": [], "source": ["heart.corr()[\"Cholesterol\"][\"HeartDisease\"]"]}, {"cell_type": "markdown", "id": "248b0c81", "metadata": {}, "source": ["### Before finishing the EDA I will transform the categorical data into dummy variables. This means that the categorical data such as Sex, that has Male of Female values will be transformed into Sex_M and Sex_F with 0 or 1 with the corresponding entry. This is useful in distance based algorithms because those kind of algorithms needs the data to be normalized. The problem with this is that we are increasing the dimension of the dataset. If we are dealing with heavy datasets after this we should use some PCA to reduce the dimensionality."]}, {"cell_type": "markdown", "id": "96dc9aeb", "metadata": {}, "source": ["### I will get the name of the columns with categorical type."]}, {"cell_type": "code", "execution_count": 1, "id": "b4c80af5", "metadata": {}, "outputs": [], "source": ["cat_cols = heart.select_dtypes(\"object\").columns.to_list()\ncat_cols"]}, {"cell_type": "code", "execution_count": 1, "id": "41bc942b", "metadata": {}, "outputs": [], "source": ["heart_dummy = pd.get_dummies(heart)"]}, {"cell_type": "markdown", "id": "c8196cb8", "metadata": {}, "source": ["### Let's see how we increased the dimensions of the dataset."]}, {"cell_type": "code", "execution_count": 1, "id": "aa23caf7", "metadata": {}, "outputs": [], "source": ["heart_dummy.head()\nlen(heart_dummy.columns),len(heart.columns)"]}, {"cell_type": "markdown", "id": "db7e35b4", "metadata": {}, "source": ["# Tree based algorithm\n### Let's use the models to compare the scores in our treated data!\n### First import the libraries\n#### Disclaimer: *Personally I don't like to import everything before starting the notebook because it seems too many information to somebody that is starting like me. I prefer to import as I am going to use in order to see clearly where I will use it.*"]}, {"cell_type": "code", "execution_count": 1, "id": "c5e0f660", "metadata": {}, "outputs": [], "source": ["from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split"]}, {"cell_type": "markdown", "id": "cbf84873", "metadata": {}, "source": ["### First let's separate the data into X and y."]}, {"cell_type": "code", "execution_count": 1, "id": "8d5b86b0", "metadata": {}, "outputs": [], "source": ["X = heart_dummy.drop(axis=1,labels=\"HeartDisease\")\ny = heart_dummy[\"HeartDisease\"]"]}, {"cell_type": "markdown", "id": "2449b405", "metadata": {}, "source": ["### Split the data into train and test."]}, {"cell_type": "code", "execution_count": 1, "id": "ef8df370", "metadata": {}, "outputs": [], "source": ["x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=10)"]}, {"cell_type": "markdown", "id": "e6c32624", "metadata": {}, "source": ["### I will use gridsearch to optimize hyperparameters and prevent overfitting over the training set."]}, {"cell_type": "code", "execution_count": 1, "id": "9e603c29", "metadata": {}, "outputs": [], "source": ["Stimator = DecisionTreeClassifier(criterion=\"entropy\",random_state = 101)\ngrid = {\"max_depth\" : [1,2,3,4,5,6,7,8,9]}"]}, {"cell_type": "code", "execution_count": 1, "id": "8645ee7d", "metadata": {}, "outputs": [], "source": ["gso = GridSearchCV(Stimator,grid,cv=5)"]}, {"cell_type": "markdown", "id": "2812a6a2", "metadata": {}, "source": ["### As we are dealing with tree based algorithms we do not need to normalize the data! So let's train."]}, {"cell_type": "code", "execution_count": 1, "id": "8187b441", "metadata": {}, "outputs": [], "source": ["gso.fit(x_train,y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "d658aebb", "metadata": {}, "outputs": [], "source": ["y_pred = gso.predict(x_test)"]}, {"cell_type": "markdown", "id": "78a375e0", "metadata": {}, "source": ["### Let's see the score of the model and the confusion matrix"]}, {"cell_type": "code", "execution_count": 1, "id": "f7fde571", "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import confusion_matrix,accuracy_score,plot_confusion_matrix"]}, {"cell_type": "code", "execution_count": 1, "id": "698f87bf", "metadata": {}, "outputs": [], "source": ["confusion_matrix(y_test,y_pred)"]}, {"cell_type": "code", "execution_count": 1, "id": "3358bc2d", "metadata": {}, "outputs": [], "source": ["plot_confusion_matrix(gso,x_test,y_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "1714e0aa", "metadata": {}, "outputs": [], "source": ["accuracy_score(y_test,y_pred)"]}, {"cell_type": "markdown", "id": "e44794a9", "metadata": {}, "source": ["### Let's see the depth of the tree."]}, {"cell_type": "code", "execution_count": 1, "id": "e0de0d31", "metadata": {}, "outputs": [], "source": ["gso.best_params_"]}, {"cell_type": "code", "execution_count": 1, "id": "04ec9674", "metadata": {}, "outputs": [], "source": ["Stimator_forest = RandomForestClassifier(criterion=\"entropy\")\n#To optimize information gain\ngrid_forest = {\"n_estimators\" : [1000,2000,3000,4000,5000]}\ngso_forest = GridSearchCV(Stimator_forest,grid_forest,cv = 5)"]}, {"cell_type": "code", "execution_count": 1, "id": "18d98a32", "metadata": {}, "outputs": [], "source": ["gso_forest.fit(x_train,y_train)\ny_predforest = gso_forest.predict(x_test)\naccuracy_score(y_test,y_predforest)"]}, {"cell_type": "code", "execution_count": 1, "id": "40eab597", "metadata": {}, "outputs": [], "source": ["gso_forest.best_params_"]}, {"cell_type": "markdown", "id": "08eaa0a1", "metadata": {}, "source": ["### Let's try with a XGBClassifier"]}, {"cell_type": "code", "execution_count": 1, "id": "14a818bd", "metadata": {}, "outputs": [], "source": ["Stimator_XGBC = XGBClassifier(use_label_encoder=False,n_estimators=2000)\nStimator_XGBC.fit(x_train,y_train)\ny_predXGBC = Stimator_XGBC.predict(x_test)\naccuracy_score(y_test,y_predXGBC)"]}, {"cell_type": "markdown", "id": "748c550e", "metadata": {}, "source": ["# Distance Based Algorithm\n### Let's import the libraries we are going to use"]}, {"cell_type": "code", "execution_count": 1, "id": "0d895b73", "metadata": {}, "outputs": [], "source": ["from sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n"]}, {"cell_type": "code", "execution_count": 1, "id": "62d84c7e", "metadata": {}, "outputs": [], "source": ["scaler = StandardScaler()\nscaled_x_train = scaler.fit_transform(x_train)\nscaled_x_test = scaler.transform(x_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "5fce12e2", "metadata": {}, "outputs": [], "source": ["Stimator_svc = SVC(kernel=\"rbf\")\ngrid_svc = {\"gamma\":[0.001,0.01,0.1,1,10],\"C\":[0.1,1,10,100,1000]}\ngso_svc = GridSearchCV(Stimator_svc,grid_svc,cv = 5)"]}, {"cell_type": "code", "execution_count": 1, "id": "9f677d11", "metadata": {}, "outputs": [], "source": ["gso_svc.fit(scaled_x_train,y_train)\ny_predsvc = gso_svc.predict(scaled_x_test)\naccuracy_score(y_test,y_predsvc)\n"]}, {"cell_type": "code", "execution_count": 1, "id": "be4276a5", "metadata": {}, "outputs": [], "source": ["Stimator_logreg = LogisticRegression()\ngrid_logreg = {\"C\" : [0.1,1,10,100,1000]}\ngso_logreg = GridSearchCV(Stimator_logreg,grid_logreg,cv=5)"]}, {"cell_type": "code", "execution_count": 1, "id": "bd439354", "metadata": {}, "outputs": [], "source": ["gso_logreg.fit(scaled_x_train,y_train)\ny_predlogreg = gso_logreg.predict(scaled_x_test)\naccuracy_score(y_test,y_predlogreg)"]}, {"cell_type": "markdown", "id": "a3899a86", "metadata": {}, "source": ["# Finished\n### To sum up, the model with best accuracy score was SVC. It can be optimized but at this point I will stop here.\n### Thanks for reading! Finally, I would like to now how would you had replaced the entries with 0 cholesterol value and. of course, if you liked the post!\n# Have a great day!"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}