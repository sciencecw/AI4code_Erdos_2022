{"cells": [{"cell_type": "code", "execution_count": 1, "id": "365ab983", "metadata": {}, "outputs": [], "source": ["import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]}, {"cell_type": "code", "execution_count": 1, "id": "fdcc187d", "metadata": {}, "outputs": [], "source": ["!python3 -m pip install --upgrade nni\n#!pip3 install autofeat\n# !pip3 install multimodal-transformers"]}, {"cell_type": "code", "execution_count": 1, "id": "ab8c2f97", "metadata": {}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nni\nfrom nni.algorithms.feature_engineering.gradient_selector import FeatureGradientSelector\nfrom sklearn.model_selection import train_test_split\n#from autofeat import AutoFeatRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder, OneHotEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import explained_variance_score, mean_squared_error\nsns.set_theme(style=\"darkgrid\")\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n"]}, {"cell_type": "code", "execution_count": 1, "id": "9dad086d", "metadata": {}, "outputs": [], "source": ["df = pd.read_csv('/kaggle/input/home-data-for-ml-course/train.csv', index_col='Id')\ndf_test = pd.read_csv('/kaggle/input/home-data-for-ml-course/test.csv', index_col='Id')\ndf.describe()"]}, {"cell_type": "code", "execution_count": 1, "id": "ca595a43", "metadata": {}, "outputs": [], "source": ["df.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "b1fe89e5", "metadata": {}, "outputs": [], "source": ["def get_cat_num_cols(df : pd.DataFrame, verbose: bool = True) -> tuple:\n    \"\"\"function to get categorical, numerical, and ordinal columns from dataframe\n    \n    Parameters\n    ----------\n        df : pd.DataFrame\n            input dataframe\n            \n        verbose : bool\n            to show individual stats.\n    Returns\n    -------\n        tuple : (cat_cols, num_cols)\n    \"\"\"\n    \n    cat_cols = [col for col in df.columns if df[col].dtype == \"O\"]\n    num_cols = [col for col in df.columns if col not in cat_cols]\n    if verbose:\n        print(f\"categorical columns: {len(cat_cols)}, numerical_cols: {len(num_cols)}\")\n    return cat_cols, num_cols"]}, {"cell_type": "code", "execution_count": 1, "id": "4b34abd7", "metadata": {}, "outputs": [], "source": ["## separate target column from features\ny = df.SalePrice\ndf.drop(['SalePrice'], axis=1, inplace=True)\ncat_cols, num_cols = get_cat_num_cols(df)"]}, {"cell_type": "markdown", "id": "ed8a2df5", "metadata": {}, "source": ["#### Handling missing values"]}, {"cell_type": "code", "execution_count": 1, "id": "6e1b3d67", "metadata": {}, "outputs": [], "source": ["## Drop columns with #(missing values) > 0.60 * #(total rows) and rows with all missing values\nmissing_cols_info = df.isnull().sum()\nbad_cols = list(filter(lambda col : missing_cols_info[col] > 0.50 * len(df), missing_cols_info.keys()))\nprint(bad_cols)\n\n## drop bad columns\ncleaned_df = df.drop(bad_cols, axis = 1)\ncat_cols, num_cols = get_cat_num_cols(cleaned_df)\n\ndf_test.drop(bad_cols, axis = 1, inplace = True)"]}, {"cell_type": "code", "execution_count": 1, "id": "df3b08c1", "metadata": {}, "outputs": [], "source": ["## remove rows with all missing values\ncleaned_df.dropna(how = \"all\", inplace = True)\ndf_test.dropna(how = \"all\", inplace = True)\nprint(len(cleaned_df), len(df_test))"]}, {"cell_type": "code", "execution_count": 1, "id": "2a35e11a", "metadata": {}, "outputs": [], "source": ["# function for comparing different approaches\ndef score_dataset(X_train : np.ndarray, X_valid : np.ndarray, y_train : np.ndarray, y_valid : np.ndarray):\n    \"\"\"function to test different feature engineering approaches.\n    \n    Parameters\n    ----------\n        X_train : np.ndarray\n            numpy array containing training features\n        X_valid : np.ndarray\n            numpy array containing validation features\n        y_train : np.ndarray\n            numpy array containing target training values\n        y_valid : np.ndarray\n            numpy array containing target validation values\n    \"\"\"\n    model = RandomForestRegressor(n_estimators=100, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    mse = mean_squared_error(y_valid, preds, squared = False)\n    r2 = model.score(X_valid, y_valid) \n    return mse, r2"]}, {"cell_type": "markdown", "id": "f8884abc", "metadata": {}, "source": ["### Select Best Numerical Features"]}, {"cell_type": "code", "execution_count": 1, "id": "9a6ba3e1", "metadata": {}, "outputs": [], "source": ["## seperate numerical cols\nnum_df = cleaned_df[num_cols]\nprint(len(num_df.columns))"]}, {"cell_type": "markdown", "id": "1c4f1c12", "metadata": {}, "source": ["#### Filling missing values in numerical columns with mean values"]}, {"cell_type": "code", "execution_count": 1, "id": "a90eb354", "metadata": {}, "outputs": [], "source": ["print(f\"# missing values before: {num_df.isnull().sum().values.sum()}\")\nnum_df[num_cols] = num_df[num_cols].fillna(num_df[num_cols].mean())\nprint(f\"# missing values after: {num_df.isnull().sum().values.sum()}\")"]}, {"cell_type": "code", "execution_count": 1, "id": "3f40da1e", "metadata": {}, "outputs": [], "source": ["## prepare training and validation data\nX_train, X_valid, y_train, y_valid = train_test_split(num_df, y, test_size = 0.2, random_state = 37)\nprint(X_train.shape, X_valid.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "5c096214", "metadata": {}, "outputs": [], "source": ["## Standardizing numerical features\nscaler = StandardScaler()\nscaler.fit(X_train)\n\nX_norm_train, X_norm_val = scaler.transform(X_train), scaler.transform(X_valid)\nprint(X_norm_train.shape, X_norm_val.shape)"]}, {"cell_type": "markdown", "id": "da94d72a", "metadata": {}, "source": ["##### Use NNI to select top 20 features"]}, {"cell_type": "code", "execution_count": 1, "id": "34493f1b", "metadata": {}, "outputs": [], "source": ["num_fgs = FeatureGradientSelector(n_epochs=1, n_features = 20) ## change n_features according to your choice\n# fit data\nnum_fgs.fit(X_norm_train, y_train)\n# get improtant features\n# will return the index with important feature here.\nfeature_idx = num_fgs.get_selected_features()\nprint(f\"Feature indices: {feature_idx}\")\n\nnum_X_final_train, num_X_final_val = pd.DataFrame(X_norm_train[:, feature_idx], columns = np.array(num_cols)[feature_idx]), pd.DataFrame(X_norm_val[:, feature_idx], columns = np.array(num_cols)[feature_idx])\nnum_X_final_train.index = X_train.index\nnum_X_final_val.index = X_valid.index\nprint(num_X_final_train.shape, num_X_final_val.shape)"]}, {"cell_type": "markdown", "id": "bc60f065", "metadata": {}, "source": ["## Compare 2 Models\n* One with all numerical features\n* Second with top-20 selected features"]}, {"cell_type": "code", "execution_count": 1, "id": "a6fdb92c", "metadata": {}, "outputs": [], "source": ["mse1, r21 = score_dataset(X_norm_train, X_norm_val, y_train, y_valid)\nmse2, r22 = score_dataset(num_X_final_train, num_X_final_val, y_train, y_valid)\n\nprint(f\"MSE \\nWith All Numerical Features: {mse1 :.2f} \\\n        With Top-20 Features: {mse2 :.2f}\")\n\nprint(f\"R2 \\nWith All Numerical Features: {r21 :.2f} \\\n        With Top-20 Features: {r22 :.2f}\")"]}, {"cell_type": "code", "execution_count": 1, "id": "677dced1", "metadata": {}, "outputs": [], "source": ["## apply changes to test data\ndf_test[num_cols] = df_test[num_cols].fillna(df_test[num_cols].mean())\ndf_test_num = df_test[num_cols]\nfinal_test_num = scaler.transform(df_test_num)\nfinal_test_num = pd.DataFrame(final_test_num[:, feature_idx], columns = np.array(num_cols)[feature_idx])\nfinal_test_num.index = df_test_num.index\nfinal_test_num.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "a3590976", "metadata": {}, "outputs": [], "source": ["final_test_num"]}, {"cell_type": "markdown", "id": "3f6eabfd", "metadata": {}, "source": ["### Selecting Categorical Features"]}, {"cell_type": "code", "execution_count": 1, "id": "3b540bca", "metadata": {}, "outputs": [], "source": ["cat_df = cleaned_df[cat_cols]\ncat_df.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "f7f60354", "metadata": {}, "outputs": [], "source": ["def cat_cols_eda(df : pd.DataFrame, cat_cols):\n    \"\"\"Function to perform analysis on categorical columns.\n    \n    Parameters\n    ----------\n        df : pd.DataFrame\n            input dataframe\n    \"\"\"\n    \n    for col in cat_cols:\n        print(f\"{col}: Unique Values\")\n        print(df[col].value_counts())\n        print(\"\\n\")"]}, {"cell_type": "code", "execution_count": 1, "id": "e0255d56", "metadata": {}, "outputs": [], "source": ["cat_cols_eda(cat_df, cat_cols)"]}, {"cell_type": "markdown", "id": "6a9afbeb", "metadata": {}, "source": ["#### Filling missing values in categorical columns with most frequent"]}, {"cell_type": "code", "execution_count": 1, "id": "23b5c88d", "metadata": {}, "outputs": [], "source": ["## Categorical Columns\nfor col in cat_cols:\n    missing_values = cat_df[col].isnull().sum()\n    if missing_values > 0:\n        print(f\"{col} : {missing_values}\")"]}, {"cell_type": "code", "execution_count": 1, "id": "1c5075c5", "metadata": {}, "outputs": [], "source": ["## Categorical Columns\ncat_df = cat_df.fillna(cat_df.mode().iloc[0])\ncat_df.isnull().sum().values.sum()"]}, {"cell_type": "code", "execution_count": 1, "id": "d01daec7", "metadata": {}, "outputs": [], "source": ["X_train, X_valid, y_train, y_valid = train_test_split(cat_df, y, test_size = 0.2, random_state = 37)\nprint(X_train.shape, X_valid.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "4fb1ec2c", "metadata": {}, "outputs": [], "source": ["## find high cardinality categorical columns\nhigh_card_cols = [col for col in cat_cols if cat_df[col].nunique() > 10]\n\nlow_card_cols = list(set(cat_cols) - set(high_card_cols))\n\nprint(f\"High Cardinality Columns: {high_card_cols}\")\nprint(f\"Low Cardinality Columns: {low_card_cols}\")"]}, {"cell_type": "code", "execution_count": 1, "id": "59b4759d", "metadata": {}, "outputs": [], "source": ["encoded_X_train, encoded_X_valid = X_train.copy(), X_valid.copy()\nprint(encoded_X_train.shape, encoded_X_valid.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "63943030", "metadata": {}, "outputs": [], "source": ["## Apply Ordinal Encoder to high cardinality columns\nordinal_encoder = OrdinalEncoder(handle_unknown = 'use_encoded_value', unknown_value = -1)\nencoded_X_train[high_card_cols] = ordinal_encoder.fit_transform(X_train[high_card_cols])\nencoded_X_valid[high_card_cols] = ordinal_encoder.transform(X_valid[high_card_cols])\nprint(encoded_X_train.shape, encoded_X_valid.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "9f795457", "metadata": {}, "outputs": [], "source": ["encoded_X_train.drop(low_card_cols, axis = 1, inplace = True)\nencoded_X_valid.drop(low_card_cols, axis = 1, inplace = True)"]}, {"cell_type": "code", "execution_count": 1, "id": "07aedebf", "metadata": {}, "outputs": [], "source": ["## Apply One Hot Encoding to low cardinality columns\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_card_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_card_cols]))\n\nOH_cols_train.index = X_train.index\nOH_cols_valid.index = X_valid.index\nprint(OH_cols_train.shape, OH_cols_valid.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "19d4e796", "metadata": {}, "outputs": [], "source": ["## concat One Hot and Ordinal Encoded Dataframes\nfinal_cat_X_train = pd.concat([encoded_X_train, OH_cols_train], axis = 1)\nfinal_cat_X_valid = pd.concat([encoded_X_valid, OH_cols_valid], axis = 1)\nprint(final_cat_X_train.shape, final_cat_X_valid.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "ab703fae", "metadata": {}, "outputs": [], "source": ["## apply transformations to test data\ndf_test = df_test.fillna(df_test.mode().iloc[0])\nencoded_df_test = df_test[cat_cols].copy()\nencoded_df_test[high_card_cols] = ordinal_encoder.transform(df_test[high_card_cols])\nencoded_df_test.drop(low_card_cols, inplace = True, axis = 1)\nOH_df_test_cat = pd.DataFrame(OH_encoder.transform(df_test[low_card_cols]))\nOH_df_test_cat.index = df_test.index\n\nfinal_cat_test = pd.concat([encoded_df_test, OH_df_test_cat], axis = 1)\nprint(final_cat_test.shape)"]}, {"cell_type": "markdown", "id": "6e99966c", "metadata": {}, "source": ["### Combine final numeric and categorical datasets"]}, {"cell_type": "code", "execution_count": 1, "id": "1736f7b3", "metadata": {}, "outputs": [], "source": ["## combine training data\nfinal_X_train = pd.concat([num_X_final_train, final_cat_X_train], axis = 1)\n\n## combine validation data\nfinal_X_val = pd.concat([num_X_final_val, final_cat_X_valid], axis = 1)\n\n## combine test data\nfinal_test_data = pd.concat([final_test_num, final_cat_test], axis = 1)\n\nprint(final_X_train.shape, final_X_val.shape, final_test_data.shape)"]}, {"cell_type": "markdown", "id": "46247e48", "metadata": {}, "source": ["## Train the final model"]}, {"cell_type": "code", "execution_count": 1, "id": "5f7075af", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import cross_val_score\nmodel = RandomForestRegressor(n_estimators=500, random_state=0)\nmodel.fit(final_X_train, y_train)\npreds = model.predict(final_X_val)\nmse = mean_squared_error(y_valid, preds, squared = False)\nr2 = model.score(final_X_val, y_valid)\n\nmse, r2"]}, {"cell_type": "markdown", "id": "3cfad568", "metadata": {}, "source": ["### getting predictions for test data"]}, {"cell_type": "code", "execution_count": 1, "id": "2089f7b0", "metadata": {}, "outputs": [], "source": ["test_preds = model.predict(final_test_data)\n# Save test predictions to file\noutput = pd.DataFrame({'Id': df_test.index,\n                       'SalePrice': test_preds})\noutput.to_csv('submission.csv', index=False)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}