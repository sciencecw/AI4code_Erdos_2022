{"cells": [{"cell_type": "code", "execution_count": 1, "id": "309e9dda", "metadata": {}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n#import numpy as np # linear algebra\n#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output."]}, {"cell_type": "markdown", "id": "bb296063", "metadata": {}, "source": ["Load necessary packages/libraries"]}, {"cell_type": "code", "execution_count": 1, "id": "35300bd3", "metadata": {}, "outputs": [], "source": ["import json, sys, random\nimport numpy as np\nimport pandas as pd\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Activation\nfrom keras.layers import Dropout\nfrom keras import regularizers\nfrom keras import optimizers\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom keras.utils import to_categorical"]}, {"cell_type": "markdown", "id": "ed9c41a7", "metadata": {}, "source": ["Download and study the dataset"]}, {"cell_type": "code", "execution_count": 1, "id": "23960b7d", "metadata": {}, "outputs": [], "source": ["# download dataset from json object\nf = open(r'../input/ships-in-satellite-imagery/shipsnet.json')\ndataset = json.load(f)\nf.close()\n\ninput_data = np.array(dataset['data']).astype('uint8')\nlabels_data = np.array(dataset['labels']).astype('uint8')"]}, {"cell_type": "markdown", "id": "6be14514", "metadata": {}, "source": ["The dataset contains 4000 images. \nEach image is represented as a vector of length 19200 elements, containing 3 layers (R,G & B) and 80x80 of weight and height:"]}, {"cell_type": "code", "execution_count": 1, "id": "044094da", "metadata": {}, "outputs": [], "source": ["n_spectrum = 3 # color chanel (RGB)\nweight = 80\nheight = 80\nX = input_data.reshape([-1, n_spectrum, weight, height])\nX.shape"]}, {"cell_type": "markdown", "id": "49e4892d", "metadata": {}, "source": ["Changing the RGB layers to the last dimension:"]}, {"cell_type": "code", "execution_count": 1, "id": "0b85f9ba", "metadata": {}, "outputs": [], "source": ["Xt=X.transpose(0,2,3,1)\nXt.shape"]}, {"cell_type": "markdown", "id": "158e9537", "metadata": {}, "source": ["Displaying each layer RGB for the 100th image:"]}, {"cell_type": "code", "execution_count": 1, "id": "0280fac0", "metadata": {}, "outputs": [], "source": ["plt.figure( figsize = (15,15))\n\n# show each channel\nfor i in range(0,3):\n    plt.subplot(1, 3, (i+1))\n    plt.imshow(Xt[100,:,:,i])"]}, {"cell_type": "markdown", "id": "36835a74", "metadata": {}, "source": ["Dsiplaying the RGB image for the first 20 images:"]}, {"cell_type": "code", "execution_count": 1, "id": "0b83a198", "metadata": {}, "outputs": [], "source": ["plt.figure( figsize = (20,20))\n\n# show each channel\nfor i in range(0,20):\n    plt.subplot(5, 4, (i+1))\n    plt.imshow(Xt[i,:,:,:])\n\n"]}, {"cell_type": "markdown", "id": "da786340", "metadata": {}, "source": ["The dataset contains of 1000 images without ships (labelled with 0) and 3000 images with ships (labelled with 1): "]}, {"cell_type": "code", "execution_count": 1, "id": "d831d281", "metadata": {}, "outputs": [], "source": ["Presence_Absence=pd.value_counts(pd.Series(labels_data))\nPresence_Absence"]}, {"cell_type": "markdown", "id": "9cd32ffd", "metadata": {}, "source": ["Splitting the dataset into Train, Validation and Test:"]}, {"cell_type": "code", "execution_count": 1, "id": "d773bf46", "metadata": {}, "outputs": [], "source": ["# Create train set\ndata_train, data_test, labels_train, labels_test = train_test_split(\n    input_data,\n    labels_data,\n    test_size=.45, random_state=0, stratify=labels_data)\n\n# Create validation and test sets\ndata_validation, data_test, labels_validation, labels_test = train_test_split(\ndata_test, labels_test,test_size=.20, random_state=0)\n\ndata_train=data_train.reshape(-1, n_spectrum, weight, height)\ndata_test=data_test.reshape(-1, n_spectrum, weight, height)\ndata_validation=data_validation.reshape(-1, n_spectrum, weight, height)\n\nprint('Train:',data_train.shape, labels_train.shape)\nprint('Test:', data_test.shape, labels_test.shape)\nprint('Validation:', data_validation.shape, labels_validation.shape)"]}, {"cell_type": "markdown", "id": "dd46ac83", "metadata": {}, "source": ["Permute the dimensions of each array:"]}, {"cell_type": "code", "execution_count": 1, "id": "3915c26c", "metadata": {}, "outputs": [], "source": ["data_t_train=data_train.transpose(0,2,3,1)\ndata_t_test=data_test.transpose(0,2,3,1)\ndata_t_validation=data_validation.transpose(0,2,3,1)"]}, {"cell_type": "markdown", "id": "9d6b45b3", "metadata": {}, "source": ["Choosing the Number of Components in a PCA for the dataset:"]}, {"cell_type": "code", "execution_count": 1, "id": "feddfbc3", "metadata": {}, "outputs": [], "source": ["pca = PCA(n_components=100)\nvalues_train=data_train.reshape(-1,(80*80*3))\npca.fit(values_train)\n\n\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative explained variance')\nplt.title('PCA')\n\n"]}, {"cell_type": "markdown", "id": "a29c5b85", "metadata": {}, "source": ["80 components would be enought to retain most of the dataset information:"]}, {"cell_type": "code", "execution_count": 1, "id": "b6274636", "metadata": {}, "outputs": [], "source": ["NCOMPONENTS = 80\n\npca = PCA(n_components=NCOMPONENTS)\n\n\ndata_pca_train = pca.fit_transform(data_train.reshape(-1,(80*80*3)))\ndata_pca_val = pca.transform(data_validation.reshape(-1,(80*80*3)))\ndata_pca_test = pca.transform(data_test.reshape(-1,(80*80*3)))\npca_std = np.std(data_pca_train)\n"]}, {"cell_type": "markdown", "id": "d501218d", "metadata": {}, "source": ["Convert the labels vectors from integers to binary class matrices:"]}, {"cell_type": "code", "execution_count": 1, "id": "5da5114b", "metadata": {}, "outputs": [], "source": ["labels_train=to_categorical(labels_train)\nlabels_test=to_categorical(labels_test)\nlabels_validation=to_categorical(labels_validation)"]}, {"cell_type": "code", "execution_count": 1, "id": "1cbad938", "metadata": {}, "outputs": [], "source": ["print(data_pca_val.shape)\nprint(labels_validation.shape)"]}, {"cell_type": "markdown", "id": "ee72f2d7", "metadata": {}, "source": ["FCNN model calibration:"]}, {"cell_type": "code", "execution_count": 1, "id": "b1ba0812", "metadata": {}, "outputs": [], "source": ["model_fcnn = Sequential([\n  Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.0001),input_shape=(80,)),\n  Dropout(0.1),\n  Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.0001)),\n  Dropout(0.1),\n  Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.0001)),\n  Dropout(0.1),\n  Dense(2, activation='softmax', kernel_regularizer=regularizers.l2(0.0001)),\n])\n\nmodel_fcnn.summary()\n\n\nmodel_fcnn.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.RMSprop(),\n              metrics=['accuracy'])\n\nmodel_fcnn.fit(\n  data_pca_train, # training data\n  labels_train, # training targets\n  validation_data=(data_pca_val, labels_validation),\n  epochs=100,\n  batch_size=200\n)"]}, {"cell_type": "code", "execution_count": 1, "id": "491a8f41", "metadata": {}, "outputs": [], "source": ["acc=model_fcnn.history.history['accuracy']\nval_acc=model_fcnn.history.history['val_accuracy']\nloss=model_fcnn.history.history['loss']\nval_loss=model_fcnn.history.history['val_loss']\n\nepochs=range(1,len(acc)+1)\n\nplt.plot(epochs,acc,'b',label='Trainning accuracy')\nplt.plot(epochs,val_acc,'r',label='Validation accuracy')\nplt.title('Trainning & Validation Accuracy - 25 epochs')\nplt.legend()\nplt.figure()\nplt.plot(epochs,loss, 'b', label='Training loss')\nplt.plot(epochs,val_loss, 'r', label='Validation loss')\nplt.title('Trainning & Validation Loss - 50 epochs')\nplt.legend()\nplt.show()\n\n\ntest_loss, test_acc =model_fcnn.evaluate(\n  data_pca_test,\n  labels_test\n)\n\nprint('Test loss', round(test_loss,2))\nprint('Test accuracy', round(test_acc,2))"]}, {"cell_type": "markdown", "id": "2130c0e7", "metadata": {}, "source": ["Label prediction for the test dataset:"]}, {"cell_type": "code", "execution_count": 1, "id": "22f6c063", "metadata": {}, "outputs": [], "source": ["prediction=model_fcnn.predict_classes(data_pca_test)"]}, {"cell_type": "markdown", "id": "200629bc", "metadata": {}, "source": ["Displaying the first 100 images from the test dataset overlaying the test labels as well as the predicted labels with the FCNN model:"]}, {"cell_type": "code", "execution_count": 1, "id": "2bfd6e03", "metadata": {}, "outputs": [], "source": ["fig=plt.figure(figsize=(20, 20))\n\nrandom.randint(0,len(data_test))\n\ndata_t_test=data_test.transpose(0,2,3,1)\n# show each channel\nfor i in range(0,100):\n    fig.add_subplot(10, 10, i+1)\n    fig.subplots_adjust(left=0.125, bottom=0.1, right=0.9, top=0.9, wspace=.2, hspace=.2)\n    plt.imshow(data_t_test[i,:,:,:])\n    plt.title('input:'+str(labels_test.argmax(axis = 1)[i])+'\\npred:'+str(prediction[i]), fontsize=18, y=.15,color='w')\n    plt.axis('off')\nplt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}