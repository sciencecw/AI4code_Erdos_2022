{"cells": [{"cell_type": "code", "execution_count": 1, "id": "d75f7b1d", "metadata": {}, "outputs": [], "source": ["!pip install seaborn --upgrade"]}, {"cell_type": "code", "execution_count": 1, "id": "ba8bf4c8", "metadata": {}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Added these..\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nprint(\"Setup Complete\")\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]}, {"cell_type": "markdown", "id": "a3d56c8d", "metadata": {}, "source": ["## 1. Read in data"]}, {"cell_type": "code", "execution_count": 1, "id": "91627d00", "metadata": {}, "outputs": [], "source": ["# Read in the data\ntrain = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest = pd.read_csv('/kaggle/input/titanic/test.csv')\n\n# Concatenate DataFrames to process features easily...\ndata = pd.concat(objs =[train,test], axis=0).reset_index(drop=True)\n\nprint(f'Exemplars: {data.shape[0]}, features: {data.shape[1]}')\ndata.head()"]}, {"cell_type": "markdown", "id": "8af904a4", "metadata": {}, "source": ["## 2. Handle missing data "]}, {"cell_type": "code", "execution_count": 1, "id": "0be02d68", "metadata": {}, "outputs": [], "source": ["# Handle missing data...\ndata.isna().sum()"]}, {"cell_type": "markdown", "id": "657bb7d5", "metadata": {}, "source": ["* Note survived missing values corresponds to the test set."]}, {"cell_type": "markdown", "id": "6be2c325", "metadata": {}, "source": ["### 2.1 Cabin data"]}, {"cell_type": "code", "execution_count": 1, "id": "7e98521c", "metadata": {}, "outputs": [], "source": ["# I would like to transform the cabin data to contain the letter component as this likely\n# refers to the area of the ship passengers stayed in which is likely to relate to class...\n\n# Check all data we have is of this format...\nfor cabin in np.unique(data['Cabin'][data['Cabin'].notnull()]):\n    if cabin[0].isalpha() != True:\n        print('Not all cabins begin with letter.')\n        break\n\n# String away letters from known cabins...\ndata[\"Cabin\"] = data['Cabin'].apply(lambda x: x if pd.isnull(x) else x[0])\n\n# Visulise letter frequency for each passenger class...\nfreq_dict = {}\nfig = plt.figure(figsize = (25,10))\nfor c in np.unique(data['Pclass']):\n    ax = fig.add_subplot(1, 3, c)\n    sns.countplot(x='Cabin', data=data[data['Pclass']==c], ax=ax, \\\n                  order = np.unique(data.loc[(data.Pclass == c),'Cabin'].dropna()));\n    plt.xticks(fontsize=25)\n    plt.yticks(fontsize=25)\n    ax.set_xlabel('Cabin', fontsize=25)\n    ax.set_ylabel('Count', fontsize=25)\n    ax.set_title(f'Class {c}', fontsize=30)\n    freqs = []\n    [freqs.append(patch.get_height()) for patch in ax.patches]\n    freqs = np.array(freqs)    \n    freqs = freqs / freqs.sum()\n    freq_dict[c] = freqs\n    "]}, {"cell_type": "code", "execution_count": 1, "id": "bc14e46b", "metadata": {}, "outputs": [], "source": ["# Fill in missing values with cabins from drawn from distribution of cabins by class...\nnp.random.seed(1)\nfor c in np.unique(data['Pclass'].dropna()):\n    choices = np.unique(data.loc[(data.Pclass == c),'Cabin'].dropna())\n    data.loc[(data.Pclass == c),'Cabin'] = data['Cabin'].apply(lambda \\\n                        x: np.random.choice(choices, p=freq_dict[c]) if pd.isnull(x) else x)\nsns.countplot(x='Cabin', data=data).set_title('Cabin distribution with updated values');\n"]}, {"cell_type": "markdown", "id": "14209da7", "metadata": {}, "source": ["### 2.2 Age missing data"]}, {"cell_type": "code", "execution_count": 1, "id": "8d465783", "metadata": {}, "outputs": [], "source": ["# Visualise age distributions by class...\nfig = plt.figure(figsize = (25,10))\nfor c in np.unique(data['Pclass']):\n    ax = fig.add_subplot(1, 3, c)\n    sns.histplot(x='Age', data=data[data['Pclass']==c], ax=ax);\n    plt.xticks(fontsize=25)\n    plt.yticks(fontsize=25)\n    ax.set_xlabel('Age', fontsize=25)\n    ax.set_ylabel('Count', fontsize=25)\n    ax.set_title(f'Class {c}', fontsize=30)"]}, {"cell_type": "code", "execution_count": 1, "id": "4277b62a", "metadata": {}, "outputs": [], "source": ["# Fill missing age values with median age according to class...\nnp.random.seed(1)\nfor c in np.unique(data['Pclass'].dropna()):\n    data.loc[(data.Pclass == c),'Age'] = data['Age'].apply(lambda \\\n                        x: data.loc[(data.Pclass == c),'Age'].median() if pd.isnull(x) else x)"]}, {"cell_type": "markdown", "id": "c838e0dd", "metadata": {}, "source": ["### 2.3 Fare missing data"]}, {"cell_type": "code", "execution_count": 1, "id": "b7bf35ce", "metadata": {}, "outputs": [], "source": ["# Visualise fare distributions by class...\nfig = plt.figure(figsize = (25,10))\nfor c in np.unique(data['Pclass']):\n    ax = fig.add_subplot(1, 3, c)\n    sns.histplot(x='Fare', data=data[data['Pclass']==c], ax=ax);\n    plt.xticks(fontsize=25)\n    plt.yticks(fontsize=25)\n    ax.set_xlabel('Fare', fontsize=25)\n    ax.set_ylabel('Count', fontsize=25)\n    ax.set_title(f'Class {c}', fontsize=30)"]}, {"cell_type": "code", "execution_count": 1, "id": "1ff80ede", "metadata": {}, "outputs": [], "source": ["# Fill missing fare values with mean age according to class...\nnp.random.seed(1)\nfor c in np.unique(data['Pclass'].dropna()):\n    data.loc[(data.Pclass == c),'Fare'] = data['Fare'].apply(lambda \\\n                        x: data.loc[(data.Pclass == c),'Fare'].mean() if pd.isnull(x) else x)"]}, {"cell_type": "markdown", "id": "e30818c7", "metadata": {}, "source": ["### 2.4 Embarked missing data"]}, {"cell_type": "code", "execution_count": 1, "id": "f19c22ce", "metadata": {}, "outputs": [], "source": ["# Fill embarked missing data with mode...\ndata['Embarked'] = data['Embarked'].fillna(data['Embarked'].mode()[0])"]}, {"cell_type": "code", "execution_count": 1, "id": "b8ff4dc3", "metadata": {}, "outputs": [], "source": ["# Check missing data has been handled\ndata.isna().sum()"]}, {"cell_type": "markdown", "id": "59698589", "metadata": {}, "source": ["## 3. Convert categorical features"]}, {"cell_type": "markdown", "id": "160d8bb2", "metadata": {}, "source": ["### 3.1 Encoding ticket data"]}, {"cell_type": "code", "execution_count": 1, "id": "84e377d8", "metadata": {}, "outputs": [], "source": ["# Handle ticket data...\ndata['Ticket'].head()\n\n# I assume the letters preceeding the ticket numbers relate the booking company/method, so I extract\n# these to use as a feature..."]}, {"cell_type": "code", "execution_count": 1, "id": "4dfe0359", "metadata": {}, "outputs": [], "source": ["data[\"Ticket\"] = data['Ticket'].apply(lambda x: 'Z' \\\n                            if x.isdigit()  else x.replace('.', ' ').replace('/', ' ').strip().split()[0])"]}, {"cell_type": "code", "execution_count": 1, "id": "1b831329", "metadata": {}, "outputs": [], "source": ["ticket_dummies = pd.get_dummies(data[['Ticket']])\ndata = pd.concat(objs=[data, ticket_dummies], axis=1)\ndata = data.drop(['Ticket'], axis=1)"]}, {"cell_type": "markdown", "id": "19b42273", "metadata": {}, "source": ["### 3.2 Encoding name feature"]}, {"cell_type": "code", "execution_count": 1, "id": "1b7e0fe0", "metadata": {}, "outputs": [], "source": ["# Extract titles from names...\ndata['Title'] = data['Name'].apply(lambda x: x.split('.')[0].split(' ')[-1])\ndata['Title'].value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "6670e209", "metadata": {}, "outputs": [], "source": ["# Group upper class (UC) titles...\ndata[\"Title\"] = data[\"Title\"].replace(\n    ['Lady','Countess','Capt', 'Mme', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona', 'Mlle'], \n    'UC')\n\n# Group Ms and Mrs\ndata[\"Title\"] = data[\"Title\"].replace(\n    'Ms', \n    'Mrs'\n)\ndata['Title'].value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "92b0ddd0", "metadata": {}, "outputs": [], "source": ["# Visualise survival probability of each title\nsns.catplot(x=\"Title\", y=\"Survived\", data=data, kind=\"bar\").set_ylabels(\"Survival probability\");"]}, {"cell_type": "code", "execution_count": 1, "id": "1dcbaea3", "metadata": {}, "outputs": [], "source": ["title_dummies = pd.get_dummies(data[['Title']])\ndata = pd.concat(objs=[data, title_dummies], axis=1)\ndata = data.drop(['Title'], axis=1)\ndata = data.drop(['Name'], axis=1)"]}, {"cell_type": "markdown", "id": "d59ca779", "metadata": {}, "source": ["### 3.3 Encoding sex feature"]}, {"cell_type": "code", "execution_count": 1, "id": "9b4d9869", "metadata": {}, "outputs": [], "source": ["# Transform male/female to 0/1...\ndata[\"Sex\"] = data['Sex'].apply(lambda x: 0 if x == 'male' else 1)"]}, {"cell_type": "markdown", "id": "88c268be", "metadata": {}, "source": ["### 3.4 Encoding embarked feature"]}, {"cell_type": "code", "execution_count": 1, "id": "3b2e9200", "metadata": {}, "outputs": [], "source": ["embrkd_dummies = pd.get_dummies(data[['Embarked']])\ndata = pd.concat(objs=[data, embrkd_dummies], axis=1)\ndata = data.drop(['Embarked'], axis=1)"]}, {"cell_type": "markdown", "id": "f9759e28", "metadata": {}, "source": ["### 3.5 Combine and encode siblings/spouse (SibSp) and parents/childen (Parch) features"]}, {"cell_type": "code", "execution_count": 1, "id": "c71f79e0", "metadata": {}, "outputs": [], "source": ["# Create family size feature...\ndata[\"fam_size\"] = data[\"SibSp\"] + data[\"Parch\"]\n\ndata['Single'] = data['fam_size'].apply(lambda x: 1 if x == 0 else 0)\ndata['Sib/Sp'] = data['SibSp'].apply(lambda x: 1 if x >= 1  else 0)\ndata['Par/Ch'] = data['Parch'].apply(lambda x: 1 if x >= 1 else 0)\n\ndata = data.drop(['SibSp'], axis=1)\ndata = data.drop(['Parch'], axis=1)"]}, {"cell_type": "markdown", "id": "0a18d9b0", "metadata": {}, "source": ["### 3.6 Encode cabin feature"]}, {"cell_type": "code", "execution_count": 1, "id": "443c2e24", "metadata": {}, "outputs": [], "source": ["cabin_dummies = pd.get_dummies(data[['Cabin']])\ndata = pd.concat(objs=[data, cabin_dummies], axis=1)\ndata = data.drop(['Cabin'], axis=1)"]}, {"cell_type": "markdown", "id": "c4d31720", "metadata": {}, "source": ["### 3.7 Encode class feature"]}, {"cell_type": "code", "execution_count": 1, "id": "3726f3d0", "metadata": {}, "outputs": [], "source": ["cabin_dummies = pd.get_dummies(data[['Pclass']])\ndata = pd.concat(objs=[data, cabin_dummies], axis=1)\ndata = data.drop(['Pclass'], axis=1)"]}, {"cell_type": "markdown", "id": "d3124daf", "metadata": {}, "source": ["### 3.8 Transform fare to log space"]}, {"cell_type": "code", "execution_count": 1, "id": "d25e1e93", "metadata": {}, "outputs": [], "source": ["# Fare data is left skewed...\nsns.displot(data[\"Fare\"], color=\"m\", label=\"Skewness : %.2f\"%(data[\"Fare\"].skew()));\nplt.legend();"]}, {"cell_type": "code", "execution_count": 1, "id": "5de0e716", "metadata": {}, "outputs": [], "source": ["# Take logs (plus 1 to handle 0s) of fares...\ndata['Fare'] = np.log1p(data['Fare'])\nsns.displot(data[\"Fare\"], color=\"m\", label=\"Skewness : %.2f\"%(data[\"Fare\"].skew()));\nplt.legend();"]}, {"cell_type": "markdown", "id": "2426081c", "metadata": {}, "source": ["### 3.9 Standardise fare and age data "]}, {"cell_type": "code", "execution_count": 1, "id": "7fa82f91", "metadata": {}, "outputs": [], "source": ["data['Fare'] = (data['Fare'] - data['Fare'].mean()) / data['Fare'].std()\ndata['Age'] = (data['Age'] - data['Age'].mean()) / data['Age'].std()"]}, {"cell_type": "markdown", "id": "85ab52ef", "metadata": {}, "source": ["## 4. Evaluate model"]}, {"cell_type": "code", "execution_count": 1, "id": "84ed2b33", "metadata": {}, "outputs": [], "source": ["# Passenger ID isn't useful to remove...\n# Extract test indicies...\ntest_indicies = data['PassengerId'][train.shape[0]:]\ndata = data.drop(['PassengerId'], axis=1)\n\n\ntrain = data[:train.shape[0]]\ntest = data[train.shape[0]:]\n\n# Split training data in train and validate\nsplit = int(train.shape[0] * 0.85)\nx =  np.array(train.loc[:, train.columns != 'Survived'])\ny = np.array(train['Survived'])\nx_train = x[:split]\nx_val = x[split:]\ny_train = y[:split]\ny_val = y[split:]\nx_test =  np.array(test.loc[:, test.columns != 'Survived'])"]}, {"cell_type": "code", "execution_count": 1, "id": "9a3a07f2", "metadata": {}, "outputs": [], "source": ["from scipy import spatial, stats\n\n# Define k_means algorithm...\ndef k_means(\n        x_train,\n        y_train,\n        x_val,\n        y_val,\n        x_test,\n        k,\n        steps=10):\n    \"\"\" Fit and evaluate k-means clusters on data\"\"\"\n    \n    assert k <= x_train.shape[0], \"K must be less than the number of traning data.\"\n\n    # Force k to be an integer...\n    k = int(k)\n\n    # Choose k random clusters to begin...\n    np.random.seed(1)\n    index = np.random.choice(x_train.shape[0], k, replace=False)\n    centroids = x_train[index, :]\n\n    # Find indicies of data closest to each centroid...\n    P = np.argmin(spatial.distance.cdist(x_train, centroids, 'euclidean'), axis=1)\n\n    for _ in range(steps):\n        # Create new centroids at the means of each centroid...\n        centroids = np.vstack([np.mean(x_train[P == i, :], axis=0)\n                               for i in range(k) if len(x_train[P == i, :]) != 0])\n\n        # Find indicies of data closests to each (new) centroid...\n        temp_index = np.argmin(\n            spatial.distance.cdist(\n                x_train,\n                centroids,\n                'euclidean'),\n            axis=1)\n\n        # If no data changes centroid, stop...\n        if np.array_equal(P, temp_index):\n            break\n\n        # Repeat process with new indicies...\n        P = temp_index\n\n    # Find the predictions (weighted average) from each cluster...\n    P = np.argmin(spatial.distance.cdist(x_train, centroids, 'euclidean'), axis=1)\n    distances = np.min(spatial.distance.cdist(x_train, centroids, 'euclidean'), axis=1)\n    cluster_preds = np.zeros(k)\n    cluster_preds = np.stack([stats.mode(y_train[P == i])[0] if len(y_train[P == i]) > 0 else np.array([0]) for i in range(k)])\n\n    # Find indicies of data closest to each final centroid for test and train\n    # data...\n\n    index_final_train = np.argmin(\n        spatial.distance.cdist(\n            x_train,\n            centroids,\n            'euclidean'),\n        axis=1)\n    \n    index_final_val = np.argmin(\n        spatial.distance.cdist(\n            x_val,\n            centroids,\n            'euclidean'),\n        axis=1)\n    \n    index_final_test = np.argmin(\n        spatial.distance.cdist(\n            x_test,\n            centroids,\n            'euclidean'),\n        axis=1)\n    pred_train = np.zeros(x_train.shape[0])\n    pred_val = np.zeros(x_val.shape[0])\n    pred_test = np.zeros(x_test.shape[0]) \n    \n    for i in range(k):\n        pred_train[index_final_train == i] = cluster_preds[i]\n        pred_val[index_final_val == i] = cluster_preds[i]\n        pred_test[index_final_test == i] = cluster_preds[i]\n       \n    \n    # Calculate accuracy on train and validation data...\n    train_accuracy = sum([1 for i, p in enumerate(y_train) if pred_train[i] == p]) / y_train.shape[0]\n    val_accuracy = sum([1 for i, p in enumerate(y_val) if pred_val[i] == p]) / y_val.shape[0]\n    \n    results = {'Train accuracy': train_accuracy,\n               'Validation accuracy': val_accuracy,\n               'test_preds': pred_test}\n\n    return results"]}, {"cell_type": "code", "execution_count": 1, "id": "16a66f06", "metadata": {}, "outputs": [], "source": ["# Use brute force to optimise k...\nks = np.arange(1,x_train.shape[0])\nres = np.zeros((len(ks),2))\nbest_accuracy = 0\nopt_preds = None\nopt_k = None\nfor i, k in enumerate(ks):\n    results = k_means(x_train, y_train, x_val, y_val, x_test, k)\n    res[i,0] = results['Train accuracy']\n    res[i,1] = results['Validation accuracy']\n    if results['Validation accuracy'] > best_accuracy:\n        opt_preds = results['test_preds']\n        opt_k = k\n        best_accuracy = results['Validation accuracy']\nprint(f\"Best accuracy of {best_accuracy*100:.02f} of validation data using a K of {opt_k}\")        "]}, {"cell_type": "code", "execution_count": 1, "id": "89e9fac9", "metadata": {}, "outputs": [], "source": ["# Visualise accuracy of each k...\nplt.plot(ks, res[:,0], label = 'Train');\nplt.plot(ks, res[:,1], label = 'Validate');\nplt.xlabel('k')\nplt.ylabel('Accuracy')\nplt.legend();"]}, {"cell_type": "code", "execution_count": 1, "id": "d1083d44", "metadata": {}, "outputs": [], "source": ["# Format and write results...\nout = pd.DataFrame(test_indicies).reset_index(drop=True)\nout['Survived'] = opt_preds.astype(int)\nout.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "fdcb53a9", "metadata": {}, "outputs": [], "source": ["out.to_csv(\"Titanic survival submission (kmeans).csv\",index=False)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}