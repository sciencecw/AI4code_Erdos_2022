{"cells": [{"cell_type": "code", "execution_count": 1, "id": "739c9f9d", "metadata": {}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output."]}, {"cell_type": "code", "execution_count": 1, "id": "57176475", "metadata": {}, "outputs": [], "source": ["import spacy \nfrom spacy.lang.en import English\nfrom spacy import displacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\n\nimport en_core_web_sm\nimport en_core_web_lg\nimport string\n\nfrom numpy import *\nfrom datetime import datetime\n"]}, {"cell_type": "code", "execution_count": 1, "id": "520de55f", "metadata": {}, "outputs": [], "source": ["#nlp = en_core_web_sm.load()\nnlp = en_core_web_lg.load()\n#nlp = spacy.load(\"en\")"]}, {"cell_type": "code", "execution_count": 1, "id": "2bccef76", "metadata": {}, "outputs": [], "source": ["nlp.max_length = 3e9"]}, {"cell_type": "code", "execution_count": 1, "id": "4f3cd4a4", "metadata": {}, "outputs": [], "source": ["filename = \"../input/AAPL-8K-20031015162950.txt\"\ndocument = open(filename).read()\ndoc_test = nlp(document)"]}, {"cell_type": "markdown", "id": "366b0b50", "metadata": {}, "source": ["**Extract Date, time, StockCode, Category, Content**"]}, {"cell_type": "code", "execution_count": 1, "id": "2301ed1c", "metadata": {}, "outputs": [], "source": ["def extract_content(orig_file_name):\n    doc = open(orig_file_name).read()\n    components = doc.split('TIME:')\n    FileName =  components[0]\n    StockCode = FileName.split('/')[0]\n    StockCode = StockCode.split(':')[1]\n\n    other = components[1]\n    # DateTime after before 'EVENT:'\n    components = other.split('EVENTS:')\n    DateTime = components[0]\n    DateTime = DateTime.replace('\\n','')\n    #DateTime\n    Date = DateTime[0:8]\n    Date = Date[0:4]+'-'+Date[4:6]+'-'+Date[6::]\n    #Time = DateTime[8::]\n    #Time = Time[0:2]+'-'+Time[2:4]+'-'+Time[4::]\n    # Categories before \"TEXT:\", content after 'TEXT:'\n    components = components[1]\n    components = components.split('TEXT:')\n    categories = components[0]\n    categories = categories.split('\\t')[1:]\n    categories[-1] = categories[-1].replace('\\n','')\n    categories = [element.upper() for element in categories]\n    #if '' in categories:\n    #    categories = categories.remove('')\n    \n    content = components[1]\n    return StockCode, Date, categories, content\n"]}, {"cell_type": "code", "execution_count": 1, "id": "0babfa9b", "metadata": {}, "outputs": [], "source": ["#example\nStockCode, Date, categories, content = extract_content(filename)\n"]}, {"cell_type": "markdown", "id": "06d440ad", "metadata": {}, "source": ["**Preprocessing content**"]}, {"cell_type": "markdown", "id": "ae491516", "metadata": {}, "source": ["**Clean up text by converting uppercase to lowercase, removing punctuation and stopword**"]}, {"cell_type": "code", "execution_count": 1, "id": "5b72ab69", "metadata": {}, "outputs": [], "source": ["\npunctuations = string.punctuation\nstopwords = spacy.lang.en.STOP_WORDS\n\ndef clean_component(doc):\n    \"\"\" Clean up text. Tokenize, lowercase, and remove punctuation and stopwords \"\"\"\n    #print(\"Running cleaner\")\n    #Replace punctuation by spacing\n    doc= doc.replace(punctuations, ' ')\n    #remove multiple inline\n    doc = doc.replace('\\n',' ')\n    doc = doc.strip()\n    #Remove multiple space\n    #doc = doc.strip()\n    # Remove symbols (#) and stopwords\n    doc = nlp(doc)\n    doc = [tok.text for tok in doc if (tok.text not in stopwords and tok.pos_ != \"PUNCT\" and tok.pos_ != \"SYM\")]\n    # Make all tokens lowercase\n    doc = [tok.lower() for tok in doc]\n    doc = ' '.join(doc)\n    return nlp.make_doc(doc)\n\ndef pipe_clean(docs, **kwargs):\n    for doc in docs:\n        yield clean_component(doc)\n\n# Yes, adding attributes to functions works...It's just a bit dirty-looking. Arguably less confusing to\n# make it a class. Shrug.\nclean_component.pipe = pipe_clean"]}, {"cell_type": "markdown", "id": "f0f9bbba", "metadata": {}, "source": ["**Create Dataframe for report**"]}, {"cell_type": "code", "execution_count": 1, "id": "35100543", "metadata": {}, "outputs": [], "source": ["Date_form = 'Date'\n\n\ndef date_index(data):\n    data['datetime'] = pd.to_datetime(data[Date_form])\n    data = data.set_index('datetime')\n    data.drop([Date_form], axis=1, inplace=True)\n    return data"]}, {"cell_type": "code", "execution_count": 1, "id": "12b7430e", "metadata": {}, "outputs": [], "source": ["def create_data(orig_file_name):\n    #extract partial information from full report\n    StockCode, datetime, categories, content = extract_content(orig_file_name)\n    #if '' in categories:\n    #    categories = categories.remove('')\n    #clean content of report\n    content_cleaned = clean_component(content)\n    #vectorize content\n    content_vector = content_cleaned.vector\n    #create data frame\n    content_vector = pd.DataFrame(content_vector).transpose()\n    data= pd.DataFrame([1] * len(categories)).transpose()\n    data.columns = categories\n    data.insert(0, Date_form, datetime) \n    data.insert(1, \"StockCode\", StockCode)   \n    \n    data = pd.concat([data, content_vector], axis=1, sort=False)\n   \n    return data"]}, {"cell_type": "markdown", "id": "732a7194", "metadata": {}, "source": ["**Join data from individual reports of a stock**"]}, {"cell_type": "code", "execution_count": 1, "id": "09d3822b", "metadata": {}, "outputs": [], "source": ["list_file = os.listdir(\"../input\")\n\ndef concat_report_data(list_file):\n    Data = pd.DataFrame() \n    for file_name in list_file:\n        orig_file_name = '../input/'+ file_name\n        data = create_data(orig_file_name)\n        Data = pd.concat([Data, data], axis=0, ignore_index=True, sort=False)\n    #fill all NaN by 0, corresponding to the case that the categorie with NaN is not in the report\n    Data = Data.fillna(0)\n    Data[Date_form] = pd.to_datetime(Data[Date_form])\n    Data = date_index(Data)\n    return Data\n#example\nconcat_report_data(list_file)\n"]}, {"cell_type": "markdown", "id": "12332952", "metadata": {}, "source": ["**Export to csv**"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}