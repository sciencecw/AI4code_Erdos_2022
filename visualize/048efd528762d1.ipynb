{"cells": [{"cell_type": "code", "execution_count": 1, "id": "5c72ed6c", "metadata": {}, "outputs": [], "source": ["!wget -P /kaggle/working/data/ http://nlp.stanford.edu/data/glove.6B.zip\n!unzip /kaggle/working/data/glove.6B.zip -d /kaggle/working/data/\n!head ./data/glove.6B.50d.txt"]}, {"cell_type": "code", "execution_count": 1, "id": "e788eb40", "metadata": {}, "outputs": [], "source": ["import re\n\nimport tensorflow as tf\nimport keras\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm"]}, {"cell_type": "code", "execution_count": 1, "id": "0b4fb252", "metadata": {}, "outputs": [], "source": ["EPOCHS = 2\nBATCH_SIZE = 128\n\nMAX_LEN = 128\nNUM_WORDS = 10000\n\nEMBEDDING_DIM = 50\nH1 = 32\n\nTHRESH = 0.5"]}, {"cell_type": "markdown", "id": "79608bf0", "metadata": {}, "source": ["## Data\nThis section talks about grabbing and pre processing the dataset."]}, {"cell_type": "code", "execution_count": 1, "id": "6fe64d1e", "metadata": {}, "outputs": [], "source": ["train_data=pd.read_csv('/kaggle/input/fake-news/train.csv')\ntest_data=pd.read_csv('/kaggle/input/fake-news/test.csv')\ntrain_data.loc[train_data[\"text\"].isnull(), \"text\"] = \"\"\ntest_data.loc[test_data[\"text\"].isnull(), \"text\"] = \"\"\nprint(train_data.shape, test_data.shape)\ntrain_data.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "40c7cacb", "metadata": {}, "outputs": [], "source": ["train_data[\"text\"] = train_data[\"text\"].map(lambda x: re.sub(\"([^a-zA-Z0-9\\s])\", r' \\1 ', x))\ntrain_data[\"text\"] = train_data[\"text\"].map(lambda x: re.sub(\"\\s+\", r' ', x))\ntest_data[\"text\"] = test_data[\"text\"].map(lambda x: re.sub(\"([^a-zA-Z0-9\\s])\", r' \\1 ', x))\ntest_data[\"text\"] = test_data[\"text\"].map(lambda x: re.sub(\"\\s+\", r' ', x))\ntrain_data.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "05aafa1d", "metadata": {}, "outputs": [], "source": ["word_vec = pd.read_table(\"./data/glove.6B.50d.txt\", sep=r\"\\s\", header=None)\nword_vec.set_index(0, inplace=True)\nword_vec.head()"]}, {"cell_type": "markdown", "id": "e64b4757", "metadata": {}, "source": ["We are required to convert words to numbers. We do this by creating a mapping of word to integer. eg. `{\"the\": 1, \"I\": 2, \"am\": 3, ...}`. `tf.keras.preprocessing.text.Tokenizer` does this for us."]}, {"cell_type": "code", "execution_count": 1, "id": "455f0c81", "metadata": {}, "outputs": [], "source": ["tokenizer = tf.keras.preprocessing.text.Tokenizer(\n    filters=\"\",\n    num_words=10000,\n    lower=True,\n    document_count=5,\n)\ntokenizer.fit_on_texts(train_data[\"text\"].values)"]}, {"cell_type": "markdown", "id": "14eb8fc9", "metadata": {}, "source": ["Note that `fit_on_texts` only creates the mapping while `texts_to_sequences` below creates the actual tokens."]}, {"cell_type": "code", "execution_count": 1, "id": "259eab38", "metadata": {}, "outputs": [], "source": ["train_tokens = tokenizer.texts_to_sequences(train_data[\"text\"])\ntest_tokens = tokenizer.texts_to_sequences(test_data[\"text\"])"]}, {"cell_type": "markdown", "id": "3f847bd1", "metadata": {}, "source": ["We cannot send in different shaped sequences in when we are doing batch processing. Therefore the data needs to be padded with zeros so that all sequences are of length `MAX_LEN`."]}, {"cell_type": "code", "execution_count": 1, "id": "f00a979a", "metadata": {}, "outputs": [], "source": ["tokenizer.index_word[9999], min(tokenizer.index_word.keys())"]}, {"cell_type": "markdown", "id": "29fa769a", "metadata": {}, "source": ["Get the missing words in our glove vectors."]}, {"cell_type": "code", "execution_count": 1, "id": "5e8e0871", "metadata": {}, "outputs": [], "source": ["words_used = [tokenizer.index_word[i] for i in range(1, 10000)]\nmissing_words = set(words_used) - set(word_vec.index.values)\nprint(len(missing_words))\nmissing_word_index = [tokenizer.word_index[word] for word in missing_words]"]}, {"cell_type": "markdown", "id": "7c295b9b", "metadata": {}, "source": ["Delete any of the above 'missing words'"]}, {"cell_type": "code", "execution_count": 1, "id": "1323439a", "metadata": {}, "outputs": [], "source": ["train_tokens = [[word for word in sentence if word not in missing_word_index] for sentence in train_tokens]\ntest_tokens = [[word for word in sentence if word not in missing_word_index] for sentence in test_tokens]"]}, {"cell_type": "code", "execution_count": 1, "id": "3639fe1c", "metadata": {}, "outputs": [], "source": ["train_X = keras.preprocessing.sequence.pad_sequences(train_tokens, maxlen = MAX_LEN)\ntest_X = keras.preprocessing.sequence.pad_sequences(test_tokens, maxlen = MAX_LEN)\ntrain_Y = train_data[\"label\"].values"]}, {"cell_type": "code", "execution_count": 1, "id": "8ab6f007", "metadata": {}, "outputs": [], "source": ["embedding_weights = np.zeros((10000, 50))\nindex_n_word = [(i, tokenizer.index_word[i]) for i in range(1, len(embedding_weights)) if tokenizer.index_word[i] in word_vec.index]\nidx, word = zip(*index_n_word)\nembedding_weights[idx, :] = word_vec.loc[word,:].values"]}, {"cell_type": "markdown", "id": "241b2407", "metadata": {}, "source": ["## Model\nConsidering all the tokenized numbers above are effectively categories, we need to pass this through an embedding layer to get the embedding. In this case each word is represented by `EMBEDDING_DIM` numbers. This is then passed through an RNN layer before passing through a final feed forward layer to calculate the probability.\n\nIf you wish to understand LSTMs at a mathematical level this is an amazing blog post: https://colah.github.io/posts/2015-08-Understanding-LSTMs/"]}, {"cell_type": "code", "execution_count": 1, "id": "e4cbdecf", "metadata": {}, "outputs": [], "source": ["model = keras.Sequential()\nmodel.add(keras.layers.Embedding(tokenizer.num_words, \n                                 EMBEDDING_DIM, \n                                 weights=[embedding_weights],\n                                 trainable=False\n                                )) # , batch_size=batch_size\nmodel.add(keras.layers.LSTM(H1))\nmodel.add(keras.layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()"]}, {"cell_type": "code", "execution_count": 1, "id": "ca5c9489", "metadata": {}, "outputs": [], "source": ["model.fit(train_X, train_Y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.1)"]}, {"cell_type": "markdown", "id": "afef106f", "metadata": {}, "source": ["## Submission"]}, {"cell_type": "code", "execution_count": 1, "id": "b268c4e3", "metadata": {}, "outputs": [], "source": ["test_y = model.predict(test_X) > THRESH\ntest_data[\"label\"] = test_y.astype(np.int)\ntest_data[[\"id\", \"label\"]].to_csv(\"submission.csv\", index=False)"]}, {"cell_type": "markdown", "id": "f12cb7c1", "metadata": {}, "source": ["## Shameless Self Promotion\nSee here for [my course](https://www.udemy.com/course/machine-learning-and-data-science-2021/?referralCode=E79228C7436D74315787) on Machine Learning and Deep Learning (Use code DEEPSCHOOL-MARCH to 85% off)."]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}