{"cells": [{"cell_type": "markdown", "id": "8bc1255f", "metadata": {}, "source": ["# Freesound Audio Tagging 2019\n## Automatically recognize sounds and apply tags of varying natures"]}, {"cell_type": "markdown", "id": "1bf64059", "metadata": {}, "source": ["Hello everyone, this kernel was developed to show a new approach of signal processing in machine learning.\nThe key here is the Wavelet Transform (WT), more information in [link 1](https://en.wikipedia.org/wiki/Wavelet_packet_decomposition) and [link 2](https://file.scirp.org/pdf/IJCNS20100300011_40520775.pdf). This is a useful tool for the analysis and classification of time-series and signal. There are diferentes implementations of WT: Continuous Wavelet Transform, Discrete Wavelet Transform and Wavelet Packet Decomposition.\n\nIn this kernel, we will use Wavelet Packet Decomposition and Random Forest Classifier.  \n(Only train_curated dataset will be used)"]}, {"cell_type": "code", "execution_count": 1, "id": "f53d16ac", "metadata": {}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra \nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfrom tqdm import tqdm, tqdm_notebook\n\nimport matplotlib.pyplot as plt\n\n#Audio\nimport IPython.display as ipd  # To play sound in the notebook\nfrom scipy.io import wavfile\nimport gc\n\n# Parallelization\nfrom joblib import Parallel, delayed\n\n#Classification\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import label_ranking_average_precision_score\nfrom sklearn.metrics import roc_auc_score\n\n# Signal processing\nimport scipy.stats"]}, {"cell_type": "code", "execution_count": 1, "id": "c7f8205e", "metadata": {}, "outputs": [], "source": ["# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n# PATH=\"../input/Santander/\" \nPATH=\"../input/\" \nprint(os.listdir(PATH))\n# Any results you write to the current directory are saved as output."]}, {"cell_type": "code", "execution_count": 1, "id": "94e0e712", "metadata": {}, "outputs": [], "source": ["%%time\ntrain_curated = pd.read_csv(PATH+\"train_curated.csv\")\n# train_noisy = pd.read_csv(PATH+\"train_noisy.csv\")\ntest = pd.read_csv(PATH+\"sample_submission.csv\")"]}, {"cell_type": "markdown", "id": "9e8040f3", "metadata": {}, "source": ["# Preparing data"]}, {"cell_type": "code", "execution_count": 1, "id": "590fc0e0", "metadata": {}, "outputs": [], "source": ["print('Train curated size:' + format(train_curated.shape))\n# print('Train noisy size:' + format(train_noisy.shape))\nprint('Test size: ' + format(test.shape))"]}, {"cell_type": "code", "execution_count": 1, "id": "d46d0722", "metadata": {}, "outputs": [], "source": ["train_curated.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "025f7006", "metadata": {}, "outputs": [], "source": ["test.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "aed6c3ed", "metadata": {}, "outputs": [], "source": ["#Creating a dictionary of labels\nlabel_columns = list( test.columns[1:] )\nlabel_mapping = dict((label, index) for index, label in enumerate(label_columns))\nlabel_mapping"]}, {"cell_type": "code", "execution_count": 1, "id": "cf8f3e3c", "metadata": {}, "outputs": [], "source": ["def split_and_label(rows_labels):\n    \n    row_labels_list = []\n    for row in rows_labels:\n        row_labels = row.split(',')\n        labels_array = np.zeros((80))\n        \n        for label in row_labels:\n            index = label_mapping[label]\n            labels_array[index] = 1\n        \n        row_labels_list.append(labels_array)\n    \n    return row_labels_list"]}, {"cell_type": "code", "execution_count": 1, "id": "b57c5a1d", "metadata": {}, "outputs": [], "source": ["train_curated_labels = split_and_label(train_curated['labels'])\n# train_noisy_labels   = split_and_label(train_noisy  ['labels'])\nlen(train_curated_labels) #, len(train_noisy_labels)"]}, {"cell_type": "code", "execution_count": 1, "id": "5ecc8d4e", "metadata": {}, "outputs": [], "source": ["for f in label_columns:\n    train_curated[f] = 0.0\n#     train_noisy[f] = 0.0\n\ntrain_curated[label_columns] = train_curated_labels\n# train_noisy[label_columns]   = train_noisy_labels\n\ntrain_curated['num_labels'] = train_curated[label_columns].sum(axis=1)\n# train_noisy['num_labels']   = train_noisy[label_columns].sum(axis=1)\n\ntrain_curated['path'] = PATH+'train_curated/'+train_curated['fname']\n# train_noisy  ['path'] = PATH+'train_noisy/'+train_noisy['fname']\n\ntrain_curated.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "6da91770", "metadata": {}, "outputs": [], "source": ["# train_noisy.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "8e465e9f", "metadata": {}, "outputs": [], "source": ["train = train_curated\n# train = pd.concat([train_curated, train_noisy],axis=0) # Using both datasets\n\ndel train_curated  #, train_noisy\ngc.collect()\n\ntrain.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "064d81c3", "metadata": {}, "outputs": [], "source": ["train.describe()"]}, {"cell_type": "markdown", "id": "71deea40", "metadata": {}, "source": ["# Wavelet packet descomposition (WPD)"]}, {"cell_type": "markdown", "id": "c1916fc2", "metadata": {}, "source": ["![image.png](attachment:image.png)\nWe will use WPD to Level 6. Where, a total of 64 new signal are obtained. For each new signal, a statistics feature group are obtanied. "]}, {"cell_type": "code", "execution_count": 1, "id": "90f15c13", "metadata": {}, "outputs": [], "source": ["import pywt\nimport scipy as sc"]}, {"cell_type": "markdown", "id": "37055b9c", "metadata": {}, "source": ["### Zero pading\nUseful information\n[Link 1](http://www.bitweenie.com/listings/fft-zero-padding/)\n[Link 2](https://www.youtube.com/watch?v=ukHTfD37THI)"]}, {"cell_type": "code", "execution_count": 1, "id": "d01e0779", "metadata": {}, "outputs": [], "source": ["def zero_padding(data, seconds):\n    fs = 44100  # 2 seconds =  88200 samples    \n    if data.shape[0] < seconds*fs:\n        zeros = np.zeros(seconds*fs - data.shape[0])\n        data = np.concatenate((data, zeros), axis=None)    \n    return data"]}, {"cell_type": "markdown", "id": "b008e374", "metadata": {}, "source": ["### Feature extraction methods"]}, {"cell_type": "code", "execution_count": 1, "id": "78a8f85e", "metadata": {}, "outputs": [], "source": ["from collections import defaultdict, Counter\nfrom scipy.stats import kurtosis\nfrom scipy.stats import skew\n\ndef _kurtosis(x):\n    return kurtosis(x)\n\ndef CPT5(x):\n    den = len(x)*np.exp(np.std(x))\n    return sum(np.exp(x))/den\n\ndef SSC(x):\n    x = np.array(x)\n    x = np.append(x[-1], x)\n    x = np.append(x,x[1])\n    xn = x[1:len(x)-1]\n    xn_i2 = x[2:len(x)]    # xn+1 \n    xn_i1 = x[0:len(x)-2]  # xn-1\n    ans = np.heaviside((xn-xn_i1)*(xn-xn_i2),0)\n    return sum(ans[1:]) \n\ndef wave_length(x):\n    x = np.array(x)\n    x = np.append(x[-1], x)\n    x = np.append(x,x[1])\n    xn = x[1:len(x)-1]\n    xn_i2 = x[2:len(x)]    # xn+1 \n    return sum(abs(xn_i2-xn))\n    \ndef norm_entropy(x):\n    tresh = 2\n    return sum(np.power(abs(x),tresh))\n\ndef SRAV(x):    \n    SRA = sum(np.sqrt(abs(x)))\n    return np.power(SRA/len(x),2)\n\ndef mean_abs(x):\n    return sum(abs(x))/len(x)\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "3db37f6e", "metadata": {}, "outputs": [], "source": ["from scipy.stats import kurtosis\nfrom scipy.stats import skew #skewness\n\ndef calculate_entropy(list_values):\n    counter_values = Counter(list_values).most_common()\n    probabilities = [elem[1]/len(list_values) for elem in counter_values]\n    entropy=scipy.stats.entropy(probabilities)\n    return entropy\n \ndef calculate_statistics(list_values):\n    n5 = np.nanpercentile(list_values, 5)\n    n25 = np.nanpercentile(list_values, 25)\n    n75 = np.nanpercentile(list_values, 75)\n    n95 = np.nanpercentile(list_values, 95)\n    median = np.nanpercentile(list_values, 50)\n    mean = np.nanmean(list_values)\n    std = np.nanstd(list_values)\n    var = np.nanvar(list_values)\n    rms = np.nanmean(np.sqrt(list_values**2))\n    # New features\n    kur = kurtosis(list_values)\n    MeanAbs = mean_abs(list_values)\n    norm_ent = norm_entropy(list_values)\n    skewness = skew(list_values)\n    CPT_5 = CPT5(list_values)\n    SSC_1 = SSC(list_values)\n    WL = wave_length(list_values)\n    SRAV_1 = SRAV(list_values)\n    return [n5, n25, n75, n95, median, mean, std, var, rms, kur, MeanAbs, norm_ent, skewness, CPT_5, SSC_1, WL, SRAV_1]\n \ndef calculate_crossings(list_values):\n    zero_crossing_indices = np.nonzero(np.diff(np.array(list_values) > 0))[0]\n    no_zero_crossings = len(zero_crossing_indices)\n    mean_crossing_indices = np.nonzero(np.diff(np.array(list_values) > np.nanmean(list_values)))[0]\n    no_mean_crossings = len(mean_crossing_indices)\n    return [no_zero_crossings, no_mean_crossings]\n "]}, {"cell_type": "code", "execution_count": 1, "id": "751a2395", "metadata": {}, "outputs": [], "source": ["def get_features(list_values):\n    entropy = calculate_entropy(list_values)\n    crossings = calculate_crossings(list_values)\n    statistics = calculate_statistics(list_values)\n    return [entropy] + crossings + statistics    "]}, {"cell_type": "code", "execution_count": 1, "id": "f7623382", "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import StandardScaler\n\n\ndef feature_extraction_wpd(path_names, level, seconds):\n    # Sampling rate\n    fs = 44100    \n    corpus = []\n    \n    for fname in tqdm_notebook(path_names):        \n\n        fs, data = wavfile.read(fname)    \n        data = data.astype(float)\n        \n        # Zero padding\n        if data.shape[0] < (seconds*fs):\n            data = zero_padding(data,seconds)\n        elif data.shape[0] > (seconds*fs):\n            data = data[0:seconds*fs]\n        elif data.shape[0] == 0:\n            raise Exception('Lenght of x should not be 0. The value of lenght of x was: {}'.format(data.shape[0]))\n            \n        # Signal standarization\n        data_std = StandardScaler().fit_transform(data.reshape(-1,1)).reshape(1,-1)[0]            \n        \n        # WPD tree\n        wptree = pywt.WaveletPacket(data=data_std, wavelet='db5', mode='symmetric', maxlevel=level)\n        levels = wptree.get_level(level, order = \"freq\")            \n        \n        #Feature extraction for each node\n        features = []        \n        for node in levels:\n            data_wp = node.data\n            # Features group\n            features.extend(get_features(data_wp))\n        corpus.append(features)\n    # Delate first row\n    return np.array(corpus)\n     "]}, {"cell_type": "code", "execution_count": 1, "id": "df57b7f2", "metadata": {}, "outputs": [], "source": ["%%time\npath_names = train.path.values\nlevel = 6\nseconds = 2\nX_train = feature_extraction_wpd(path_names,level,seconds)"]}, {"cell_type": "code", "execution_count": 1, "id": "931818f7", "metadata": {}, "outputs": [], "source": ["path_names.shape, X_train.shape"]}, {"cell_type": "markdown", "id": "3d042181", "metadata": {}, "source": ["Train set ready!"]}, {"cell_type": "code", "execution_count": 1, "id": "583c9b2c", "metadata": {}, "outputs": [], "source": ["test.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "4ea994c2", "metadata": {}, "outputs": [], "source": ["%%time\npath_test = PATH+'test/'\npath_names = path_test + test['fname'].values\nX_test = feature_extraction_wpd(path_names,level, seconds) "]}, {"cell_type": "markdown", "id": "020210e4", "metadata": {}, "source": ["Test set ready!"]}, {"cell_type": "markdown", "id": "2fe082ef", "metadata": {}, "source": ["Cleaning Nan and Inf values"]}, {"cell_type": "code", "execution_count": 1, "id": "8d057f5d", "metadata": {}, "outputs": [], "source": ["X_train[~np.isfinite(X_train)] = 0\nX_test[~np.isfinite(X_test)] = 0"]}, {"cell_type": "code", "execution_count": 1, "id": "71da5aa4", "metadata": {}, "outputs": [], "source": ["X_train = np.float32(X_train)\nX_test = np.float32(X_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "35f850ad", "metadata": {}, "outputs": [], "source": ["X_train[~np.isfinite(X_train)] = 0\nX_test[~np.isfinite(X_test)] = 0"]}, {"cell_type": "markdown", "id": "8ef8570a", "metadata": {}, "source": ["# Classification\n\nInformation about Random Forest classifier:\n[Link](https://medium.com/machine-learning-101/chapter-5-random-forest-classifier-56dc7425c3e1)"]}, {"cell_type": "code", "execution_count": 1, "id": "cec99235", "metadata": {}, "outputs": [], "source": ["n_fold = 5\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=69)\n\nPREDTRAIN = np.zeros( (X_train.shape[0],80))\nPREDTEST  = np.zeros( (X_test.shape[0],80))\nfor f in range(len(label_columns)):\n    y = train[ label_columns[f]].values\n    oof      = np.zeros( X_train.shape[0] )\n    oof_test = np.zeros( X_test.shape[0] )\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train,y)):\n        \n        # Random Forest classifier\n        model = RandomForestClassifier(n_estimators=500, random_state=0, n_jobs=-1)\n        model.fit(X_train[trn_idx,:], y[trn_idx])\n        \n        oof[val_idx] = model.predict_proba(X_train[val_idx,:])[:,1] \n        oof_test += model.predict_proba(X_test)[:,1]/5.0\n\n    PREDTRAIN[:,f] = oof    \n    PREDTEST [:,f] = oof_test\n    \n    print( f, str(roc_auc_score( y, oof ))[:6], label_columns[f] )"]}, {"cell_type": "code", "execution_count": 1, "id": "091ae89e", "metadata": {}, "outputs": [], "source": ["def calculate_overall_lwlrap_sklearn(truth, scores):\n    \"\"\"Calculate the overall lwlrap using sklearn.metrics.lrap.\"\"\"\n    # sklearn doesn't correctly apply weighting to samples with no labels, so just skip them.\n    sample_weight = np.sum(truth > 0, axis=1)\n    nonzero_weight_sample_indices = np.flatnonzero(sample_weight > 0)\n    overall_lwlrap = label_ranking_average_precision_score(\n        truth[nonzero_weight_sample_indices, :] > 0, \n        scores[nonzero_weight_sample_indices, :], \n        sample_weight=sample_weight[nonzero_weight_sample_indices])\n    return overall_lwlrap\n\nprint( 'CV:', calculate_overall_lwlrap_sklearn( train[label_columns].values, PREDTRAIN ) )"]}, {"cell_type": "code", "execution_count": 1, "id": "2bc3a6a8", "metadata": {}, "outputs": [], "source": ["PREDTEST.shape, test.shape, X_test.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "f1afd2d8", "metadata": {}, "outputs": [], "source": ["test[label_columns] = PREDTEST \ntest.to_csv('submission.csv', index=False)\ntest.head()"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}