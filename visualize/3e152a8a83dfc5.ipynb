{"cells": [{"cell_type": "markdown", "id": "75850573", "metadata": {}, "source": ["## Content\nThe data is about Asteroids - NeoWs.\nNeoWs (Near Earth Object Web Service) is a RESTful web service for near earth Asteroid information. With NeoWs a user can: search for Asteroids based on their closest approach date to Earth, lookup a specific Asteroid with its NASA JPL small body id, as well as browse the overall data-set.\n\n## Acknowledgements\nData-set: All the data is from the (http://neo.jpl.nasa.gov/). This API is maintained by SpaceRocks Team: David Greenfield, Arezu Sarvestani, Jason English and Peter Baunach.\n\n## Tasks\nGiven this dataset, we think about three tasks:\n1. Develop a model that predicts if an asteroid is going to be hazardous (or not!)\n2. Identify the features responsible for claiming an asteroid to be hazardous\n3. Identify clusters of asteroids and reveal its characteristics\n\n## Approach\nTo deal with it, we will approach with different methods, and in the end, compare which performed the best on the data :) . Some of them are listed below:\n- Decision tree\n- Random forest\n- SVM\n- XGBoosting\n- K-Means\n- PCA\n- and others..."]}, {"cell_type": "markdown", "id": "453139ea", "metadata": {}, "source": ["## Libraries imported"]}, {"cell_type": "code", "execution_count": 1, "id": "52223217", "metadata": {}, "outputs": [], "source": ["import pandas as pd\nimport numpy as np\n\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier, plot_importance\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# seed fixing\nSEED = 42"]}, {"cell_type": "code", "execution_count": 1, "id": "44159d01", "metadata": {}, "outputs": [], "source": ["df = pd.read_csv('../input/nasa-asteroids-classification/nasa.csv', parse_dates=['Close Approach Date', 'Orbit Determination Date', 'Epoch Date Close Approach'])\ndf"]}, {"cell_type": "code", "execution_count": 1, "id": "c89b54c6", "metadata": {}, "outputs": [], "source": ["df.dtypes"]}, {"cell_type": "markdown", "id": "2c339d29", "metadata": {}, "source": ["## Exploratory Data Analysis"]}, {"cell_type": "code", "execution_count": 1, "id": "550e715d", "metadata": {}, "outputs": [], "source": ["df.info()"]}, {"cell_type": "code", "execution_count": 1, "id": "871dc9c6", "metadata": {}, "outputs": [], "source": ["df.describe()"]}, {"cell_type": "markdown", "id": "d1844ef1", "metadata": {}, "source": ["## Checking null values"]}, {"cell_type": "code", "execution_count": 1, "id": "6a657d2b", "metadata": {}, "outputs": [], "source": ["import missingno as msno\n\nmsno.matrix(df)"]}, {"cell_type": "code", "execution_count": 1, "id": "5bc022f4", "metadata": {}, "outputs": [], "source": ["## Dimensionality reduction"]}, {"cell_type": "code", "execution_count": 1, "id": "3182c898", "metadata": {}, "outputs": [], "source": ["sns.set(rc={'figure.figsize':(30,20)})\n\nmask = np.triu(df.corr())\n\nsns.heatmap(df.corr(), vmin=-1, vmax=1, cmap=\"viridis\", mask = mask)\nplt.show()\nplt.close()"]}, {"cell_type": "code", "execution_count": 1, "id": "cac01dec", "metadata": {}, "outputs": [], "source": ["df = df.drop(['Est Dia in M(min)', 'Est Dia in M(max)', 'Est Dia in Miles(min)', 'Est Dia in Miles(max)', 'Est Dia in Feet(min)', 'Est Dia in Feet(max)', 'Est Dia in KM(max)',\n              'Relative Velocity km per hr', 'Miles per hour',\n              'Miss Dist.(Astronomical)', 'Miss Dist.(lunar)', 'Miss Dist.(miles)',\n              'Semi Major Axis',\n              'Neo Reference ID', 'Name',\n              'Close Approach Date', 'Epoch Date Close Approach', 'Orbit Determination Date'],axis=1)\n\n\nmask = np.triu(df.corr())\n\nsns.heatmap(df.corr(), vmin=-1, vmax=1, cmap=\"viridis\", mask = mask, annot=True)\nplt.show()\nplt.close()"]}, {"cell_type": "markdown", "id": "5ad292e1", "metadata": {}, "source": ["## Categorical Feature Encoding"]}, {"cell_type": "code", "execution_count": 1, "id": "2508d017", "metadata": {}, "outputs": [], "source": ["encoder = LabelEncoder()\n\ndf['hazardous'] = encoder.fit_transform(df.Hazardous)\n\n# Dropping these categorical features since they are repeated among all observations\ndf = df.drop(['Orbiting Body', 'Equinox', 'Hazardous'], axis = 1)"]}, {"cell_type": "markdown", "id": "004900fd", "metadata": {}, "source": ["## Train-test split"]}, {"cell_type": "code", "execution_count": 1, "id": "2ee33508", "metadata": {}, "outputs": [], "source": ["features = df.drop('hazardous', axis = 1).values\ntarget = df['hazardous'].values\n\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.7, stratify = target, random_state = SEED)"]}, {"cell_type": "code", "execution_count": 1, "id": "53b3d859", "metadata": {}, "outputs": [], "source": ["# Creating the Features/Label split as numpy arrays\nfeatures = df.drop('hazardous', axis=1).values\nlabel = df.hazardous.values\n\n# Creating the test/train split\ntraining_features, test_features, training_label, test_label = train_test_split(features, label,\n                                                                                test_size=0.8,\n                                                                                stratify=label,\n                                                                                random_state=SEED)\n\ndf_graph = df.copy()\nfeature_names = df_graph.drop('hazardous', axis=1).columns.tolist()"]}, {"cell_type": "code", "execution_count": 1, "id": "d2e2d771", "metadata": {}, "outputs": [], "source": ["feature_names"]}, {"cell_type": "markdown", "id": "58c1b68a", "metadata": {}, "source": ["## Models"]}, {"cell_type": "markdown", "id": "140287fa", "metadata": {}, "source": ["## Decision Tree"]}, {"cell_type": "code", "execution_count": 1, "id": "61a777df", "metadata": {}, "outputs": [], "source": ["hyperparameters_decision_tree = {'max_depth'        : np.arange(0, 25, 1),\n                                 'criterion'        : ['gini', 'entropy'],\n                                 'min_samples_leaf' : np.arange(0, 1, 0.05),\n                                  'random_state'    : [SEED]}\n\n# Applying GridSearchCV\ndecision_tree_grid = GridSearchCV(estimator = DecisionTreeClassifier(),\n                                  param_grid = hyperparameters_decision_tree,\n                                  scoring = 'accuracy',\n                                  cv = 10)\n\ndecision_tree_grid.fit(X_train, y_train)\ndecision_tree_opt = decision_tree_grid.best_params_\n\ndecision_tree_score = (decision_tree_grid.best_score_*100).round(2)\ndecision_tree_est = decision_tree_grid.best_estimator_\ndecision_tree_features = decision_tree_est.feature_importances_\n\n# Score on holdout data\ndecision_tree_holdout_score = (decision_tree_grid.score(test_features, test_label)*100).round(2)\n\nprint('Optimal Hyperparameters:')\nprint(decision_tree_opt)\nprint('Optimal Estimator:')\nprint(decision_tree_est)\n\nprint('Training Accuracy {}'.format(decision_tree_score))\nprint('Testing Accuracy {}'.format(decision_tree_holdout_score))"]}, {"cell_type": "code", "execution_count": 1, "id": "d16297cd", "metadata": {}, "outputs": [], "source": ["# Plotting the optimal tree\nplt.subplot(1, 2, 1)\nplot_tree(decision_tree_est,\n          feature_names = feature_names,  \n          class_names = ['Non-Hazardous [0]', 'Hazardous [1]'],\n          filled = True)\n\n# Plotting feature importnace\nplt.subplot(1, 2, 2)\nplt.barh(feature_names, decision_tree_features)\n\nplt.show()\nplt.close"]}, {"cell_type": "markdown", "id": "3caf1608", "metadata": {}, "source": ["## Random Forest"]}, {"cell_type": "code", "execution_count": 1, "id": "e640f332", "metadata": {}, "outputs": [], "source": ["rf_params = {'max_depth': np.arange(0, 30, 1),\n             'criterion': ['gini', 'entropy'],\n             'min_samples_leaf': np.arange(0, 1, 0.05),\n             'random_state': [SEED],\n             'n_estimators': np.arange(0, 20, 1)}\n\n\nrf_grid = GridSearchCV(estimator=RandomForestClassifier(), param_grid=rf_params,\n                       scoring='accuracy', cv=5)\nrf_grid.fit(X_train, y_train)\nrf_opt_param = rf_grid.best_params_\nrf_best_score = (rf_grid.best_score_*100).round(2)\nrf_best_est = rf_grid.best_estimator_\nrf_feat_imp = rf_best_est.feature_importances_\n\nrf_holdout_score = (rf_grid.score(X_test, y_test)*100).round(2)\n\nprint('Optimal Hyperparameters:')\nprint(rf_opt_param)\nprint('Optimal Estimator:')\nprint(rf_best_est)\nprint('Training Accuracy {}'.format(rf_best_score))\nprint('Testing Accuracy {}'.format(rf_holdout_score))"]}, {"cell_type": "code", "execution_count": 1, "id": "c8531c03", "metadata": {}, "outputs": [], "source": ["# Plotting feature importance\nplt.barh(feature_names, rf_feat_imp)\nplt.show()\nplt.close"]}, {"cell_type": "markdown", "id": "57cc77c1", "metadata": {}, "source": ["## Support Vector Machines"]}, {"cell_type": "code", "execution_count": 1, "id": "ce838526", "metadata": {}, "outputs": [], "source": ["svm_pipe = Pipeline([('Scaling', StandardScaler()),\n                     ('SVM', SVC())])\n\nsvm_params = {'SVM__C': np.arange(0, 20, 0.1),\n              'SVM__gamma': [0.01, 0.1, 1, 2, 5],        \n              'SVM__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n              'SVM__random_state': [SEED]}\n\nsvm_grid = GridSearchCV(estimator=svm_pipe, param_grid=svm_params,\n                        scoring='accuracy', cv=5)\nsvm_grid.fit(X_train, y_train)\nsvm_opt_param = svm_grid.best_params_\nsvm_best_score = (svm_grid.best_score_*100).round(2)\nsvm_best_est = svm_grid.best_estimator_\n\nsvm_holdout_score = (svm_grid.score(X_test, y_test)*100).round(2)\n\nprint('Optimal Hyperparameters:')\nprint(svm_opt_param)\nprint('Optimal Estimator:')\nprint(svm_best_est)\n\nprint('Training Accuracy {}'.format(svm_best_score))\nprint('Testing Accuracy {}'.format(svm_holdout_score))"]}, {"cell_type": "markdown", "id": "cbf3f89d", "metadata": {}, "source": ["## XGBoosting"]}, {"cell_type": "code", "execution_count": 1, "id": "9b606276", "metadata": {}, "outputs": [], "source": ["# Creating hyperparameter options\nxgb_params = {'max_depth': np.arange(0, 5, 1),\n              'objective': ['binary:logistic'],\n              'random_state': [SEED],\n              'alpha': [0, 0.01, 0.1, 1],\n              'lambda': [0, 0.01, 0.1, 1],\n              'subsample': [0.25, 0.5, 0.75],\n              'colsample_bytree': [0.25, 0.5, 0.75],\n              'eval_metric': ['logloss']}\n\n# GridSearcCV\nxgb_grid = GridSearchCV(estimator=XGBClassifier(), param_grid=xgb_params,\n                            scoring='accuracy', cv=5)\nxgb_grid.fit(X_train, y_train)\nxgb_opt_param = xgb_grid.best_params_\nxgb_best_score = (xgb_grid.best_score_*100).round(2)\nxgb_best_est = xgb_grid.best_estimator_\nxgb_feat_imp = xgb_best_est.feature_importances_\n\n# Score on holdout data\nxgb_holdout_score = (xgb_grid.score(X_test, y_test)*100).round(2)\n\nprint('Optimal Hyperparameters:')\nprint(xgb_opt_param)\nprint('Optimal Estimator:')\nprint(xgb_best_est)\nprint('\\n')\nprint('Training Accuracy {}'.format(xgb_best_score))\nprint('Testing Accuracy {}'.format(xgb_holdout_score))"]}, {"cell_type": "code", "execution_count": 1, "id": "d66ff989", "metadata": {}, "outputs": [], "source": ["# Plotting feature importance\nplt.barh(feature_names, xgb_feat_imp)"]}, {"cell_type": "markdown", "id": "cd67d8f7", "metadata": {}, "source": ["## KMeans and PCA - Approach to identifying asteroids groups"]}, {"cell_type": "code", "execution_count": 1, "id": "bc4da6f5", "metadata": {}, "outputs": [], "source": ["scaler = StandardScaler()\ndf = df.drop(['hazardous', 'Orbit ID'], axis = 1)\ndf_scaled = scaler.fit_transform(df)\ndf_scaled"]}, {"cell_type": "code", "execution_count": 1, "id": "f94cb589", "metadata": {}, "outputs": [], "source": ["from sklearn.cluster import KMeans\n\nscores_1 = []\n\nrange_values = range(1, 20)\n\nfor i in range_values:\n    kmeans = KMeans(n_clusters= i)\n    kmeans.fit(df_scaled)\n    scores_1.append(kmeans.inertia_)\n\nplt.figure(figsize=(16,9)) \nplt.plot(scores_1, 'bx-')\nplt.title('Optimal cluster number')\nplt.xlabel('Clusters')\nplt.ylabel('scores') \nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "0a74abae", "metadata": {}, "outputs": [], "source": ["kmeans = KMeans(4)\nkmeans.fit(df_scaled)\nlabels = kmeans.labels_\n\nkmeans.cluster_centers_.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "72c84b3a", "metadata": {}, "outputs": [], "source": ["cluster_centers = pd.DataFrame(data = kmeans.cluster_centers_, columns = [df.columns])\n\ncluster_centers = scaler.inverse_transform(cluster_centers)\ncluster_centers = pd.DataFrame(data = cluster_centers, columns = [df.columns])\ncluster_centers"]}, {"cell_type": "markdown", "id": "e55c444c", "metadata": {}, "source": ["Simply, according to our KMeans approach, the asteroids can be subgrouped in 4 clusters.\n- **Cluster 0**: higher magnitude, lowest `Est Dia in KM` and highest orbit uncertainty\n- **Cluster 1**: mean magnitude, highest relative velocity (km/sec), highest Miss Dist (kilometers) and lowest orbit uncertainty, most inclinated and most `mean motion` value.\n- **Cluster 2**: Highest eccentricity and greater orbital period\n- **Cluster 3**: Highest magnitude and lowest relative velocity, smaller eccentricity and least inclination among other clusters."]}, {"cell_type": "code", "execution_count": 1, "id": "c64c6126", "metadata": {}, "outputs": [], "source": ["from sklearn.decomposition import PCA\n\n# Obtain the principal components \npca = PCA(n_components=2)\nprincipal_comp = pca.fit_transform(df_scaled)\nprincipal_comp"]}, {"cell_type": "code", "execution_count": 1, "id": "b9f92c1e", "metadata": {}, "outputs": [], "source": ["# Create a dataframe with the two components\npca_df = pd.DataFrame(data = principal_comp, columns =['pca1','pca2'])\npca_df.head()\n\n# Concatenate the clusters labels to the dataframe\npca_df = pd.concat([pca_df,pd.DataFrame({'cluster':labels})], axis = 1)\npca_df.head()\n\n\nax = sns.scatterplot(x = pca_df['pca1'], y=pca_df['pca2'], hue = \"cluster\", data = pca_df)\nax.set(xlabel = f\"PC0 {pca.explained_variance_ratio_[0]*100:.2f}%\", ylabel=f\"PC1 {pca.explained_variance_ratio_[1]*100:.2f}%\")\nplt.show()"]}, {"cell_type": "markdown", "id": "9b3690e2", "metadata": {}, "source": ["## Conclusion\n### Model Performance on Unseen Data\n\n|Model |Accuracy  |\n--- | --- \n|Decision Tree|99,55%|\n|Random Forest|98,93%|\n|Support Vector Machine|94,39%|\n|XGBoosting|99,57%|\n\n\nClearly, using decision tree provided the best performance on unseen data, of course, considering its simplicity and fast execution, and thus is the best model for this classifcation problem out of those tested.\n\n### Feature Importance\nLooking into each tree based model we can see a pattern on feature importance of the best estimator found for each model:\n\n- Minimum Orbit Intersection\n- Absolute Magnitude\n- Est Dia in Km"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}