{"cells": [{"cell_type": "markdown", "id": "faefa914", "metadata": {}, "source": ["<h1 style=\"font-size:350%;\"><center><b style=\"color:navy;\">BREAST CANCER - EXPLORATORY DATA ANALYSIS</b></center></h1>"]}, {"cell_type": "markdown", "id": "5072c857", "metadata": {}, "source": ["<h1 style=\"font-size:200%;\"><b>OBJECTIVE</b></h1>\n<ul>\n    <li style=\"font-size:150%;\">The goal of this kernel is to perfom <b>EXPLORATORY DATA ANALYSIS</b> on Breast Cancer Dataset and build a Machine Learning Model with good Accuracy. This will help in understand the importance of attributes thereby helping in predicting breast cancer depending these attributes.</li>\n</ul>\n\n<h1 style=\"font-size:200%;\"><b>STEPS PERFORMED</b></h1>\n<ul>\n    <li style=\"font-size:150%;\">Data Cleaning</li>\n    <li style=\"font-size:150%;\">Data Visualization</li>\n    <li style=\"font-size:150%;\">PCA</li>\n    <li style=\"font-size:150%;\">Model Building</li>\n    <li style=\"font-size:150%;\">Conclusion</li>\n</ul>"]}, {"cell_type": "code", "execution_count": 1, "id": "23d99b2e", "metadata": {}, "outputs": [], "source": ["# Importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns"]}, {"cell_type": "code", "execution_count": 1, "id": "1e0c774f", "metadata": {}, "outputs": [], "source": ["# Importing the dataset\ndata = pd.read_csv(\"../input/breast-cancer-wisconsin-data/data.csv\")"]}, {"cell_type": "markdown", "id": "a56047d6", "metadata": {}, "source": ["# DATA CLEANING"]}, {"cell_type": "code", "execution_count": 1, "id": "337d6b59", "metadata": {}, "outputs": [], "source": ["# Printing the 1st 5 columns\ndata.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "7196e9e2", "metadata": {}, "outputs": [], "source": ["# get the dimenions of data\ndata.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "e4e560e0", "metadata": {}, "outputs": [], "source": ["# get the columns list:\ndata.columns"]}, {"cell_type": "code", "execution_count": 1, "id": "6e014922", "metadata": {}, "outputs": [], "source": ["# Target Variable:\ndata.diagnosis.value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "08715a52", "metadata": {}, "outputs": [], "source": ["#get the datatype of columns:\ndata.dtypes"]}, {"cell_type": "markdown", "id": "61235a3f", "metadata": {}, "source": ["## MISSING VALUES"]}, {"cell_type": "code", "execution_count": 1, "id": "78ecf3be", "metadata": {}, "outputs": [], "source": ["# Check for null values:\ndata.isnull().sum()"]}, {"cell_type": "code", "execution_count": 1, "id": "e3d0e600", "metadata": {}, "outputs": [], "source": ["#drop the unnamed column:\ndata.drop(['Unnamed: 32', 'id'], axis=1, inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "98e7e4e1", "metadata": {}, "outputs": [], "source": ["# statistics of our data:\ndata.describe().T"]}, {"cell_type": "markdown", "id": "cafb4552", "metadata": {}, "source": ["# DATA VISUALIZATION"]}, {"cell_type": "code", "execution_count": 1, "id": "d8fe95ec", "metadata": {}, "outputs": [], "source": ["# Finding out the correlation between the features\ncorr = data.corr()\ncorr.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "93a97b4a", "metadata": {}, "outputs": [], "source": ["# Plotting the heatmap of correlation between features\nplt.figure(figsize=(20,20))\nsns.heatmap(corr, cbar=True, square= True, fmt='.1f', annot=True, annot_kws={'size':15}, cmap='Greens')\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "a2cf316a", "metadata": {}, "outputs": [], "source": ["# Analyzing the target variable\n\nplt.title('Count of cancer type')\nsns.countplot(data['diagnosis'])\nplt.xlabel('Cancer lethality')\nplt.ylabel('Count')\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "917f270f", "metadata": {}, "outputs": [], "source": ["#plot the histograms for each feature:\ndata.hist(figsize = (30,30), color = 'orange')\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "2e859dce", "metadata": {}, "outputs": [], "source": ["melted_data = pd.melt(data,id_vars = \"diagnosis\",value_vars = ['radius_worst', 'texture_worst', 'perimeter_worst'])\nplt.figure(figsize = (15,10))\nsns.boxplot(x = \"variable\", y = \"value\", hue=\"diagnosis\",data= melted_data)\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "0be49026", "metadata": {}, "outputs": [], "source": ["data.columns"]}, {"cell_type": "code", "execution_count": 1, "id": "9e35a1a0", "metadata": {}, "outputs": [], "source": ["#generate a scatter plot with the following columns:\n\ncolumns = ['diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']\n\nsns.pairplot(data=data[columns], hue=\"diagnosis\", palette='rocket')"]}, {"cell_type": "code", "execution_count": 1, "id": "f986c437", "metadata": {}, "outputs": [], "source": ["# Distribution density plot KDE (kernel density estimate)\nsns.FacetGrid(data, hue=\"diagnosis\", height=6).map(sns.kdeplot, \"radius_mean\").add_legend()\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "c8d99b8d", "metadata": {}, "outputs": [], "source": ["# Plotting the distribution of the mean radius\nsns.stripplot(x=\"diagnosis\", y=\"radius_mean\", data=data, jitter=True, edgecolor=\"gray\")\nplt.show()"]}, {"cell_type": "markdown", "id": "23ea48f9", "metadata": {}, "source": ["<h1>Drop the Columns with high correlation</h1>\n\n<ul>\n    <li style=\"font-size:130%;\">Multicollinearity is a problem as it undermines the significance of independent varibales and we fix it by removing the highly correlated predictors.</li>\n    <li style=\"font-size:130%;\">we can verify the presence of multicollinearity between some of the variables. For instance, the radius_mean column has a correlation of 1 and 0.99 with perimeter_mean and area_mean columns, respectively. This is because the three columns essentially contain the same information, which is the physical size of the observation (the cell). Therefore we should only pick ONE of the three columns when we go into further analysis.</li>\n    <li style=\"font-size:130%;\">Another place where multicollienartiy is apparent is between the \"mean\" columns and the \"worst\" column. For instance, the radius_mean column has a correlation of 0.97 with the radius_worst column.\nalso there is multicollinearity between the attributes compactness, concavity, and concave points. So we can choose just ONE out of these, I am going for Compactness.</li>\n</ul>"]}, {"cell_type": "code", "execution_count": 1, "id": "345c3af2", "metadata": {}, "outputs": [], "source": ["#From the correlation matrix we got to knwo that these columns are highly correlated with radius_mean, perimeter, area columns.\n# So we are dropping these columns:\n\n# first, drop all \"worst\" columns\ncols = ['radius_worst', \n        'texture_worst', \n        'perimeter_worst', \n        'area_worst', \n        'smoothness_worst', \n        'compactness_worst', \n        'concavity_worst',\n        'concave points_worst', \n        'symmetry_worst', \n        'fractal_dimension_worst']\ndata = data.drop(cols, axis=1)\n\n# then, drop all columns related to the \"perimeter\" and \"area\" attributes\ncols = ['perimeter_mean',\n        'perimeter_se', \n        'area_mean', \n        'area_se']\ndata = data.drop(cols, axis=1)\n\n# lastly, drop all columns related to the \"concavity\" and \"concave points\" attributes\ncols = ['concavity_mean',\n        'concavity_se', \n        'concave points_mean', \n        'concave points_se']\ndata = data.drop(cols, axis=1)\n\n# verify remaining columns\ndata.columns"]}, {"cell_type": "code", "execution_count": 1, "id": "86a97f61", "metadata": {}, "outputs": [], "source": ["# Draw the heatmap again, with the new correlation matrix\ncorr = data.corr().round(2)\n\n# Define custom colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nf, ax = plt.subplots(figsize=(20, 20))\nsns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\nplt.tight_layout()"]}, {"cell_type": "markdown", "id": "790e1d8b", "metadata": {}, "source": ["# TRAIN TEST SPLIT"]}, {"cell_type": "code", "execution_count": 1, "id": "56b86310", "metadata": {}, "outputs": [], "source": ["# Spliting target variable and independent variables\nX = data.drop(['diagnosis'], axis = 1)\ny = data['diagnosis']"]}, {"cell_type": "code", "execution_count": 1, "id": "96eda7c5", "metadata": {}, "outputs": [], "source": ["# Splitting the data into training set and testset\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)\nprint(\"Size of training set:\", X_train.shape)\nprint(\"Size of training set:\", X_test.shape)"]}, {"cell_type": "markdown", "id": "44fd5b9f", "metadata": {}, "source": ["# Random Forest Model"]}, {"cell_type": "code", "execution_count": 1, "id": "8111a6ca", "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report"]}, {"cell_type": "code", "execution_count": 1, "id": "b6344a91", "metadata": {}, "outputs": [], "source": ["# Random Forest Classifier\n\n# Import library of RandomForestClassifier model\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Create a Random Forest Classifier\nrf = RandomForestClassifier()\n\n# Hyperparameter Optimization\nparameters = {'n_estimators': [4, 6, 9, 10, 15], \n              'max_features': ['log2', 'sqrt','auto'], \n              'criterion': ['entropy', 'gini'],\n              'max_depth': [2, 3, 5, 10], \n              'min_samples_split': [2, 3, 5],\n              'min_samples_leaf': [1, 5, 8]\n             }\n\n# Run the grid search\ngrid_obj = GridSearchCV(rf, parameters)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the rf to the best combination of parameters\nrf = grid_obj.best_estimator_\n\n# Train the model using the training sets \nrf.fit(X_train,y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "011dfd2b", "metadata": {}, "outputs": [], "source": ["# Prediction on test data\ny_pred = rf.predict(X_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "b0d3365b", "metadata": {}, "outputs": [], "source": ["from sklearn import metrics\n# Calculating the accuracy\nacc_rf = round( metrics.accuracy_score(y_test, y_pred) * 100 , 2 )\nprint( 'Accuracy of Random Forest model : ', acc_rf )"]}, {"cell_type": "markdown", "id": "8e519de6", "metadata": {}, "source": ["# Support Vector Machine"]}, {"cell_type": "code", "execution_count": 1, "id": "b8b56aa0", "metadata": {}, "outputs": [], "source": ["# SVM Classifier\n\n# Creating scaled set to be used in model to improve the results\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "219c3f71", "metadata": {}, "outputs": [], "source": ["# Import Library of Support Vector Machine model\nfrom sklearn import svm\n\n# Create a Support Vector Classifier\nsvc = svm.SVC()\n\n# Hyperparameter Optimization\nparameters = [\n  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n]\n\n# Run the grid search\ngrid_obj = GridSearchCV(svc, parameters)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the svc to the best combination of parameters\nsvc = grid_obj.best_estimator_\n\n# Train the model using the training sets \nsvc.fit(X_train,y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "fa01c316", "metadata": {}, "outputs": [], "source": ["# Prediction on test data\ny_pred = svc.predict(X_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "a86dc0d6", "metadata": {}, "outputs": [], "source": ["# Calculating the accuracy\nacc_svm = round( metrics.accuracy_score(y_test, y_pred) * 100, 2 )\nprint( 'Accuracy of SVM model : ', acc_svm )"]}, {"cell_type": "markdown", "id": "75720f18", "metadata": {}, "source": ["# CONCLUSION"]}, {"cell_type": "markdown", "id": "855c89e0", "metadata": {}, "source": ["<p style=\"font-size:180%;\">In this Kernel i have performed EDA, and built the machine learning models(RF, SVM)</p>\n\n<p style=\"font-size:180%;\">If you like the kernel, please give an upvote.</p>"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}