{"cells": [{"cell_type": "code", "execution_count": 1, "id": "a4cb2bc1", "metadata": {}, "outputs": [], "source": ["import numpy as np\nimport pandas as pd \nfrom sklearn.preprocessing import LabelEncoder\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport seaborn as sns"]}, {"cell_type": "code", "execution_count": 1, "id": "a7453445", "metadata": {}, "outputs": [], "source": ["print(os.listdir(\"../input/\"))"]}, {"cell_type": "markdown", "id": "498f8c20", "metadata": {}, "source": ["## Basic LightGBM - First Step"]}, {"cell_type": "code", "execution_count": 1, "id": "45eac8d8", "metadata": {}, "outputs": [], "source": ["import lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nMAX_EVALS = 5"]}, {"cell_type": "code", "execution_count": 1, "id": "54e94a49", "metadata": {}, "outputs": [], "source": ["features=pd.read_csv('../input/application_train.csv')\nfeatures=features.sample(n=16000,random_state=42)\nprint(features.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "38206f5d", "metadata": {}, "outputs": [], "source": ["features.dtypes.value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "24770d43", "metadata": {}, "outputs": [], "source": ["features = features.select_dtypes('number')\nlabels = np.array(features['TARGET'])\nfeatures = features.drop(columns = ['TARGET', 'SK_ID_CURR'])\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 6000, random_state = 50)"]}, {"cell_type": "code", "execution_count": 1, "id": "e53fc0f9", "metadata": {}, "outputs": [], "source": ["print(\"Training features shape: \", train_features.shape)\nprint(\"Testing features shape: \", test_features.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "13bf1f6b", "metadata": {}, "outputs": [], "source": ["train_set = lgb.Dataset(data = train_features, label = train_labels)\ntest_set = lgb.Dataset(data = test_features, label = test_labels)"]}, {"cell_type": "code", "execution_count": 1, "id": "bbd38be6", "metadata": {}, "outputs": [], "source": ["model = lgb.LGBMClassifier()\ndefault_params = model.get_params()\n\n# Remove the number of estimators because we set this to 10000 in the cv call\ndel default_params['n_estimators']\n\n# Cross validation with early stopping\ncv_results = lgb.cv(default_params, train_set, num_boost_round = 10000, early_stopping_rounds = 100, \n                    metrics = 'auc', nfold = 5, seed = 42)"]}, {"cell_type": "code", "execution_count": 1, "id": "78594b4e", "metadata": {}, "outputs": [], "source": ["print(max(cv_results['auc-mean']))\nprint(len(cv_results['auc-mean']))"]}, {"cell_type": "code", "execution_count": 1, "id": "221f3484", "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import roc_auc_score"]}, {"cell_type": "code", "execution_count": 1, "id": "1eb48047", "metadata": {}, "outputs": [], "source": ["model.n_estimators = len(cv_results['auc-mean'])\n# Train and make predicions with model\nmodel.fit(train_features, train_labels)\npreds = model.predict_proba(test_features)[:, 1]\nbaseline_auc = roc_auc_score(test_labels, preds)\nprint('The model scores {:.5f} ROC AUC on the test set.'.format(baseline_auc))"]}, {"cell_type": "code", "execution_count": 1, "id": "4bb0683e", "metadata": {}, "outputs": [], "source": ["def objective(hyperparameters, iteration):\n    \n    if 'n_estimators' in hyperparameters.keys():\n        del hyperparameters['n_estimators']   \n\n    cv_results = lgb.cv(hyperparameters, train_set, num_boost_round = 10000, nfold =5, \n                        early_stopping_rounds = 100, metrics = 'auc', seed = 42)\n\n    score = cv_results['auc-mean'][-1]\n    estimators = len(cv_results['auc-mean'])\n    hyperparameters['n_estimators'] = estimators     \n    return [score, hyperparameters, iteration]"]}, {"cell_type": "code", "execution_count": 1, "id": "9febeab7", "metadata": {}, "outputs": [], "source": ["score, params, iteration = objective(default_params, 1)\n\nprint('The cross-validation ROC AUC was {:.5f}.'.format(score))"]}, {"cell_type": "code", "execution_count": 1, "id": "6da89161", "metadata": {}, "outputs": [], "source": ["model = lgb.LGBMModel()\nmodel.get_params()"]}, {"cell_type": "code", "execution_count": 1, "id": "92bd757e", "metadata": {}, "outputs": [], "source": ["param_grid = {\n    'boosting_type': ['gbdt', 'goss', 'dart'],\n    'num_leaves': list(range(20, 150)),\n    'learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 1000)),\n    'subsample_for_bin': list(range(20000, 300000, 20000)),\n    'min_child_samples': list(range(20, 500, 5)),\n    'reg_alpha': list(np.linspace(0, 1)),\n    'reg_lambda': list(np.linspace(0, 1)),\n    'colsample_bytree': list(np.linspace(0.6, 1, 10)),\n    'subsample': list(np.linspace(0.5, 1, 100)),\n    'is_unbalance': [True, False]\n}"]}, {"cell_type": "code", "execution_count": 1, "id": "0e8d236a", "metadata": {}, "outputs": [], "source": ["import random\n\nrandom.seed(50)\n\n# Randomly sample a boosting type\nboosting_type = random.sample(param_grid['boosting_type'], 1)[0]\n\n# Set subsample depending on boosting type\nsubsample = 1.0 if boosting_type == 'goss' else random.sample(param_grid['subsample'], 1)[0]\n\nprint('Boosting type: ', boosting_type)\nprint('Subsample ratio: ', subsample)"]}, {"cell_type": "code", "execution_count": 1, "id": "3076300c", "metadata": {}, "outputs": [], "source": ["random_results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                              index = list(range(MAX_EVALS)))\n\ngrid_results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                              index = list(range(MAX_EVALS)))"]}, {"cell_type": "code", "execution_count": 1, "id": "8fd98d11", "metadata": {}, "outputs": [], "source": ["import itertools\n\ndef grid_search(param_grid, max_evals = MAX_EVALS):\n    \"\"\"Grid search algorithm (with limit on max evals)\"\"\"\n    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                              index = list(range(MAX_EVALS)))\n    keys, values = zip(*param_grid.items())    \n    i = 0\n    for v in itertools.product(*values):\n        hyperparameters = dict(zip(keys, v))\n        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']\n        eval_results = objective(hyperparameters, i)       \n        results.loc[i, :] = eval_results\n        i += 1\n        if i > MAX_EVALS:\n            break\n    results.sort_values('score', ascending = False, inplace = True)\n    results.reset_index(inplace = True)    \n    return results    "]}, {"cell_type": "code", "execution_count": 1, "id": "0a849f12", "metadata": {}, "outputs": [], "source": ["grid_results = grid_search(param_grid)\nprint('The best validation score was {:.5f}'.format(grid_results.loc[0, 'score']))\nprint('\\nThe best hyperparameters were:')\nimport pprint\npprint.pprint(grid_results.loc[0, 'params'])"]}, {"cell_type": "code", "execution_count": 1, "id": "0ff24cdd", "metadata": {}, "outputs": [], "source": ["grid_search_params = grid_results.loc[0, 'params']\nmodel = lgb.LGBMClassifier(**grid_search_params, random_state=42)\nmodel.fit(train_features, train_labels)\n\npreds = model.predict_proba(test_features)[:, 1]\n\nprint('The best model from grid search scores {:.5f} ROC AUC on the test set.'.format(roc_auc_score(test_labels, preds)))"]}, {"cell_type": "code", "execution_count": 1, "id": "3e1b479b", "metadata": {}, "outputs": [], "source": ["random.seed(50)\n\n# Randomly sample from dictionary\nrandom_params = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n# Deal with subsample ratio\nrandom_params['subsample'] = 1.0 if random_params['boosting_type'] == 'goss' else random_params['subsample']\n\nrandom_params"]}, {"cell_type": "code", "execution_count": 1, "id": "8ab8c4e7", "metadata": {}, "outputs": [], "source": ["def random_search(param_grid, max_evals = MAX_EVALS):\n    \"\"\"Random search for hyperparameter optimization\"\"\"\n    \n    # Dataframe for results\n    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                                  index = list(range(MAX_EVALS)))\n    \n    # Keep searching until reach max evaluations\n    for i in range(MAX_EVALS):\n        \n        # Choose random hyperparameters\n        hyperparameters = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']\n\n        # Evaluate randomly selected hyperparameters\n        eval_results = objective(hyperparameters, i)\n        \n        results.loc[i, :] = eval_results\n    \n    # Sort with best score on top\n    results.sort_values('score', ascending = False, inplace = True)\n    results.reset_index(inplace = True)\n    return results "]}, {"cell_type": "code", "execution_count": 1, "id": "093c7e1b", "metadata": {}, "outputs": [], "source": ["random_results = random_search(param_grid)\n\nprint('The best validation score was {:.5f}'.format(random_results.loc[0, 'score']))\nprint('\\nThe best hyperparameters were:')\n\nimport pprint\npprint.pprint(random_results.loc[0, 'params'])"]}, {"cell_type": "code", "execution_count": 1, "id": "f2c01a8e", "metadata": {}, "outputs": [], "source": ["random_search_params = random_results.loc[0, 'params']\n\n# Create, train, test model\nmodel = lgb.LGBMClassifier(**random_search_params, random_state = 42)\nmodel.fit(train_features, train_labels)\n\npreds = model.predict_proba(test_features)[:, 1]\n\nprint('The best model from random search scores {:.5f} ROC AUC on the test set.'.format(roc_auc_score(test_labels, preds)))"]}, {"cell_type": "code", "execution_count": 1, "id": "5db3aa39", "metadata": {}, "outputs": [], "source": ["train = pd.read_csv('../input/application_train.csv')\ntest = pd.read_csv('../input/application_test.csv')\n\n# Extract the test ids and train labels\ntest_ids = test['SK_ID_CURR']\ntrain_labels = np.array(train['TARGET'].astype(np.int32)).reshape((-1, ))\n\ntrain = train.drop(columns = ['SK_ID_CURR', 'TARGET'])\ntest = test.drop(columns = ['SK_ID_CURR'])\n\nprint('Training shape: ', train.shape)\nprint('Testing shape: ', test.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "79628205", "metadata": {}, "outputs": [], "source": ["le = LabelEncoder()\nle_count = 0\nfor col in train:\n    if train[col].dtype == 'object':\n        if len(list(train[col].unique())) <= 2:\n            le.fit(train[col])\n            train[col] = le.transform(train[col])\n            test[col] = le.transform(test[col])\n            le_count += 1\nprint('%d columns were label encoded.' % le_count)"]}, {"cell_type": "code", "execution_count": 1, "id": "d33bcfca", "metadata": {}, "outputs": [], "source": ["train = pd.get_dummies(train)\ntest=pd.get_dummies(test)\nprint(train.shape,test.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "fca0ced7", "metadata": {}, "outputs": [], "source": ["train, test = train.align(test, join = 'inner', axis = 1)\n\n# Add the target back in\n#train['TARGET'] = train_labels\n\nprint('Training Features shape: ', train.shape)\nprint('Testing Features shape: ', test.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "4e9438a7", "metadata": {}, "outputs": [], "source": ["train_set = lgb.Dataset(train, label = train_labels)\n\nhyperparameters = dict(**random_results.loc[0, 'params'])\ndel hyperparameters['n_estimators']\n\n# Cross validation with n_folds and early stopping\ncv_results = lgb.cv(hyperparameters, train_set,\n                    num_boost_round = 10000, early_stopping_rounds = 100, \n                    metrics = 'auc', nfold = 5)"]}, {"cell_type": "code", "execution_count": 1, "id": "522c55ae", "metadata": {}, "outputs": [], "source": ["model = lgb.LGBMClassifier(n_estimators = len(cv_results['auc-mean']), **hyperparameters)\nmodel.fit(train, train_labels)\n                        \n# Predictions on the test data\npreds = model.predict_proba(test)[:, 1]"]}, {"cell_type": "code", "execution_count": 1, "id": "7dfd6510", "metadata": {}, "outputs": [], "source": ["submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': preds})\nsubmission.to_csv('submission_simple_features_random.csv', index = False)"]}, {"cell_type": "markdown", "id": "1ad6c17b", "metadata": {}, "source": ["## LightGBM - Second Step"]}, {"cell_type": "code", "execution_count": 1, "id": "1b81c7fa", "metadata": {}, "outputs": [], "source": ["app_train = pd.read_csv('../input/application_train.csv')\nprint('Training data shape: ', app_train.shape)\napp_train.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "3a884ce6", "metadata": {}, "outputs": [], "source": ["app_test = pd.read_csv('../input/application_test.csv')\nprint('Testing data shape: ', app_test.shape)\napp_test.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "d22f34cd", "metadata": {}, "outputs": [], "source": ["app_train['TARGET'].value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "53d99bf5", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(10,5))\nsns.set(style=\"whitegrid\",font_scale=1)\ng=sns.distplot(app_train['TARGET'],kde=False,hist_kws={\"alpha\": 1, \"color\": \"#DA1A32\"})\nplt.title('Distribution of target (1:default, 0:no default)',size=15)\nplt.show()"]}, {"cell_type": "markdown", "id": "6c368541", "metadata": {}, "source": ["## About missing values"]}, {"cell_type": "code", "execution_count": 1, "id": "c85e846c", "metadata": {}, "outputs": [], "source": ["def missing_values_table(df):\n        mis_val = df.isnull().sum()\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        return mis_val_table_ren_columns"]}, {"cell_type": "code", "execution_count": 1, "id": "43ddfbc5", "metadata": {}, "outputs": [], "source": ["missing_values = missing_values_table(app_train)\nmissing_values.head(10)"]}, {"cell_type": "markdown", "id": "6d66e941", "metadata": {}, "source": ["## Column Types"]}, {"cell_type": "code", "execution_count": 1, "id": "1df7315b", "metadata": {}, "outputs": [], "source": ["# Number of each type of column\napp_train.dtypes.value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "d33fd920", "metadata": {}, "outputs": [], "source": ["# Number of unique classes in each object column\napp_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)"]}, {"cell_type": "code", "execution_count": 1, "id": "8f63de52", "metadata": {}, "outputs": [], "source": ["le = LabelEncoder()\nle_count = 0\nfor col in app_train:\n    if app_train[col].dtype == 'object':\n        if len(list(app_train[col].unique())) <= 2:\n            le.fit(app_train[col])\n            app_train[col] = le.transform(app_train[col])\n            app_test[col] = le.transform(app_test[col])\n            le_count += 1\nprint('%d columns were label encoded.' % le_count)"]}, {"cell_type": "code", "execution_count": 1, "id": "1e40193f", "metadata": {}, "outputs": [], "source": ["app_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "ea874893", "metadata": {}, "outputs": [], "source": ["train_labels_last=app_train['TARGET']"]}, {"cell_type": "code", "execution_count": 1, "id": "c1294d37", "metadata": {}, "outputs": [], "source": ["train_labels = app_train['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\napp_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n\n# Add the target back in\napp_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "0b8e3e06", "metadata": {}, "outputs": [], "source": ["(app_train['DAYS_BIRTH'] / -365).describe()"]}, {"cell_type": "code", "execution_count": 1, "id": "171ae9f7", "metadata": {}, "outputs": [], "source": ["app_train['DAYS_EMPLOYED'].describe()"]}, {"cell_type": "code", "execution_count": 1, "id": "1e9948d8", "metadata": {}, "outputs": [], "source": ["plt.hist(app_train['DAYS_EMPLOYED'],color=\"#DA1A32\")\nplt.title('Ditribution of employed days')\nplt.show()"]}, {"cell_type": "markdown", "id": "74d70622", "metadata": {}, "source": ["**350000 days of employment seems wrong.**"]}, {"cell_type": "code", "execution_count": 1, "id": "709e16c8", "metadata": {}, "outputs": [], "source": ["anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\nnon_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\nprint('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\nprint('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\nprint('There are %d anomalous days of employment' % len(anom))"]}, {"cell_type": "markdown", "id": "7462d140", "metadata": {}, "source": ["**Replace it with nan.**"]}, {"cell_type": "code", "execution_count": 1, "id": "2944f7fb", "metadata": {}, "outputs": [], "source": ["# Create an anomalous flag column\napp_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\n# Replace the anomalous values with nan\napp_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\napp_train['DAYS_EMPLOYED'].plot.hist(title = 'Ditribution of employed days',color=\"#DA1A32\")\nplt.xlabel('Days')"]}, {"cell_type": "code", "execution_count": 1, "id": "f9c6b89a", "metadata": {}, "outputs": [], "source": ["app_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\napp_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\nprint('There are %d anomalies in the test data out of %d entries' % (app_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test)))"]}, {"cell_type": "markdown", "id": "9b9811f1", "metadata": {}, "source": ["## Correlations"]}, {"cell_type": "code", "execution_count": 1, "id": "852cbfd5", "metadata": {}, "outputs": [], "source": ["# Find correlations with the target and sort\ncorrelations = app_train.corr()['TARGET'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))"]}, {"cell_type": "code", "execution_count": 1, "id": "4eab437a", "metadata": {}, "outputs": [], "source": ["app_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])"]}, {"cell_type": "code", "execution_count": 1, "id": "7303df54", "metadata": {}, "outputs": [], "source": ["# Extract the EXT_SOURCE variables and show correlations\next_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\next_data_corrs = ext_data.corr()\next_data_corrs"]}, {"cell_type": "code", "execution_count": 1, "id": "89342b64", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize = (8, 6))\n# Heatmap of correlations\nsns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap');"]}, {"cell_type": "markdown", "id": "cd9630f0", "metadata": {}, "source": ["## Feature engineering"]}, {"cell_type": "markdown", "id": "449e29cd", "metadata": {}, "source": ["<font size=\"4\">Manual features. </font>"]}, {"cell_type": "code", "execution_count": 1, "id": "6eb81fc6", "metadata": {}, "outputs": [], "source": ["manual_features=app_train[['SK_ID_CURR','AMT_CREDIT','AMT_ANNUITY','AMT_INCOME_TOTAL',\n                           'DAYS_BIRTH','DAYS_EMPLOYED']]\nmanual_features_test=app_test[['SK_ID_CURR','AMT_CREDIT','AMT_ANNUITY','AMT_INCOME_TOTAL',\n                           'DAYS_BIRTH','DAYS_EMPLOYED']]"]}, {"cell_type": "code", "execution_count": 1, "id": "bac90356", "metadata": {}, "outputs": [], "source": ["cols=list(manual_features.columns)\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(strategy = 'median')\nmanual_features = imputer.fit_transform(manual_features)\nmanual_features_test = imputer.transform(manual_features_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "8ec3df45", "metadata": {}, "outputs": [], "source": ["manual_features=pd.DataFrame(manual_features,columns=cols)\nmanual_features_test=pd.DataFrame(manual_features_test,columns=cols)"]}, {"cell_type": "code", "execution_count": 1, "id": "fb8dd518", "metadata": {}, "outputs": [], "source": ["manual_features['CREDIT_INCOME_PERCENT'] = manual_features['AMT_CREDIT'] /manual_features['AMT_INCOME_TOTAL']\nmanual_features['ANNUITY_INCOME_PERCENT'] = manual_features['AMT_ANNUITY'] / manual_features['AMT_INCOME_TOTAL']\nmanual_features['CREDIT_TERM'] = manual_features['AMT_ANNUITY'] / manual_features['AMT_CREDIT']\nmanual_features['DAYS_EMPLOYED_PERCENT'] = manual_features['DAYS_EMPLOYED'] / manual_features['DAYS_BIRTH']"]}, {"cell_type": "code", "execution_count": 1, "id": "2624d014", "metadata": {}, "outputs": [], "source": ["manual_features_test['CREDIT_INCOME_PERCENT'] = manual_features_test['AMT_CREDIT'] / manual_features_test['AMT_INCOME_TOTAL']\nmanual_features_test['ANNUITY_INCOME_PERCENT'] = manual_features_test['AMT_ANNUITY'] / manual_features_test['AMT_INCOME_TOTAL']\nmanual_features_test['CREDIT_TERM'] = manual_features_test['AMT_ANNUITY'] / manual_features_test['AMT_CREDIT']\nmanual_features_test['DAYS_EMPLOYED_PERCENT'] = manual_features_test['DAYS_EMPLOYED'] / manual_features_test['DAYS_BIRTH']"]}, {"cell_type": "markdown", "id": "42401531", "metadata": {}, "source": ["<font size=\"4\">Polynomial features & Imputation of missing values. </font>"]}, {"cell_type": "code", "execution_count": 1, "id": "11faa971", "metadata": {}, "outputs": [], "source": ["poly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\npoly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(strategy = 'median')\n\npoly_target = poly_features['TARGET']\npoly_features = poly_features.drop(columns = ['TARGET'])\n\npoly_features = imputer.fit_transform(poly_features)\npoly_features_test = imputer.transform(poly_features_test)\n\nfrom sklearn.preprocessing import PolynomialFeatures\n                                \npoly_transformer = PolynomialFeatures(degree = 3)"]}, {"cell_type": "code", "execution_count": 1, "id": "db14caf6", "metadata": {}, "outputs": [], "source": ["poly_transformer.fit(poly_features)\n\npoly_features = poly_transformer.transform(poly_features)\npoly_features_test = poly_transformer.transform(poly_features_test)\nprint('Polynomial Features shape: ', poly_features.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "5755206a", "metadata": {}, "outputs": [], "source": ["poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])"]}, {"cell_type": "code", "execution_count": 1, "id": "25a2e0f9", "metadata": {}, "outputs": [], "source": ["poly_features = pd.DataFrame(poly_features, \n                             columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\npoly_features['TARGET'] = poly_target\n\npoly_corrs = poly_features.corr()['TARGET'].sort_values()\n\nprint(poly_corrs.head(10))\nprint(poly_corrs.tail(5))"]}, {"cell_type": "code", "execution_count": 1, "id": "32c7f6b1", "metadata": {}, "outputs": [], "source": ["poly_features_test = pd.DataFrame(poly_features_test, \n                                  columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n\npoly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\napp_train_poly_1 = app_train.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\napp_train_poly_3=app_train_poly_1.merge(manual_features,on = 'SK_ID_CURR', how = 'left')\n\npoly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\napp_test_poly_2 = app_test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')\napp_test_poly_4=app_test_poly_2.merge(manual_features_test,on = 'SK_ID_CURR', how = 'left')\n\napp_train_poly, app_test_poly = app_train_poly_3.align(app_test_poly_4, join = 'inner', axis = 1)\n\nprint('Training data with polynomial features shape: ', app_train_poly.shape)\nprint('Testing data with polynomial features shape:  ', app_test_poly.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "bf811156", "metadata": {}, "outputs": [], "source": ["app_train_poly=app_train_poly_3.T.drop_duplicates().T"]}, {"cell_type": "code", "execution_count": 1, "id": "7cad77ef", "metadata": {}, "outputs": [], "source": ["app_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)"]}, {"cell_type": "code", "execution_count": 1, "id": "d51b55c9", "metadata": {}, "outputs": [], "source": ["app_train_poly.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "93a67960", "metadata": {}, "outputs": [], "source": ["app_train_poly.isnull().sum()"]}, {"cell_type": "code", "execution_count": 1, "id": "75220863", "metadata": {}, "outputs": [], "source": ["col_2=list(app_train_poly.columns)"]}, {"cell_type": "code", "execution_count": 1, "id": "4cf37ce9", "metadata": {}, "outputs": [], "source": ["print(app_train_poly.shape,app_test_poly.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "db928d83", "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import Imputer\n\nif 'TARGET' in app_train_poly:\n    train = app_train_poly.drop(columns = ['TARGET'])\nelse:\n    train = app_train_poly.copy()\n\nfeatures = list(train.columns)\n\ntest = app_test_poly.copy()\n\nimputer = Imputer(strategy = 'median')\n\nimputer.fit(train)\n\ntrain = imputer.transform(train)\ntest = imputer.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "6fad002d", "metadata": {}, "outputs": [], "source": ["train=pd.DataFrame(train,columns=col_2)\ntest=pd.DataFrame(test,columns=col_2)"]}, {"cell_type": "code", "execution_count": 1, "id": "38f10312", "metadata": {}, "outputs": [], "source": ["train['TARGET']=app_train['TARGET']"]}, {"cell_type": "code", "execution_count": 1, "id": "f2fd0e13", "metadata": {}, "outputs": [], "source": ["train.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "b951cc28", "metadata": {}, "outputs": [], "source": ["#train_last=train.copy()\n#test_last=test.copy()"]}, {"cell_type": "markdown", "id": "66f03674", "metadata": {}, "source": ["## LightGBM"]}, {"cell_type": "code", "execution_count": 1, "id": "08c83c93", "metadata": {}, "outputs": [], "source": ["import lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nMAX_EVALS = 5"]}, {"cell_type": "code", "execution_count": 1, "id": "7037373e", "metadata": {}, "outputs": [], "source": ["features=train.sample(n=16000,random_state=42)"]}, {"cell_type": "code", "execution_count": 1, "id": "b108dcf2", "metadata": {}, "outputs": [], "source": ["print(features.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "72b260c9", "metadata": {}, "outputs": [], "source": ["labels = np.array(features['TARGET'])\nfeatures = features.drop(columns = ['TARGET', 'SK_ID_CURR'])\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 6000, random_state = 50)"]}, {"cell_type": "code", "execution_count": 1, "id": "a82839ac", "metadata": {}, "outputs": [], "source": ["print(\"Training features shape: \", train_features.shape)\nprint(\"Testing features shape: \", test_features.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "21fe8274", "metadata": {}, "outputs": [], "source": ["train_set = lgb.Dataset(data = train_features, label = train_labels)\ntest_set = lgb.Dataset(data = test_features, label = test_labels)"]}, {"cell_type": "code", "execution_count": 1, "id": "9a8f0497", "metadata": {}, "outputs": [], "source": ["model = lgb.LGBMClassifier()\ndefault_params = model.get_params()\n\n# Remove the number of estimators because we set this to 10000 in the cv call\ndel default_params['n_estimators']\n\n# Cross validation with early stopping\ncv_results = lgb.cv(default_params, train_set, num_boost_round = 10000, early_stopping_rounds = 100, \n                    metrics = 'auc', nfold = 5, seed = 42)"]}, {"cell_type": "code", "execution_count": 1, "id": "d9ef0f07", "metadata": {}, "outputs": [], "source": ["print(max(cv_results['auc-mean']))"]}, {"cell_type": "code", "execution_count": 1, "id": "11096d33", "metadata": {}, "outputs": [], "source": ["print(len(cv_results['auc-mean']))"]}, {"cell_type": "code", "execution_count": 1, "id": "48de05fd", "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import roc_auc_score"]}, {"cell_type": "code", "execution_count": 1, "id": "f6655472", "metadata": {}, "outputs": [], "source": ["model.n_estimators = len(cv_results['auc-mean'])\n# Train and make predicions with model\nmodel.fit(train_features, train_labels)\npreds = model.predict_proba(test_features)[:, 1]\nbaseline_auc = roc_auc_score(test_labels, preds)\nprint('The baseline model scores {:.5f} ROC AUC on the test set.'.format(baseline_auc))"]}, {"cell_type": "code", "execution_count": 1, "id": "fa302116", "metadata": {}, "outputs": [], "source": ["def objective(hyperparameters, iteration):\n    \"\"\"Objective function for grid and random search. Returns\n       the cross validation score from a set of hyperparameters.\"\"\"\n    \n    # Number of estimators will be found using early stopping\n    if 'n_estimators' in hyperparameters.keys():\n        del hyperparameters['n_estimators']   \n     # Perform n_folds cross validation\n    cv_results = lgb.cv(hyperparameters, train_set, num_boost_round = 10000, nfold =5, \n                        early_stopping_rounds = 100, metrics = 'auc', seed = 42)\n    # results to retun\n    score = cv_results['auc-mean'][-1]\n    estimators = len(cv_results['auc-mean'])\n    hyperparameters['n_estimators'] = estimators     \n    return [score, hyperparameters, iteration]"]}, {"cell_type": "code", "execution_count": 1, "id": "1487f21f", "metadata": {}, "outputs": [], "source": ["score, params, iteration = objective(default_params, 1)\n\nprint('The cross-validation ROC AUC was {:.5f}.'.format(score))"]}, {"cell_type": "code", "execution_count": 1, "id": "fba89f08", "metadata": {}, "outputs": [], "source": ["model = lgb.LGBMModel()\nmodel.get_params()"]}, {"cell_type": "markdown", "id": "740c3d90", "metadata": {}, "source": ["## Grid search"]}, {"cell_type": "code", "execution_count": 1, "id": "a79aeb17", "metadata": {}, "outputs": [], "source": ["param_grid = {\n    'boosting_type': ['gbdt', 'goss', 'dart'],\n    'num_leaves': list(range(20, 150)),\n    'learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 1000)),\n    'subsample_for_bin': list(range(20000, 300000, 20000)),\n    'min_child_samples': list(range(20, 500, 5)),\n    'reg_alpha': list(np.linspace(0, 1)),\n    'reg_lambda': list(np.linspace(0, 1)),\n    'colsample_bytree': list(np.linspace(0.6, 1, 10)),\n    'subsample': list(np.linspace(0.5, 1, 100)),\n    'is_unbalance': [True, False]\n}"]}, {"cell_type": "code", "execution_count": 1, "id": "7aaf24d9", "metadata": {}, "outputs": [], "source": ["import random\n\nrandom.seed(50)\n\n# Randomly sample a boosting type\nboosting_type = random.sample(param_grid['boosting_type'], 1)[0]\n\n# Set subsample depending on boosting type\nsubsample = 1.0 if boosting_type == 'goss' else random.sample(param_grid['subsample'], 1)[0]\n\nprint('Boosting type: ', boosting_type)\nprint('Subsample ratio: ', subsample)"]}, {"cell_type": "code", "execution_count": 1, "id": "e7cbb815", "metadata": {}, "outputs": [], "source": ["random_results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                              index = list(range(MAX_EVALS)))\n\ngrid_results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                              index = list(range(MAX_EVALS)))"]}, {"cell_type": "code", "execution_count": 1, "id": "40ee9d2c", "metadata": {}, "outputs": [], "source": ["import itertools\n\ndef grid_search(param_grid, max_evals = MAX_EVALS):\n    \"\"\"Grid search algorithm (with limit on max evals)\"\"\"\n    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                              index = list(range(MAX_EVALS)))\n    keys, values = zip(*param_grid.items())    \n    i = 0\n    for v in itertools.product(*values):\n        hyperparameters = dict(zip(keys, v))\n        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']\n        eval_results = objective(hyperparameters, i)       \n        results.loc[i, :] = eval_results\n        i += 1\n        if i > MAX_EVALS:\n            break\n    results.sort_values('score', ascending = False, inplace = True)\n    results.reset_index(inplace = True)    \n    return results    "]}, {"cell_type": "code", "execution_count": 1, "id": "07f6be11", "metadata": {}, "outputs": [], "source": ["grid_results = grid_search(param_grid)\nprint('The best validation score was {:.5f}'.format(grid_results.loc[0, 'score']))\nprint('\\nThe best hyperparameters were:')\nimport pprint\npprint.pprint(grid_results.loc[0, 'params'])"]}, {"cell_type": "code", "execution_count": 1, "id": "d8d319d9", "metadata": {}, "outputs": [], "source": ["grid_search_params = grid_results.loc[0, 'params']\nmodel = lgb.LGBMClassifier(**grid_search_params, random_state=42)\nmodel.fit(train_features, train_labels)\n\npreds = model.predict_proba(test_features)[:, 1]\n\nprint('The best model from grid search scores {:.5f} ROC AUC on the test set.'.format(roc_auc_score(test_labels, preds)))"]}, {"cell_type": "markdown", "id": "934498ea", "metadata": {}, "source": ["## Random search"]}, {"cell_type": "code", "execution_count": 1, "id": "8087e743", "metadata": {}, "outputs": [], "source": ["random.seed(50)\nrandom_params = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\nrandom_params['subsample'] = 1.0 if random_params['boosting_type'] == 'goss' else random_params['subsample']"]}, {"cell_type": "code", "execution_count": 1, "id": "fb908af7", "metadata": {}, "outputs": [], "source": ["def random_search(param_grid, max_evals = MAX_EVALS):\n   \n    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                                 index = list(range(MAX_EVALS)))\n    for i in range(MAX_EVALS):\n        hyperparameters = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']\n        eval_results = objective(hyperparameters, i)\n        results.loc[i, :] = eval_results\n        \n    results.sort_values('score', ascending = False, inplace = True)\n    results.reset_index(inplace = True)\n    return results "]}, {"cell_type": "code", "execution_count": 1, "id": "d4820f46", "metadata": {}, "outputs": [], "source": ["random_results = random_search(param_grid)\n\nprint('The best validation score was {:.5f}'.format(random_results.loc[0, 'score']))\nprint('\\nThe best hyperparameters were:')\n\nimport pprint\npprint.pprint(random_results.loc[0, 'params'])"]}, {"cell_type": "code", "execution_count": 1, "id": "b9e47d1c", "metadata": {}, "outputs": [], "source": ["random_search_params = random_results.loc[0, 'params']\n\nmodel = lgb.LGBMClassifier(**random_search_params, random_state = 42)\nmodel.fit(train_features, train_labels)\n\npreds = model.predict_proba(test_features)[:, 1]\n\nprint('The best model from random search scores {:.5f} ROC AUC on the test set.'.format(roc_auc_score(test_labels, preds)))"]}, {"cell_type": "code", "execution_count": 1, "id": "476ab5f9", "metadata": {}, "outputs": [], "source": ["#train = pd.read_csv('../input/application_train.csv')\n#test = pd.read_csv('../input/application_test.csv')\n\n# Extract the test ids and train labels\n#test_ids = test['SK_ID_CURR']\n#train_labels = np.array(train['TARGET'].astype(np.int32)).reshape((-1, ))\n\n#train = train.drop(columns = ['SK_ID_CURR', 'TARGET'])\n#test = test.drop(columns = ['SK_ID_CURR'])\n\n#print('Training shape: ', train.shape)\n#print('Testing shape: ', test.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "e7efff3a", "metadata": {}, "outputs": [], "source": ["#le = LabelEncoder()\n#le_count = 0\n#for col in train:\n#    if train[col].dtype == 'object':\n#        if len(list(train[col].unique())) <= 2:\n#            le.fit(train[col])\n#            train[col] = le.transform(train[col])\n#            test[col] = le.transform(test[col])\n#            le_count += 1\n#print('%d columns were label encoded.' % le_count)"]}, {"cell_type": "code", "execution_count": 1, "id": "e73fcdd6", "metadata": {}, "outputs": [], "source": ["#train = pd.get_dummies(train)\n#test=pd.get_dummies(test)\n#print(train.shape,test.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "59a11ca1", "metadata": {}, "outputs": [], "source": ["\n# Align the training and testing data, keep only columns present in both dataframes\n#train, test = train.align(test, join = 'inner', axis = 1)\n\n# Add the target back in\n#train['TARGET'] = train_labels\n\n#print('Training Features shape: ', train.shape)\n#print('Testing Features shape: ', test.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "241ffb59", "metadata": {}, "outputs": [], "source": ["#train=train.select_dtypes('number')\n#test=test.select_dtypes('number')"]}, {"cell_type": "code", "execution_count": 1, "id": "7274fb17", "metadata": {}, "outputs": [], "source": ["train_set = lgb.Dataset(train, label = train_labels_last)\n\nhyperparameters = dict(**random_results.loc[0, 'params'])\ndel hyperparameters['n_estimators']\n\n# Cross validation with n_folds and early stopping\ncv_results = lgb.cv(hyperparameters, train_set,\n                    num_boost_round = 10000, early_stopping_rounds = 100, \n                    metrics = 'auc', nfold = 5)\nprint(max(cv_results['auc-mean']))"]}, {"cell_type": "code", "execution_count": 1, "id": "4c70a185", "metadata": {}, "outputs": [], "source": ["test['SK_ID_CURR'].describe()"]}, {"cell_type": "code", "execution_count": 1, "id": "3c9e0e02", "metadata": {}, "outputs": [], "source": ["test_ids = test['SK_ID_CURR']"]}, {"cell_type": "code", "execution_count": 1, "id": "0d088d38", "metadata": {}, "outputs": [], "source": ["train=train.drop(columns=['TARGET','SK_ID_CURR'])\ntest=test.drop(columns='SK_ID_CURR')"]}, {"cell_type": "code", "execution_count": 1, "id": "378ae7a9", "metadata": {}, "outputs": [], "source": ["print(train.shape,test.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "22ea3e81", "metadata": {}, "outputs": [], "source": ["model = lgb.LGBMClassifier(n_estimators = len(cv_results['auc-mean']), **hyperparameters)\nmodel.fit(train, train_labels_last)\n                        \n# Predictions on the test data\npreds = model.predict_proba(test)[:, 1]\n#auc1=roc_auc_score(y_test, preds)\n#print(auc1)"]}, {"cell_type": "code", "execution_count": 1, "id": "20e64d91", "metadata": {}, "outputs": [], "source": ["#submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': preds})\n#submission['SK_ID_CURR']=submission['SK_ID_CURR'].astype('int32')\n#submission.to_csv('submission_simple_features_random.csv', index = False)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}