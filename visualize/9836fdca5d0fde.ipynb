{"cells": [{"cell_type": "code", "execution_count": 1, "id": "b129d29b", "metadata": {}, "outputs": [], "source": ["import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# import useful tools\nfrom glob import glob\nfrom PIL import Image\nimport cv2\n\n# import data visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport seaborn as sns\n\nfrom bokeh.plotting import figure\nfrom bokeh.io import output_notebook, show, output_file\nfrom bokeh.models import ColumnDataSource, HoverTool, Panel\nfrom bokeh.models.widgets import Tabs\n\n# import data augmentation\nimport albumentations as albu"]}, {"cell_type": "markdown", "id": "cbdd77fa", "metadata": {}, "source": ["![image](https://raw.githubusercontent.com/Lexie88rus/GlobalWheatDetection/master/wheat_image_cropped.png)"]}, {"cell_type": "markdown", "id": "215a537f", "metadata": {}, "source": ["# Global Wheat Detection Competition EDA"]}, {"cell_type": "markdown", "id": "b875adc8", "metadata": {}, "source": ["Detection of wheat spikes with computer vision opens a lot of opportunity for the farmers and breeders like:\n* controlling the growth stage of the plants on the field: the number oand the area of spikes is raising closer to the harvest date;\n* controlling the heatlth of the plants: unusually small number or small size of plants might be a signal of deceased plants;\n* spikes density characteristic and approximate yield estimation for different varieties of wheat.\n\nIn this notebook I am exploring the data and giving some thoughts on what to pay attention when making and validating the models."]}, {"cell_type": "markdown", "id": "2dda02d8", "metadata": {}, "source": ["## General Dataset Information"]}, {"cell_type": "markdown", "id": "573c55b9", "metadata": {}, "source": ["Let's just look at the numbers first. Those numbers will give us a hint which images might be interesting to look at."]}, {"cell_type": "markdown", "id": "ecdefda1", "metadata": {}, "source": ["`1` The number of train and test images:"]}, {"cell_type": "code", "execution_count": 1, "id": "9f042060", "metadata": {}, "outputs": [], "source": ["# Setup the paths to train and test images\nTRAIN_DIR = '../input/global-wheat-detection/train/'\nTEST_DIR = '../input/global-wheat-detection/test/'\nTRAIN_CSV_PATH = '../input/global-wheat-detection/train.csv'\n\n# Glob the directories and get the lists of train and test images\ntrain_fns = glob(TRAIN_DIR + '*')\ntest_fns = glob(TEST_DIR + '*')"]}, {"cell_type": "markdown", "id": "b8b46250", "metadata": {}, "source": ["Compute at the number of train and test images:"]}, {"cell_type": "code", "execution_count": 1, "id": "f8bef15b", "metadata": {}, "outputs": [], "source": ["print('Number of train images is {}'.format(len(train_fns)))\nprint('Number of test images is {}'.format(len(test_fns)))"]}, {"cell_type": "markdown", "id": "66793ade", "metadata": {}, "source": ["We have only 10 test images here, other test images will be used to evaluate the prediction models during the submission.\n\nJust `3422` images for the training seems to be not much at all. Data augmentation techniques will be definetely required in this competition."]}, {"cell_type": "markdown", "id": "013e1167", "metadata": {}, "source": ["`2` The number of bounding boxes (wheat spikes) per image:"]}, {"cell_type": "markdown", "id": "bd7014bc", "metadata": {}, "source": ["Construct dataframe with all images (images with no bboxes will have nan values in all columns except `image_id`):"]}, {"cell_type": "code", "execution_count": 1, "id": "f60ea498", "metadata": {}, "outputs": [], "source": ["# Load the dataframe with the bounding boxes\ntrain = pd.read_csv(TRAIN_CSV_PATH)\n\n# Create a dataframe with all train images\nall_train_images = pd.DataFrame([fns.split('/')[-1][:-4] for fns in train_fns])\nall_train_images.columns=['image_id']\n\n# Merge all train images with the bounding boxes dataframe\nall_train_images = all_train_images.merge(train, on='image_id', how='left')\n\n# replace nan values with zeros\nall_train_images['bbox'] = all_train_images.bbox.fillna('[0,0,0,0]')\n\n# split bbox column\nbbox_items = all_train_images.bbox.str.split(',', expand=True)\nall_train_images['bbox_xmin'] = bbox_items[0].str.strip('[ ').astype(float)\nall_train_images['bbox_ymin'] = bbox_items[1].str.strip(' ').astype(float)\nall_train_images['bbox_width'] = bbox_items[2].str.strip(' ').astype(float)\nall_train_images['bbox_height'] = bbox_items[3].str.strip(' ]').astype(float)"]}, {"cell_type": "code", "execution_count": 1, "id": "55b2ff73", "metadata": {}, "outputs": [], "source": ["print('{} images without wheat heads.'.format(len(all_train_images) - len(train)))"]}, {"cell_type": "markdown", "id": "b9219307", "metadata": {}, "source": ["Let's plot some image examples:"]}, {"cell_type": "code", "execution_count": 1, "id": "861b025d", "metadata": {}, "outputs": [], "source": ["def get_all_bboxes(df, image_id):\n    image_bboxes = df[df.image_id == image_id]\n    \n    bboxes = []\n    for _,row in image_bboxes.iterrows():\n        bboxes.append((row.bbox_xmin, row.bbox_ymin, row.bbox_width, row.bbox_height))\n        \n    return bboxes\n\ndef plot_image_examples(df, rows=3, cols=3, title='Image examples'):\n    fig, axs = plt.subplots(rows, cols, figsize=(10,10))\n    for row in range(rows):\n        for col in range(cols):\n            idx = np.random.randint(len(df), size=1)[0]\n            img_id = df.iloc[idx].image_id\n            \n            img = Image.open(TRAIN_DIR + img_id + '.jpg')\n            axs[row, col].imshow(img)\n            \n            bboxes = get_all_bboxes(df, img_id)\n            \n            for bbox in bboxes:\n                rect = patches.Rectangle((bbox[0],bbox[1]),bbox[2],bbox[3],linewidth=1,edgecolor='r',facecolor='none')\n                axs[row, col].add_patch(rect)\n            \n            axs[row, col].axis('off')\n            \n    plt.suptitle(title)"]}, {"cell_type": "code", "execution_count": 1, "id": "625c647b", "metadata": {}, "outputs": [], "source": ["plot_image_examples(all_train_images)"]}, {"cell_type": "markdown", "id": "95b0c270", "metadata": {}, "source": ["We can see images taken at different lighting conditions and plant maturity stages!"]}, {"cell_type": "markdown", "id": "c58dcf81", "metadata": {}, "source": ["Count number of bounding boxes per image:"]}, {"cell_type": "code", "execution_count": 1, "id": "b105b1df", "metadata": {}, "outputs": [], "source": ["# compute the number of bounding boxes per train image\nall_train_images['count'] = all_train_images.apply(lambda row: 1 if np.isfinite(row.width) else 0, axis=1)\ntrain_images_count = all_train_images.groupby('image_id').sum().reset_index()"]}, {"cell_type": "code", "execution_count": 1, "id": "92ea6ff8", "metadata": {}, "outputs": [], "source": ["# See this article on how to plot bar charts with Bokeh:\n# https://towardsdatascience.com/interactive-histograms-with-bokeh-202b522265f3\ndef hist_hover(dataframe, column, colors=[\"#94c8d8\", \"#ea5e51\"], bins=30, title=''):\n    hist, edges = np.histogram(dataframe[column], bins = bins)\n    \n    hist_df = pd.DataFrame({column: hist,\n                             \"left\": edges[:-1],\n                             \"right\": edges[1:]})\n    hist_df[\"interval\"] = [\"%d to %d\" % (left, right) for left, \n                           right in zip(hist_df[\"left\"], hist_df[\"right\"])]\n\n    src = ColumnDataSource(hist_df)\n    plot = figure(plot_height = 400, plot_width = 600,\n          title = title,\n          x_axis_label = column,\n          y_axis_label = \"Count\")    \n    plot.quad(bottom = 0, top = column,left = \"left\", \n        right = \"right\", source = src, fill_color = colors[0], \n        line_color = \"#35838d\", fill_alpha = 0.7,\n        hover_fill_alpha = 0.7, hover_fill_color = colors[1])\n        \n    hover = HoverTool(tooltips = [('Interval', '@interval'),\n                              ('Count', str(\"@\" + column))])\n    plot.add_tools(hover)\n    \n    output_notebook()\n    show(plot)"]}, {"cell_type": "code", "execution_count": 1, "id": "3d0d794a", "metadata": {}, "outputs": [], "source": ["hist_hover(train_images_count, 'count', title='Number of wheat spikes per image')"]}, {"cell_type": "markdown", "id": "19e2d035", "metadata": {}, "source": ["Most of the images have 20-50 wheat spikes on them."]}, {"cell_type": "markdown", "id": "471c5fef", "metadata": {}, "source": ["Let's plot some examples with small number of spikes per image:"]}, {"cell_type": "code", "execution_count": 1, "id": "532d4215", "metadata": {}, "outputs": [], "source": ["less_spikes_ids = train_images_count[train_images_count['count'] < 10].image_id\nplot_image_examples(all_train_images[all_train_images.image_id.isin(less_spikes_ids)], title='Example images with small number of spikes')"]}, {"cell_type": "markdown", "id": "a19c2dd5", "metadata": {}, "source": ["These are some very strange examples:\n* on some of the images all we see is the ground;\n* some of the images are just zoomed in a lot."]}, {"cell_type": "markdown", "id": "999941dd", "metadata": {}, "source": ["Plot the images with many spikes:"]}, {"cell_type": "code", "execution_count": 1, "id": "0404623f", "metadata": {}, "outputs": [], "source": ["many_spikes_ids = train_images_count[train_images_count['count'] > 100].image_id\nplot_image_examples(all_train_images[all_train_images.image_id.isin(many_spikes_ids)], title='Example images with large number of spikes')"]}, {"cell_type": "markdown", "id": "4906e5b3", "metadata": {}, "source": ["These actually look much better than those with small number of spikes."]}, {"cell_type": "markdown", "id": "76ba3ae4", "metadata": {}, "source": ["`3` Area of bounding boxes:"]}, {"cell_type": "code", "execution_count": 1, "id": "8570a768", "metadata": {}, "outputs": [], "source": ["# compute bounding box areas\nall_train_images['bbox_area'] = all_train_images['bbox_width'] * all_train_images['bbox_height']"]}, {"cell_type": "code", "execution_count": 1, "id": "a44f8d19", "metadata": {}, "outputs": [], "source": ["# plot a histogram of bounding box areas\nhist_hover(all_train_images, 'bbox_area', title='Area of a single bounding box')"]}, {"cell_type": "markdown", "id": "bcc86a1c", "metadata": {}, "source": ["The maximum area of bounding box:"]}, {"cell_type": "code", "execution_count": 1, "id": "6944c3b0", "metadata": {}, "outputs": [], "source": ["all_train_images.bbox_area.max()"]}, {"cell_type": "markdown", "id": "b37f1104", "metadata": {}, "source": ["The distribution of individual areas of bounding boxes has a very long tail. It would be interesting to look at the images with those large bounding boxes."]}, {"cell_type": "markdown", "id": "cf043c00", "metadata": {}, "source": ["Let's plot some examples of large bounding boxes:"]}, {"cell_type": "code", "execution_count": 1, "id": "8aca312f", "metadata": {}, "outputs": [], "source": ["large_boxes_ids = all_train_images[all_train_images['bbox_area'] > 200000].image_id\nplot_image_examples(all_train_images[all_train_images.image_id.isin(large_boxes_ids)], title='Example images with large bbox area')"]}, {"cell_type": "markdown", "id": "9ebd1880", "metadata": {}, "source": ["__What are these anomally large bounding boxes?? I think those should be removed while training!!!__"]}, {"cell_type": "markdown", "id": "4836eb31", "metadata": {}, "source": ["Similarly, let's look at very small bounding boxes:"]}, {"cell_type": "code", "execution_count": 1, "id": "459a6f5f", "metadata": {}, "outputs": [], "source": ["min_area = all_train_images[all_train_images['bbox_area'] > 0].bbox_area.min()\nprint('The smallest bounding box area is {}'.format(min_area))"]}, {"cell_type": "code", "execution_count": 1, "id": "5fa37d12", "metadata": {}, "outputs": [], "source": ["small_boxes_ids = all_train_images[(all_train_images['bbox_area'] < 50) & (all_train_images['bbox_area'] > 0)].image_id\nplot_image_examples(all_train_images[all_train_images.image_id.isin(small_boxes_ids)], title='Example images with large bbox area')"]}, {"cell_type": "markdown", "id": "8cc9f0ba", "metadata": {}, "source": ["If you look very close, you can probably see those tinyest bounding boxes near the corners and borders of the images. Probably, the boundries were drawn first, than the images were cut into several ones. That is why we see those strange small bounsing boxes in the corners.\n\nIt is not necessary to clean these, because they won't have much effect on the IOU metric."]}, {"cell_type": "markdown", "id": "07142f36", "metadata": {}, "source": ["`4` Area of bounding boxes per image:"]}, {"cell_type": "code", "execution_count": 1, "id": "a7b85b4c", "metadata": {}, "outputs": [], "source": ["# compute the total bounding boxes area per image\narea_per_image = all_train_images.groupby(by='image_id').sum().reset_index()\n\n# compute the percentage of the image area covered by bounding boxes\narea_per_image_percentage = area_per_image.copy()\narea_per_image_percentage['bbox_area'] = area_per_image_percentage['bbox_area'] / (1024*1024) * 100"]}, {"cell_type": "code", "execution_count": 1, "id": "46a9aac9", "metadata": {}, "outputs": [], "source": ["hist_hover(area_per_image_percentage, 'bbox_area', title='Percentage of image area covered by bounding boxes')"]}, {"cell_type": "markdown", "id": "a05293b6", "metadata": {}, "source": ["This looks like a nice normal distribution! 20-40% of the image area is covered by the bounding boxes.\n\nThis observation can be used to validate the predictions of the resulting model. The percentage of predicted bounding boxes area should be normally distributed too.\n\nWe can also see that the maximum is actually greater than 100%. This means that the bounding boxes are overlapping."]}, {"cell_type": "markdown", "id": "45d14916", "metadata": {}, "source": ["Let's look at some examples of images with small areas covered by bounding boxes:"]}, {"cell_type": "code", "execution_count": 1, "id": "a481b95a", "metadata": {}, "outputs": [], "source": ["small_area_perc_ids = area_per_image_percentage[area_per_image_percentage['bbox_area'] < 7].image_id\nplot_image_examples(all_train_images[all_train_images.image_id.isin(small_area_perc_ids)], title='Example images with small percentage of area covered by bounding boxes')"]}, {"cell_type": "markdown", "id": "d7bd64ee", "metadata": {}, "source": ["Now let's plot the images with large areas covered by bounding boxes:"]}, {"cell_type": "code", "execution_count": 1, "id": "bf9a6a73", "metadata": {}, "outputs": [], "source": ["large_area_perc_ids = area_per_image_percentage[area_per_image_percentage['bbox_area'] > 95].image_id\nplot_image_examples(all_train_images[all_train_images.image_id.isin(large_area_perc_ids)], title='Example images with large percentage of area covered by bounding boxes')"]}, {"cell_type": "markdown", "id": "16977bad", "metadata": {}, "source": ["`5` The brightness of the images:"]}, {"cell_type": "code", "execution_count": 1, "id": "002d0e23", "metadata": {}, "outputs": [], "source": ["def get_image_brightness(image):\n    # convert to grayscale\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    \n    # get average brightness\n    return np.array(gray).mean()\n\ndef add_brightness(df):\n    brightness = []\n    for _, row in df.iterrows():\n        img_id = row.image_id  \n        image = cv2.imread(TRAIN_DIR + img_id + '.jpg')\n        brightness.append(get_image_brightness(image))\n        \n    brightness_df = pd.DataFrame(brightness)\n    brightness_df.columns = ['brightness']\n    df = pd.concat([df, brightness_df], ignore_index=True, axis=1)\n    df.columns = ['image_id', 'brightness']\n    \n    return df"]}, {"cell_type": "code", "execution_count": 1, "id": "690814b3", "metadata": {}, "outputs": [], "source": ["images_df = pd.DataFrame(all_train_images.image_id.unique())\nimages_df.columns = ['image_id']"]}, {"cell_type": "code", "execution_count": 1, "id": "c9d5230d", "metadata": {}, "outputs": [], "source": ["# add brightness to the dataframe\nimages_df = pd.DataFrame(all_train_images.image_id.unique())\nimages_df.columns = ['image_id']\nbrightness_df = add_brightness(images_df)\n\nall_train_images = all_train_images.merge(brightness_df, on='image_id')"]}, {"cell_type": "code", "execution_count": 1, "id": "289c7686", "metadata": {}, "outputs": [], "source": ["hist_hover(all_train_images, 'brightness', title='Images brightness distribution')"]}, {"cell_type": "markdown", "id": "f1cd1708", "metadata": {}, "source": ["Plot the darkest images:"]}, {"cell_type": "code", "execution_count": 1, "id": "200ffdfc", "metadata": {}, "outputs": [], "source": ["dark_ids = all_train_images[all_train_images['brightness'] < 30].image_id\nplot_image_examples(all_train_images[all_train_images.image_id.isin(dark_ids)], title='Darkest images')"]}, {"cell_type": "markdown", "id": "939ce361", "metadata": {}, "source": ["On some of the images, it is even hard for human to see the spikes!"]}, {"cell_type": "markdown", "id": "fd4b162f", "metadata": {}, "source": ["Plot the brightest images:"]}, {"cell_type": "code", "execution_count": 1, "id": "9738cfce", "metadata": {}, "outputs": [], "source": ["bright_ids = all_train_images[all_train_images['brightness'] > 130].image_id\nplot_image_examples(all_train_images[all_train_images.image_id.isin(bright_ids)], title='Brightest images')"]}, {"cell_type": "markdown", "id": "1db7a2ce", "metadata": {}, "source": ["These are very different from the dark ones. Some filters needed here to make the spikes more clear.\n\nI also see the missing boundaries."]}, {"cell_type": "markdown", "id": "b3284273", "metadata": {}, "source": ["`6` The most and the least green and yellow images:"]}, {"cell_type": "markdown", "id": "98d662dd", "metadata": {}, "source": ["I would like to plot images with different dominant colors. The idea is that the most green images will represent healthy plants. The most yellow images will contain plants close to maturity. The most brown images will have ground on them."]}, {"cell_type": "code", "execution_count": 1, "id": "081ed696", "metadata": {}, "outputs": [], "source": ["def get_percentage_of_green_pixels(image):\n    # convert to HSV\n    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n    \n    # get the green mask\n    hsv_lower = (40, 40, 40) \n    hsv_higher = (70, 255, 255)\n    green_mask = cv2.inRange(hsv, hsv_lower, hsv_higher)\n    \n    return float(np.sum(green_mask)) / 255 / (1024 * 1024)\n\ndef get_percentage_of_yellow_pixels(image):\n    # convert to HSV\n    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n    \n    # get the green mask\n    hsv_lower = (25, 40, 40) \n    hsv_higher = (35, 255, 255)\n    yellow_mask = cv2.inRange(hsv, hsv_lower, hsv_higher)\n    \n    return float(np.sum(yellow_mask)) / 255 / (1024 * 1024)\n\ndef add_green_pixels_percentage(df):\n    green = []\n    for _, row in df.iterrows():\n        img_id = row.image_id  \n        image = cv2.imread(TRAIN_DIR + img_id + '.jpg')\n        green.append(get_percentage_of_green_pixels(image))\n        \n    green_df = pd.DataFrame(green)\n    green_df.columns = ['green_pixels']\n    df = pd.concat([df, green_df], ignore_index=True, axis=1)\n    df.columns = ['image_id', 'green_pixels']\n    \n    return df\n\ndef add_yellow_pixels_percentage(df):\n    yellow = []\n    for _, row in df.iterrows():\n        img_id = row.image_id  \n        image = cv2.imread(TRAIN_DIR + img_id + '.jpg')\n        yellow.append(get_percentage_of_yellow_pixels(image))\n        \n    yellow_df = pd.DataFrame(yellow)\n    yellow_df.columns = ['yellow_pixels']\n    df = pd.concat([df, yellow_df], ignore_index=True, axis=1)\n    df.columns = ['image_id', 'yellow_pixels']\n    \n    return df"]}, {"cell_type": "code", "execution_count": 1, "id": "03ba1fad", "metadata": {}, "outputs": [], "source": ["# add a column with the percentage of green pixels\ngreen_pixels_df = add_green_pixels_percentage(images_df)\nall_train_images = all_train_images.merge(green_pixels_df, on='image_id')"]}, {"cell_type": "code", "execution_count": 1, "id": "dac67782", "metadata": {}, "outputs": [], "source": ["hist_hover(all_train_images, 'green_pixels', title='Percentage of green pixels distribution', colors=['#c3ea84', '#3e7a17'])"]}, {"cell_type": "markdown", "id": "ef3f2d8c", "metadata": {}, "source": ["Surprisingly, the most green images have only aoung 60% green pixels.\n\nThe majority of the images are not green at all! Most probably, they are more yellow and are the images of the plants close to harvest."]}, {"cell_type": "markdown", "id": "0d63322c", "metadata": {}, "source": ["The most green images:"]}, {"cell_type": "code", "execution_count": 1, "id": "7e415d26", "metadata": {}, "outputs": [], "source": ["green_ids = all_train_images[all_train_images['green_pixels'] > 0.55].image_id\nplot_image_examples(all_train_images[all_train_images.image_id.isin(green_ids)], title='The most green images')"]}, {"cell_type": "markdown", "id": "e90e4533", "metadata": {}, "source": ["The most green images mostly contain the plants with very small spikes, which are just starting to appear."]}, {"cell_type": "code", "execution_count": 1, "id": "c6deab11", "metadata": {}, "outputs": [], "source": ["# add a column with the percentage of yellow pixels\nyellow_pixels_df = add_yellow_pixels_percentage(images_df)\nall_train_images = all_train_images.merge(yellow_pixels_df, on='image_id')"]}, {"cell_type": "code", "execution_count": 1, "id": "2da2de4b", "metadata": {}, "outputs": [], "source": ["hist_hover(all_train_images, 'yellow_pixels', title='Percentage of yellow pixels distribution', colors=['#fffedb', '#fffeab'])"]}, {"cell_type": "markdown", "id": "01b528ee", "metadata": {}, "source": ["Let's look at the most yellow images:"]}, {"cell_type": "code", "execution_count": 1, "id": "e6b33fe4", "metadata": {}, "outputs": [], "source": ["yellow_ids = all_train_images[all_train_images['yellow_pixels'] > 0.55].image_id\nplot_image_examples(all_train_images[all_train_images.image_id.isin(yellow_ids)], title='The most yellow images')"]}, {"cell_type": "markdown", "id": "3e24eaa2", "metadata": {}, "source": ["## Thoughts on Data Augmentation\n\nData augmentation is critical in this competition, because there is a relatively small training set. Data augmentation will allow to build robust models under given circumstances.\nWhat augmentations/filers might work:\n* flipping images horizontally and vertically, because the orientation of original images is different;\n* crop-resize, because we can see spikes at different zoom levels;\n* different filters to adjust the lighting conditions. I suggest looking at [this competition](https://www.kaggle.com/c/aptos2019-blindness-detection) for example.\n\nWhat to do with caution:\n* rotation might not work, because rotation messes up the bounding boxes."]}, {"cell_type": "markdown", "id": "4d031c85", "metadata": {}, "source": ["Example augmentation pipeline:"]}, {"cell_type": "code", "execution_count": 1, "id": "1fa7cab3", "metadata": {}, "outputs": [], "source": ["# setup an example augmentation pipeline\n# be sure to use bbox safe functions for data augmentation\nexample_transforms = albu.Compose([\n    albu.RandomSizedBBoxSafeCrop(512, 512, erosion_rate=0.0, interpolation=1, p=1.0),\n    albu.HorizontalFlip(p=0.5),\n    albu.VerticalFlip(p=0.5),\n    albu.OneOf([albu.RandomContrast(),\n                albu.RandomGamma(),\n                albu.RandomBrightness()], p=1.0),\n    albu.CLAHE(p=1.0)], p=1.0, bbox_params=albu.BboxParams(format='coco', label_fields=['category_id']))"]}, {"cell_type": "code", "execution_count": 1, "id": "87b87f4e", "metadata": {}, "outputs": [], "source": ["def apply_transforms(transforms, df, n_transforms=3):\n    idx = np.random.randint(len(df), size=1)[0]\n    \n    image_id = df.iloc[idx].image_id\n    bboxes = []\n    for _, row in df[df.image_id == image_id].iterrows():\n        bboxes.append([row.bbox_xmin, row.bbox_ymin, row.bbox_width, row.bbox_height])\n        \n    image = Image.open(TRAIN_DIR + image_id + '.jpg')\n    \n    fig, axs = plt.subplots(1, n_transforms+1, figsize=(15,7))\n    \n    # plot the original image\n    axs[0].imshow(image)\n    axs[0].set_title('original')\n    for bbox in bboxes:\n        rect = patches.Rectangle((bbox[0],bbox[1]),bbox[2],bbox[3],linewidth=1,edgecolor='r',facecolor='none')\n        axs[0].add_patch(rect)\n    \n    # apply transforms n_transforms times\n    for i in range(n_transforms):\n        params = {'image': np.asarray(image),\n                  'bboxes': bboxes,\n                  'category_id': [1 for j in range(len(bboxes))]}\n        augmented_boxes = transforms(**params)\n        bboxes_aug = augmented_boxes['bboxes']\n        image_aug = augmented_boxes['image']\n\n        # plot the augmented image and augmented bounding boxes\n        axs[i+1].imshow(image_aug)\n        axs[i+1].set_title('augmented_' + str(i+1))\n        for bbox in bboxes_aug:\n            rect = patches.Rectangle((bbox[0],bbox[1]),bbox[2],bbox[3],linewidth=1,edgecolor='r',facecolor='none')\n            axs[i+1].add_patch(rect)\n    plt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "3a0501b9", "metadata": {}, "outputs": [], "source": ["apply_transforms(example_transforms, all_train_images, n_transforms=3)"]}, {"cell_type": "code", "execution_count": 1, "id": "70754e7d", "metadata": {}, "outputs": [], "source": ["apply_transforms(example_transforms, all_train_images, n_transforms=3)"]}, {"cell_type": "markdown", "id": "6ed3695d", "metadata": {}, "source": ["Note how CLAHE emphasizes the features in darker spots. It is a must have for this competition."]}, {"cell_type": "markdown", "id": "d8a3fffa", "metadata": {}, "source": ["## Conclusions:\n\n1. Images are taken at different zoom levels. Crop and resize data augmentations to be used for model training.\n2. Images are taken at various lighting conditions. Special filters should be used to address that.\n3. Bounding boxes are messy! \n    * Giant bounding boxes should be filtered out by area and removed before model training.\n    * Micro bounding boxes. These can stay. They won't have much effect on the IOU metric.\n    * Some spikes are not surrounded by a bounding box (missing bounding boxes)."]}, {"cell_type": "markdown", "id": "6a0d94af", "metadata": {}, "source": ["## Related Papers and Links\nI found some interesting papers and articles:\n1. [Deep learning based banana plant detection and counting using high-resolution red-green-blue (RGB) images collected from unmanned aerial vehicle (UAV)](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0223906). The objective of this research is focused to automate the count of banana plants during early stage of growth, in which they are prone to lose maturity.\n2. [Fine-grained recognition of plants from images](https://plantmethods.biomedcentral.com/articles/10.1186/s13007-017-0265-4). Fine-grained recognition of plants from images is a challenging computer vision task, due to the diverse appearance and complex structure of plants, high intra-class variability and small inter-class differences. We review the state-of-the-art and discuss plant recognition tasks, from identification of plants from specific plant organs to general plant recognition \u201cin the wild\u201d.\n3. [Review of deep learning algorithms for object detection](https://medium.com/zylapp/review-of-deep-learning-algorithms-for-object-detection-c1f3d437b852)\n4. [Fast Detection Models](https://lilianweng.github.io/lil-log/2018/12/27/object-detection-part-4.html)\n5. [SSD Tutorial](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection), [SSD implementation in pytorch](https://github.com/qfgaohao/pytorch-ssd)\n6. [EfficientDet](https://arxiv.org/pdf/1911.09070.pdf)\n7. [CutMix paper](https://arxiv.org/pdf/1905.04899.pdf)"]}, {"cell_type": "markdown", "id": "3804ec53", "metadata": {}, "source": ["## References & Credits\n1. [Article on how to plot bar charts with Bokeh](https://towardsdatascience.com/interactive-histograms-with-bokeh-202b522265f3)"]}, {"cell_type": "markdown", "id": "f41c04fd", "metadata": {}, "source": ["__This EDA is being updated. Your questions and comments are very welcome!__"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}