{"cells": [{"cell_type": "markdown", "id": "734543e3", "metadata": {}, "source": ["### Table of Contents\n\n* [Load Libraries](#chapter1)\n* [Load the data](#chapter2)\n* [Cleaning and Preparing the data](#chapter3)\n* [Exploratory Data Analysis](#chapter4)\n* [Correlation Matrix](#chapter5)\n* [Diagnosis vs Features](#chapter6)\n* [Outlier Detection](#chapter7)\n* [Drop Outliers](#chapter8)\n* [Creating Test and Train Dataset](#chapter9)\n* [Standardization](#chapter10)\n* [Classification and Build a Model](#chapter11)\n* [Logistic Regression](#chapter12)\n* [Decision Tree](#chapter13)\n* [Random Forest](#chapter14)\n* [Conclusion](#chapter15)"]}, {"cell_type": "markdown", "id": "f64262b7", "metadata": {}, "source": ["### Load Libraries <a class=\"anchor\" id=\"chapter1\"></a>"]}, {"cell_type": "code", "execution_count": 1, "id": "acc49513", "metadata": {}, "outputs": [], "source": ["import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.colors import ListedColormap\n%matplotlib inline \n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis, LocalOutlierFactor\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn import metrics\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")"]}, {"cell_type": "markdown", "id": "ff00622b", "metadata": {}, "source": ["### Load the data <a class=\"anchor\" id=\"chapter2\"></a>"]}, {"cell_type": "code", "execution_count": 1, "id": "1ef5030a", "metadata": {}, "outputs": [], "source": ["data = pd.read_csv(\"../input/breast-cancer-wisconsin-data/data.csv\")"]}, {"cell_type": "code", "execution_count": 1, "id": "9dfe819c", "metadata": {}, "outputs": [], "source": ["data.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "c5caf9dc", "metadata": {}, "outputs": [], "source": ["data.info()"]}, {"cell_type": "markdown", "id": "3242a5ab", "metadata": {}, "source": ["### Cleaning and Preparing the data <a class=\"anchor\" id=\"chapter3\"></a>"]}, {"cell_type": "markdown", "id": "bf5f1265", "metadata": {}, "source": ["Since we don't need the *id* column and the empty *Unnamed: 32* column in the dataset, we remove them."]}, {"cell_type": "code", "execution_count": 1, "id": "a8dfacad", "metadata": {}, "outputs": [], "source": ["data.drop(\"id\", axis=1, inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "a4465196", "metadata": {}, "outputs": [], "source": ["data.drop('Unnamed: 32', axis=1, inplace=True)"]}, {"cell_type": "markdown", "id": "b656a2c3", "metadata": {}, "source": ["Let's look at the values of the column we are going to predict"]}, {"cell_type": "code", "execution_count": 1, "id": "5cb4ffc9", "metadata": {}, "outputs": [], "source": ["data.diagnosis.unique()"]}, {"cell_type": "markdown", "id": "0ab0d8cb", "metadata": {}, "source": ["Columns are initials of Malignant and Bening results. These values are not the kind our algorithms will understand. For this, we will define Malignant as 1 and Bening as 0."]}, {"cell_type": "code", "execution_count": 1, "id": "58be55ee", "metadata": {}, "outputs": [], "source": ["data.diagnosis = data['diagnosis'].map({'M':1, 'B':0})"]}, {"cell_type": "markdown", "id": "ffc0661a", "metadata": {}, "source": ["### Exploratory Data Analysis <a class=\"anchor\" id=\"chapter4\"></a>"]}, {"cell_type": "markdown", "id": "8632b478", "metadata": {}, "source": ["Let's look at the statistical information of our dataset"]}, {"cell_type": "code", "execution_count": 1, "id": "81231988", "metadata": {}, "outputs": [], "source": ["data.describe()"]}, {"cell_type": "markdown", "id": "e621065d", "metadata": {}, "source": ["Let's look at the density of our target value"]}, {"cell_type": "code", "execution_count": 1, "id": "8df3a00f", "metadata": {}, "outputs": [], "source": ["sns.countplot(data[\"diagnosis\"])"]}, {"cell_type": "markdown", "id": "3c5c5dee", "metadata": {}, "source": ["On average, 350 of our samples have benign tumors and 200 have malignant tumors."]}, {"cell_type": "markdown", "id": "7d3c2943", "metadata": {}, "source": ["### Correlation Matrix <a class=\"anchor\" id=\"chapter5\"></a>"]}, {"cell_type": "code", "execution_count": 1, "id": "392ceafa", "metadata": {}, "outputs": [], "source": ["f, ax = plt.subplots(figsize = (20,20))\nsns.heatmap(data.corr(), annot=True, fmt='.1f',\n            ax=ax, cmap='coolwarm', vmin=-1, vmax=1)\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.title('Correlation Map', size=14);"]}, {"cell_type": "markdown", "id": "e70b9c67", "metadata": {}, "source": ["Let's plot the traits with a correlation greater than 0.75"]}, {"cell_type": "code", "execution_count": 1, "id": "5cb081cf", "metadata": {}, "outputs": [], "source": ["flt = np.abs(data.corr()['diagnosis']) > .75"]}, {"cell_type": "code", "execution_count": 1, "id": "04b00c7e", "metadata": {}, "outputs": [], "source": ["corr_feat = data.corr().columns[flt].tolist()"]}, {"cell_type": "code", "execution_count": 1, "id": "1ee8017d", "metadata": {}, "outputs": [], "source": ["f, ax = plt.subplots(figsize = (8,8))\nsns.heatmap(data[corr_feat].corr(), annot=True, fmt='.2f',\n           ax=ax, cmap='coolwarm',vmin=-1,vmax=1)\nplt.xticks(rotation=60)\nplt.yticks(rotation=0)\nplt.title('Correlation Between Features (Th>0.75)');"]}, {"cell_type": "markdown", "id": "257a45df", "metadata": {}, "source": ["Here we see the 5 features that are most correlated with each other. One of them is our target value. This is good"]}, {"cell_type": "markdown", "id": "2ea2054c", "metadata": {}, "source": ["Now let's look at the distribution of our values"]}, {"cell_type": "code", "execution_count": 1, "id": "da549d95", "metadata": {}, "outputs": [], "source": ["def melt(dataset, param):\n    data_melted = pd.melt(dataset, id_vars=param,\n                     var_name=\"features\",\n                     value_name=\"value\")\n    return data_melted\n\ndef boxplot(dataset, param):\n    plt.figure(figsize= (14,8))\n    sns.boxplot(x=\"features\", y=\"value\", hue=param, data=dataset)\n    plt.xticks(rotation = 90)\n    return plt.show()\n\ndef pairplot(dataset, param):\n    sns.pairplot(dataset, diag_kind='kde', markers='+', hue=param);"]}, {"cell_type": "code", "execution_count": 1, "id": "59843173", "metadata": {}, "outputs": [], "source": ["boxplot(melt(data,\"diagnosis\"),\"diagnosis\")"]}, {"cell_type": "markdown", "id": "09702e65", "metadata": {}, "source": ["It looks like there are outliers. If we remove them and standardize our values, we get more accurate results."]}, {"cell_type": "code", "execution_count": 1, "id": "e99eea91", "metadata": {}, "outputs": [], "source": ["pairplot(data[corr_feat],\"diagnosis\")"]}, {"cell_type": "markdown", "id": "292d00ef", "metadata": {}, "source": ["When we plot the highly correlated features by classifying them according to the target value, we see that the data with benign tumors and data with malignant tumors are grouped in separate values. In other words, the fact that one of the features we are comparing has a high value causes the other to have a high value. The analysis we will do in this way will not give exactly correct results. Because when the value of one of the features with high correlation increases, the other increases as well."]}, {"cell_type": "markdown", "id": "f95cf62f", "metadata": {}, "source": ["### Diagnosis vs Features <a class=\"anchor\" id=\"chapter6\"></a>"]}, {"cell_type": "markdown", "id": "8753352f", "metadata": {}, "source": ["As a result, if we divide our data into patients and non-patients, our prediction model will give more accurate results."]}, {"cell_type": "code", "execution_count": 1, "id": "888672ba", "metadata": {}, "outputs": [], "source": ["feat = list(data.columns[1:11])\ndataM = data[data.diagnosis ==1]\ndataB = data[data.diagnosis ==0]"]}, {"cell_type": "markdown", "id": "ac419dea", "metadata": {}, "source": ["Let's plot the distribution of the values in the features by those who are sick and those who are not."]}, {"cell_type": "code", "execution_count": 1, "id": "852e592d", "metadata": {}, "outputs": [], "source": ["plt.rcParams.update({'font.size': 8})\nf, axes = plt.subplots(nrows=5, ncols=2, figsize=(8,10))\naxes = axes.ravel()\nfor i,ax in enumerate(axes):\n    ax.figure\n    binwidth= (max(data[feat[i]]) - min(data[feat[i]]))/50\n    ax.hist([dataM[feat[i]],dataB[feat[i]]], \n            bins=np.arange(min(data[feat[i]]), max(data[feat[i]]) + binwidth, binwidth), \n            alpha=0.5,stacked=True, density=True, label=['M','B'],color=['r','g'])\n    ax.legend(loc='upper right')\n    ax.set_title(feat[i])\nplt.tight_layout()\nplt.show()"]}, {"cell_type": "markdown", "id": "80797ac1", "metadata": {}, "source": ["* Unlike the above, the mean values of cell *radius*, *perimeter*, *area*, *compactness*, *concavity* and *concave points* can be used in classification of the cancer. Larger values of these parameters tends to show a correlation with malignant tumors."]}, {"cell_type": "markdown", "id": "84460bdc", "metadata": {}, "source": ["* mean values of texture, smoothness, symmetry or fractual dimension does not show a particular preference of one diagnosis over the other. In any of the histograms there are no noticeable large outliers that warrants further cleanup."]}, {"cell_type": "markdown", "id": "da4f9dda", "metadata": {}, "source": ["### Outlier Detection <a class=\"anchor\" id=\"chapter7\"></a>"]}, {"cell_type": "markdown", "id": "4838efda", "metadata": {}, "source": ["Let's detect outliers."]}, {"cell_type": "code", "execution_count": 1, "id": "dd2ffdec", "metadata": {}, "outputs": [], "source": ["X = data.drop(['diagnosis'], axis=1)\ny = data.diagnosis"]}, {"cell_type": "code", "execution_count": 1, "id": "bed9119a", "metadata": {}, "outputs": [], "source": ["column = X.columns.tolist()"]}, {"cell_type": "code", "execution_count": 1, "id": "2313c656", "metadata": {}, "outputs": [], "source": ["LOF = LocalOutlierFactor()\ny_pred = LOF.fit_predict(X)\nX_score = LOF.negative_outlier_factor_"]}, {"cell_type": "code", "execution_count": 1, "id": "4a06d836", "metadata": {}, "outputs": [], "source": ["outlier_score = pd.DataFrame()\noutlier_score['score'] = X_score\noutlier_score.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "576e3196", "metadata": {}, "outputs": [], "source": ["radius = (X_score.max() - X_score) / (X_score.max() - X_score.min())\noutlier_score[\"radius\"] = radius\nfilt = outlier_score[\"score\"] < -2.5\noutlier_index = outlier_score[filt].index.tolist()\nplt.figure(figsize = (12,8))\nplt.scatter(X.iloc[outlier_index,0], X.iloc[outlier_index,1], \n            color = \"blue\", s = 50, label = \"Outliers\" )\nplt.scatter(X.iloc[:,0], X.iloc[:,1], color =\"k\", s=3, label = \"Data Points\" )\nplt.scatter(X.iloc[:,0],X.iloc[:,1], s=1000*radius, edgecolor = \"r\", \n            facecolors = \"none\", label=\"Outlier Scores\")\nplt.legend()\nplt.show()"]}, {"cell_type": "markdown", "id": "5c312734", "metadata": {}, "source": ["We see outliers. Let's remove them now"]}, {"cell_type": "markdown", "id": "c4ca8e4d", "metadata": {}, "source": ["### Drop Outliers <a class=\"anchor\" id=\"chapter8\"></a>"]}, {"cell_type": "code", "execution_count": 1, "id": "4c19d38d", "metadata": {}, "outputs": [], "source": ["X = X.drop(outlier_index)\ny = y.drop(outlier_index).values"]}, {"cell_type": "markdown", "id": "1abc07d8", "metadata": {}, "source": ["### Creating Test and Train Dataset <a class=\"anchor\" id=\"chapter9\"></a>"]}, {"cell_type": "markdown", "id": "e4b8a26e", "metadata": {}, "source": ["Since this data set is not ordered, I am going to do a simple 70:30 split to create a training data set and a test data set."]}, {"cell_type": "code", "execution_count": 1, "id": "eb3dd2de", "metadata": {}, "outputs": [], "source": ["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"]}, {"cell_type": "markdown", "id": "a8756d96", "metadata": {}, "source": ["### Standardization <a class=\"anchor\" id=\"chapter10\"></a>"]}, {"cell_type": "markdown", "id": "59cba85e", "metadata": {}, "source": ["In order for our model to give more accurate results, we need to standardize our data."]}, {"cell_type": "code", "execution_count": 1, "id": "85ed886b", "metadata": {}, "outputs": [], "source": ["scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nX_train_df = pd.DataFrame(X_train, columns= column)"]}, {"cell_type": "code", "execution_count": 1, "id": "190e2072", "metadata": {}, "outputs": [], "source": ["X_train_df.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "5c468928", "metadata": {}, "outputs": [], "source": ["X_train_df.describe()"]}, {"cell_type": "code", "execution_count": 1, "id": "a9ed77cb", "metadata": {}, "outputs": [], "source": ["df = X_train_df"]}, {"cell_type": "code", "execution_count": 1, "id": "0d36617e", "metadata": {}, "outputs": [], "source": ["df['diagnosis'] = y_train"]}, {"cell_type": "code", "execution_count": 1, "id": "a9c21438", "metadata": {}, "outputs": [], "source": ["boxplot(melt(df,'diagnosis'),'diagnosis')"]}, {"cell_type": "markdown", "id": "84f3bb12", "metadata": {}, "source": ["looks better."]}, {"cell_type": "code", "execution_count": 1, "id": "9ed1b9ef", "metadata": {}, "outputs": [], "source": ["pairplot(df[corr_feat],'diagnosis')"]}, {"cell_type": "markdown", "id": "9cda281e", "metadata": {}, "source": ["### Classification and Build a Model <a class=\"anchor\" id=\"chapter11\"></a>"]}, {"cell_type": "markdown", "id": "4734da4d", "metadata": {}, "source": ["Here we are going to build a classification model and evaluate its performance using the training set."]}, {"cell_type": "code", "execution_count": 1, "id": "e45cb304", "metadata": {}, "outputs": [], "source": ["def classification_and_fit_model(model, data, predictors, outcome):\n    model.fit(data[predictors], data[outcome])\n    predictions = model.predict(data[predictors])\n    accuracy =metrics.accuracy_score(predictions, data[outcome])\n    print('Accuracy : %s' % '{0:.3%}'.format(accuracy))\n    kf = KFold(n_splits=5)\n    error = []\n    for train, test in kf.split(data):\n        train_predictors = data[predictors].iloc[train,:]\n        train_target = data[outcome].iloc[train]\n        model.fit(train_predictors, train_target)\n        error.append(model.score(data[predictors].iloc[test,:],\n                                data[outcome].iloc[test]))\n        print('Cross-Validation Score : %s' % '{0:.3%}'.format(np.mean(error)))\n    model.fit(data[predictors],data[outcome])\n        "]}, {"cell_type": "markdown", "id": "cea4d38b", "metadata": {}, "source": ["### Logistic Regression <a class=\"anchor\" id=\"chapter12\"></a>"]}, {"cell_type": "markdown", "id": "f39138c8", "metadata": {}, "source": ["* Based on the observations in the histogram plots, we can reasonably hypothesize that the cancer diagnosis depends on the mean cell radius, mean perimeter, mean area, mean compactness, mean concavity and mean concave points. We can then perform a logistic regression analysis using those features as follows:"]}, {"cell_type": "code", "execution_count": 1, "id": "02890628", "metadata": {}, "outputs": [], "source": ["predictor = ['radius_mean', 'perimeter_mean', 'area_mean', 'compactness_mean', 'concave points_mean']"]}, {"cell_type": "code", "execution_count": 1, "id": "58ddde57", "metadata": {}, "outputs": [], "source": ["outcome = 'diagnosis'"]}, {"cell_type": "code", "execution_count": 1, "id": "ab66039c", "metadata": {}, "outputs": [], "source": ["model = LogisticRegression()"]}, {"cell_type": "code", "execution_count": 1, "id": "2a8bbb72", "metadata": {}, "outputs": [], "source": ["classification_and_fit_model(model, X_train_df, predictor, outcome)"]}, {"cell_type": "markdown", "id": "cadee4ab", "metadata": {}, "source": ["The prediction accuracy is good. What happens if we use just one predictor? Use the mean_radius:"]}, {"cell_type": "code", "execution_count": 1, "id": "a1bb704e", "metadata": {}, "outputs": [], "source": ["predictor1 = ['radius_mean']"]}, {"cell_type": "code", "execution_count": 1, "id": "b0b997c1", "metadata": {}, "outputs": [], "source": ["classification_and_fit_model(model, X_train_df, predictor1, outcome)"]}, {"cell_type": "markdown", "id": "31912bbd", "metadata": {}, "source": ["This gives a similar prediction accuracy and a cross-validation score."]}, {"cell_type": "markdown", "id": "fb6d6882", "metadata": {}, "source": ["The accuracy of the predictions are good but not great. The cross-validation scores are reasonable. Can we do better with another model?"]}, {"cell_type": "markdown", "id": "399a3b88", "metadata": {}, "source": ["### Decision Tree <a class=\"anchor\" id=\"chapter13\"></a>"]}, {"cell_type": "code", "execution_count": 1, "id": "334aa346", "metadata": {}, "outputs": [], "source": ["model = DecisionTreeClassifier()"]}, {"cell_type": "code", "execution_count": 1, "id": "eae37265", "metadata": {}, "outputs": [], "source": ["classification_and_fit_model(model, X_train_df, predictor, outcome)"]}, {"cell_type": "markdown", "id": "e929a0c1", "metadata": {}, "source": ["Here we are over-fitting the model probably due to the large number of predictors. Let use a single predictor, the obvious one is the radius of the cell."]}, {"cell_type": "code", "execution_count": 1, "id": "c8b56331", "metadata": {}, "outputs": [], "source": ["classification_and_fit_model(model, X_train_df, predictor1, outcome)"]}, {"cell_type": "markdown", "id": "06889311", "metadata": {}, "source": ["The accuracy of the prediction is much much better here. But does it depend on the predictor?"]}, {"cell_type": "markdown", "id": "f4990406", "metadata": {}, "source": ["Using a single predictor gives a 97% prediction accuracy for this model but the cross-validation score is not that great."]}, {"cell_type": "markdown", "id": "78d341e7", "metadata": {}, "source": ["### Random Forest <a class=\"anchor\" id=\"chapter14\"></a>"]}, {"cell_type": "code", "execution_count": 1, "id": "f2d585a1", "metadata": {}, "outputs": [], "source": ["features_mean = list(X_train_df.columns[1:11])"]}, {"cell_type": "code", "execution_count": 1, "id": "6127d83b", "metadata": {}, "outputs": [], "source": ["model = RandomForestClassifier(n_estimators=100, min_samples_split=25, max_depth=7, max_features=2)"]}, {"cell_type": "code", "execution_count": 1, "id": "bd014172", "metadata": {}, "outputs": [], "source": ["classification_and_fit_model(model, X_train_df, features_mean, outcome)"]}, {"cell_type": "markdown", "id": "c90eb6a5", "metadata": {}, "source": ["Using all the features improves the prediction accuracy and the cross-validation score is great."]}, {"cell_type": "markdown", "id": "6a03bd55", "metadata": {}, "source": ["An advantage with Random Forest is that it returns a feature importance matrix which can be used to select features. So lets select the top 5 features and use them as predictors"]}, {"cell_type": "code", "execution_count": 1, "id": "b92ca31d", "metadata": {}, "outputs": [], "source": ["feature_importance = pd.Series(model.feature_importances_, index=features_mean).sort_values(ascending=False)\nfeature_importance"]}, {"cell_type": "markdown", "id": "2c1c3c9a", "metadata": {}, "source": ["Using top 5 features"]}, {"cell_type": "code", "execution_count": 1, "id": "e027a128", "metadata": {}, "outputs": [], "source": ["model = RandomForestClassifier(n_estimators=100, min_samples_split=25, max_depth=7, max_features=2)"]}, {"cell_type": "code", "execution_count": 1, "id": "466772bb", "metadata": {}, "outputs": [], "source": ["classification_and_fit_model(model, X_train_df, predictor, outcome)"]}, {"cell_type": "markdown", "id": "44e2ad2e", "metadata": {}, "source": ["Using the top 5 features only changes the prediction accuracy a bit but I think we get a better result if we use all the predictors."]}, {"cell_type": "markdown", "id": "236da477", "metadata": {}, "source": ["What happens if we use a single predictor as before? Just check."]}, {"cell_type": "code", "execution_count": 1, "id": "bd103df5", "metadata": {}, "outputs": [], "source": ["model = RandomForestClassifier(n_estimators=100)"]}, {"cell_type": "code", "execution_count": 1, "id": "dfee688c", "metadata": {}, "outputs": [], "source": ["classification_and_fit_model(model, X_train_df, predictor1, outcome)"]}, {"cell_type": "markdown", "id": "1edaa7b8", "metadata": {}, "source": ["This gives a better prediction accuracy too but the cross-validation is not great"]}, {"cell_type": "markdown", "id": "53b864e8", "metadata": {}, "source": ["### Conclusion <a class=\"anchor\" id=\"chapter15\"></a>"]}, {"cell_type": "markdown", "id": "bda87cfc", "metadata": {}, "source": ["The best model to be used for diagnosing breast cancer as found in this analysis is the Random Forest model with the top 5 predictors, 'concave points_mean','area_mean','radius_mean','perimeter_mean','concavity_mean'. It gives a prediction accuracy of ~95% and a cross-validation score ~ 95% for the test data set."]}, {"cell_type": "markdown", "id": "bbebe93c", "metadata": {}, "source": ["I will see if I can improve this more by tweaking the model further and trying out other models in a later version of this analysis."]}, {"cell_type": "markdown", "id": "7d49e389", "metadata": {}, "source": ["#### Thanks for viewing my notebook :)\nI will be very happy if you vote."]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}