{"cells": [{"cell_type": "markdown", "id": "c928b23e", "metadata": {}, "source": ["#Iris Datset"]}, {"cell_type": "code", "execution_count": 1, "id": "1d99dea7", "metadata": {}, "outputs": [], "source": ["import sys\nimport scipy\nimport numpy\nimport matplotlib\nimport pandas\nimport sklearn\n\nprint('Python: {}'.format(sys.version))\nprint('scipy: {}'.format(scipy.__version__))\nprint('numpy: {}'.format(numpy.__version__))\nprint('matplotlib: {}'.format(matplotlib.__version__))\nprint('pandas: {}'.format(pandas.__version__))\nprint('sklearn: {}'.format(sklearn.__version__))"]}, {"cell_type": "markdown", "id": "f88feb55", "metadata": {}, "source": ["## 1. Import libraries\n\nImport all of the modules, functions, and objects we will use in this tutorial."]}, {"cell_type": "code", "execution_count": 1, "id": "2ab834a6", "metadata": {}, "outputs": [], "source": ["from pandas.plotting import scatter_matrix\nimport matplotlib.pyplot as plt\nfrom sklearn import model_selection\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC"]}, {"cell_type": "markdown", "id": "a51f57fb", "metadata": {}, "source": ["## 2. Load the Dataset\n\nWe will be using the iris flowers dataset, which contains 150 observations of iris flowers. There are four columns of measurements and the species of flower observed.  Only three species are present in this dataset.\n\nThe data can be loaded directly from the UCI Machine Learning Repository"]}, {"cell_type": "code", "execution_count": 1, "id": "417c515f", "metadata": {}, "outputs": [], "source": ["# Load Dataset\ndataset=pandas.read_csv('../input/Iris.csv')"]}, {"cell_type": "markdown", "id": "03994478", "metadata": {}, "source": ["## 2.1 Dataset Properties\n\nLets take a look at the dataset by observing its dimensions, the first few rows of data, a statistical summary of the attributes, and a breakdown of the data by the class variable."]}, {"cell_type": "code", "execution_count": 1, "id": "18260541", "metadata": {}, "outputs": [], "source": ["del dataset['Id']"]}, {"cell_type": "code", "execution_count": 1, "id": "378c2d18", "metadata": {}, "outputs": [], "source": ["# Shape\nprint(dataset.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "40e7df0f", "metadata": {}, "outputs": [], "source": ["# Head\nprint(dataset.head(20))"]}, {"cell_type": "code", "execution_count": 1, "id": "30664ad3", "metadata": {}, "outputs": [], "source": ["# descriptions\nprint(dataset.describe())"]}, {"cell_type": "code", "execution_count": 1, "id": "df587450", "metadata": {}, "outputs": [], "source": ["# class distribution\nprint(dataset.groupby('Species').mean())"]}, {"cell_type": "code", "execution_count": 1, "id": "e37d4563", "metadata": {}, "outputs": [], "source": ["print(dataset.groupby('Species').size())"]}, {"cell_type": "code", "execution_count": 1, "id": "4fd5676e", "metadata": {}, "outputs": [], "source": ["print(dataset.groupby('Species').median())"]}, {"cell_type": "markdown", "id": "12024f9f", "metadata": {}, "source": ["## 2.2 Data Visualizations\n\nLets visualize the data so we can understand the distribution of the input attributes. We will use histograms of each attribute, as well as some multivariate plots so that we can view the interactions between variables."]}, {"cell_type": "code", "execution_count": 1, "id": "f983dddf", "metadata": {}, "outputs": [], "source": ["# histograms\ndataset.hist(color='green')\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "9acad44a", "metadata": {}, "outputs": [], "source": ["# scatter plot matrix\nscatter_matrix(dataset)\nplt.show()"]}, {"cell_type": "markdown", "id": "d3563949", "metadata": {}, "source": ["## 3. Evaluate Algorithms\n\nLets create some models of the data and estimate their accuracy on unseen data.\n\nWe are going to,\n\n* Create a validation dataset\n* Set-up cross validation\n* Build three different models to predict species from flower measurement\n* Select the best model\n\n## 3.1 Create Validation Dataset\n\nLets split the loaded dataset into two.  80% of the data will be used for training, while 20% will be used for validation."]}, {"cell_type": "code", "execution_count": 1, "id": "aff941d3", "metadata": {}, "outputs": [], "source": ["# Split-out validation dataset\narray = dataset.values\nX = array[:,0:4]\nY = array[:,4]\nvalidation_size = 0.30\nseed = 7\nX_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size = validation_size, random_state = seed)"]}, {"cell_type": "markdown", "id": "fd637d4e", "metadata": {}, "source": ["## 3.2 10-fold Cross Validation\n\nThis will split our dataset into 10 parts, train on 9 and test on 1 and repeate for all combinations of train-test splits"]}, {"cell_type": "code", "execution_count": 1, "id": "3e31da31", "metadata": {}, "outputs": [], "source": ["# Test options and evaluation metric\nseed = 7\nscoring = 'accuracy'"]}, {"cell_type": "markdown", "id": "9543b9d8", "metadata": {}, "source": ["## 3.3 Build Models\n\nLets evaluate three models:\n\n* Logistic Regression (LR)\n* K-Nearest Neighbors (KNN)\n* Support Vector Machine (SVM)"]}, {"cell_type": "code", "execution_count": 1, "id": "b3dd6e70", "metadata": {}, "outputs": [], "source": ["models = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('SVM', SVC()))\n\n# evaluate each model in turn\nresults = []\nnames = []\n\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=10, random_state = seed)\n    cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)"]}, {"cell_type": "markdown", "id": "e3042ba3", "metadata": {}, "source": ["## 4. Make Predictions\n\nLets test the model on the validation set to make sure that our algorithms can generalize to new data.  Otherwise, we may be overfitting the training data.  "]}, {"cell_type": "code", "execution_count": 1, "id": "5895cd96", "metadata": {}, "outputs": [], "source": ["# Make predictions on validation dataset\n\nfor name, model in models:\n    model.fit(X_train, Y_train)\n    predictions = model.predict(X_validation)\n    print(name)\n    print(accuracy_score(Y_validation, predictions))\n    print(classification_report(Y_validation, predictions))"]}, {"cell_type": "code", "execution_count": 1, "id": "f7b5c8c8", "metadata": {}, "outputs": [], "source": ["# if u find it helpful please upvote the notebook"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}