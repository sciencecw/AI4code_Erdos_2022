{"cells": [{"cell_type": "code", "execution_count": 1, "id": "a57d20dd", "metadata": {}, "outputs": [], "source": ["# Input data files are available in the \"../input/\" directory.\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output."]}, {"cell_type": "markdown", "id": "5bb59480", "metadata": {}, "source": ["The objective of this kernel is to forecast demand for customer orders using an appropriate model. \n\nThere are multiple ways of forecasting demand, with linear models being one of the most popular choice (tried/tested) for data that exhibits a linear trend and/or when the residuals have a normal distribution. \n\nThe following dataset is highly skewed and lacks a normal distribution thus one needs to look at alternate models for building accurate forecasts. Either by transforming the data using box-cox or log transformation which 'normalizes' the data. Alternatively, a model that does not require a linear trend on continuous features can be used too. Some models include the grand dad ARIMA model or the newer ones including facbook's prophet or even deep learning. I will implement them in subsequent commits. For now, ARIMA is done end to end.\n\nI have gone with a tried and tested ARIMA model to forecast the order demand.\n\nComments and suggestions always welcome."]}, {"cell_type": "markdown", "id": "cf282f80", "metadata": {}, "source": ["***STEPS/Work Flow***\n\n* Import data and libaries: Self Explanatory\n* Explore the Dataset: Check Nulls, Skew, Data Types, Univariate and Bivariate Analysis.\n* Explore the Dataset as a Time Series: Min/Max Dates, Check Seasonality, Trends etc.\n* ARIMA Model - Some Theory, Choose Best Params (Grid Seach), Build/Fit, Validation, Forecast Accuracy. \n* The END.\n\nI will try and implement some other models including deep network and Facebook's Prophet in later commits."]}, {"cell_type": "code", "execution_count": 1, "id": "a7a2558c", "metadata": {}, "outputs": [], "source": ["#Import Libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sb\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom scipy.stats import norm, skew #for some statistics\nfrom scipy import stats #qqplot\nimport statsmodels.api as sm #for decomposing the trends, seasonality etc.\n\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX #the big daddy"]}, {"cell_type": "code", "execution_count": 1, "id": "edd908e7", "metadata": {}, "outputs": [], "source": ["#Import the data and parse dates.\ndf  = pd.read_csv('../input/productdemandforecasting/Historical Product Demand.csv', parse_dates=['Date'])"]}, {"cell_type": "code", "execution_count": 1, "id": "3e928555", "metadata": {}, "outputs": [], "source": ["#Don't judge it by the cover.\ndf.head(5)"]}, {"cell_type": "code", "execution_count": 1, "id": "39195d2f", "metadata": {}, "outputs": [], "source": ["#Check the cardinality.\ndf.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "33006229", "metadata": {}, "outputs": [], "source": ["#Check the data types.\ndf.dtypes"]}, {"cell_type": "code", "execution_count": 1, "id": "5989b6fc", "metadata": {}, "outputs": [], "source": ["# Check any number of columns with NaN\nprint(df.isnull().any().sum(), ' / ', len(df.columns))\n# Check any number of data points with NaN\nprint(df.isnull().any(axis=1).sum(), ' / ', len(df))"]}, {"cell_type": "code", "execution_count": 1, "id": "52c65791", "metadata": {}, "outputs": [], "source": ["#Lets check where these nulls are.\nprint (df.isna().sum())\nprint ('Null to Dataset Ratio in Dates: ',df.isnull().sum()[3]/df.shape[0]*100)\n#There are missing values in Dates."]}, {"cell_type": "code", "execution_count": 1, "id": "a542db93", "metadata": {}, "outputs": [], "source": ["#Drop na's.\n\n#Since the number of missing values are about 1%, I am taking an 'executive decision' of removing them. ;) \ndf.dropna(axis=0, inplace=True) #remove all rows with na's.\ndf.reset_index(drop=True)\ndf.sort_values('Date')[10:20] #Some of the values have () in them."]}, {"cell_type": "code", "execution_count": 1, "id": "af3a6623", "metadata": {}, "outputs": [], "source": ["#Target Feature - Order_Demand\n#Removing () from the target feature.\ndf['Order_Demand'] = df['Order_Demand'].str.replace('(',\"\")\ndf['Order_Demand'] = df['Order_Demand'].str.replace(')',\"\")\n\n#Next step is to change the data type.\ndf['Order_Demand'] = df['Order_Demand'].astype('int64')"]}, {"cell_type": "code", "execution_count": 1, "id": "ec317239", "metadata": {}, "outputs": [], "source": ["#Get the lowest and highest dates in the dataset.\ndf['Date'].min() , df['Date'].max()\n#There is data for 6 years. great."]}, {"cell_type": "code", "execution_count": 1, "id": "f971d0b5", "metadata": {}, "outputs": [], "source": ["#Lets start with 2012 and cap it 2016 december. Since the dates before 2012 have a lot of missing values - inspected and checked using basic time series plot.\ndf = df[(df['Date']>='2012-01-01') & (df['Date']<='2016-12-31')].sort_values('Date', ascending=True)"]}, {"cell_type": "markdown", "id": "c53eba3f", "metadata": {}, "source": ["Lets inspect the data to see why a linear model without any transformation would fail to capture the essence of the data."]}, {"cell_type": "code", "execution_count": 1, "id": "ccef5fd9", "metadata": {}, "outputs": [], "source": ["#Lets check the ditribution of the target variable (Order_Demand)\nfrom matplotlib import rcParams\n# figure size in inches\nrcParams['figure.figsize'] = 4,2\n\nsb.distplot(df['Order_Demand'], fit=norm)\n\n#Get the QQ-plot\nfig = plt.figure()\nres = stats.probplot(df['Order_Demand'], plot=plt)\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "920c1f6a", "metadata": {}, "outputs": [], "source": ["#The data is highly skewed, but since we'll be applying ARIMA, it's fine.\ndf['Order_Demand'].skew()"]}, {"cell_type": "code", "execution_count": 1, "id": "f31e3a1d", "metadata": {}, "outputs": [], "source": ["#Just in case if there needs to be some transformation, it can be done by either taking log values or using box cox.\n\n## In case you need to normalize data, use Box Cox. Pick the one that looks MOST like a normal distribution.\n# for i in [1,2,3,4,5,6,7,8]:\n#     plt.hist(df['Order_Demand']**(1/i), bins= 40, normed=False)\n#     plt.title(\"Box Cox transformation: 1/{}\". format(str(i)))\n#     plt.show()"]}, {"cell_type": "markdown", "id": "d66d1cf1", "metadata": {}, "source": ["Univariate Analysis - Warehouse, Product Category.\n"]}, {"cell_type": "code", "execution_count": 1, "id": "0377b822", "metadata": {}, "outputs": [], "source": ["#Warehouse shipping by orders.\ndf['Warehouse'].value_counts().sort_values(ascending = False)"]}, {"cell_type": "code", "execution_count": 1, "id": "a5a10cfb", "metadata": {}, "outputs": [], "source": ["#The amount of orders shipped by each warehouse.\ndf.groupby('Warehouse').sum().sort_values('Order_Demand', ascending = False)\n#Warehouse J is clearly shipping most orders. Although S is shipping more quantity within fewer requested orders."]}, {"cell_type": "code", "execution_count": 1, "id": "53f411e6", "metadata": {}, "outputs": [], "source": ["#Product Category.\n\nprint (len(df['Product_Category'].value_counts()))\n\nrcParams['figure.figsize'] = 50,14\nsb.countplot(df['Product_Category'].sort_values(ascending = True))\n\n#There's a lot of orders on category19."]}, {"cell_type": "markdown", "id": "98e45ee2", "metadata": {}, "source": ["Bivariate Analysis - Warehouse, Product Category with target variable.\n"]}, {"cell_type": "code", "execution_count": 1, "id": "596ab21e", "metadata": {}, "outputs": [], "source": ["#Lets check the orders by warehouse.\n\n#Checking with Boxplots\nfrom matplotlib import rcParams\n# figure size in inches\nrcParams['figure.figsize'] = 16,4\nf, axes = plt.subplots(1, 2)\n#Regular Data\nfig3 = sb.boxplot( df['Warehouse'],df['Order_Demand'], ax = axes[0])\n#Data with Log Transformation\nfig4 = sb.boxplot( df['Warehouse'], np.log1p(df['Order_Demand']),ax = axes[1])\n\ndel fig3, fig4"]}, {"cell_type": "code", "execution_count": 1, "id": "27df56ac", "metadata": {}, "outputs": [], "source": ["#Lets check the Orders by Product Category.\nrcParams['figure.figsize'] = 50,12\n#Taking subset of data temporarily for in memory compute.\ndf_temp = df.sample(n=20000).reset_index()\nfig5 = sb.boxplot( df_temp['Product_Category'].sort_values(),np.log1p(df_temp['Order_Demand']))\ndel df_temp, fig5"]}, {"cell_type": "markdown", "id": "a7c562d9", "metadata": {}, "source": ["**Exploring the Data as TIME SERIES**"]}, {"cell_type": "code", "execution_count": 1, "id": "ef457909", "metadata": {}, "outputs": [], "source": ["df = df.groupby('Date')['Order_Demand'].sum().reset_index()\n#This gives us the total orders placed on each day."]}, {"cell_type": "code", "execution_count": 1, "id": "a8dfca55", "metadata": {}, "outputs": [], "source": ["#Index the date\ndf = df.set_index('Date')\ndf.index #Lets check the index"]}, {"cell_type": "code", "execution_count": 1, "id": "4c22cf60", "metadata": {}, "outputs": [], "source": ["#Averages daily sales value for the month, and we are using the start of each month as the timestamp.\ny = df['Order_Demand'].resample('MS').mean()"]}, {"cell_type": "code", "execution_count": 1, "id": "522be474", "metadata": {}, "outputs": [], "source": ["#In case there are Null values, they can be imputed using bfill.\n#y = y.fillna(y.bfill())"]}, {"cell_type": "code", "execution_count": 1, "id": "cf285f79", "metadata": {}, "outputs": [], "source": ["#Visualizing time series.\n\ny.plot(figsize=(12,5))\nplt.show()\n\n#Takeaway: The sales are always low for the beginning of the year and the highest peak in demand every year is in the\n#last quarter. The observed trend shows that orders were higher during 2014-2016 then reducing.\n\n#Lets check it by decomposition."]}, {"cell_type": "markdown", "id": "e3506a84", "metadata": {}, "source": ["Decomposition is a great way to check the seasonality, trends and residuals."]}, {"cell_type": "code", "execution_count": 1, "id": "285de552", "metadata": {}, "outputs": [], "source": ["#The best part about time series data and decomposition is that you can break down the data into the following:\n#Time Series Decomposition. \nfrom pylab import rcParams\nimport statsmodels.api as sm\nrcParams['figure.figsize'] = 18, 8\ndecomposition = sm.tsa.seasonal_decompose(y, model='additive')\nfig = decomposition.plot()\nplt.show()"]}, {"cell_type": "markdown", "id": "fdcbf2d3", "metadata": {}, "source": ["**Theory on ARIMA**\n\nAn ARIMA model is characterized by 3 terms: p, d, q where these three parameters account for seasonality (p), trend (d), and noise in data (q):\n\n* p is the order of the AR term (number of lags of Y to be used as predictors). If it snowed for the last wee, it is likely it will snow tomorrow.\n* q is the order of the MA term (moving average).  \n* d is the number of differencing required to make the time series stationary. if already stationary d=0.\n* \nBut when dealing with SEASONALITY, it is best to incorporate it as 's'. \nARIMA(p,d,q)(P,D,Q)s.\nWhere 'pdq' are non seasonal params and 's' is the perdiocity of the time series. 4:quarter, 12:yearly etc.\n\nIf a time series, has seasonal patterns, then you need to add seasonal terms and it becomes SARIMA, \nshort for \u2018Seasonal ARIMA\u2019."]}, {"cell_type": "markdown", "id": "494d7393", "metadata": {}, "source": ["**Grid Search**\n\nSince ARIMA has hyper params that can be tuned, the objective here is to find the best params using Grid Search."]}, {"cell_type": "code", "execution_count": 1, "id": "f2cb5ca9", "metadata": {}, "outputs": [], "source": ["#GRID SEARCH for Param Tuning.\n#Sample params for seasonal arima. (SARIMAX).\n\n#For each combination of parameters, we fit a new seasonal ARIMA model with the SARIMAX() function \n#from the statsmodels module and assess its overall quality.\n\nimport itertools\np = d = q = range(0, 2)\npdq = list(itertools.product(p, d, q))\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\nprint('Examples of parameter combinations for Seasonal ARIMA...')\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))"]}, {"cell_type": "code", "execution_count": 1, "id": "baf06c6b", "metadata": {}, "outputs": [], "source": ["#Get the best params for the data. Choose the lowest AIC.\n\n# The Akaike information criterion (AIC) is an estimator of the relative quality of statistical models for a \n# given set of data. \n# AIC measures how well a model fits the data while taking into account the overall complexity of the model.\n# Large AIC: Model fits very well using a lot of features.\n# Small AIC: Model fits similar fit but using lesser features. \n# Hence LOWER THE AIC, the better it is.\n\n#The code tests the given params using sarimax and outputs the AIC scores.\n\nfor param in pdq:\n    for param_seasonal in seasonal_pdq:\n        try:\n            mod = sm.tsa.statespace.SARIMAX(y,\n                                            order=param,\n                                            seasonal_order=param_seasonal,\n                                            enforce_stationarity=False,\n                                            enforce_invertibility=False)\n\n            results = mod.fit()\n\n            print('SARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n        except:\n            continue"]}, {"cell_type": "markdown", "id": "1eb22c98", "metadata": {}, "source": ["Fit the Model"]}, {"cell_type": "code", "execution_count": 1, "id": "96196ba1", "metadata": {}, "outputs": [], "source": ["#Fit the model with the best params.\n#ARIMA(1, 1, 1)x(1, 1, 0, 12)12 - AIC:960.5164122018646\n\n\n#The above output suggests that ARIMA(1, 1, 1)x(1, 1, 0, 12) yields the lowest AIC value: 960.516\n#Therefore we should consider this to be optimal option.\n\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nmod = sm.tsa.statespace.SARIMAX(y,\n                                order=(1, 1, 1),\n                                seasonal_order=(1, 1, 0, 12),\n                                enforce_stationarity=False,\n                                enforce_invertibility=False)\nresults = mod.fit()\nprint(results.summary().tables[1])"]}, {"cell_type": "markdown", "id": "dbf1b831", "metadata": {}, "source": ["Interpreting the table:\n\ncoeff: Shows weight/impotance how each feature impacts the time series.\nPvalue: Shows the significance of each feature weight. Can test hypothesis using this.\nIf p value is <.05 then they are statitically significant.\n\nRefresher on null hyp and pvalues.\nBy default we take the null hyp as 'there is no relationship bw them'\nIf p value < .05 (significance level) then you reject the Null Hypthesis\nIf p value > .05 , then you fail to reject the Null Hypothesis.\n\nSo, if the p-value is < .05 then there is a relationship between the response and predictor. Hence, significant."]}, {"cell_type": "code", "execution_count": 1, "id": "55d7dc92", "metadata": {}, "outputs": [], "source": ["#Plotting the diagnostics.\n\n#The plot_diagnostics object allows us to quickly generate model diagnostics and investigate for any unusual behavior.\nresults.plot_diagnostics(figsize=(16, 8))\nplt.show()\n\n#What to look for?\n#1. Residuals SHOULD be Normally Distributed ; Check\n#Top Right: The (orange colored) KDE line should be closely matched with green colored N(0,1) line. This is the standard notation\n#for normal distribution with mean 0 and sd 1.\n#Bottom Left: The qq plot shows the ordered distribution of residuals (blue dots) follows the linear trend of the samples \n#taken from a standard normal distribution with N(0, 1). \n\n#2. #Residuals are not correlated; Check\n#Top Left: The standard residuals don\u2019t display any obvious seasonality and appear to be white noise. \n#Bottom Right: The autocorrelation (i.e. correlogram) plot on the bottom right, which shows that the time series residuals have \n#low correlation with its own lagged versions.\n"]}, {"cell_type": "markdown", "id": "0433d8a4", "metadata": {}, "source": ["Model Validation"]}, {"cell_type": "code", "execution_count": 1, "id": "f6332ebe", "metadata": {}, "outputs": [], "source": ["#Lets get the predictions and confidence interval for those predictions.\n#Get the predictions. The forecasts start from the 1st of Jan 2017 but the previous line shows how it fits to the data.\npred = results.get_prediction(start=pd.to_datetime('2014-05-01'), dynamic=False) #false is when using the entire history.\n#Confidence interval.\npred_ci = pred.conf_int()\n\n#Plotting real and forecasted values.\nax = y['2013':].plot(label='observed')\npred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7))\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='blue', alpha=.2)\nax.set_xlabel('Date')\nax.set_ylabel('Order_Demand')\nplt.legend()\nplt.show()\n\n#Takeaway: The forecats seems to be fitting well to the data. The Blue/purple thicker plot shows the confidence level in the forecasts. "]}, {"cell_type": "markdown", "id": "55c50e39", "metadata": {}, "source": ["Forecast Accuracy"]}, {"cell_type": "code", "execution_count": 1, "id": "f2e7e61d", "metadata": {}, "outputs": [], "source": ["#Getting the mean squared error (average error of forecasts).\ny_forecasted = pred.predicted_mean\ny_truth = y['2016-01-01':]\nmse = ((y_forecasted - y_truth) ** 2).mean()\nprint('MSE {}'.format(round(mse, 2)))\n\n#Smaller the better."]}, {"cell_type": "code", "execution_count": 1, "id": "577790d6", "metadata": {}, "outputs": [], "source": ["print('RMSE: {}'.format(round(np.sqrt(mse), 2)))"]}, {"cell_type": "code", "execution_count": 1, "id": "45486735", "metadata": {}, "outputs": [], "source": ["#The time can be changed using steps.\npred_uc = results.get_forecast(steps=50)\npred_ci = pred_uc.conf_int()\nax = y.plot(label='observed', figsize=(14, 7))\npred_uc.predicted_mean.plot(ax=ax, label='Forecast')\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.25)\nax.set_xlabel('Date')\nax.set_ylabel('Order_Demand')\nplt.legend()\nplt.show()\n\n#Far out values are naturally more prone to variance. The grey area is the confidence we have in the predictions."]}, {"cell_type": "code", "execution_count": 1, "id": "04b78ea9", "metadata": {}, "outputs": [], "source": ["#And that is how you build a somewhat accurate forecast of the demand. Next steps include checking and modeling demand by category type."]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}