{"cells": [{"cell_type": "markdown", "id": "9f6c6df9", "metadata": {}, "source": ["# Hi, this notebook is all about learning and experimenting Unsupervised Learning (on famous Iris Dataset)\n## You'll find: \n\n1- <a href=\"#eda\">EDA</a>\n\n2-  <a href=\"#1\">Adjusting the Dataset for Unsupervised Learning</a>\n\n3-  <a href=\"#2\">Implemeting the K Means Clustering</a>\n\n4-  <a href=\"#3\">Finding the best amount of clusters to get most accurate results (KMeans)</a>\n\n5-  <a href=\"#4\">Implemeting the Hierarchical Clustering</a>\n\n6-  <a href=\"#5\">Finding the best amount of clusters to get most accurate results (Hierarchy)</a>\n\n7-  <a href=\"#6\">Evaluating the Results and Comparing them</a>"]}, {"cell_type": "code", "execution_count": 1, "id": "b8eda217", "metadata": {}, "outputs": [], "source": ["import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")"]}, {"cell_type": "markdown", "id": "7d113705", "metadata": {}, "source": ["<div id=\"eda\"></div>\n# **BASIC EDA**"]}, {"cell_type": "code", "execution_count": 1, "id": "be3cc434", "metadata": {}, "outputs": [], "source": ["df = pd.read_csv(\"../input/Iris.csv\")   # reading the data"]}, {"cell_type": "code", "execution_count": 1, "id": "3d702707", "metadata": {}, "outputs": [], "source": ["df.head()    # first 5 rows"]}, {"cell_type": "markdown", "id": "e0ccad9a", "metadata": {}, "source": ["Id column is not a real feature of our flowers. I will drop it"]}, {"cell_type": "code", "execution_count": 1, "id": "19e0e08a", "metadata": {}, "outputs": [], "source": ["df.drop([\"Id\"],axis=1,inplace=True)    # dropped\n\ndf.tail()   # last 5 rows"]}, {"cell_type": "code", "execution_count": 1, "id": "6fa11c08", "metadata": {}, "outputs": [], "source": ["df.info()   # all non-null and numeric [except the labels]"]}, {"cell_type": "code", "execution_count": 1, "id": "2bfbe64f", "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\nimport seaborn as sns"]}, {"cell_type": "code", "execution_count": 1, "id": "f02189a8", "metadata": {}, "outputs": [], "source": ["sns.pairplot(data=df,hue=\"Species\",palette=\"Set2\")\nplt.show()"]}, {"cell_type": "markdown", "id": "ed056b39", "metadata": {}, "source": ["### We see that iris-setosa is easily separable from the other two. Especially when we can see in different colors for corresponding Labels like above.\n\n### But our mission was finding the Labels that we didn't knew at all, So Let's create a suitable scenario."]}, {"cell_type": "markdown", "id": "641e94b7", "metadata": {}, "source": ["<div id=\"1\"></div>\n# ** Adjusting the Dataset for Unsupervised Learning **\n\n### I will simply do not use labels column on my *\"new\"* Dataset"]}, {"cell_type": "code", "execution_count": 1, "id": "cdbdd39a", "metadata": {}, "outputs": [], "source": ["features = df.loc[:,[\"SepalLengthCm\",\"SepalWidthCm\",\"PetalLengthCm\",\"PetalWidthCm\"]]"]}, {"cell_type": "markdown", "id": "03fc3092", "metadata": {}, "source": ["### From now on, we don't know the real labels or amount of labels anymore (Shhh!)"]}, {"cell_type": "markdown", "id": "bbf0e123", "metadata": {}, "source": ["<div id=\"2\"></div>\n# ** Implemeting the K Means Clustering **\n\n### SciKit-Learn implementation is very simple, it only takes 2 lines of code and 1 parameter"]}, {"cell_type": "code", "execution_count": 1, "id": "4d7fad71", "metadata": {}, "outputs": [], "source": ["from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=99)        # 99 CLUSTERS ?????"]}, {"cell_type": "markdown", "id": "b16f4e55", "metadata": {}, "source": ["### WHY 99 ????? Because I don't know the right amount of Labels. Don't worry, There is a solution for it.\n\n\n<div id=\"3\"></div>\n# **Finding the best amount of clusters to get most accurate results (KMeans) **\n\n### I will use ELBOW RULE, which is basically looking for a plot line that respectively has a slope nearest to 90 degrees compared to y axis and be smallest possible. (yes, looks like an elbow )"]}, {"cell_type": "code", "execution_count": 1, "id": "6e3e92b4", "metadata": {}, "outputs": [], "source": ["from sklearn.cluster import KMeans\nwcss = []\n\nfor k in range(1,15):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(features)\n    wcss.append(kmeans.inertia_)\n\n\nplt.figure(figsize=(20,8))\nplt.title(\"WCSS / K Chart\", fontsize=18)\nplt.plot(range(1,15),wcss,\"-o\")\nplt.grid(True)\nplt.xlabel(\"Amount of Clusters\",fontsize=14)\nplt.ylabel(\"Inertia\",fontsize=14)\nplt.xticks(range(1,20))\nplt.tight_layout()\nplt.show()\n"]}, {"cell_type": "markdown", "id": "7130155d", "metadata": {}, "source": ["### **3 or 2** seems to be our ** Best ** value(s) for clusters. (By the ** Elbow Rule**)"]}, {"cell_type": "markdown", "id": "154f6036", "metadata": {}, "source": ["## Let's Double Check it "]}, {"cell_type": "code", "execution_count": 1, "id": "57c817b8", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(24,4))\n\nplt.suptitle(\"K Means Clustering\",fontsize=20)\n\n\nplt.subplot(1,5,1)\nplt.title(\"K = 1\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nplt.ylabel(\"PetalWidthCm\")\nplt.scatter(features.PetalLengthCm,features.PetalWidthCm)\n\n\nplt.subplot(1,5,2)\nplt.title(\"K = 2\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nkmeans = KMeans(n_clusters=2)\nfeatures[\"labels\"] = kmeans.fit_predict(features)\nplt.scatter(features.PetalLengthCm[features.labels == 0],features.PetalWidthCm[features.labels == 0])\nplt.scatter(features.PetalLengthCm[features.labels == 1],features.PetalWidthCm[features.labels == 1])\n\n# I drop labels since we only want to use features.\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(1,5,4)\nplt.title(\"K = 3\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nkmeans = KMeans(n_clusters=3)\nfeatures[\"labels\"] = kmeans.fit_predict(features)\nplt.scatter(features.PetalLengthCm[features.labels == 0],features.PetalWidthCm[features.labels == 0])\nplt.scatter(features.PetalLengthCm[features.labels == 1],features.PetalWidthCm[features.labels == 1])\nplt.scatter(features.PetalLengthCm[features.labels == 2],features.PetalWidthCm[features.labels == 2])\n\n# I drop labels since we only want to use features.\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(1,5,3)\nplt.title(\"K = 4\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nkmeans = KMeans(n_clusters=4)\nfeatures[\"labels\"] = kmeans.fit_predict(features)\nplt.scatter(features.PetalLengthCm[features.labels == 0],features.PetalWidthCm[features.labels == 0])\nplt.scatter(features.PetalLengthCm[features.labels == 1],features.PetalWidthCm[features.labels == 1])\nplt.scatter(features.PetalLengthCm[features.labels == 2],features.PetalWidthCm[features.labels == 2])\nplt.scatter(features.PetalLengthCm[features.labels == 3],features.PetalWidthCm[features.labels == 3])\n\n# I drop labels since we only want to use features.\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(1,5,5)\nplt.title(\"Original Labels\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nplt.scatter(df.PetalLengthCm[df.Species == \"Iris-setosa\"],df.PetalWidthCm[df.Species == \"Iris-setosa\"])\nplt.scatter(df.PetalLengthCm[df.Species == \"Iris-versicolor\"],df.PetalWidthCm[df.Species == \"Iris-versicolor\"])\nplt.scatter(df.PetalLengthCm[df.Species == \"Iris-virginica\"],df.PetalWidthCm[df.Species == \"Iris-virginica\"])\n\nplt.subplots_adjust(top=0.8)\nplt.show()"]}, {"cell_type": "markdown", "id": "60e9d492", "metadata": {}, "source": ["### Kmeans visibly did an amazing job with **3** clusters. Except few data points, I can say prediction is identical to the original with labels. Which shows that our ELBOW chart was right.\n\n<div id=\"4\"></div>\n# ** Implemeting the Hierarchical Clustering**\n\n### Again, Super Simple with SciKit-Learn. "]}, {"cell_type": "code", "execution_count": 1, "id": "38266152", "metadata": {}, "outputs": [], "source": ["from sklearn.cluster import AgglomerativeClustering\nhc_cluster = AgglomerativeClustering(n_clusters=99)    # 99 CLUSTERS ????? "]}, {"cell_type": "markdown", "id": "83b8020b", "metadata": {}, "source": ["### AGAIN 99 ????? Yes, Because I don't know the right amount of Labels. Again, There is also solution for it.\n<div id=\"5\"></div>\n# **Finding the best amount of clusters to get most accurate results (Hierarchy)**\n\n### Longest Vertical line between Horizontal Lines."]}, {"cell_type": "code", "execution_count": 1, "id": "7c6812b5", "metadata": {}, "outputs": [], "source": ["from scipy.cluster.hierarchy import dendrogram, linkage\n\nmerg = linkage(features,method=\"ward\")\n\nplt.figure(figsize=(18,6))\ndendrogram(merg, leaf_rotation=90)\nplt.xlabel(\"data points\")\nplt.ylabel(\"euclidian distance\")\n\nplt.suptitle(\"DENDROGRAM\",fontsize=18)\nplt.show()"]}, {"cell_type": "markdown", "id": "a0560855", "metadata": {}, "source": ["### we see that longest vertical line without any perpendecular matching lines (euclidian distances). If we draw a horizontal line between that values, we will have ** 2 or 3 ** interceptions which are representing ideal amount of labels.\n\n"]}, {"cell_type": "markdown", "id": "e2b9e1f7", "metadata": {}, "source": ["## Double Check!"]}, {"cell_type": "code", "execution_count": 1, "id": "803ac58b", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(24,4))\n\nplt.suptitle(\"Hierarchical Clustering\",fontsize=20)\n\nplt.subplot(1,5,1)\nplt.title(\"K = 1\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nplt.ylabel(\"PetalWidthCm\")\nplt.scatter(features.SepalLengthCm,features.SepalWidthCm)\n\n\nplt.subplot(1,5,2)\nplt.title(\"K = 2\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nhc_cluster = AgglomerativeClustering(n_clusters=2)\nfeatures[\"labels\"] = hc_cluster.fit_predict(features)\nplt.scatter(features.SepalLengthCm[features.labels == 0],features.SepalWidthCm[features.labels == 0])\nplt.scatter(features.SepalLengthCm[features.labels == 1],features.SepalWidthCm[features.labels == 1])\n\n# I drop labels since we only want to use features.\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(1,5,3)\nplt.title(\"K = 4\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nhc_cluster = AgglomerativeClustering(n_clusters=4)\nfeatures[\"labels\"] = hc_cluster.fit_predict(features)\nplt.scatter(features.SepalLengthCm[features.labels == 0],features.SepalWidthCm[features.labels == 0])\nplt.scatter(features.SepalLengthCm[features.labels == 1],features.SepalWidthCm[features.labels == 1])\nplt.scatter(features.SepalLengthCm[features.labels == 2],features.SepalWidthCm[features.labels == 2])\nplt.scatter(features.SepalLengthCm[features.labels == 3],features.SepalWidthCm[features.labels == 3])\n\n# I drop labels since we only want to use features.\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(1,5,4)\nplt.title(\"K = 3\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nhc_cluster = AgglomerativeClustering(n_clusters=3)\nfeatures[\"labels\"] = hc_cluster.fit_predict(features)\nplt.scatter(features.SepalLengthCm[features.labels == 0],features.SepalWidthCm[features.labels == 0])\nplt.scatter(features.SepalLengthCm[features.labels == 1],features.SepalWidthCm[features.labels == 1])\nplt.scatter(features.SepalLengthCm[features.labels == 2],features.SepalWidthCm[features.labels == 2])\n\n\nplt.subplot(1,5,5)\nplt.title(\"Original Labels\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nplt.scatter(df.SepalLengthCm[df.Species == \"Iris-setosa\"],df.SepalWidthCm[df.Species == \"Iris-setosa\"])\nplt.scatter(df.SepalLengthCm[df.Species == \"Iris-versicolor\"],df.SepalWidthCm[df.Species == \"Iris-versicolor\"])\nplt.scatter(df.SepalLengthCm[df.Species == \"Iris-virginica\"],df.SepalWidthCm[df.Species == \"Iris-virginica\"])\n\nplt.subplots_adjust(top=0.8)\nplt.show()"]}, {"cell_type": "markdown", "id": "c691b0af", "metadata": {}, "source": ["### Again, Our double checking method showed that **3** is more accurate than ** 2 ** value by simply looking to the graph above. \n\n### Reason behind this is basically \"iris-setosa\" being too easy to separate while the other two is quite mixed and it made our Dendrogram method a bit unclear."]}, {"cell_type": "markdown", "id": "9d450c3d", "metadata": {}, "source": ["<div id=\"6\"></div>\n# ** Evaluating the Results and Comparing them**"]}, {"cell_type": "code", "execution_count": 1, "id": "13fa68e2", "metadata": {}, "outputs": [], "source": ["# I drop labels since we only want to use features.\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\n# kmeans\nkmeans = KMeans(n_clusters=3)\nkmeans_predict = kmeans.fit_predict(features)\n\n# cross tabulation table for kmeans\ndf1 = pd.DataFrame({'labels':kmeans_predict,\"Species\":df['Species']})\nct1 = pd.crosstab(df1['labels'],df1['Species'])\n\n\n# hierarchy\nhc_cluster = AgglomerativeClustering(n_clusters=3)\nhc_predict = hc_cluster.fit_predict(features)\n\n# cross tabulation table for Hierarchy\ndf2 = pd.DataFrame({'labels':hc_predict,\"Species\":df['Species']})\nct2 = pd.crosstab(df2['labels'],df2['Species'])\n\n\nplt.figure(figsize=(24,8))\nplt.suptitle(\"CROSS TABULATIONS\",fontsize=18)\nplt.subplot(1,2,1)\nplt.title(\"KMeans\")\nsns.heatmap(ct1,annot=True,cbar=False,cmap=\"Blues\")\n\nplt.subplot(1,2,2)\nplt.title(\"Hierarchy\")\nsns.heatmap(ct2,annot=True,cbar=False,cmap=\"Blues\")\n\nplt.show()"]}, {"cell_type": "markdown", "id": "00af9088", "metadata": {}, "source": ["# **Conclusion**\n\n### The both Failed on 16 data points over 150 data points, which is equal to 90%\n\n### We also see that clustering \"iris-setosa\" was easy for both of them (50/50 success) because it's data points are all easily differentiable\n\n### 15 mistakes of all 16 is coming from \"iris-virginica\". Which shows that it was hard to cluster for my models.\n\n### ***Thanks for reading this far. If you enjoyed please be sure to vote, or if you have some issues don't be shy to comment. Best Regards. Efe***"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}