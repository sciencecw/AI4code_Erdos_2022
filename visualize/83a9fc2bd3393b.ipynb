{"cells": [{"cell_type": "markdown", "id": "29260ee9", "metadata": {}, "source": ["# ML-Ensemble using the House Prices competition data\n![image.png](attachment:image.png)\n\n> *ML-Ensemble a Python library for memory efficient parallelized ensemble learning. In particular, ML-Ensemble is a Scikit-learn compatible library for building deep ensemble networks in just a few lines of code.*\n\nThis notebook is adapted from part of the extensive [Getting started tutorial](http://ml-ensemble.com/info/tutorials/start.html) on the [ML-Ensemble website](http://ml-ensemble.com/).\n\nFor the ensemble meta learners we shall use [XGBoost](https://github.com/dmlc/xgboost), [CatBoost](https://github.com/catboost/catboost), and the [Regularized Greedy Forest (RGF)](https://github.com/RGF-team/rgf/tree/master/python-package) (See my notebook [\"Introduction to the Regularized Greedy Forest\"](https://www.kaggle.com/carlmcbrideellis/introduction-to-the-regularized-greedy-forest) for more details).\nFor the meta estimator we shall use the [Random Forest Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html).\n\n### Install both ML-Ensemble (`mlens`) and the Regularized Greedy Forest (`rgf_python`):"]}, {"cell_type": "code", "execution_count": 1, "id": "72b2c3bb", "metadata": {}, "outputs": [], "source": ["!pip install mlens\n!pip install rgf_python"]}, {"cell_type": "markdown", "id": "1d584fe0", "metadata": {}, "source": ["### set up the House Prices competition data"]}, {"cell_type": "code", "execution_count": 1, "id": "3f4bf015", "metadata": {}, "outputs": [], "source": ["import pandas  as pd\nimport numpy   as np\n\n#===========================================================================\n# read in the data\n#===========================================================================\ntrain_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest_data  = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n\n#===========================================================================\n# select some features of interest\n#===========================================================================\nfeatures = ['OverallQual' , 'GrLivArea' , 'TotalBsmtSF' , 'BsmtFinSF1' ,\n            '2ndFlrSF'    , 'GarageArea', '1stFlrSF'    , 'YearBuilt'  ]\n\n#===========================================================================\n#===========================================================================\nX_train       = train_data[features]\ny_train       = train_data[\"SalePrice\"]\nX_test        = test_data[features]\n\n#===========================================================================\n# imputation; substitute any 'NaN' with mean value\n#===========================================================================\nX_train      = X_train.fillna(X_train.mean())\nX_test       = X_test.fillna(X_test.mean())"]}, {"cell_type": "markdown", "id": "b4e5c174", "metadata": {}, "source": ["### create a House Prices scoring function\nThe House Prices competition uses the root of the [mean squared logarithmic error regression loss](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_log_error.html) (Note: Be sure to adapt this function to the needs of your particular dataset/competition)"]}, {"cell_type": "code", "execution_count": 1, "id": "292cd3fd", "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import mean_squared_log_error\n\ndef RMSLE(y,y_hat):\n    return np.sqrt( mean_squared_log_error(y,y_hat) )"]}, {"cell_type": "markdown", "id": "7cf17fb1", "metadata": {}, "source": ["### build and run the ensemble"]}, {"cell_type": "code", "execution_count": 1, "id": "5ca0df07", "metadata": {}, "outputs": [], "source": ["from mlens.ensemble import SuperLearner\nfrom rgf.sklearn import RGFRegressor\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n# --- Build ---\n# Passing a scoring function will create cv scores during fitting\n# the scorer should be a simple function accepting to vectors and returning a scalar\nensemble = SuperLearner(scorer=RMSLE, verbose=2)\n\n# Build the first layer\nensemble.add([xgb.XGBRegressor(n_estimators  = 750,learning_rate = 0.02, max_depth = 4),\n              CatBoostRegressor(loss_function='RMSE', verbose=False),\n              RGFRegressor(max_leaf=300, algorithm=\"RGF_Sib\", test_interval=100, loss=\"LS\")])\n\n\n# Attach the final meta estimator\nensemble.add_meta(RandomForestRegressor())\n\n# --- Use ---\n\n# Fit ensemble\nensemble.fit(X_train, y_train)\n\n# Predict\npredictions = ensemble.predict(X_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "bc8f146c", "metadata": {}, "outputs": [], "source": ["print(\"Fit data:\\n%r\" % ensemble.data)"]}, {"cell_type": "markdown", "id": "e28a0917", "metadata": {}, "source": ["### now write out the `submission.csv` file:"]}, {"cell_type": "code", "execution_count": 1, "id": "c19d4e16", "metadata": {}, "outputs": [], "source": ["output = pd.DataFrame({\"Id\":test_data.Id, \"SalePrice\":predictions})\noutput.to_csv('submission.csv', index=False)"]}, {"cell_type": "markdown", "id": "ea85d2d1", "metadata": {}, "source": ["Here we have looked at the ensembling, however the final score can  be improved upon with better feature selection (see for example [\"Feature selection using Boruta-SHAP\"](https://www.kaggle.com/carlmcbrideellis/feature-selection-using-borutashap)) as well as both feature engineering, and hyperparameter tuning of the individual estimators. The ensembling its-self is not so much about producing a better score (although this can happen) but about reducing the variance (or increasing the [explained variance score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.explained_variance_score.html)). Indeed our result may not beat the score of the best individual estimator that we used in the ensemble. However, what it can do is reduce the overfitting that leads to unwelcome surprises during the competition shakeup."]}, {"cell_type": "code", "execution_count": 1, "id": "631557d7", "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\nfrom mlens.visualization import exp_var_plot\nfrom sklearn.decomposition import PCA\nexp_var_plot(X_train, PCA(), marker='s', where='post');"]}, {"cell_type": "markdown", "id": "f3da4a6f", "metadata": {}, "source": ["# Links\n* [ML-Ensemble](http://ml-ensemble.com/)\n* [GitHub: mlens](https://github.com/flennerhag/mlens) \n\n# Related notebooks\n* [ML-Ensemble: Scikit-learn style ensemble learning ](https://www.kaggle.com/flennerhag/ml-ensemble-scikit-learn-style-ensemble-learning) by [flnr](https://www.kaggle.com/flennerhag) (the author of `mlens`)\n* [Ensemble learning lib: MLens [99% accuracy]](https://www.kaggle.com/vipulgote4/ensemble-learning-lib-mlens-99-accuracy) by [vipul](https://www.kaggle.com/vipulgote4)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}