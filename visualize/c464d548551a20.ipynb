{"cells": [{"cell_type": "markdown", "id": "aee135d5", "metadata": {}, "source": ["# Performance of JAX vs PyTorch\n\nLet's compare how fast two libraries can calculate a gradient of the same function: JAX vs PyTorch. No hardware acceleration will be enabled, we will use just CPU (GPU is disabled in this notebook)."]}, {"cell_type": "markdown", "id": "1dccf91a", "metadata": {}, "source": ["## JAX"]}, {"cell_type": "markdown", "id": "e0e42183", "metadata": {}, "source": ["Install and import JAX, enable usage of 64-bit floats and CPU for computations."]}, {"cell_type": "code", "execution_count": 1, "id": "277c776d", "metadata": {}, "outputs": [], "source": ["!pip -q install jax jaxlib\n\n%env JAX_ENABLE_X64=1\n%env JAX_PLATFORM_NAME=cpu\n\nimport jax.numpy as np\nfrom jax import grad, ops, jit, lax"]}, {"cell_type": "markdown", "id": "8364dbf5", "metadata": {}, "source": ["Next we will define a toy function which will be used for our tests. It receives an array as input, performs some computations on it and returns a scalar. The problem is O(n) hard - the longer the input array, the linearly longer it takes to calculate the result.\n\nAs gnecula and mattjj kindly [explained](https://github.com/google/jax/issues/1832), it's better to use lax.scan in this case instead of a `for` loop, so it is commented out."]}, {"cell_type": "code", "execution_count": 1, "id": "2932d6e0", "metadata": {}, "outputs": [], "source": ["def func_jax(x):\n    t = len(x)\n    f = np.zeros(t)\n    \n    #for i in range(1, t):\n    #    f = ops.index_update(f, i, x[i]+f[i-1])\n    \n    f = lax.scan(lambda f, i: (ops.index_update(f, i, x[i] + f[i-1]), None), f, np.arange(1, t))\n    \n    return np.sum(f[0])"]}, {"cell_type": "markdown", "id": "c2e50f85", "metadata": {}, "source": ["Perform a sanity check."]}, {"cell_type": "code", "execution_count": 1, "id": "5bfce997", "metadata": {}, "outputs": [], "source": ["func_jax(np.ones(100))"]}, {"cell_type": "markdown", "id": "3f049e52", "metadata": {}, "source": ["Measure performance of a gradient calculation for different array length. Run only one loop to exclude any caching if it exists. Also add a `.block_until_ready()` so we are not just timing dispatch time (due to asynchronous dispatch)."]}, {"cell_type": "code", "execution_count": 1, "id": "3569a7f0", "metadata": {}, "outputs": [], "source": ["%timeit grad(func_jax)(np.ones(10)).block_until_ready()"]}, {"cell_type": "code", "execution_count": 1, "id": "49067ec9", "metadata": {}, "outputs": [], "source": ["%timeit grad(func_jax)(np.ones(100)).block_until_ready()"]}, {"cell_type": "code", "execution_count": 1, "id": "585a742f", "metadata": {}, "outputs": [], "source": ["%timeit grad(func_jax)(np.ones(1000)).block_until_ready()"]}, {"cell_type": "code", "execution_count": 1, "id": "e158e990", "metadata": {}, "outputs": [], "source": ["%timeit grad(func_jax)(np.ones(10000)).block_until_ready()"]}, {"cell_type": "markdown", "id": "42e51860", "metadata": {}, "source": ["Jitted versions have almost the same timings. "]}, {"cell_type": "code", "execution_count": 1, "id": "c1e3bab9", "metadata": {}, "outputs": [], "source": ["%timeit jit(grad(func_jax))(np.ones(10)).block_until_ready()"]}, {"cell_type": "code", "execution_count": 1, "id": "58dd831f", "metadata": {}, "outputs": [], "source": ["%timeit jit(grad(func_jax))(np.ones(100)).block_until_ready()"]}, {"cell_type": "code", "execution_count": 1, "id": "5971b38b", "metadata": {}, "outputs": [], "source": ["%timeit jit(grad(func_jax))(np.ones(1000)).block_until_ready()"]}, {"cell_type": "code", "execution_count": 1, "id": "7ba9eee4", "metadata": {}, "outputs": [], "source": ["%timeit jit(grad(func_jax))(np.ones(10000)).block_until_ready()"]}, {"cell_type": "markdown", "id": "2d72a85b", "metadata": {}, "source": ["Looks like the problem is O(1) now!"]}, {"cell_type": "markdown", "id": "5c928465", "metadata": {}, "source": ["## PyTorch"]}, {"cell_type": "markdown", "id": "5226ac44", "metadata": {}, "source": ["Let's do the same with PyTorch."]}, {"cell_type": "code", "execution_count": 1, "id": "31a70f5a", "metadata": {}, "outputs": [], "source": ["import torch"]}, {"cell_type": "markdown", "id": "c6b8500d", "metadata": {}, "source": ["A test function is still the same with a slight cosmetic modifications."]}, {"cell_type": "code", "execution_count": 1, "id": "802bb57e", "metadata": {}, "outputs": [], "source": ["def func_torch(x):\n    t = len(x)\n    f = torch.zeros(t)\n    for i in range(1, t):\n        f[i] = x[i] + f[i-1]\n    return f.sum()"]}, {"cell_type": "markdown", "id": "91092d7d", "metadata": {}, "source": ["A sanity check again. Just a quick test to check that we implemented the same function."]}, {"cell_type": "code", "execution_count": 1, "id": "88d115bd", "metadata": {}, "outputs": [], "source": ["func_torch(torch.ones(100, dtype=torch.float64))"]}, {"cell_type": "markdown", "id": "59d54939", "metadata": {}, "source": ["Define a gradient function."]}, {"cell_type": "code", "execution_count": 1, "id": "39a9ce6a", "metadata": {}, "outputs": [], "source": ["def grad_torch(length):\n    x = torch.ones(length, requires_grad=True, dtype=torch.float64)\n    func_torch(x).backward()\n    return x.grad"]}, {"cell_type": "markdown", "id": "714712ba", "metadata": {}, "source": ["Now measure performance."]}, {"cell_type": "code", "execution_count": 1, "id": "35c41baa", "metadata": {}, "outputs": [], "source": ["%timeit grad_torch(10)"]}, {"cell_type": "code", "execution_count": 1, "id": "09ee61f9", "metadata": {}, "outputs": [], "source": ["%timeit grad_torch(100)"]}, {"cell_type": "code", "execution_count": 1, "id": "0b227ea1", "metadata": {}, "outputs": [], "source": ["%timeit grad_torch(1000)"]}, {"cell_type": "code", "execution_count": 1, "id": "ca1a02cf", "metadata": {}, "outputs": [], "source": ["%timeit grad_torch(10000)"]}, {"cell_type": "markdown", "id": "59ff2af2", "metadata": {}, "source": ["PyTorch is fast, but looks like it solves the problem in O(n) time."]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}