{"cells": [{"cell_type": "markdown", "id": "feba8a4e", "metadata": {}, "source": ["# Importing Libraries"]}, {"cell_type": "code", "execution_count": 1, "id": "fcca7e54", "metadata": {}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk #for natural language processing\nimport string\nfrom PIL import Image\nfrom sklearn.feature_extraction.text import TfidfVectorizer,TfidfTransformer\n\nfrom nltk.corpus import stopwords\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport emoji\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport re\nfrom nltk.stem import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport collections\nfrom sklearn.model_selection import train_test_split\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output."]}, {"cell_type": "markdown", "id": "48fe304c", "metadata": {}, "source": ["# Importing Files"]}, {"cell_type": "code", "execution_count": 1, "id": "0ae796b6", "metadata": {}, "outputs": [], "source": ["train_df=pd.read_csv(\"../input/train.csv\")\ntest_df=pd.read_csv(\"../input/test.csv\")\n"]}, {"cell_type": "markdown", "id": "ca8fdc55", "metadata": {}, "source": ["# General Analysis"]}, {"cell_type": "code", "execution_count": 1, "id": "059d8889", "metadata": {}, "outputs": [], "source": ["#To check first five rows\ntrain_df.head()\ntest_df.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "5043879e", "metadata": {}, "outputs": [], "source": ["#To check type of columns\ntrain_df.info()"]}, {"cell_type": "code", "execution_count": 1, "id": "05c44fd5", "metadata": {}, "outputs": [], "source": ["#To check size of data\nprint(train_df.shape)\nprint(\"training set has 3339 rows and 12 columns\\n \")\n#To check no. of null values\nprint(train_df.isnull().sum())\nprint(\"\\nFollowing columns have null values more than 1:-\\r\\n1. negativereason\\r\\n2. negativereason_confidence\\r\\n3. tweet_created\\r\\n4. tweet_location\\r\\n5. usertimezone \")"]}, {"cell_type": "markdown", "id": "b49e4fc6", "metadata": {}, "source": ["* The main objective here is to determine whether the  tweet is negative or positive .\n* airline_sentiment column shows the type of tweet : Negative, Positive or Neutral\n* negativereason column shows the overall reason for a negative tweet. Its not applicable for Positive or Neutral tweets\n* airline column shows the name of the airline as a particular airline may have more negative tweets than other . We will look into that.\n* text column is the actual tweet which contains words deciding whether the tweet is negative or positive.\n* Rest of the columns have not been used in this notebook as of now.\n"]}, {"cell_type": "code", "execution_count": 1, "id": "f289ef71", "metadata": {}, "outputs": [], "source": ["#To count the no. of negative , positive and neutral tweets in training data\nmood_count=train_df[\"airline_sentiment\"].value_counts()\nmood_count"]}, {"cell_type": "markdown", "id": "2243cfff", "metadata": {}, "source": ["* Most of the tweets are negative which makes sense as people tweet mostly when they had some issues with the flight."]}, {"cell_type": "code", "execution_count": 1, "id": "9a59b303", "metadata": {}, "outputs": [], "source": ["# To plot the abouve stats\nplt.bar([\"Negative\",\"Neutral\",'Positive'],mood_count)\nplt.xlabel(\"Mood\")\nplt.ylabel(\"Mood_Count\")\nplt.xticks(rotation=45)\nplt.title(\"Count of Moods\")"]}, {"cell_type": "code", "execution_count": 1, "id": "ec98238f", "metadata": {}, "outputs": [], "source": ["# To find the count of tweets for different airlines\ntrain_df[\"airline\"].value_counts()\n"]}, {"cell_type": "markdown", "id": "5346cc10", "metadata": {}, "source": ["* United airlines have most no. of tweets."]}, {"cell_type": "code", "execution_count": 1, "id": "59bf63fa", "metadata": {}, "outputs": [], "source": ["# To plot the sentiment count airline wise\ndef plot_airline_wise_sentiments(Airline):\n    df=train_df[train_df[\"airline\"]==Airline]\n    count=df[\"airline_sentiment\"].value_counts()\n    plt.bar([\"Negative\",\"Neutral\",\"Positive\"],count)\n    plt.xlabel(\"Moods\")\n    plt.ylabel(\"Moood_Counts\")\n    plt.title(\"Mood counts for {}\".format(Airline))\nplt.figure(1,figsize=(20,12))\nplt.subplot(231)\nplot_airline_wise_sentiments(\"United\")\nplt.subplot(232)\nplot_airline_wise_sentiments(\"Virgin America\")\n\n"]}, {"cell_type": "markdown", "id": "bd3b8e48", "metadata": {}, "source": ["* From Graphs , its clear that United airlines has much more negative tweets and almost same no. of neutral and positive tweets while for Virgin      America, sentiments are somewhat balanced"]}, {"cell_type": "code", "execution_count": 1, "id": "06dc6cf3", "metadata": {}, "outputs": [], "source": ["# To get the count of negative reasons . Here dict function is used to create dictionary.\nNR_count=dict(train_df[\"negativereason\"].value_counts())\nprint(NR_count)\n"]}, {"cell_type": "code", "execution_count": 1, "id": "3bf43aa6", "metadata": {}, "outputs": [], "source": ["# To get airline wise count of negative reasons\ndef NR_count(Airline):\n    if(Airline==\"All\"):\n        df=train_df\n    else:\n        df=train_df[train_df[\"airline\"]==Airline]\n    count=dict(df[\"negativereason\"].value_counts())\n    unique_reason=list(train_df[\"negativereason\"].unique())\n    unique_reason=[x for x in unique_reason if str(x)!='nan'] # To remove none values\n    print(type(unique_reason))\n    reason_frame=pd.DataFrame({'Reasons':unique_reason})\n    reason_frame['Count']=reason_frame['Reasons'].apply(lambda x:count[x])\n    return reason_frame\n    "]}, {"cell_type": "code", "execution_count": 1, "id": "edec7a03", "metadata": {}, "outputs": [], "source": ["# To plot airline wise count of negative reason\ndef plot_reason(Airline):\n    df=NR_count(Airline)\n    index=df[\"Reasons\"]\n    plt.figure(figsize=(12,12))\n    plt.bar(index,df[\"Count\"])\n    plt.xticks(rotation=45)\n    plt.tick_params(top='off', bottom='on', left='off', right='off', labelleft='on', labelbottom='on')\n    plt.xlabel(\"Negative Reasons\")\n    plt.ylabel(\"Count\")\n    plt.title(\"Negative reason count for \"+Airline)\n\nplot_reason(\"All\")"]}, {"cell_type": "code", "execution_count": 1, "id": "8aac54a2", "metadata": {}, "outputs": [], "source": ["plot_reason(\"United\")"]}, {"cell_type": "code", "execution_count": 1, "id": "692290fc", "metadata": {}, "outputs": [], "source": ["plot_reason(\"Virgin America\")"]}, {"cell_type": "markdown", "id": "53a686ff", "metadata": {}, "source": ["# Feature Engineering"]}, {"cell_type": "code", "execution_count": 1, "id": "7f48c50d", "metadata": {}, "outputs": [], "source": ["# Functions to remove unnecesary words, symbols from text .\ndef remove_mentions(input_text): # To remove @....\n    return re.sub(r'@\\w+','',str(input_text))\ndef remove_urls(input_text): # To remove http.......\n    return re.sub(r'http.?://[^\\s]+[\\s]?', '', str(input_text))\ndef emoji_oneword(input_text): # To remove emojis\n    return input_text.replace('_','')\ndef remove_punctuation(input_text): # To remove punctuations(, . ! ') \n    punct=string.punctuation # punct now has all the punctuations used in english \n    trantab=str.maketrans(punct,len(punct)*' ') #every punctuation in punct will be mapped to ' ' and stored in trantab in a table\n    return  input_text.translate(trantab) # Here punctuations in text will be replaced by ' ' as defined in trantab.\ndef remove_digits(input_text): # To remove digits \n     return re.sub(r'\\d+','',str(input_text))\ndef to_lower(input_text): # To convert each word in lower case\n     return input_text.lower()\ndef remove_stopWords(input_text): # To remove stop words like the, is , in ,not.\n    stopwords_list = stopwords.words('english')\n    whitelist=[\"n't\",\"no\",\"not\"] # Some words which might indicate a certain sentiment are kept via a whitelist\n    words=input_text.split() # By default it will split the words by ' '\n    clean_words=[word for word in words if(word not in stopwords_list or word in whitelist) and len(word)>1]\n    return \" \".join(clean_words)\ndef stemming(input_text): # stemming means getting word to its original form eg: Difficulty -> Difficult\n    porter=PorterStemmer()\n    words=input_text.split()\n    stemmed_words = [porter.stem(word) for word in words]\n    return \" \".join(stemmed_words)\n "]}, {"cell_type": "code", "execution_count": 1, "id": "fdc4cb19", "metadata": {}, "outputs": [], "source": ["pd.options.mode.chained_assignment = None  # default='warn' # To hide warnings\ndf=train_df[train_df[\"airline_sentiment\"]==\"negative\"]\ndf[\"text\"]=df[\"text\"].fillna(\"No Value Found\")\ndf[\"text\"]=df[\"text\"].apply(lambda x: emoji.demojize(x)) #To covert emoji in txt\ndf[\"text\"]=df[\"text\"].apply(lambda x: remove_mentions(x))\ndf[\"text\"]=df[\"text\"].apply(lambda x: remove_urls(x))\ndf[\"text\"]=df[\"text\"].apply(lambda x: emoji_oneword(x))\ndf[\"text\"]=df[\"text\"].apply(lambda x: remove_punctuation(x))\ndf[\"text\"]=df[\"text\"].apply(lambda x: remove_digits(x))\ndf[\"text\"]=df[\"text\"].apply(lambda x: to_lower(x))\ndf[\"text\"]=df[\"text\"].apply(lambda x: remove_stopWords(x))"]}, {"cell_type": "code", "execution_count": 1, "id": "28166ea9", "metadata": {}, "outputs": [], "source": ["# To create Word Cloud of most frequent negative words\nwords=' '.join( x for x in str(df.text.values).split())             \nfrom wordcloud import WordCloud, STOPWORDS,ImageColorGenerator\nd = os.path.dirname(\"../input/\")\nplane_coloring = np.array(Image.open(os.path.join(d, \"Airplane_Transparent_PNG_Clipart.png\")))\n\nwordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='white',\n                      width=3000,\n                      height=2500,\n                      colormap=\"Blues\",\n                      max_words=15,\n                      mask=plane_coloring\n                     ).generate(words)\nimage_colors = ImageColorGenerator(plane_coloring)\n"]}, {"cell_type": "code", "execution_count": 1, "id": "02064230", "metadata": {}, "outputs": [], "source": ["plt.figure(1,figsize=(25,25))\nplt.imshow(wordcloud,interpolation=\"bilinear\")\nplt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\")\n#plt.imshow(plane_coloring, cmap=plt.cm.gray, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "cbdcaa5f", "metadata": {}, "outputs": [], "source": ["# Function to get meaningful words from training file\ndef tweet_to_words(raw_tweet):\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \",str(raw_tweet)) \n    words = letters_only.lower().split()                             \n    stops = set(stopwords.words(\"english\"))                  \n    meaningful_words = [w for w in words if not w in stops] \n    return( \" \".join( meaningful_words ))"]}, {"cell_type": "code", "execution_count": 1, "id": "b22d08d9", "metadata": {}, "outputs": [], "source": ["# Function to get length of meaningful words from training file\ndef clean_tweet_length(raw_tweet):\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \",str(raw_tweet))\n    words = letters_only.lower().split()                             \n    stops = set(stopwords.words(\"english\"))                  \n    meaningful_words = [w for w in words if not w in stops] \n    return(len(meaningful_words))"]}, {"cell_type": "code", "execution_count": 1, "id": "0ad7e827", "metadata": {}, "outputs": [], "source": ["# Changing the sentiment column values in numerical categorical form as we will only predict whether the sentiment is positive or negative, considering neutral sentiments as positive\ntrain_df['sentiment']=train_df['airline_sentiment'].apply(lambda x: -1 if x=='negative' else(0 if x=='neutral' else 1))\n#test_df['sentiment']=test_df['airline_sentiment'].apply(lambda x: -1 if x=='negative' else(0 if x=='neutral' else 1))\n"]}, {"cell_type": "code", "execution_count": 1, "id": "a44f7136", "metadata": {}, "outputs": [], "source": ["#train_df['clean_tweet']=train_df['text'].apply(lambda x: tweet_to_words(x))\ntrain_df[\"text\"]=train_df[\"text\"].fillna(\"No Value Found\")\ntrain_df[\"text\"]=train_df[\"text\"].apply(lambda x: emoji.demojize(x)) #To covert emoji in txt\ntrain_df[\"text\"]=train_df[\"text\"].apply(lambda x: remove_mentions(x))\ntrain_df[\"text\"]=train_df[\"text\"].apply(lambda x: remove_urls(x))\ntrain_df[\"text\"]=train_df[\"text\"].apply(lambda x: emoji_oneword(x))\ntrain_df[\"text\"]=train_df[\"text\"].apply(lambda x: remove_punctuation(x))\ntrain_df[\"text\"]=train_df[\"text\"].apply(lambda x: remove_digits(x))\ntrain_df[\"text\"]=train_df[\"text\"].apply(lambda x: to_lower(x))\ntrain_df[\"text\"]=train_df[\"text\"].apply(lambda x: remove_stopWords(x))\ntrain_df['clean_tweet']=train_df['text']\ntrain_df['Tweet_length']=train_df['text'].apply(lambda x: clean_tweet_length(x))\ntrain,test = train_test_split(train_df,test_size=0.2,random_state=42)\n# for original test data to be used later\n#test_df['clean_tweet']=test_df['text'].apply(lambda x: tweet_to_words(x))\ntest_df[\"text\"]=test_df[\"text\"].fillna(\"No Value Found\")\ntest_df[\"text\"]=test_df[\"text\"].apply(lambda x: emoji.demojize(x)) #To covert emoji in txt\ntest_df[\"text\"]=test_df[\"text\"].apply(lambda x: remove_mentions(x))\ntest_df[\"text\"]=test_df[\"text\"].apply(lambda x: remove_urls(x))\ntest_df[\"text\"]=test_df[\"text\"].apply(lambda x: emoji_oneword(x))\ntest_df[\"text\"]=test_df[\"text\"].apply(lambda x: remove_punctuation(x))\ntest_df[\"text\"]=test_df[\"text\"].apply(lambda x: remove_digits(x))\ntest_df[\"text\"]=test_df[\"text\"].apply(lambda x: to_lower(x))\ntest_df[\"text\"]=test_df[\"text\"].apply(lambda x: remove_stopWords(x))\ntest_df['clean_tweet']=test_df['text']\ntest_df['Tweet_length']=test_df['text'].apply(lambda x: clean_tweet_length(x))"]}, {"cell_type": "code", "execution_count": 1, "id": "043598c1", "metadata": {}, "outputs": [], "source": ["# Creating train and test clean tweet words data\ntrain_clean_tweet=[]\nfor tweet in train['clean_tweet']:\n    train_clean_tweet.append(tweet)\ntest_clean_tweet=[]\nfor tweet in test['clean_tweet']:\n    test_clean_tweet.append(tweet)\n    \n# for original train and test data to be used later\ntrain_original_clean_tweet=[]\nfor tweet in train_df['clean_tweet']:\n    train_original_clean_tweet.append(tweet)\ntest_original_clean_tweet=[]\nfor tweet in test_df['clean_tweet']:\n    test_original_clean_tweet.append(tweet)\n"]}, {"cell_type": "code", "execution_count": 1, "id": "d39e03c4", "metadata": {}, "outputs": [], "source": ["from sklearn.feature_extraction.text import CountVectorizer\n#v = CountVectorizer(analyzer = \"word\")\nv = TfidfVectorizer(analyzer=\"word\")\ntrain_features= v.fit_transform(train_clean_tweet)\n#print(train_features)\nword_freq = dict(zip(v.get_feature_names(), np.asarray(train_features.sum(axis=0)).ravel())) #zip function is used for mapping values in different lists\n#print(train_features.sum(axis=0))\n#print(word_freq)\nword_counter = collections.Counter(word_freq)\n#print(word_counter)\nword_counter_df = pd.DataFrame(word_counter.most_common(20), columns = ['word', 'freq'])\nfig, ax = plt.subplots(figsize=(12, 10))\nsns.barplot(x=\"word\", y=\"freq\", data=word_counter_df, palette=\"PuBuGn_d\", ax=ax)\nplt.xticks(rotation=45)\nplt.show();\ntest_features=v.transform(test_clean_tweet)\n\n# for original train and test data to be used later\ntrain_original_features= v.fit_transform(train_original_clean_tweet)\ntest_original_features=v.transform(test_original_clean_tweet)"]}, {"cell_type": "code", "execution_count": 1, "id": "1e70fbb5", "metadata": {}, "outputs": [], "source": ["from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import accuracy_score"]}, {"cell_type": "code", "execution_count": 1, "id": "e348bad7", "metadata": {}, "outputs": [], "source": ["Classifiers = [\n    LogisticRegression(C=0.000000001,solver='liblinear',max_iter=200),\n    KNeighborsClassifier(3),\n    SVC(kernel=\"rbf\", C=0.025, probability=True,gamma='auto'),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators=200),\n    AdaBoostClassifier(),\n    GaussianNB()]"]}, {"cell_type": "code", "execution_count": 1, "id": "035f880c", "metadata": {}, "outputs": [], "source": ["dense_features=train_features.toarray()\ndense_test= test_features.toarray()\nAccuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    try:\n        fit = classifier.fit(train_features,train['sentiment'])\n        pred = fit.predict(test_features)\n    except Exception:\n        fit = classifier.fit(dense_features,train['sentiment'])\n        pred = fit.predict(dense_test)\n    \n    accuracy = accuracy_score(pred,test['sentiment'])\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    print('Accuracy of '+classifier.__class__.__name__+' is '+str(accuracy))"]}, {"cell_type": "code", "execution_count": 1, "id": "a508c984", "metadata": {}, "outputs": [], "source": ["Index = [1,2,3,4,5,6,7]\nplt.bar(Index,Accuracy)\nplt.xticks(Index, Model,rotation=45)\nplt.ylabel('Accuracy')\nplt.xlabel('Model')\nplt.title('Accuracies of Models')"]}, {"cell_type": "markdown", "id": "f49ee790", "metadata": {}, "source": ["* Using the best model"]}, {"cell_type": "code", "execution_count": 1, "id": "c0d25eac", "metadata": {}, "outputs": [], "source": ["dense_original_features=train_original_features.toarray()\ndense_original_test= test_original_features.toarray()\nindex=Accuracy.index(max(Accuracy))\nclassifier=Classifiers[index]\ntry:\n    fit = classifier.fit(train_original_features,train_df['sentiment'])\n    pred = fit.predict(test_original_features)\nexcept Exception:\n    fit = classifier.fit(dense_original_features,train_df['sentiment'])\n    pred = fit.predict(dense_original_test)\npred=pred.astype(object)\npred[pred==0]='neutral'\npred[pred==-1]='negative'\npred[pred==1]='positive'\nd={\"tweet_id\":test_df.tweet_id,\"airline_sentiment\":pred}\nsubmission=pd.DataFrame(d)\nsubmission.to_csv('submission.csv', index=False)"]}, {"cell_type": "markdown", "id": "ff473857", "metadata": {}, "source": [" # References\n \n ## Text Analytics Reference\n* http://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/\n* https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n* https://www.kaggle.com/bertcarremans/predicting-sentiment-with-text-features\n* https://www.kaggle.com/adamschroeder/countvectorizer-tfidfvectorizer-predict-comments\n\n## Predictive Model Reference\n* https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/  (KNN Algorithm)\n* https://www.analyticsvidhya.com/blog/2015/05/boosting-algorithms-simplified/ (AdaBoost)\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}