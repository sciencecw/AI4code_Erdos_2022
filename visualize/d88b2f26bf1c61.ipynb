{"cells": [{"cell_type": "code", "execution_count": 1, "id": "8900c809", "metadata": {}, "outputs": [], "source": ["import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#matplotlib inline"]}, {"cell_type": "code", "execution_count": 1, "id": "82b16cd5", "metadata": {}, "outputs": [], "source": ["# read the data\nflights = pd.read_csv('../input/feb-2020-us-flight-delay/feb-20-us-flight-delay.csv')"]}, {"cell_type": "code", "execution_count": 1, "id": "e2b73fde", "metadata": {}, "outputs": [], "source": ["flights.columns"]}, {"cell_type": "markdown", "id": "b1256e7b", "metadata": {}, "source": ["### Data Format\n* **MONTH** - Month\n* **DAY_OF_MONTH** - Day of Month\n* **DAY_OF_WEEK** - Day of Week\n* **OP_UNIQUE_CARRIER** - Unique Carrier Code\n* **ORIGIN** - Origin airport location\n* **DEST** - Destination airport location\n* **DEP_TIME** - Actual Departure Time (local time: hhmm)\n* **DEP_DEL15** - Departure Delay Indicator, 15 Minutes or More (1=Yes, 0=No) [TARGET VARIABLE]\n* **DISTANCE** - Distance between airports (miles)"]}, {"cell_type": "code", "execution_count": 1, "id": "bbc0a2a6", "metadata": {}, "outputs": [], "source": ["flights.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "1b2adaa7", "metadata": {}, "outputs": [], "source": ["flights['Unnamed: 9'].unique()"]}, {"cell_type": "markdown", "id": "983f8eb4", "metadata": {}, "source": ["* 'Unnamed: 9' is an extra-empty column, so we will get rid of it."]}, {"cell_type": "code", "execution_count": 1, "id": "b3aed733", "metadata": {}, "outputs": [], "source": ["flights.drop('Unnamed: 9', axis=1, inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "e6351d8d", "metadata": {}, "outputs": [], "source": ["flights.columns"]}, {"cell_type": "markdown", "id": "1c640f00", "metadata": {}, "source": ["# Data preprocessing and visualization:"]}, {"cell_type": "code", "execution_count": 1, "id": "bb4e8327", "metadata": {}, "outputs": [], "source": ["flights.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "245b2b91", "metadata": {}, "outputs": [], "source": ["# Rename the DEP_DEL15 to is_dealy\nflights.rename(columns={'DEP_DEL15':'is_delay'}, inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "1f0a1848", "metadata": {}, "outputs": [], "source": ["# Look for null values\nflights.isnull().sum()"]}, {"cell_type": "code", "execution_count": 1, "id": "e7b1df68", "metadata": {}, "outputs": [], "source": ["print(f'\\'is_delay\\' missing values are {100*4951/flights.shape[0]}%')\nprint(f'\\'DEP_TIME\\' missing values are {100*4938/flights.shape[0]}%')"]}, {"cell_type": "markdown", "id": "e1ae4f68", "metadata": {}, "source": ["* The missing 'is_delay' values represent only 0.86%, so we can safely remove them.\n* The same thing applies for 'DEP_TIME', the missing is ~0.86%."]}, {"cell_type": "markdown", "id": "4988fc67", "metadata": {}, "source": ["**Note:** As will start to do some preprocessing and cleaning, we will make a new copy of the data to work on without changing the original one."]}, {"cell_type": "code", "execution_count": 1, "id": "f16d6897", "metadata": {}, "outputs": [], "source": ["data = flights.copy()"]}, {"cell_type": "markdown", "id": "bdc275da", "metadata": {}, "source": ["### - Remove the missing values"]}, {"cell_type": "code", "execution_count": 1, "id": "577a7cec", "metadata": {}, "outputs": [], "source": ["data = data.dropna()"]}, {"cell_type": "code", "execution_count": 1, "id": "53204bc1", "metadata": {}, "outputs": [], "source": ["data.isnull().sum()"]}, {"cell_type": "markdown", "id": "8954c86c", "metadata": {}, "source": ["## Visualization"]}, {"cell_type": "markdown", "id": "c1a6e92f", "metadata": {}, "source": ["### - Delay vs No Delay"]}, {"cell_type": "code", "execution_count": 1, "id": "36457192", "metadata": {}, "outputs": [], "source": ["sns.countplot(x=data['is_delay'])"]}, {"cell_type": "code", "execution_count": 1, "id": "316ebcc0", "metadata": {}, "outputs": [], "source": ["data.groupby('is_delay').size()/len(data)"]}, {"cell_type": "markdown", "id": "651635d1", "metadata": {}, "source": ["**We see that the data is highly imbalanced; 85.6% is 'no delay' vs 14.4% 'delay' flights.**"]}, {"cell_type": "markdown", "id": "22ca6cec", "metadata": {}, "source": ["### Now there are some questions we need to ask:\n\n* What day of the week has the most delays?\n* Which origin and destination airports have the most delays?\n* Is flight distance a factor in the delays?\n* Which carrier has the most delays?"]}, {"cell_type": "code", "execution_count": 1, "id": "9e765b10", "metadata": {}, "outputs": [], "source": ["sns.countplot(x='DAY_OF_WEEK', hue=\"is_delay\", data=data)"]}, {"cell_type": "code", "execution_count": 1, "id": "da85e33d", "metadata": {}, "outputs": [], "source": ["print(f'Number of Origin airports is {data.ORIGIN.nunique()}')\nprint(f'Number of Dest airports is {data.DEST.nunique()}')"]}, {"cell_type": "markdown", "id": "4fd58056", "metadata": {}, "source": ["- We are goning to create a dataframe for 'Origin' and 'Dest' as the plot will be not clear due to the large numbers."]}, {"cell_type": "code", "execution_count": 1, "id": "80b92cce", "metadata": {}, "outputs": [], "source": ["origins = pd.DataFrame(data.groupby('ORIGIN').is_delay.count())\norigins.rename(columns={'is_delay':'flights'}, inplace=True)\norigins['delayed'] = data.groupby('ORIGIN').is_delay.sum()\norigins['delayed_perc'] = 100*origins.delayed/origins.flights\norigins.reset_index(level=0, inplace=True)\norigins.sort_values(by=['delayed_perc'], inplace=True, ascending=False, ignore_index=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "448d9929", "metadata": {}, "outputs": [], "source": ["origins.head(10)"]}, {"cell_type": "code", "execution_count": 1, "id": "31e34173", "metadata": {}, "outputs": [], "source": ["dests = pd.DataFrame(data.groupby('DEST').is_delay.count())\ndests.rename(columns={'is_delay':'flights'}, inplace=True)\ndests['delayed'] = data.groupby('DEST').is_delay.sum()\ndests['delayed_perc'] = 100*dests.delayed/dests.flights\ndests.reset_index(level=0, inplace=True)\ndests.sort_values(by=['delayed_perc'], inplace=True, ascending=False, ignore_index=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "304b1ab0", "metadata": {}, "outputs": [], "source": ["dests.head(10)"]}, {"cell_type": "markdown", "id": "77ce115f", "metadata": {}, "source": ["- Most delays happens at HGR airport."]}, {"cell_type": "markdown", "id": "f680796e", "metadata": {}, "source": ["- Thre are 350 airports for both 'DEST, nad 'ORIGIN', this will result in 700 new features when doing one-hot-encoding, so we will use the top 10 airports only, and set the rest "]}, {"cell_type": "code", "execution_count": 1, "id": "307aa4c5", "metadata": {}, "outputs": [], "source": ["origins.sort_values(by=['flights'], inplace=True, ascending=False, ignore_index=True)\ndests.sort_values(by=['flights'], inplace=True, ascending=False, ignore_index=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "b57bc99f", "metadata": {}, "outputs": [], "source": ["origins.head(10)"]}, {"cell_type": "code", "execution_count": 1, "id": "a0f2c38b", "metadata": {}, "outputs": [], "source": ["dests.head(10)"]}, {"cell_type": "markdown", "id": "25e2c2b5", "metadata": {}, "source": ["- Top airports for 'dest' and 'origin' are:  ['ATL', 'ORD', 'DFW', 'DEN', 'CLT', 'LAX', 'PHX', 'IAH', 'LAS', 'SFO']"]}, {"cell_type": "code", "execution_count": 1, "id": "c6477019", "metadata": {}, "outputs": [], "source": ["airports = ['ATL', 'ORD', 'DFW', 'DEN', 'CLT', 'LAX', 'PHX', 'IAH', 'LAS', 'SFO']\n\ndata['ORIGIN'].loc[~data['ORIGIN'].isin(airports)] = 'others'\ndata['DEST'].loc[~data['DEST'].isin(airports)] = 'others'"]}, {"cell_type": "code", "execution_count": 1, "id": "889c5165", "metadata": {}, "outputs": [], "source": ["print(f'values in \\'ORIGIN\\': {data.ORIGIN.unique()}')\nprint(f'values in \\'DEST\\': {data.DEST.unique()}')"]}, {"cell_type": "code", "execution_count": 1, "id": "75ce13b1", "metadata": {}, "outputs": [], "source": ["sns.lmplot( x=\"is_delay\", y=\"DISTANCE\", data=data, fit_reg=False, hue='is_delay', legend=False)"]}, {"cell_type": "markdown", "id": "e6e54e3e", "metadata": {}, "source": ["- Delay happens in both short and long distances."]}, {"cell_type": "code", "execution_count": 1, "id": "a155d663", "metadata": {}, "outputs": [], "source": ["sns.countplot(x='OP_UNIQUE_CARRIER', hue='is_delay', data=data)"]}, {"cell_type": "markdown", "id": "2da49fe8", "metadata": {}, "source": ["## Check the features\n- Drop uncessary ones if exist.\n- Encoding\n- Scale if needed."]}, {"cell_type": "code", "execution_count": 1, "id": "8059ec99", "metadata": {}, "outputs": [], "source": ["# Check coulmns types\ndata.dtypes"]}, {"cell_type": "code", "execution_count": 1, "id": "1586d3c3", "metadata": {}, "outputs": [], "source": ["data.describe()"]}, {"cell_type": "markdown", "id": "633b09d9", "metadata": {}, "source": ["- Distance range is high ---> needs to be scaled.\n- Dep_TIME needs to be in 24-hour format."]}, {"cell_type": "code", "execution_count": 1, "id": "72ad704e", "metadata": {}, "outputs": [], "source": ["data['DISTANCE'] = (data['DISTANCE']-data['DISTANCE'].mean())/data['DISTANCE'].std()#\ndata['DEP_TIME'] = (data['DEP_TIME']//100)"]}, {"cell_type": "code", "execution_count": 1, "id": "2c85ae84", "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import MinMaxScaler\n\nto_scale = ['DISTANCE', 'DAY_OF_WEEK', 'DEP_TIME', 'DAY_OF_MONTH']\nscaler = MinMaxScaler()\ndata[to_scale] = scaler.fit_transform(data[to_scale])"]}, {"cell_type": "code", "execution_count": 1, "id": "6631b58e", "metadata": {}, "outputs": [], "source": ["data.describe()"]}, {"cell_type": "code", "execution_count": 1, "id": "0b74a3bf", "metadata": {}, "outputs": [], "source": ["# The month is not important here as it is the same for all samples (Feb)\ndata.drop(columns=['MONTH'], axis=1, inplace=True)\n\ncategorical_columns  = ['DAY_OF_MONTH', 'DAY_OF_WEEK','OP_UNIQUE_CARRIER', \n                        'ORIGIN', 'DEST', 'is_delay']\n\ncategorical_columns.remove('is_delay') # Remove the target variable before converting to categorical\n\n# Convert them to categorical dtype\nfor c in categorical_columns:\n    data[c] = data[c].astype('category')"]}, {"cell_type": "code", "execution_count": 1, "id": "91d3cce6", "metadata": {}, "outputs": [], "source": ["data.dtypes"]}, {"cell_type": "code", "execution_count": 1, "id": "cfdcd9cb", "metadata": {}, "outputs": [], "source": ["data_dummies = pd.get_dummies(data[categorical_columns], drop_first=True)\ndata = pd.concat([data, data_dummies], axis = 1)\ndata.drop(categorical_columns,axis=1, inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "a055aeeb", "metadata": {}, "outputs": [], "source": ["data.columns"]}, {"cell_type": "markdown", "id": "e881076c", "metadata": {}, "source": ["# Bulid the Baseline Model"]}, {"cell_type": "code", "execution_count": 1, "id": "53143da3", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_recall_fscore_support"]}, {"cell_type": "code", "execution_count": 1, "id": "537c5beb", "metadata": {}, "outputs": [], "source": ["# Extract the target column\ntarget = data.is_delay\ndata.drop(columns=['is_delay'], axis=1, inplace=True)"]}, {"cell_type": "markdown", "id": "cd44255c", "metadata": {}, "source": ["### Split twice to get train, and test sets"]}, {"cell_type": "code", "execution_count": 1, "id": "12faa59a", "metadata": {}, "outputs": [], "source": ["# Split the dataset in the ratio train 80% and test 20%\nx_train, x_test, y_train, y_test = train_test_split(data, target, test_size=0.20, random_state=95) "]}, {"cell_type": "markdown", "id": "5666ad1d", "metadata": {}, "source": ["## 1 - Logistic Regression"]}, {"cell_type": "code", "execution_count": 1, "id": "90e4b20b", "metadata": {}, "outputs": [], "source": ["logReg =LogisticRegression()\nlogReg.fit(x_train, y_train)\ny_pred_logReg = logReg.predict(x_test)"]}, {"cell_type": "markdown", "id": "cc6396c1", "metadata": {}, "source": ["## 2- Random Forest"]}, {"cell_type": "code", "execution_count": 1, "id": "0f01bc49", "metadata": {}, "outputs": [], "source": ["randForest = RandomForestClassifier()\nrandForest.fit(x_train, y_train)\ny_pred_randForest = randForest.predict(x_test)"]}, {"cell_type": "markdown", "id": "e2487e15", "metadata": {}, "source": ["## 3- XGBoost"]}, {"cell_type": "code", "execution_count": 1, "id": "c65cbde4", "metadata": {}, "outputs": [], "source": ["xgb = XGBClassifier(use_label_encoder=False)\nxgb.fit(x_train, y_train)\ny_pred_xgb = xgb.predict(x_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "445b0c5c", "metadata": {}, "outputs": [], "source": ["# Calculate accuracy\ndef evaluate_model(labels, preds):\n    accuracy = (preds == labels).sum() / preds.shape[0]\n    print(f'Accuracy: {accuracy}')\n\n    auc = roc_auc_score(labels, preds)\n    print(f'AUC     : {auc}')\n\n    precision, recall, f1_score, _ = precision_recall_fscore_support(labels, preds, average = 'binary')\n    print(f'Precision: {precision}')\n    print(f'Recall: {recall}')\n    print(f'F1_score: {f1_score}')\n\n    confusion_matrix = pd.crosstab(index=labels, columns=np.round(preds), \n                                   rownames=['True'], colnames=['predictions']).astype(int)\n    plt.figure(figsize = (5,5))\n    sns.heatmap(confusion_matrix, annot=True, fmt='.2f', cmap=\"YlGnBu\").set_title('Confusion Matrix') "]}, {"cell_type": "code", "execution_count": 1, "id": "dfdb7ed0", "metadata": {}, "outputs": [], "source": ["evaluate_model(y_pred_logReg, y_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "f0af9177", "metadata": {}, "outputs": [], "source": ["evaluate_model(y_pred_randForest, y_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "10947796", "metadata": {}, "outputs": [], "source": ["evaluate_model(y_pred_xgb, y_test)"]}, {"cell_type": "markdown", "id": "5fed6d06", "metadata": {}, "source": ["### since the data is unbalanced, we will look at the F1-Score.\n* f1-score of the RandomForest model is the highest one, so will go with this model."]}, {"cell_type": "markdown", "id": "cce73150", "metadata": {}, "source": ["# Model Tuning\n\n### We will use the **Grid Search**\u00b6 algorithm to tune the hyperparameters\n\n### Most important hyperparameters of Random Forest:\n\n* n_estimators = number of trees, larger --> more complex.\n* max_features =  number of maximum features provided to each tree, the default value is the best 'square root of the number of features'.\n* max_depth = max number of levels in each decision tree, if it's too large --> overfitting.\n* min_samples_split = min number of data points placed in a node before the node is split, larger values prevent overfitting.\n* max_leaf_nodes = number of leaf nodes, very small --> underfitting, and very large --> overfitting.\n* min_samples_leaf = min number of data points allowed in a leaf node, very large --> underfitting, and very small --> overfitting.\n* bootstrap = method for sampling data points (with or without replacement)"]}, {"cell_type": "code", "execution_count": 1, "id": "fd46f9c0", "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import f1_score\nfrom sklearn.metrics import make_scorer\n\nf1_scorer = make_scorer(f1_score, greater_is_better=True)\n\nparam_grid = [\n{'n_estimators': [10, 25],\n 'max_depth': [10, 50],\n 'min_samples_split': [5, 10, 15],\n 'bootstrap': [True, False]}\n]\n\ngrid_search_forest = GridSearchCV(randForest, param_grid, cv=10, scoring=f1_scorer)\ngrid_search_forest.fit(x_train, y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "bbdbf62a", "metadata": {}, "outputs": [], "source": ["cvres = grid_search_forest.cv_results_\n\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(mean_score, params)"]}, {"cell_type": "code", "execution_count": 1, "id": "ce9403c5", "metadata": {}, "outputs": [], "source": ["grid_search_forest.best_estimator_"]}, {"cell_type": "code", "execution_count": 1, "id": "44bb6cd9", "metadata": {}, "outputs": [], "source": ["grid_search_forest.best_score_"]}, {"cell_type": "code", "execution_count": 1, "id": "783ee561", "metadata": {}, "outputs": [], "source": ["grid_best= grid_search_forest.best_estimator_.predict(x_test)\nevaluate_model(grid_best, y_test)"]}, {"cell_type": "markdown", "id": "cce81d31", "metadata": {}, "source": ["## Final Notes:\n- The final f1-score increased slightly, but if we increased the number of trees i.e.(n_estimators), it may get better.\n- Adding more features like (weather) will result in a better performance, but it's not availble right now."]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}