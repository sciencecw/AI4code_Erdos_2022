{"cells": [{"cell_type": "markdown", "id": "9423566f", "metadata": {}, "source": ["# Speech Emotion Recognition with CNN"]}, {"cell_type": "markdown", "id": "f4454e06", "metadata": {}, "source": ["## Data Exploration"]}, {"cell_type": "code", "execution_count": 1, "id": "b11887d6", "metadata": {}, "outputs": [], "source": ["# Import libraries \nimport librosa\nimport librosa.display\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom matplotlib.pyplot import specgram\nimport pandas as pd\nimport glob \nfrom sklearn.metrics import confusion_matrix\nimport IPython.display as ipd  # To play sound in the notebook\nimport os\nimport sys\nimport warnings\n# ignore warnings \nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) "]}, {"cell_type": "code", "execution_count": 1, "id": "85226c40", "metadata": {}, "outputs": [], "source": ["#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\nTESS = \"/kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"\nRAV = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\nSAVEE = \"/kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/\"\nCREMA = \"/kaggle/input/cremad/AudioWAV/\"\n\n# Run one example \ndir_list = os.listdir(SAVEE)\ndir_list[0:5]"]}, {"cell_type": "markdown", "id": "766fbb5a", "metadata": {}, "source": ["### Surrey Audio-Visual Expressed Emotion (SAVEE)"]}, {"cell_type": "code", "execution_count": 1, "id": "85d7a415", "metadata": {}, "outputs": [], "source": ["# Get the data location for SAVEE\ndir_list = os.listdir(SAVEE)\n\n# parse the filename to get the emotions\nemotion=[]\npath = []\nfor i in dir_list:\n    if i[-8:-6]=='_a':\n        emotion.append('male_angry')\n    elif i[-8:-6]=='_d':\n        emotion.append('male_disgust')\n    elif i[-8:-6]=='_f':\n        emotion.append('male_fear')\n    elif i[-8:-6]=='_h':\n        emotion.append('male_happy')\n    elif i[-8:-6]=='_n':\n        emotion.append('male_neutral')\n    elif i[-8:-6]=='sa':\n        emotion.append('male_sad')\n    elif i[-8:-6]=='su':\n        emotion.append('male_surprise')\n    else:\n        emotion.append('male_error') \n    path.append(SAVEE + i)\n\n# Now check out the label count distribution \nSAVEE_df = pd.DataFrame(emotion, columns = ['labels'])\nSAVEE_df['source'] = 'SAVEE'\nSAVEE_df = pd.concat([SAVEE_df, pd.DataFrame(path, columns = ['path'])], axis = 1)\nSAVEE_df.labels.value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "a0967915", "metadata": {}, "outputs": [], "source": ["# use the well known Librosa library for this task \nfname = SAVEE + 'DC_f11.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)"]}, {"cell_type": "code", "execution_count": 1, "id": "5bf3ca86", "metadata": {}, "outputs": [], "source": ["# Lets play a happy track\nfname = SAVEE + 'DC_h11.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)"]}, {"cell_type": "markdown", "id": "0716aecb", "metadata": {}, "source": ["### Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) "]}, {"cell_type": "code", "execution_count": 1, "id": "a383583f", "metadata": {}, "outputs": [], "source": ["dir_list = os.listdir(RAV)\ndir_list.sort()\n\nemotion = []\ngender = []\npath = []\nfor i in dir_list:\n    fname = os.listdir(RAV + i)\n    for f in fname:\n        part = f.split('.')[0].split('-')\n        emotion.append(int(part[2]))\n        temp = int(part[6])\n        if temp%2 == 0:\n            temp = \"female\"\n        else:\n            temp = \"male\"\n        gender.append(temp)\n        path.append(RAV + i + '/' + f)\n\n        \nRAV_df = pd.DataFrame(emotion)\nRAV_df = RAV_df.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'})\nRAV_df = pd.concat([pd.DataFrame(gender),RAV_df],axis=1)\nRAV_df.columns = ['gender','emotion']\nRAV_df['labels'] =RAV_df.gender + '_' + RAV_df.emotion\nRAV_df['source'] = 'RAVDESS'  \nRAV_df = pd.concat([RAV_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nRAV_df = RAV_df.drop(['gender', 'emotion'], axis=1)\nRAV_df.labels.value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "e617af7b", "metadata": {}, "outputs": [], "source": ["# Pick a fearful track\nfname = RAV + 'Actor_14/03-01-06-02-02-02-14.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)"]}, {"cell_type": "code", "execution_count": 1, "id": "583afaa3", "metadata": {}, "outputs": [], "source": ["# Pick a happy track\nfname = RAV + 'Actor_14/03-01-03-02-02-02-14.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)"]}, {"cell_type": "markdown", "id": "4bb358c3", "metadata": {}, "source": ["### Toronto Emotional Speech Set (TESS) "]}, {"cell_type": "code", "execution_count": 1, "id": "3a83b268", "metadata": {}, "outputs": [], "source": ["dir_list = os.listdir(TESS)\ndir_list.sort()\ndir_list"]}, {"cell_type": "code", "execution_count": 1, "id": "6b112309", "metadata": {}, "outputs": [], "source": ["path = []\nemotion = []\n\nfor i in dir_list:\n    fname = os.listdir(TESS + i)\n    for f in fname:\n        if i == 'OAF_angry' or i == 'YAF_angry':\n            emotion.append('female_angry')\n        elif i == 'OAF_disgust' or i == 'YAF_disgust':\n            emotion.append('female_disgust')\n        elif i == 'OAF_Fear' or i == 'YAF_fear':\n            emotion.append('female_fear')\n        elif i == 'OAF_happy' or i == 'YAF_happy':\n            emotion.append('female_happy')\n        elif i == 'OAF_neutral' or i == 'YAF_neutral':\n            emotion.append('female_neutral')                                \n        elif i == 'OAF_Pleasant_surprise' or i == 'YAF_pleasant_surprised':\n            emotion.append('female_surprise')               \n        elif i == 'OAF_Sad' or i == 'YAF_sad':\n            emotion.append('female_sad')\n        else:\n            emotion.append('Unknown')\n        path.append(TESS + i + \"/\" + f)\n\nTESS_df = pd.DataFrame(emotion, columns = ['labels'])\nTESS_df['source'] = 'TESS'\nTESS_df = pd.concat([TESS_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nTESS_df.labels.value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "9fcba47b", "metadata": {}, "outputs": [], "source": ["# lets play a fearful track \nfname = TESS + 'YAF_fear/YAF_dog_fear.wav' \n\ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)"]}, {"cell_type": "code", "execution_count": 1, "id": "5f61831d", "metadata": {}, "outputs": [], "source": ["# lets play a happy track \nfname =  TESS + 'YAF_happy/YAF_dog_happy.wav' \n\ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)"]}, {"cell_type": "markdown", "id": "e484c582", "metadata": {}, "source": ["### Crowd Sourced Emotional Multimodal Actors Dataset (CREDMA-D) "]}, {"cell_type": "code", "execution_count": 1, "id": "799414fd", "metadata": {}, "outputs": [], "source": ["dir_list = os.listdir(CREMA)\ndir_list.sort()\nprint(dir_list[0:10])"]}, {"cell_type": "code", "execution_count": 1, "id": "1fb6277a", "metadata": {}, "outputs": [], "source": ["gender = []\nemotion = []\npath = []\nfemale = [1002,1003,1004,1006,1007,1008,1009,1010,1012,1013,1018,1020,1021,1024,1025,1028,1029,1030,1037,1043,1046,1047,1049,\n          1052,1053,1054,1055,1056,1058,1060,1061,1063,1072,1073,1074,1075,1076,1078,1079,1082,1084,1089,1091]\n\nfor i in dir_list: \n    part = i.split('_')\n    if int(part[0]) in female:\n        temp = 'female'\n    else:\n        temp = 'male'\n    gender.append(temp)\n    if part[2] == 'SAD' and temp == 'male':\n        emotion.append('male_sad')\n    elif part[2] == 'ANG' and temp == 'male':\n        emotion.append('male_angry')\n    elif part[2] == 'DIS' and temp == 'male':\n        emotion.append('male_disgust')\n    elif part[2] == 'FEA' and temp == 'male':\n        emotion.append('male_fear')\n    elif part[2] == 'HAP' and temp == 'male':\n        emotion.append('male_happy')\n    elif part[2] == 'NEU' and temp == 'male':\n        emotion.append('male_neutral')\n    elif part[2] == 'SAD' and temp == 'female':\n        emotion.append('female_sad')\n    elif part[2] == 'ANG' and temp == 'female':\n        emotion.append('female_angry')\n    elif part[2] == 'DIS' and temp == 'female':\n        emotion.append('female_disgust')\n    elif part[2] == 'FEA' and temp == 'female':\n        emotion.append('female_fear')\n    elif part[2] == 'HAP' and temp == 'female':\n        emotion.append('female_happy')\n    elif part[2] == 'NEU' and temp == 'female':\n        emotion.append('female_neutral')\n    else:\n        emotion.append('Unknown')\n    path.append(CREMA + i)\n    \nCREMA_df = pd.DataFrame(emotion, columns = ['labels'])\nCREMA_df['source'] = 'CREMA'\nCREMA_df = pd.concat([CREMA_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nCREMA_df.labels.value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "28bf51de", "metadata": {}, "outputs": [], "source": ["# use the well known Librosa library for this task \nfname = CREMA + '1012_IEO_HAP_HI.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)"]}, {"cell_type": "code", "execution_count": 1, "id": "ac492907", "metadata": {}, "outputs": [], "source": ["# A fearful track\nfname = CREMA + '1012_IEO_FEA_HI.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)"]}, {"cell_type": "markdown", "id": "c1db497e", "metadata": {}, "source": ["### Combining the 4 Datasets "]}, {"cell_type": "code", "execution_count": 1, "id": "9a692765", "metadata": {}, "outputs": [], "source": ["df = pd.concat([SAVEE_df, RAV_df, TESS_df, CREMA_df], axis = 0)\nprint(df.labels.value_counts())\ndf.head()\ndf.to_csv(\"Data_path.csv\",index=False)"]}, {"cell_type": "markdown", "id": "16341dde", "metadata": {}, "source": ["## Feature Extraction "]}, {"cell_type": "code", "execution_count": 1, "id": "ec6e3c18", "metadata": {}, "outputs": [], "source": ["# Import our libraries\nimport librosa\nimport librosa.display\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import specgram\nimport pandas as pd\nimport os\nimport IPython.display as ipd  # To play sound in the notebook"]}, {"cell_type": "code", "execution_count": 1, "id": "b9a900bb", "metadata": {}, "outputs": [], "source": ["# Source - RAVDESS; Gender - Female; Emotion - Angry \npath = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_08/03-01-05-02-01-01-08.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n\n# audio wave\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.waveplot(X, sr=sample_rate)\nplt.title('Audio sampled at 44100 hrz')\n\n# MFCC\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis='time')\nplt.ylabel('MFCC')\nplt.colorbar()\n\nipd.Audio(path)"]}, {"cell_type": "code", "execution_count": 1, "id": "92b25f3f", "metadata": {}, "outputs": [], "source": ["# Source - RAVDESS; Gender - Male; Emotion - Angry \npath = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_09/03-01-05-01-01-01-09.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n\n# audio wave\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.waveplot(X, sr=sample_rate)\nplt.title('Audio sampled at 44100 hrz')\n\n# MFCC\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis='time')\nplt.ylabel('MFCC')\nplt.colorbar()\n\nipd.Audio(path)"]}, {"cell_type": "code", "execution_count": 1, "id": "e2d86aa1", "metadata": {}, "outputs": [], "source": ["# Source - RAVDESS; Gender - Female; Emotion - Happy \npath = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_12/03-01-03-01-02-01-12.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n\n# audio wave\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.waveplot(X, sr=sample_rate)\nplt.title('Audio sampled at 44100 hrz')\n\n# MFCC\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis='time')\nplt.ylabel('MFCC')\nplt.colorbar()\n\nipd.Audio(path)"]}, {"cell_type": "code", "execution_count": 1, "id": "27b05f2a", "metadata": {}, "outputs": [], "source": ["# Source - RAVDESS; Gender - Male; Emotion - Happy \npath = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_11/03-01-03-01-02-02-11.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n\n# audio wave\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.waveplot(X, sr=sample_rate)\nplt.title('Audio sampled at 44100 hrz')\n\n# MFCC\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis='time')\nplt.ylabel('MFCC')\nplt.colorbar()\n\nipd.Audio(path)\n"]}, {"cell_type": "code", "execution_count": 1, "id": "4f779eae", "metadata": {}, "outputs": [], "source": ["# Source - RAVDESS; Gender - Female; Emotion - Angry \npath = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_08/03-01-05-02-01-01-08.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nfemale = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\nfemale = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\nprint(len(female))\n\n# Source - RAVDESS; Gender - Male; Emotion - Angry \npath = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_09/03-01-05-01-01-01-09.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmale = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\nmale = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\nprint(len(male))\n\n# audio wave\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nplt.plot(female, label='female')\nplt.plot(male, label='male')\nplt.legend()"]}, {"cell_type": "code", "execution_count": 1, "id": "7137eec4", "metadata": {}, "outputs": [], "source": ["# Source - RAVDESS; Gender - Female; Emotion - happy \npath = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_12/03-01-03-01-02-01-12.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nfemale = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\nfemale = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\nprint(len(female))\n\n# Source - RAVDESS; Gender - Male; Emotion - happy \npath = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_11/03-01-03-01-02-02-11.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmale = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\nmale = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\nprint(len(male))\n\n# Plot the two audio waves together\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nplt.plot(female, label='female')\nplt.plot(male, label='male')\nplt.legend()"]}, {"cell_type": "markdown", "id": "47916f6c", "metadata": {}, "source": ["## Data Preparation  & Processing "]}, {"cell_type": "code", "execution_count": 1, "id": "93e33f8a", "metadata": {}, "outputs": [], "source": ["# Importing required libraries \n# Keras\nimport keras\nfrom keras import regularizers\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model, model_from_json\nfrom keras.layers import Dense, Embedding, LSTM\nfrom keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\nfrom keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\nfrom keras.utils import np_utils, to_categorical\nfrom keras.callbacks import ModelCheckpoint\n\n# sklearn\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Other  \nimport librosa\nimport librosa.display\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom matplotlib.pyplot import specgram\nimport pandas as pd\nimport seaborn as sns\nimport glob \nimport os\nimport pickle\nimport IPython.display as ipd  # To play sound in the notebook"]}, {"cell_type": "code", "execution_count": 1, "id": "c1d4ad73", "metadata": {}, "outputs": [], "source": ["# lets pick up the meta-data that we got from our first part of the Kernel\nref = pd.read_csv(\"/kaggle/input/datapath/Data_path.csv\")\nref.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "1d91f2eb", "metadata": {}, "outputs": [], "source": ["# Note this takes a couple of minutes (~10 mins) as we're iterating over 4 datasets \ndf = pd.DataFrame(columns=['feature'])\n\n# loop feature extraction over the entire dataset\ncounter=0\nfor index,path in enumerate(ref.path):\n    X, sample_rate = librosa.load(path\n                                  , res_type='kaiser_fast'\n                                  ,duration=2.5\n                                  ,sr=44100\n                                  ,offset=0.5\n                                 )\n    sample_rate = np.array(sample_rate)\n    \n    # mean as the feature. Could do min and max etc as well. \n    mfccs = np.mean(librosa.feature.mfcc(y=X, \n                                        sr=sample_rate, \n                                        n_mfcc=13),\n                    axis=0)\n    df.loc[counter] = [mfccs]\n    counter=counter+1   \n\n# Check a few records to make sure its processed successfully\nprint(len(df))\ndf.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "517627eb", "metadata": {}, "outputs": [], "source": ["# Now extract the mean bands to its own feature columns\ndf = pd.concat([ref,pd.DataFrame(df['feature'].values.tolist())],axis=1)\ndf[:5]"]}, {"cell_type": "code", "execution_count": 1, "id": "031ce5a8", "metadata": {}, "outputs": [], "source": ["# replace NA with 0\ndf=df.fillna(0)\nprint(df.shape)\ndf[:5]"]}, {"cell_type": "markdown", "id": "1c45d5ab", "metadata": {}, "source": ["### Splitting the Dataset into Training and Test "]}, {"cell_type": "code", "execution_count": 1, "id": "72337aaa", "metadata": {}, "outputs": [], "source": ["# Split between train and test \nX_train, X_test, y_train, y_test = train_test_split(df.drop(['path','labels','source'],axis=1)\n                                                    , df.labels\n                                                    , test_size=0.25\n                                                    , shuffle=True\n                                                    , random_state=42\n                                                   )\n\n# Lets see how the data present itself before normalisation \nX_train[150:160]"]}, {"cell_type": "code", "execution_count": 1, "id": "c3cd5992", "metadata": {}, "outputs": [], "source": ["# Lts do data normalization \nmean = np.mean(X_train, axis=0)\nstd = np.std(X_train, axis=0)\n\nX_train = (X_train - mean)/std\nX_test = (X_test - mean)/std\n\n# Check the dataset now \nX_train[150:160]\n"]}, {"cell_type": "code", "execution_count": 1, "id": "6d60073f", "metadata": {}, "outputs": [], "source": ["# Lets few preparation steps to get it into the correct format for Keras \nX_train = np.array(X_train)\ny_train = np.array(y_train)\nX_test = np.array(X_test)\ny_test = np.array(y_test)\n\n# one hot encode the target \nlb = LabelEncoder()\ny_train = np_utils.to_categorical(lb.fit_transform(y_train))\ny_test = np_utils.to_categorical(lb.fit_transform(y_test))\n\nprint(X_train.shape)\nprint(lb.classes_)\n#print(y_train[0:10])\n#print(y_test[0:10])\n\n# Pickel the lb object for future use \nfilename = 'labels'\noutfile = open(filename,'wb')\npickle.dump(lb,outfile)\noutfile.close()"]}, {"cell_type": "code", "execution_count": 1, "id": "d2d8f558", "metadata": {}, "outputs": [], "source": ["X_train = np.expand_dims(X_train, axis=2)\nX_test = np.expand_dims(X_test, axis=2)\nX_train.shape"]}, {"cell_type": "markdown", "id": "f45a98c5", "metadata": {}, "source": ["## Modelling "]}, {"cell_type": "code", "execution_count": 1, "id": "cf85ada7", "metadata": {}, "outputs": [], "source": ["# New model\nmodel = Sequential()\nmodel.add(Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1],1)))  # X_train.shape[1] = No. of Columns\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(256, 8, padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\nmodel.add(MaxPooling1D(pool_size=(8)))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\nmodel.add(MaxPooling1D(pool_size=(8)))\nmodel.add(Conv1D(64, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(64, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(14)) # Target class number\nmodel.add(Activation('softmax'))\n# opt = keras.optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)\n# opt = keras.optimizers.Adam(lr=0.0001)\nopt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\nmodel.summary()"]}, {"cell_type": "markdown", "id": "cfb34c3e", "metadata": {}, "source": ["### Description of CNN-Model"]}, {"cell_type": "code", "execution_count": 1, "id": "98624848", "metadata": {}, "outputs": [], "source": ["model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\nmodel_history=model.fit(X_train, y_train, batch_size=16, epochs=100, validation_data=(X_test, y_test))"]}, {"cell_type": "code", "execution_count": 1, "id": "c42aabaa", "metadata": {}, "outputs": [], "source": ["plt.plot(model_history.history['loss'])\nplt.plot(model_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "3a8c2eaa", "metadata": {}, "outputs": [], "source": ["# Save model and weights\nmodel_name = 'Emotion_Model.h5'\nsave_dir = os.path.join(os.getcwd(), 'saved_models')\n\nif not os.path.isdir(save_dir):\n    os.makedirs(save_dir)\nmodel_path = os.path.join(save_dir, model_name)\nmodel.save(model_path)\nprint('Save model and weights at %s ' % model_path)\n\n# Save the model to disk\nmodel_json = model.to_json()\nwith open(\"model_json.json\", \"w\") as json_file:\n    json_file.write(model_json)"]}, {"cell_type": "code", "execution_count": 1, "id": "e103199b", "metadata": {}, "outputs": [], "source": ["# loading json and model architecture \njson_file = open('model_json.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n\n# load weights into new model\nloaded_model.load_weights(\"saved_models/Emotion_Model.h5\")\nprint(\"Loaded model from disk\")\n \n# Keras optimiser\nopt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\nloaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\nscore = loaded_model.evaluate(X_test, y_test, verbose=0)\nprint(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"]}, {"cell_type": "code", "execution_count": 1, "id": "857c4dec", "metadata": {}, "outputs": [], "source": ["preds = loaded_model.predict(X_test, \n                         batch_size=16, \n                         verbose=1)\n\npreds=preds.argmax(axis=1)\npreds"]}, {"cell_type": "code", "execution_count": 1, "id": "abe99812", "metadata": {}, "outputs": [], "source": ["# predictions \npreds = preds.astype(int).flatten()\npreds = (lb.inverse_transform((preds)))\npreds = pd.DataFrame({'predictedvalues': preds})\n\n# Actual labels\nactual=y_test.argmax(axis=1)\nactual = actual.astype(int).flatten()\nactual = (lb.inverse_transform((actual)))\nactual = pd.DataFrame({'actualvalues': actual})\n\n# Lets combined both of them into a single dataframe\nfinaldf = actual.join(preds)\nfinaldf[170:180]"]}, {"cell_type": "code", "execution_count": 1, "id": "eb236f71", "metadata": {}, "outputs": [], "source": ["# Write out the predictions to disk\nfinaldf.to_csv('Predictions.csv', index=False)\nfinaldf.groupby('predictedvalues').count()"]}, {"cell_type": "markdown", "id": "d0213ab6", "metadata": {}, "source": ["### Emotion vs Gender Accuracy "]}, {"cell_type": "code", "execution_count": 1, "id": "71d4f2ed", "metadata": {}, "outputs": [], "source": ["# the confusion matrix heat map plot\ndef print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n    \n    Arguments\n    ---------\n    confusion_matrix: numpy.ndarray\n        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n        Similarly constructed ndarrays can also be used.\n    class_names: list\n        An ordered list of class names, in the order they index the given confusion matrix.\n    figsize: tuple\n        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n        the second determining the vertical size. Defaults to (10,7).\n    fontsize: int\n        Font size for axes labels. Defaults to 14.\n        \n    Returns\n    -------\n    matplotlib.figure.Figure\n        The resulting confusion matrix figure\n    \"\"\"\n    df_cm = pd.DataFrame(\n        confusion_matrix, index=class_names, columns=class_names, \n    )\n    fig = plt.figure(figsize=figsize)\n    try:\n        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n    except ValueError:\n        raise ValueError(\"Confusion matrix values must be integers.\")\n        \n    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Gender recode function\ndef gender(row):\n    if row == 'female_disgust' or 'female_fear' or 'female_happy' or 'female_sad' or 'female_surprise' or 'female_neutral':\n        return 'female'\n    elif row == 'male_angry' or 'male_fear' or 'male_happy' or 'male_sad' or 'male_surprise' or 'male_neutral' or 'male_disgust':\n        return 'male'"]}, {"cell_type": "code", "execution_count": 1, "id": "6885370d", "metadata": {}, "outputs": [], "source": ["# Get the predictions file \nfinaldf = pd.read_csv(\"Predictions.csv\")\nclasses = finaldf.actualvalues.unique()\nclasses.sort()    \n\n# Confusion matrix \nc = confusion_matrix(finaldf.actualvalues, finaldf.predictedvalues)\nprint(accuracy_score(finaldf.actualvalues, finaldf.predictedvalues))\nprint_confusion_matrix(c, class_names = classes)"]}, {"cell_type": "code", "execution_count": 1, "id": "3113fce8", "metadata": {}, "outputs": [], "source": ["# Classification report \nclasses = finaldf.actualvalues.unique()\nclasses.sort()    \nprint(classification_report(finaldf.actualvalues, finaldf.predictedvalues, target_names=classes))"]}, {"cell_type": "markdown", "id": "bf5cb2e7", "metadata": {}, "source": ["### Gender Accuracy "]}, {"cell_type": "code", "execution_count": 1, "id": "266073f7", "metadata": {}, "outputs": [], "source": ["modidf = finaldf\nmodidf['actualvalues'] = finaldf.actualvalues.replace({'female_angry':'female'\n                                       , 'female_disgust':'female'\n                                       , 'female_fear':'female'\n                                       , 'female_happy':'female'\n                                       , 'female_sad':'female'\n                                       , 'female_surprise':'female'\n                                       , 'female_neutral':'female'\n                                       , 'male_angry':'male'\n                                       , 'male_fear':'male'\n                                       , 'male_happy':'male'\n                                       , 'male_sad':'male'\n                                       , 'male_surprise':'male'\n                                       , 'male_neutral':'male'\n                                       , 'male_disgust':'male'\n                                      })\n\nmodidf['predictedvalues'] = finaldf.predictedvalues.replace({'female_angry':'female'\n                                       , 'female_disgust':'female'\n                                       , 'female_fear':'female'\n                                       , 'female_happy':'female'\n                                       , 'female_sad':'female'\n                                       , 'female_surprise':'female'\n                                       , 'female_neutral':'female'\n                                       , 'male_angry':'male'\n                                       , 'male_fear':'male'\n                                       , 'male_happy':'male'\n                                       , 'male_sad':'male'\n                                       , 'male_surprise':'male'\n                                       , 'male_neutral':'male'\n                                       , 'male_disgust':'male'\n                                      })\n\nclasses = modidf.actualvalues.unique()  \nclasses.sort() \n\n# Confusion matrix \nc = confusion_matrix(modidf.actualvalues, modidf.predictedvalues)\nprint(accuracy_score(modidf.actualvalues, modidf.predictedvalues))\nprint_confusion_matrix(c, class_names = classes)"]}, {"cell_type": "code", "execution_count": 1, "id": "887b8d5e", "metadata": {}, "outputs": [], "source": ["# Classification report \nclasses = modidf.actualvalues.unique()\nclasses.sort()    \nprint(classification_report(modidf.actualvalues, modidf.predictedvalues, target_names=classes))"]}, {"cell_type": "markdown", "id": "3af9fed2", "metadata": {}, "source": ["### Emotion Accuracy "]}, {"cell_type": "code", "execution_count": 1, "id": "7bab8c0e", "metadata": {}, "outputs": [], "source": ["modidf = pd.read_csv(\"Predictions.csv\")\nmodidf['actualvalues'] = modidf.actualvalues.replace({'female_angry':'angry'\n                                       , 'female_disgust':'disgust'\n                                       , 'female_fear':'fear'\n                                       , 'female_happy':'happy'\n                                       , 'female_sad':'sad'\n                                       , 'female_surprise':'surprise'\n                                       , 'female_neutral':'neutral'\n                                       , 'male_angry':'angry'\n                                       , 'male_fear':'fear'\n                                       , 'male_happy':'happy'\n                                       , 'male_sad':'sad'\n                                       , 'male_surprise':'surprise'\n                                       , 'male_neutral':'neutral'\n                                       , 'male_disgust':'disgust'\n                                      })\n\nmodidf['predictedvalues'] = modidf.predictedvalues.replace({'female_angry':'angry'\n                                       , 'female_disgust':'disgust'\n                                       , 'female_fear':'fear'\n                                       , 'female_happy':'happy'\n                                       , 'female_sad':'sad'\n                                       , 'female_surprise':'surprise'\n                                       , 'female_neutral':'neutral'\n                                       , 'male_angry':'angry'\n                                       , 'male_fear':'fear'\n                                       , 'male_happy':'happy'\n                                       , 'male_sad':'sad'\n                                       , 'male_surprise':'surprise'\n                                       , 'male_neutral':'neutral'\n                                       , 'male_disgust':'disgust'\n                                      })\n\nclasses = modidf.actualvalues.unique() \nclasses.sort() \n\n# Confusion matrix \nc = confusion_matrix(modidf.actualvalues, modidf.predictedvalues)\nprint(accuracy_score(modidf.actualvalues, modidf.predictedvalues))\nprint_confusion_matrix(c, class_names = classes)"]}, {"cell_type": "code", "execution_count": 1, "id": "c2baff96", "metadata": {}, "outputs": [], "source": ["# Classification report \nclasses = modidf.actualvalues.unique()\nclasses.sort()    \nprint(classification_report(modidf.actualvalues, modidf.predictedvalues, target_names=classes))"]}, {"cell_type": "markdown", "id": "d535d4f5", "metadata": {}, "source": ["## Testing the Model on External Audio "]}, {"cell_type": "code", "execution_count": 1, "id": "47769526", "metadata": {}, "outputs": [], "source": ["from keras.models import Sequential, Model, model_from_json\nimport matplotlib.pyplot as plt\nimport keras \nimport pickle\nimport wave  # !pip install wave\nimport os\nimport pandas as pd\nimport numpy as np\nimport sys\nimport warnings\nimport librosa\nimport librosa.display\nimport IPython.display as ipd  # To play sound in the notebook\n\n# ignore warnings \nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")"]}, {"cell_type": "code", "execution_count": 1, "id": "bcecabec", "metadata": {}, "outputs": [], "source": ["data, sampling_rate = librosa.load('/kaggle/input/externaltest/DC_d02.wav')\nipd.Audio('/kaggle/input/externaltest/DC_d02.wav')"]}, {"cell_type": "code", "execution_count": 1, "id": "48e6b137", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)"]}, {"cell_type": "code", "execution_count": 1, "id": "faeba910", "metadata": {}, "outputs": [], "source": ["# loading json and model architecture \njson_file = open('model_json.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n\n# load weights into new model\nloaded_model.load_weights(\"saved_models/Emotion_Model.h5\")\nprint(\"Loaded model from disk\")\n \n# Keras optimiser\nopt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\nloaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"]}, {"cell_type": "code", "execution_count": 1, "id": "58d28e1d", "metadata": {}, "outputs": [], "source": ["# Lets transform the dataset so we can apply the predictions\nX, sample_rate = librosa.load('/kaggle/input/externaltest/DC_d02.wav'\n                              ,res_type='kaiser_fast'\n                              ,duration=2.5\n                              ,sr=44100\n                              ,offset=0.5\n                             )\n\nsample_rate = np.array(sample_rate)\nmfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13),axis=0)\nnewdf = pd.DataFrame(data=mfccs).T\nnewdf\n"]}, {"cell_type": "markdown", "id": "28e2525d", "metadata": {}, "source": ["### Predictions "]}, {"cell_type": "code", "execution_count": 1, "id": "56cf37c1", "metadata": {}, "outputs": [], "source": ["# Apply predictions\nnewdf= np.expand_dims(newdf, axis=2)\nnewpred = loaded_model.predict(newdf, \n                         batch_size=16, \n                         verbose=1)\n\nnewpred"]}, {"cell_type": "code", "execution_count": 1, "id": "c1d28cae", "metadata": {}, "outputs": [], "source": ["filename = '/kaggle/input/labels/labels'\ninfile = open(filename,'rb')\nlb = pickle.load(infile)\ninfile.close()\n\n# Get the final predicted label\nfinal = newpred.argmax(axis=1)\nfinal = final.astype(int).flatten()\nfinal = (lb.inverse_transform((final)))\nprint(final) #emo(final) #gender(final) "]}, {"cell_type": "markdown", "id": "d927a06a", "metadata": {}, "source": ["## Second Test "]}, {"cell_type": "code", "execution_count": 1, "id": "da911bb2", "metadata": {}, "outputs": [], "source": ["data, sampling_rate = librosa.load('/kaggle/input/externaltest/DC_d06.wav')\nipd.Audio('/kaggle/input/externaltest/DC_d06.wav')"]}, {"cell_type": "code", "execution_count": 1, "id": "4207d68e", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)"]}, {"cell_type": "code", "execution_count": 1, "id": "584189d7", "metadata": {}, "outputs": [], "source": ["# loading json and model architecture \njson_file = open('model_json.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n\n# load weights into new model\nloaded_model.load_weights(\"saved_models/Emotion_Model.h5\")\nprint(\"Loaded model from disk\")\n \n# Keras optimiser\nopt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\nloaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"]}, {"cell_type": "code", "execution_count": 1, "id": "c8b33004", "metadata": {}, "outputs": [], "source": ["# Lets transform the dataset so we can apply the predictions\nX, sample_rate = librosa.load('/kaggle/input/externaltest/DC_d06.wav'\n                              ,res_type='kaiser_fast'\n                              ,duration=2.5\n                              ,sr=44100\n                              ,offset=0.5\n                             )\n\nsample_rate = np.array(sample_rate)\nmfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13),axis=0)\nnewdf = pd.DataFrame(data=mfccs).T\nnewdf"]}, {"cell_type": "code", "execution_count": 1, "id": "5f038115", "metadata": {}, "outputs": [], "source": ["# Apply predictions\nnewdf= np.expand_dims(newdf, axis=2)\nnewpred = loaded_model.predict(newdf, \n                         batch_size=16, \n                         verbose=1)\n\nnewpred"]}, {"cell_type": "code", "execution_count": 1, "id": "d7cdd794", "metadata": {}, "outputs": [], "source": ["filename = '/kaggle/input/labels/labels'\ninfile = open(filename,'rb')\nlb = pickle.load(infile)\ninfile.close()\n\n# Get the final predicted label\nfinal = newpred.argmax(axis=1)\nfinal = final.astype(int).flatten()\nfinal = (lb.inverse_transform((final)))\nprint(final) #emo(final) #gender(final) "]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}