{"cells": [{"cell_type": "markdown", "id": "10e93c8e", "metadata": {}, "source": ["In this notebook, we're going to make a simple model to classify an article as promotional or not promotional. This notebook serves as a basic beginner's guide to LSTMs and text processing."]}, {"cell_type": "code", "execution_count": 1, "id": "fbf014e2", "metadata": {}, "outputs": [], "source": ["import numpy as np\nRANDOM_SEED = 4\nnp.random.seed(RANDOM_SEED) # set random seed for reproducability"]}, {"cell_type": "markdown", "id": "f7b0f00f", "metadata": {}, "source": ["# Import Data"]}, {"cell_type": "code", "execution_count": 1, "id": "a549b066", "metadata": {}, "outputs": [], "source": ["import pandas as pd\ndf_neut = pd.read_csv(\"../input/wikipedia-promotional-articles/good.csv\")\ndf_prom = pd.read_csv(\"../input/wikipedia-promotional-articles/promotional.csv\")"]}, {"cell_type": "markdown", "id": "748046ab", "metadata": {}, "source": ["In both datasets, we only need the text."]}, {"cell_type": "code", "execution_count": 1, "id": "ee24b14a", "metadata": {}, "outputs": [], "source": ["df_prom = df_prom.drop(df_prom.columns[1:], axis=1)\ndf_neut = df_neut.drop(df_neut.columns[1:], axis=1)"]}, {"cell_type": "markdown", "id": "3ed671a8", "metadata": {}, "source": ["Our data is structured where each row corresponds to the text of one article."]}, {"cell_type": "code", "execution_count": 1, "id": "d993ea69", "metadata": {}, "outputs": [], "source": ["df_neut.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "e8cb0b29", "metadata": {}, "outputs": [], "source": ["df_prom.head()"]}, {"cell_type": "markdown", "id": "501bc393", "metadata": {}, "source": ["It will be easier to use we combine both into one dataframe. When we do this, we need to add labels corresponding to promotional or neutral as well. Here, I'm using promotional = 1 and neutral = 0."]}, {"cell_type": "code", "execution_count": 1, "id": "e8b99665", "metadata": {}, "outputs": [], "source": ["df_neut.insert(1, 'label', 0) # neutral labels\ndf_prom.insert(1, 'label', 1) # promotional labels"]}, {"cell_type": "code", "execution_count": 1, "id": "1966927d", "metadata": {}, "outputs": [], "source": ["df_prom.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "630632fc", "metadata": {}, "outputs": [], "source": ["df_neut.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "9b7a7ab4", "metadata": {}, "outputs": [], "source": ["df = pd.concat((df_neut, df_prom), ignore_index=True, axis=0) # merge dataframes"]}, {"cell_type": "markdown", "id": "75d32012", "metadata": {}, "source": ["Currently our data has all neutral articles first and all promotional articles second. The promotional articles are alphabetically sorted as well. Because of this, we're going to randomize the order."]}, {"cell_type": "code", "execution_count": 1, "id": "1c9969d1", "metadata": {}, "outputs": [], "source": ["df.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "78e2b255", "metadata": {}, "outputs": [], "source": ["df = df.reindex(np.random.permutation(df.index))\ndf.head()"]}, {"cell_type": "markdown", "id": "3a6a87d6", "metadata": {}, "source": ["Prior to training, we need to split the data into training/testing sets as well."]}, {"cell_type": "code", "execution_count": 1, "id": "29bf6a81", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=RANDOM_SEED)"]}, {"cell_type": "markdown", "id": "0bfa5d4f", "metadata": {}, "source": ["# Text Processing"]}, {"cell_type": "markdown", "id": "b8164c6a", "metadata": {}, "source": ["When we process text data, we need to vectorize it, that is, convert it to data that can be interpreted by the model. To do this, we're using the Tokenizer class in Keras.\n\nTokenizer preprocesses the words, removing symbols and making the text lowercass, then adds all unique words to a dictionary. When we then vectorize text, that text gets converted to a sequence of indices, corresponding to a word's position in the dictionary."]}, {"cell_type": "code", "execution_count": 1, "id": "892c0c39", "metadata": {}, "outputs": [], "source": ["from keras.preprocessing.text import Tokenizer\n\n# The maximum number of words to be used. (most frequent)\nMAX_NB_WORDS = 50000\n\ntext_data = [str(txt) for txt in df_train['text'].values] # convert text data to strings\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True) # create tokenizer object\ntokenizer.fit_on_texts(text_data) # make dictionary\n\nx_train = tokenizer.texts_to_sequences(text_data) # vectorize dataset"]}, {"cell_type": "markdown", "id": "a235c245", "metadata": {}, "source": ["We need to then convert our data to a fixed shape. The following Keras function will pad each document with a default value of 0 if a given sequence is smaller than the target sequence length and truncate if it is larger."]}, {"cell_type": "code", "execution_count": 1, "id": "22fc772a", "metadata": {}, "outputs": [], "source": ["from keras.preprocessing import sequence\n\n# Max number of words in each sequence\nMAX_SEQUENCE_LENGTH = 400\n\nx_train = sequence.pad_sequences(x_train, maxlen=MAX_SEQUENCE_LENGTH)"]}, {"cell_type": "markdown", "id": "769d6cb3", "metadata": {}, "source": ["Getting the test labels as well"]}, {"cell_type": "code", "execution_count": 1, "id": "49656906", "metadata": {}, "outputs": [], "source": ["y_train = df_train['label'].values"]}, {"cell_type": "markdown", "id": "ee7d5486", "metadata": {}, "source": ["# Creating the Model"]}, {"cell_type": "code", "execution_count": 1, "id": "e84147fd", "metadata": {}, "outputs": [], "source": ["from keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.optimizers import Adam\n\nmodel = Sequential()"]}, {"cell_type": "markdown", "id": "d14238e8", "metadata": {}, "source": ["**An issue with word representation:**\n\nRecall that when we vectorized our data, we converted each word into indices. This allows words to be processed, but it creates the issue that the values lose their meaning. In image data, the magnitude of a datapoint corresponds the the brightness of a pixel, so a larger value will correspond to a brighter pixel. However, the magnitude of datapoints in a vectorized dataset don't have that meaning -- a point with value 5000 doesn't necessarily have more meaning than a point with the value 0. If we represent the text as categorical data with one-hot arrays, we get the issue of sparcity: with a vocabulary of 50000 words, where only 400 words are used per document, this method would be extremely inefficient.\n\nThis is why we use an embedding layer. An embedding layer is simply a matrix where each row corresponds to a representation of a word in the vocabulary. Each representation is a 1D vector of real numbers that is learned when training. Now, we can process words with vectors of managable, fixed sizes and, because these representations are learned, they often are semantically reasonable as well."]}, {"cell_type": "markdown", "id": "f7b1c8b2", "metadata": {}, "source": ["![Embedding Layer](https://miro.medium.com/max/1596/1*1hPDk0gPyIBg0D5SzY5t7Q.png)\n\nSource: https://towardsdatascience.com/what-the-heck-is-word-embedding-b30f67f01c81"]}, {"cell_type": "markdown", "id": "5682e3f2", "metadata": {}, "source": ["Embedding layers aren't limited to word representations either, they can be useful in many types of data, especially when it is sparse."]}, {"cell_type": "markdown", "id": "88e5fa0d", "metadata": {}, "source": ["We create an embedding layer by specifying the input size of the mapping (number of words in vocabulary + 1), output size (length of vector representation), and input length (amount of words per sequence).\n\nWe add 1 to the vocab size because 0 is a reserved value that we use for padding."]}, {"cell_type": "code", "execution_count": 1, "id": "5bc2b622", "metadata": {}, "outputs": [], "source": ["EMBEDDING_DIM = 100\nmodel.add(Embedding(MAX_NB_WORDS+1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))"]}, {"cell_type": "markdown", "id": "0268c450", "metadata": {}, "source": ["**LSTM Layer**\n\nInitially, Recurrent Neural Networks (RNNs) allowed for neural networks to effectively process time series data by taking in past outputs as input:\n\n![rnn](http://www.easy-tensorflow.com/images/NN/01.png)\n\nSource: http://www.easy-tensorflow.com/tf-tutorials/recurrent-neural-networks/vanilla-rnn-for-classification\n\nHowever, this type of network had a very short memory. It wasn't possible for the network to take into account context information from very far in the past, something very important for processing documents. LSTMs offered a solution through a structure that consists of a cell state, hidden state, and multiple gates:\n\n![lstm](https://www.researchgate.net/publication/324600237/figure/fig3/AS:616974623178753@1524109621725/Long-Short-term-Memory-Neural-Network.png)\n\nSource: https://www.researchgate.net/figure/Long-Short-term-Memory-Neural-Network_fig3_324600237\n\nThe hidden state (\\\\(h_{t-1}\\\\)) of an LSTM consists of the output from the previous time step.\nThe cell state (\\\\(C_{t}\\\\)) passes through each time step, and its alteration is decided by a series of gates. The gates are essentially one layer neural networks that use the current input along with the hidden state to decide what values within the cell state to forget and what new information to add. Another gate also decides what information to output.\n\nThis is an extreme oversimplification and I would recommend anyone interested to check out this [great article](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)."]}, {"cell_type": "markdown", "id": "2b46a912", "metadata": {}, "source": ["Adding the LSTM layer to the model, we need to set the number of units. This number corresponds to the dimensionality of the output space and thus also the dimensionality of the cell state, the hidden state, and the neural network gates."]}, {"cell_type": "code", "execution_count": 1, "id": "8f34906d", "metadata": {}, "outputs": [], "source": ["model.add(LSTM(80))"]}, {"cell_type": "markdown", "id": "ef6a476f", "metadata": {}, "source": ["To complete the classifier, we can then add two dense layers, the last one being a sigmoid output, producing a range between 0 and 1, corresponding to not promotional and promotional respectively.\nThe dropout layer randomly sets a proportion of its input units to zero. This is added to prevent overfitting as it reduces the neural network's dependence on certain features."]}, {"cell_type": "code", "execution_count": 1, "id": "04f22fe6", "metadata": {}, "outputs": [], "source": ["model.add(Dropout(0.5))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))"]}, {"cell_type": "markdown", "id": "7b91c804", "metadata": {}, "source": ["Since we're doing binary classification, we're using [binary cross entropy loss](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a). The Adam optimizer is used to train to model."]}, {"cell_type": "code", "execution_count": 1, "id": "a19c7e7d", "metadata": {}, "outputs": [], "source": ["model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])"]}, {"cell_type": "markdown", "id": "92ea5bc5", "metadata": {}, "source": ["# Training"]}, {"cell_type": "markdown", "id": "a53d84a9", "metadata": {}, "source": ["We don't need to train for too many epochs as this model will overfit fairly easily."]}, {"cell_type": "code", "execution_count": 1, "id": "a5b5c2eb", "metadata": {}, "outputs": [], "source": ["EPOCHS = 2\nBATCH_SIZE = 64\n\nhistory = model.fit(x_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.15)"]}, {"cell_type": "markdown", "id": "bf91f5ca", "metadata": {}, "source": ["# Testing"]}, {"cell_type": "markdown", "id": "c68fb912", "metadata": {}, "source": ["Converting the test set using the previously trained tokenizer"]}, {"cell_type": "code", "execution_count": 1, "id": "b5c2d7af", "metadata": {}, "outputs": [], "source": ["x_test = np.array(tokenizer.texts_to_sequences([str(txt) for txt in df_test['text'].values]))\nx_test = sequence.pad_sequences(x_test, maxlen=MAX_SEQUENCE_LENGTH)\n\ny_test = df_test['label'].values"]}, {"cell_type": "markdown", "id": "c8e15a56", "metadata": {}, "source": ["Evaluating the model"]}, {"cell_type": "code", "execution_count": 1, "id": "da3bc6f1", "metadata": {}, "outputs": [], "source": ["scores = model.evaluate(x_test, y_test, batch_size=128)\nprint(\"The model has a test loss of %.2f and a test accuracy of %.1f%%\" % (scores[0], scores[1]*100))"]}, {"cell_type": "markdown", "id": "73f9f670", "metadata": {}, "source": ["This model alone obviously isn't ideal: a couple ideas for improvement are to increase the number of epochs, change the intial learning rate (by setting the parameter \"learning_rate\" in Adam()), or change the number of units in the LSTM."]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}