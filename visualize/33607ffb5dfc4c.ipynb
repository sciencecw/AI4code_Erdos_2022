{"cells": [{"cell_type": "code", "execution_count": 1, "id": "22b2e524", "metadata": {}, "outputs": [], "source": ["import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn"]}, {"cell_type": "code", "execution_count": 1, "id": "8b418690", "metadata": {}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]}, {"cell_type": "markdown", "id": "bd9b536b", "metadata": {}, "source": ["# Load dataset"]}, {"cell_type": "code", "execution_count": 1, "id": "1a963916", "metadata": {}, "outputs": [], "source": ["df_train = pd.read_csv('/kaggle/input/titanic/train.csv')\ndf_test = pd.read_csv('/kaggle/input/titanic/test.csv')\ndf_train.head()"]}, {"cell_type": "markdown", "id": "5b967995", "metadata": {}, "source": ["# EDA"]}, {"cell_type": "code", "execution_count": 1, "id": "5445835b", "metadata": {}, "outputs": [], "source": ["df_train.info()"]}, {"cell_type": "code", "execution_count": 1, "id": "965b48e9", "metadata": {}, "outputs": [], "source": ["df_train.describe()"]}, {"cell_type": "code", "execution_count": 1, "id": "7eb98a9a", "metadata": {}, "outputs": [], "source": ["np.round(df_train.isna().sum() / df_train.shape[0], 1)"]}, {"cell_type": "code", "execution_count": 1, "id": "38034edd", "metadata": {}, "outputs": [], "source": ["sns.heatmap(df_train.corr())\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "29f9d144", "metadata": {}, "outputs": [], "source": ["print(df_train['Survived'].value_counts())\nprint(df_train['Pclass'].value_counts())\nprint(df_train['Sex'].value_counts())\nprint(df_train['SibSp'].value_counts())\nprint(df_train['Parch'].value_counts())\nprint(df_train['Embarked'].value_counts())"]}, {"cell_type": "code", "execution_count": 1, "id": "86cbc510", "metadata": {}, "outputs": [], "source": ["plt.subplots(figsize=(4, 4))\nsns.countplot(x='Sex', hue='Survived', data=df_train)\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "d6e165d5", "metadata": {}, "outputs": [], "source": ["plt.subplots(figsize=(4, 4))\nsns.countplot(x='Embarked', hue='Survived', data=df_train)\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "1a9a67d9", "metadata": {}, "outputs": [], "source": ["plt.subplots(figsize=(4, 4))\nsns.countplot(x='Pclass', hue='Survived', data=df_train)\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "e2b733bd", "metadata": {}, "outputs": [], "source": ["plt.subplots(figsize=(4, 4))\nsns.countplot(x='Parch', hue='Survived', data=df_train)\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "ec8259b1", "metadata": {}, "outputs": [], "source": ["f, ax = plt.subplots(figsize=(4, 4))\nax = sns.kdeplot(df_train[df_train['Survived'] == 0]['Age'])\nax = sns.kdeplot(df_train[df_train['Survived'] == 1]['Age'])"]}, {"cell_type": "markdown", "id": "cc6a1916", "metadata": {}, "source": ["# Preprocessing"]}, {"cell_type": "code", "execution_count": 1, "id": "0ca77dd0", "metadata": {}, "outputs": [], "source": ["df_train = df_train.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'])\ndf_train.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "52375c09", "metadata": {}, "outputs": [], "source": ["df_train['Sex'] = df_train['Sex'].replace('male', 0)\ndf_train['Sex'] = df_train['Sex'].replace('female', 1)\ndf_train['Sex'].value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "77dd8fe5", "metadata": {}, "outputs": [], "source": ["df_train['Embarked'] = df_train['Embarked'].replace('S', 0)\ndf_train['Embarked'] = df_train['Embarked'].replace('C', 1)\ndf_train['Embarked'] = df_train['Embarked'].replace('Q', 2)\ndf_train['Embarked'].value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "85f803aa", "metadata": {}, "outputs": [], "source": ["df_train = df_train.dropna()"]}, {"cell_type": "code", "execution_count": 1, "id": "bdf82e6e", "metadata": {}, "outputs": [], "source": ["df_train.count()"]}, {"cell_type": "markdown", "id": "98af66f4", "metadata": {}, "source": ["# Train models"]}, {"cell_type": "code", "execution_count": 1, "id": "ab9ab863", "metadata": {}, "outputs": [], "source": ["from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import classification_report"]}, {"cell_type": "code", "execution_count": 1, "id": "923c6d4d", "metadata": {}, "outputs": [], "source": ["X = df_train[[c for c in df_train.columns if c != \"Survived\"]]\ny = df_train[\"Survived\"]\nX.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "204b8507", "metadata": {}, "outputs": [], "source": ["y.head()"]}, {"cell_type": "markdown", "id": "1446b39d", "metadata": {}, "source": ["## 1. Decision Tree"]}, {"cell_type": "code", "execution_count": 1, "id": "c43abef8", "metadata": {}, "outputs": [], "source": ["model_dt = DecisionTreeClassifier()"]}, {"cell_type": "code", "execution_count": 1, "id": "5985b22c", "metadata": {}, "outputs": [], "source": ["print(\"Decision Tree: \")\nkf = KFold(n_splits=5, random_state=32, shuffle=True)\ndt_score_list = list()\ncnt = 0\n\nfor train_index, test_index in kf.split(X):\n    \n    print(\"Fold {}:\".format(cnt + 1))\n    cnt += 1\n    \n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    model_dt.fit(X_train, y_train)\n    y_hat = model_dt.predict(X_test)\n    prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_hat)\n    \n    df_temp = pd.DataFrame()\n    df_temp[\"Precision\"] = [prec[0], prec[1]]\n    df_temp[\"Recall\"] = [rec[0], rec[1]]\n    df_temp[\"F1-score\"] = [f1[0], f1[1]]\n    df_temp.index.name = 'Class'\n    print(df_temp)\n    \n    score = {'precision' : df_temp['Precision'].tolist(), \n             'recall' : df_temp['Recall'].tolist(),\n             'f1': df_temp['F1-score'].tolist()}\n    dt_score_list.append(score)"]}, {"cell_type": "code", "execution_count": 1, "id": "049981cd", "metadata": {}, "outputs": [], "source": ["average_f1 = np.mean([np.mean(ele['f1']) for ele in dt_score_list])\nprint(\"Average f-score of Decision Tree: {:.2f}\".format(average_f1))"]}, {"cell_type": "markdown", "id": "d9737541", "metadata": {}, "source": ["## 2. Multi-layer Perceptron"]}, {"cell_type": "code", "execution_count": 1, "id": "2b7e4328", "metadata": {}, "outputs": [], "source": ["model_mlp = MLPClassifier(hidden_layer_sizes=(150,100,50),\n                          max_iter=300,\n                          activation = 'relu',\n                          solver='sgd',\n                          random_state=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "de1e23b2", "metadata": {}, "outputs": [], "source": ["print(\"Multi-layer Perceptron: \")\nkf = KFold(n_splits=5, random_state=32, shuffle=True)\nmlp_score_list = list()\ncnt = 0\n\nfor train_index, test_index in kf.split(X):\n    \n    print(\"Fold {}:\".format(cnt + 1))\n    cnt += 1\n    \n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    model_mlp.fit(X_train, y_train)\n    y_hat = model_mlp.predict(X_test)\n    prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_hat)\n    \n    df_temp = pd.DataFrame()\n    df_temp[\"Precision\"] = [prec[0], prec[1]]\n    df_temp[\"Recall\"] = [rec[0], rec[1]]\n    df_temp[\"F1-score\"] = [f1[0], f1[1]]\n    df_temp.index.name = 'Class'\n    print(df_temp)\n    \n    score = {'precision' : df_temp['Precision'].tolist(), \n             'recall' : df_temp['Recall'].tolist(),\n             'f1': df_temp['F1-score'].tolist()}\n    mlp_score_list.append(score)"]}, {"cell_type": "code", "execution_count": 1, "id": "f5794eb4", "metadata": {}, "outputs": [], "source": ["average_f1 = np.mean([np.mean(ele['f1']) for ele in mlp_score_list])\nprint(\"Average f-score of Multi-layer Perceptron: {:.2f}\".format(average_f1))"]}, {"cell_type": "markdown", "id": "d152e396", "metadata": {}, "source": ["## 2. Gaussian Naive Bayes"]}, {"cell_type": "code", "execution_count": 1, "id": "da3d53e4", "metadata": {}, "outputs": [], "source": ["model_gnb = GaussianNB()"]}, {"cell_type": "code", "execution_count": 1, "id": "bdc0e9d2", "metadata": {}, "outputs": [], "source": ["print(\"Decision Tree: \")\nkf = KFold(n_splits=5, random_state=32, shuffle=True)\ngnb_score_list = list()\ncnt = 0\n\nfor train_index, test_index in kf.split(X):\n    \n    print(\"Fold {}:\".format(cnt + 1))\n    cnt += 1\n    \n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    model_gnb.fit(X_train, y_train)\n    y_hat = model_gnb.predict(X_test)\n    prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_hat)\n    \n    df_temp = pd.DataFrame()\n    df_temp[\"Precision\"] = [prec[0], prec[1]]\n    df_temp[\"Recall\"] = [rec[0], rec[1]]\n    df_temp[\"F1-score\"] = [f1[0], f1[1]]\n    df_temp.index.name = 'Class'\n    print(df_temp)\n    \n    score = {'precision' : df_temp['Precision'].tolist(), \n             'recall' : df_temp['Recall'].tolist(),\n             'f1': df_temp['F1-score'].tolist()}\n    gnb_score_list.append(score)"]}, {"cell_type": "code", "execution_count": 1, "id": "bbe1152a", "metadata": {}, "outputs": [], "source": ["average_f1 = np.mean([np.mean(ele['f1']) for ele in gnb_score_list])\nprint(\"Average f-score of Gaussian Naive Bayes: {:.2f}\".format(average_f1))"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}