{"cells": [{"cell_type": "markdown", "id": "6af029bf", "metadata": {}, "source": ["Description: Shrimp , (Mexico), west coast, frozen, white, No. 1, shell-on, headless, 26 to 30 count per pound, wholesale price at New York.\n\nUnit: US Dollars per Kilogram.\n\n[Data](https://www.kaggle.com/mruanova/shrimp-prices)"]}, {"cell_type": "code", "execution_count": 1, "id": "d37de36c", "metadata": {}, "outputs": [], "source": ["import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport datetime\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom scipy import stats\nfrom bokeh.plotting import figure, show, output_notebook\nfrom bokeh.models import ColumnDataSource\nimport sklearn\nfrom sklearn.linear_model import LinearRegression\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))"]}, {"cell_type": "code", "execution_count": 1, "id": "af1dee32", "metadata": {}, "outputs": [], "source": ["path = '../input/shrimp-prices/shrimp-prices.csv'\ndf = pd.read_csv(path, delimiter=',')\nprint(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")"]}, {"cell_type": "code", "execution_count": 1, "id": "b7a06514", "metadata": {}, "outputs": [], "source": ["df.tail(3)"]}, {"cell_type": "code", "execution_count": 1, "id": "686a6c1f", "metadata": {}, "outputs": [], "source": ["df.dtypes"]}, {"cell_type": "code", "execution_count": 1, "id": "98c5e59f", "metadata": {}, "outputs": [], "source": ["df.Month = df.Month.apply(pd.to_datetime) # pd.to_datetime(df['Date'])\ndf.columns = [\"Date\", \"Price\", \"Change\"]\ndf.tail(3)"]}, {"cell_type": "code", "execution_count": 1, "id": "c1cd25f3", "metadata": {}, "outputs": [], "source": ["df.dtypes"]}, {"cell_type": "code", "execution_count": 1, "id": "732017ad", "metadata": {}, "outputs": [], "source": ["from datetime import datetime\n# Interesting milestones\nunersupply1 = {'date':datetime(year=2014, month=8, day=1), 'label':'Peak 1', 'color':'green'}\noversupply1 = {'date':datetime(year=2015, month=9, day=1), 'label':'Valley 1', 'color':'red'}\nmilestones  = [unersupply1, oversupply1]\n# Create the datetime line plot\nplt.figure() # figsize=(WIDTH, HEIGHT)\nlegends={\"Price\":\"Price\"}\nsns.lineplot(x=df['Date'].rename(legends), y=df['Price'], label=legends['Price'])\n# Plot the different milestones\nif milestones:\n    minimum = df['Price'].min()\n    maximum = df['Price'].max()\n    for milestone in milestones:\n        plt.plot([milestone['date'], milestone['date']], [minimum, maximum], color=milestone['color'], linestyle='dashed', ms=10)\n        plt.text(milestone['date'], minimum, milestone['label'], color=milestone['color'], rotation=45)\nplt.ylabel('Price')\ntitle = 'Shrimp Prices (US Dollars per Kilogram) 2011-2021'\nplt.title(title)\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "c46262af", "metadata": {}, "outputs": [], "source": ["sns.jointplot(data=df, x=\"Date\", y=\"Price\")"]}, {"cell_type": "code", "execution_count": 1, "id": "ab71e48f", "metadata": {}, "outputs": [], "source": ["df['day'] = df['Date'].apply(lambda x:x.day)\ndf['month'] = df['Date'].apply(lambda x:x.month)\ndf['year'] = df['Date'].apply(lambda x:x.year)\ndf['dayofweek'] = df['Date'].apply(lambda x:x.dayofweek)\n#df['weekofyear'] = df['Date'].apply(lambda x:x.isocalendar().week)\n#df['is_weekend'] = df['Date'].apply(lambda x:x.dayofweek // 5)\ndf.drop('Date',axis=1,inplace=True)\ndf.dtypes"]}, {"cell_type": "code", "execution_count": 1, "id": "3ba5a6c7", "metadata": {}, "outputs": [], "source": ["df.head(3)"]}, {"cell_type": "code", "execution_count": 1, "id": "169269b2", "metadata": {}, "outputs": [], "source": ["ax = sns.distplot(df['Price']) # histogram distribution"]}, {"cell_type": "code", "execution_count": 1, "id": "143d3b00", "metadata": {}, "outputs": [], "source": ["plt.figure() # figsize=(9,5)\nsns.countplot(df['month'])\nplt.title('Monthwise Distribution of Sales',fontdict={'fontsize':25});"]}, {"cell_type": "markdown", "id": "160a6c74", "metadata": {}, "source": ["## Splitting data into training and test set"]}, {"cell_type": "code", "execution_count": 1, "id": "f73c0993", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\ntarget = df['Price']\nX_train, X_test, y_train, y_test = train_test_split(df,target,test_size=0.30)\nprint(f\"X_train.shape: {X_train.shape}, X_test.shape: {X_test.shape}, y_train.shape: {y_train.shape}, y_test.shape: {y_test.shape}\")"]}, {"cell_type": "markdown", "id": "1f9dd196", "metadata": {}, "source": ["## Select and evaluate the model"]}, {"cell_type": "code", "execution_count": 1, "id": "31a1e220", "metadata": {}, "outputs": [], "source": ["from xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\nmodel = XGBRegressor()\nmodel.fit(X_train, y_train)\nY_pred = model.predict(X_test)\nscore = model.score(X_train, y_train)\nprint('Training Score:', score)\nscore = model.score(X_test, y_test)\nprint('Testing Score:', score)\noutput = pd.DataFrame({'Predicted':Y_pred})"]}, {"cell_type": "code", "execution_count": 1, "id": "5651922d", "metadata": {}, "outputs": [], "source": ["mae = np.round(mean_absolute_error(y_test,Y_pred),3)\nprint('Mean Absolute Error:', mae)\nmse = np.round(mean_squared_error(y_test,Y_pred),3)\nprint('Mean Squared Error:', mse)\nscore = np.round(r2_score(y_test,Y_pred),3)\nprint('R2 Score:', score)"]}, {"cell_type": "code", "execution_count": 1, "id": "ce70de57", "metadata": {}, "outputs": [], "source": ["output.head(3)"]}, {"cell_type": "markdown", "id": "eaffd7ee", "metadata": {}, "source": ["## ARIMA (AutoRegressive Integrated Moving Average)"]}, {"cell_type": "code", "execution_count": 1, "id": "2becb1d2", "metadata": {}, "outputs": [], "source": ["from statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.arima_model import ARIMA # Autoregressive integrated moving average"]}, {"cell_type": "code", "execution_count": 1, "id": "e28571a0", "metadata": {}, "outputs": [], "source": ["df = pd.read_csv(path, delimiter=',')\ndf.drop('Change',axis=1,inplace=True)\ndf.Month = df.Month.apply(pd.to_datetime) # pd.to_datetime(df['Date'])\ndf.set_index('Month')\nindex='Month'\ntarget='Price'\nindexedDataset = df.set_index([index]) # date month"]}, {"cell_type": "markdown", "id": "65a4e7f2", "metadata": {}, "source": ["From the plot below, we can see that there is a Trend component in the series. \n\nHence, we now check for stationarity of the data"]}, {"cell_type": "code", "execution_count": 1, "id": "16058df1", "metadata": {}, "outputs": [], "source": ["## plot graph\nplt.xlabel(index)\nplt.ylabel(target)\nplt.plot(indexedDataset)"]}, {"cell_type": "markdown", "id": "6dad3702", "metadata": {}, "source": ["### Determine rolling statistics"]}, {"cell_type": "code", "execution_count": 1, "id": "5ebb01b5", "metadata": {}, "outputs": [], "source": ["rolmean = indexedDataset.rolling(window=12).mean() # window size 12 denotes 12 months, giving rolling mean at yearly level\nrolmean"]}, {"cell_type": "code", "execution_count": 1, "id": "797c8f73", "metadata": {}, "outputs": [], "source": ["rolstd = indexedDataset.rolling(window=12).std()\nrolstd"]}, {"cell_type": "code", "execution_count": 1, "id": "3bd2c50c", "metadata": {}, "outputs": [], "source": ["# Plot rolling statistics\norig = plt.plot(indexedDataset, color='blue', label='Original')\nmean = plt.plot(rolmean, color='red', label='Rolling Mean')\nstd = plt.plot(rolstd, color='black', label='Rolling Std')\nplt.legend(loc='best')\nplt.title('Rolling Mean & Standard Deviation')\nplt.show(block=False)"]}, {"cell_type": "markdown", "id": "fe9c9a84", "metadata": {}, "source": ["From the above graph, we see that rolling mean itself has a trend component even though rolling standard deviation is fairly constant with time. \n\nFor our time series to be stationary, we need to ensure that both the rolling statistics ie: mean & std. dev. remain time invariant or constant with time. \n\nThus the curves for both of them have to be parallel to the x-axis, which in our case is not so.\n\nTo further augment our hypothesis that the time series is not stationary, let us perform the ADCF test."]}, {"cell_type": "code", "execution_count": 1, "id": "1a65b2ee", "metadata": {}, "outputs": [], "source": ["# Perform Augmented Dickey\u2013Fuller test:\nprint('Results of Dickey Fuller Test:')\nresults = adfuller(indexedDataset, autolag='AIC')\ndfoutput = pd.Series(results[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\nfor key,value in results[4].items():\n    dfoutput['Critical Value (%s)'%key] = value\ndfoutput"]}, {"cell_type": "markdown", "id": "55abcab1", "metadata": {}, "source": ["For a Time series to be stationary, its ADCF test should have:\n\np-value to be low (according to the null hypothesis)\nThe critical values at 1%,5%,10% confidence intervals should be as close as possible to the Test Statistics\n\nFrom the above ADCF test result, we see that p-value(at max can be 1.0) is very large. \n\nAlso critical values are no where close to the Test Statistics. \n\nHence, we can safely say that our Time Series at the moment is not stationary\n\nData Transformation to achieve Stationarity \n\nThere are a couple of ways to achieve stationarity through data transformation like taking log 10\n , loge\n , square, square root, cube, cube root, exponential decay, time shift and so on ...\n\nIn our notebook, lets start of with log transformations. \n\nOur objective is to remove the trend component. \n\nHence, flatter curves( ie: paralle to x-axis) for time series and rolling mean after taking log would say that our data transformation did a good job."]}, {"cell_type": "markdown", "id": "e7615972", "metadata": {}, "source": ["## Log Scale Transformation"]}, {"cell_type": "code", "execution_count": 1, "id": "96da6054", "metadata": {}, "outputs": [], "source": ["# Estimating trend\nindexedDataset_logScale = np.log(indexedDataset)\nplt.plot(indexedDataset_logScale)"]}, {"cell_type": "code", "execution_count": 1, "id": "cb7ea39a", "metadata": {}, "outputs": [], "source": ["# The below transformation is required to make series stationary\nmovingAverage = indexedDataset_logScale.rolling(window=12).mean()\nmovingSTD = indexedDataset_logScale.rolling(window=12).std()\nplt.plot(indexedDataset_logScale)\nplt.plot(movingAverage, color='red')"]}, {"cell_type": "markdown", "id": "3f0f3543", "metadata": {}, "source": ["From above graph, we see that even though rolling mean is not stationary, it is still better than the previous case, where no transfromation were applied to series. So we can atleast say that we are heading in the right direction.\n\nWe know from above graph that both the Time series with log scale as well as its moving average have a trend component. \n\nThus we can apply a elementary intuition: subtraction one from the other should remove the trend component of both. Its like:\n\nlogscaleL=stationarypart(L1)+trend(LT)\n \nmovingavgoflogscaleA=stationarypart(A1)+trend(AT)\n \nresultseriesR=L\u2212A=(L1+LT)\u2212(A1+AT)=(L1\u2212A1)+(LT\u2212AT)\n \n\nSince, L & A are series & it moving avg, their trend will be more or less same, Hence\nLT-AT nearly equals to 0\n\nThus trend component will be almost removed. And we have,\n\nR=L1\u2212A1\n , our final non-trend curve"]}, {"cell_type": "code", "execution_count": 1, "id": "6433cfdf", "metadata": {}, "outputs": [], "source": ["datasetLogScaleMinusMovingAverage = indexedDataset_logScale - movingAverage\ndatasetLogScaleMinusMovingAverage.head(12)\n\n# Remove NAN values\ndatasetLogScaleMinusMovingAverage.dropna(inplace=True)\ndatasetLogScaleMinusMovingAverage.head(10)"]}, {"cell_type": "code", "execution_count": 1, "id": "3fcc0d9a", "metadata": {}, "outputs": [], "source": ["def test_stationarity(timeseries):\n    #Determine rolling statistics\n    movingAverage = timeseries.rolling(window=12).mean()\n    movingSTD = timeseries.rolling(window=12).std()\n    #Plot rolling statistics\n    orig = plt.plot(timeseries, color='blue', label='Original')\n    mean = plt.plot(movingAverage, color='red', label='Rolling Mean')\n    std = plt.plot(movingSTD, color='black', label='Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)\n    # Perform Dickey\u2013Fuller test:\n    print('Results of Dickey Fuller Test:')\n    dftest = adfuller(timeseries[target], autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print(dfoutput)\ntest_stationarity(datasetLogScaleMinusMovingAverage)"]}, {"cell_type": "markdown", "id": "5fe2e6ed", "metadata": {}, "source": ["From above graph, we observe that our intuition that \"subtracting two related series having similar trend components will make the result stationary\" is true. \n\nWe find that:\n\np-value has reduced from 0.99 to 0.022.\n\nThe critical values at 1%,5%,10% confidence intervals are pretty close to the Test Statistic. \n\nThus, from above 2 points, we can say that our given series is stationary.\n\nBut, in the spirit of getting higher accuracy, let us explore & try to find a better scale than our current log.\n\nLet us try out Exponential decay.\n\nFor further info, refer to my answer 12 at the top of the notebook on it."]}, {"cell_type": "markdown", "id": "2bc70f0f", "metadata": {}, "source": ["## Exponential Decay Transformation"]}, {"cell_type": "code", "execution_count": 1, "id": "fc2c0736", "metadata": {}, "outputs": [], "source": ["exponentialDecayWeightedAverage = indexedDataset_logScale.ewm(halflife=12, min_periods=0, adjust=True).mean()\nplt.plot(indexedDataset_logScale)\nplt.plot(exponentialDecayWeightedAverage, color='red')"]}, {"cell_type": "markdown", "id": "f91fd87b", "metadata": {}, "source": ["From above graph, it seems that exponential decay is not holding any advantage over log scale as both the corresponding curves are similar. \n\nBut, in statistics, inferences cannot be drawn simply by looking at the curves. \n\nHence, we perform the ADCF test again on the decay series below."]}, {"cell_type": "code", "execution_count": 1, "id": "ff2d0e20", "metadata": {}, "outputs": [], "source": ["datasetLogScaleMinusExponentialMovingAverage = indexedDataset_logScale - exponentialDecayWeightedAverage\ntest_stationarity(datasetLogScaleMinusExponentialMovingAverage)"]}, {"cell_type": "markdown", "id": "b410ecab", "metadata": {}, "source": ["We observe that the Time Series is stationary & also the series for moving avg & std. dev. is almost parallel to x-axis thus they also have no trend.\n\nAlso,\n\np-value has decreased from 0.022 to 0.005.\n\nTest Statistic value is very much closer to the Critical values.\n\nBoth the points say that our current transformation is better than the previous logarithmic transformation. \n\nEven though, we couldn't observe any differences by visually looking at the graphs, the tests confirmed decay to be much better.\n\nBut lets try one more time & find if an even better solution exists. \n\nWe will try out the simple time shift technique, which is simply:\n\nGiven a set of observation on the time series:\nx0,x1,x2,x3,....xn\n \n\nThe shifted values will be:\nnull,x0,x1,x2,....xn\n  <---- basically all xi's shifted by 1 pos to right\n\nThus, the time series with time shifted values are:\nnull,(x1\u2212x0),(x2\u2212x1),(x3\u2212x2),(x4\u2212x3),....(xn\u2212xn\u22121)"]}, {"cell_type": "markdown", "id": "895152de", "metadata": {}, "source": ["## Time Shift Transformation "]}, {"cell_type": "code", "execution_count": 1, "id": "c59ecf2c", "metadata": {}, "outputs": [], "source": ["# Time Shift Transformation\ndatasetLogDiffShifting = indexedDataset_logScale - indexedDataset_logScale.shift()\nplt.plot(datasetLogDiffShifting)"]}, {"cell_type": "code", "execution_count": 1, "id": "d9f1b29a", "metadata": {}, "outputs": [], "source": ["datasetLogDiffShifting.dropna(inplace=True)\ntest_stationarity(datasetLogDiffShifting)"]}, {"cell_type": "markdown", "id": "aef9729b", "metadata": {}, "source": ["From above 2 graphs, we can see that, visually this is the best result as our series along with rolling statistic values of moving avg & moving std. dev. is very much flat & stationary. \n\nBut, the ADCF test shows us that:\n\np-value of 0.07 is not as good as 0.005 of exponential decay.\n\nTest Statistic value not as close to the critical values as that for exponential decay.\n\nWe have thus tried out 3 different transformation: log, exp decay & time shift. \n\nFor simplicity, we will go with the log scale. \n\nThe reason for doing this is that we can revert back to the original scale during forecasting.\n\nLet us now break down the 3 components of the log scale series using a system libary function. \n\nOnce, we separate our the components, we can simply ignore trend & seasonality and check on the nature of the residual part."]}, {"cell_type": "code", "execution_count": 1, "id": "cc3ede36", "metadata": {}, "outputs": [], "source": ["decomposition = seasonal_decompose(indexedDataset_logScale) \n\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nplt.subplot(411)\nplt.plot(indexedDataset_logScale, label='Original')\nplt.legend(loc='best')\n\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend(loc='best')\n\nplt.subplot(411)\nplt.plot(seasonal, label='Seasonality')\nplt.legend(loc='best')\n\nplt.subplot(411)\nplt.plot(residual, label='Residuals')\nplt.legend(loc='best')\n\nplt.tight_layout()"]}, {"cell_type": "code", "execution_count": 1, "id": "7ecb76d6", "metadata": {}, "outputs": [], "source": ["# there can be cases where an observation simply consisted of trend & seasonality. \n# In that case, there won't be \n# any residual component & that would be a null or NaN.\n# Hence, we also remove such cases.\ndecomposedLogData = residual\ndecomposedLogData.dropna(inplace=True)\n# test_stationarity(decomposedLogData)"]}, {"cell_type": "markdown", "id": "8f55c026", "metadata": {}, "source": ["Plotting ACF & PACF"]}, {"cell_type": "code", "execution_count": 1, "id": "1cf636bc", "metadata": {}, "outputs": [], "source": ["#ACF & PACF plots\n\nlag_acf = acf(datasetLogDiffShifting, nlags=20)\nlag_pacf = pacf(datasetLogDiffShifting, nlags=20, method='ols')\n\n#Plot ACF:\nplt.subplot(121)\nplt.plot(lag_acf)\nplt.axhline(y=0, linestyle='--', color='gray')\nplt.axhline(y=-1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')\nplt.axhline(y=1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')\nplt.title('Autocorrelation Function')            \n\n#Plot PACF\nplt.subplot(122)\nplt.plot(lag_pacf)\nplt.axhline(y=0, linestyle='--', color='gray')\nplt.axhline(y=-1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')\nplt.axhline(y=1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')\nplt.title('Partial Autocorrelation Function')\n            \nplt.tight_layout()"]}, {"cell_type": "markdown", "id": "7dfc32a7", "metadata": {}, "source": ["From the ACF graph, we see that curve touches y=0.0 line at x=2. \n\nThus, from theory, Q = 2 From the PACF graph, we see that curve touches y=0.0 line at x=2. \n\nThus, from theory, P = 2\n\nARIMA is AR + I + MA. \n\nBefore, we see an ARIMA model, let us check the results of the individual AR & MA model. Note that, these models will give a value of RSS.\n\nLower RSS values indicate a better model."]}, {"cell_type": "markdown", "id": "5ef0ef5d", "metadata": {}, "source": ["Thanks [freespirit08](https://www.kaggle.com/freespirit08/time-series-for-beginners-with-arima)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}