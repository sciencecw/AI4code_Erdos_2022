{"cells": [{"cell_type": "code", "execution_count": 1, "id": "274336e6", "metadata": {}, "outputs": [], "source": ["import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n%matplotlib inline"]}, {"cell_type": "markdown", "id": "353ac836", "metadata": {}, "source": ["# Loading the Dataset"]}, {"cell_type": "code", "execution_count": 1, "id": "b543781b", "metadata": {}, "outputs": [], "source": ["# '../input/house-prices-advanced-regression-techniques' for Kaggle\ndef load_housing_data(data_name):\n    csv_path = os.path.join('../input/house-prices-advanced-regression-techniques', data_name)\n    return pd.read_csv(csv_path)"]}, {"cell_type": "code", "execution_count": 1, "id": "b5343184", "metadata": {}, "outputs": [], "source": ["train, test = load_housing_data(\"train.csv\"), load_housing_data(\"test.csv\")"]}, {"cell_type": "code", "execution_count": 1, "id": "5673fd83", "metadata": {}, "outputs": [], "source": ["train.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "28431c3a", "metadata": {}, "outputs": [], "source": ["train.shape"]}, {"cell_type": "markdown", "id": "063dff09", "metadata": {}, "source": ["# Exploring and Preprocessing"]}, {"cell_type": "markdown", "id": "a1065688", "metadata": {}, "source": ["## Feature Types"]}, {"cell_type": "markdown", "id": "62ce1c03", "metadata": {}, "source": ["### Numerical Features"]}, {"cell_type": "code", "execution_count": 1, "id": "02af2167", "metadata": {}, "outputs": [], "source": ["train.select_dtypes(include='number').columns"]}, {"cell_type": "markdown", "id": "48e5eab3", "metadata": {}, "source": ["### Categorical Features"]}, {"cell_type": "code", "execution_count": 1, "id": "fe88a727", "metadata": {}, "outputs": [], "source": ["train.select_dtypes(include='object').columns"]}, {"cell_type": "markdown", "id": "a325855b", "metadata": {}, "source": ["### These Features are Categorical "]}, {"cell_type": "code", "execution_count": 1, "id": "5ca00034", "metadata": {}, "outputs": [], "source": ["train['YrSold'].value_counts()"]}, {"cell_type": "markdown", "id": "6b010f3f", "metadata": {}, "source": ["Also, we have `MoSold` which is the month in which the house was sold. This is obviously categorical as there isn't any reason for July to be better than June. <br>\nAlso, `MSSubClass` just describes the type of the dwelling. Categorical again. "]}, {"cell_type": "code", "execution_count": 1, "id": "d75e365c", "metadata": {}, "outputs": [], "source": ["def to_str(feature, data=train):\n    data[feature] = data[feature].astype(str)"]}, {"cell_type": "code", "execution_count": 1, "id": "1109ada9", "metadata": {}, "outputs": [], "source": ["to_str('YrSold')\nto_str('MoSold')\nto_str('MSSubClass')\nto_str('YrSold',test)\nto_str('MoSold',test)\nto_str('MSSubClass',test)"]}, {"cell_type": "code", "execution_count": 1, "id": "db794247", "metadata": {}, "outputs": [], "source": ["train.dtypes.value_counts()"]}, {"cell_type": "markdown", "id": "4342d0b8", "metadata": {}, "source": ["## The Target Feature"]}, {"cell_type": "markdown", "id": "398e9843", "metadata": {}, "source": ["### Is It Normal?"]}, {"cell_type": "markdown", "id": "b38e62d8", "metadata": {}, "source": ["Let's use Seaborn to see the distribution of SalePrice, our target feature."]}, {"cell_type": "code", "execution_count": 1, "id": "58d74d35", "metadata": {}, "outputs": [], "source": ["sns.set_style(\"darkgrid\")"]}, {"cell_type": "code", "execution_count": 1, "id": "fe31c7ff", "metadata": {}, "outputs": [], "source": ["def plot_SalePrice(data=train['SalePrice']):    \n    sns.distplot(data)\n    plt.ylabel(\"Frequency\")\n    plt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "01f11600", "metadata": {}, "outputs": [], "source": ["plot_SalePrice()"]}, {"cell_type": "code", "execution_count": 1, "id": "89cda192", "metadata": {}, "outputs": [], "source": ["#skewness and kurtosis\nprint(\"Skewness: \" + str(train['SalePrice'].skew()))\nprint(\"Kurtosis: \" + str(train['SalePrice'].kurt()))"]}, {"cell_type": "markdown", "id": "86b18f23", "metadata": {}, "source": ["#### Conclusion\n* The target feature `SalePrice` is obviously not normally distributed.\n* The distribution is right-skewed.\n* We will have to fix this later on."]}, {"cell_type": "markdown", "id": "f4a63058", "metadata": {}, "source": ["## Correlations"]}, {"cell_type": "code", "execution_count": 1, "id": "8cc7a5ce", "metadata": {}, "outputs": [], "source": ["corr_matrix = train.corr()\n(corr_matrix[\"SalePrice\"]**2).sort_values(ascending=False)"]}, {"cell_type": "code", "execution_count": 1, "id": "d933e03a", "metadata": {}, "outputs": [], "source": ["corr = train.corr()**2\nplt.subplots(figsize=(12,12))\nsns.heatmap(corr, vmax=0.9, square=True)\nplt.show()"]}, {"cell_type": "markdown", "id": "251e2cc7", "metadata": {}, "source": ["## Looking at Individual Correlations"]}, {"cell_type": "markdown", "id": "767192e7", "metadata": {}, "source": ["### Scatterplots"]}, {"cell_type": "code", "execution_count": 1, "id": "1cbf193a", "metadata": {}, "outputs": [], "source": ["def scatterplot(x, y='SalePrice', data=train):\n#     plt.subplots(figsize=(12,8))\n    sns.scatterplot(x=x, y=y, data=data)\n    plt.show()"]}, {"cell_type": "markdown", "id": "f9e05273", "metadata": {}, "source": ["#### OverallQual"]}, {"cell_type": "code", "execution_count": 1, "id": "e769e03b", "metadata": {}, "outputs": [], "source": ["sns.boxplot(x='OverallQual', y='SalePrice', data=train)\nplt.show()"]}, {"cell_type": "markdown", "id": "14605411", "metadata": {}, "source": ["SalePrice increases with OverallQual."]}, {"cell_type": "markdown", "id": "e7dbb845", "metadata": {}, "source": ["#### GrLivArea"]}, {"cell_type": "code", "execution_count": 1, "id": "33600dd5", "metadata": {}, "outputs": [], "source": ["scatterplot('GrLivArea')"]}, {"cell_type": "markdown", "id": "71b96c9b", "metadata": {}, "source": ["There's 2 very obvious outliers!"]}, {"cell_type": "markdown", "id": "8b200f55", "metadata": {}, "source": ["#### GarageCars and GarageArea"]}, {"cell_type": "code", "execution_count": 1, "id": "c604923a", "metadata": {}, "outputs": [], "source": ["scatterplot('GarageCars')"]}, {"cell_type": "code", "execution_count": 1, "id": "4b3f411d", "metadata": {}, "outputs": [], "source": ["scatterplot('GarageArea')"]}, {"cell_type": "markdown", "id": "376958c4", "metadata": {}, "source": ["#### TotalBsmtSF"]}, {"cell_type": "code", "execution_count": 1, "id": "a58b7167", "metadata": {}, "outputs": [], "source": ["scatterplot('TotalBsmtSF')"]}, {"cell_type": "markdown", "id": "fd52cdf0", "metadata": {}, "source": ["#### 1stFlrSF"]}, {"cell_type": "code", "execution_count": 1, "id": "81d3bf4f", "metadata": {}, "outputs": [], "source": ["scatterplot('1stFlrSF')"]}, {"cell_type": "markdown", "id": "e2837ebf", "metadata": {}, "source": ["#### TotalRmsAbvGrd"]}, {"cell_type": "code", "execution_count": 1, "id": "86b77bff", "metadata": {}, "outputs": [], "source": ["scatterplot('TotRmsAbvGrd')"]}, {"cell_type": "markdown", "id": "68b60e4d", "metadata": {}, "source": ["#### YearBuilt and YearRemodAdd"]}, {"cell_type": "code", "execution_count": 1, "id": "5c633e38", "metadata": {}, "outputs": [], "source": ["scatterplot('YearBuilt')"]}, {"cell_type": "code", "execution_count": 1, "id": "c50b9af0", "metadata": {}, "outputs": [], "source": ["scatterplot('YearRemodAdd')"]}, {"cell_type": "markdown", "id": "429bd449", "metadata": {}, "source": ["GarageYrBuilt was a similar graph, and it felt kinda useless so I'm excluding it to save space."]}, {"cell_type": "markdown", "id": "90a07c0d", "metadata": {}, "source": ["#### MasVnrArea"]}, {"cell_type": "code", "execution_count": 1, "id": "391c05dc", "metadata": {}, "outputs": [], "source": ["scatterplot('MasVnrArea')"]}, {"cell_type": "markdown", "id": "a82546ac", "metadata": {}, "source": ["#### Conclusions\n* There are outliers we need to remove.\n* There is heteroscedasticity."]}, {"cell_type": "markdown", "id": "9eb97e53", "metadata": {}, "source": ["## Deleting Outliers"]}, {"cell_type": "markdown", "id": "81b90144", "metadata": {}, "source": ["### From GrLivArea\nWe found 2 outliers while plotting GrLivArea. Let's remove those."]}, {"cell_type": "code", "execution_count": 1, "id": "af4bdbbc", "metadata": {}, "outputs": [], "source": ["train.sort_values('GrLivArea', ascending=False).head(2)['GrLivArea']"]}, {"cell_type": "code", "execution_count": 1, "id": "c25f3258", "metadata": {}, "outputs": [], "source": ["train = train[train['GrLivArea'] < 4676]"]}, {"cell_type": "code", "execution_count": 1, "id": "9debcf7a", "metadata": {}, "outputs": [], "source": ["train.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "4ae52859", "metadata": {}, "outputs": [], "source": ["train.sort_values('GrLivArea', ascending=False).head(2)['GrLivArea']"]}, {"cell_type": "markdown", "id": "4f92a3ed", "metadata": {}, "source": ["## Fixing the SalePrice Skew"]}, {"cell_type": "code", "execution_count": 1, "id": "a39a98cc", "metadata": {}, "outputs": [], "source": ["plot_SalePrice()"]}, {"cell_type": "markdown", "id": "b9c0fcb7", "metadata": {}, "source": ["Let's use logarithmic transformation (log1p is log(1+x) so that x=0 poses no problems)"]}, {"cell_type": "code", "execution_count": 1, "id": "8e8af705", "metadata": {}, "outputs": [], "source": ["train['SalePrice'] = np.log1p(train['SalePrice'])"]}, {"cell_type": "code", "execution_count": 1, "id": "7198cdb3", "metadata": {}, "outputs": [], "source": ["plot_SalePrice(train['SalePrice'])"]}, {"cell_type": "markdown", "id": "6d3e8e7f", "metadata": {}, "source": ["Looks good!"]}, {"cell_type": "markdown", "id": "10c53dc4", "metadata": {}, "source": ["## Missing %"]}, {"cell_type": "code", "execution_count": 1, "id": "800ac617", "metadata": {}, "outputs": [], "source": ["test_dummy = test.copy()\ntest_dummy['SalePrice'] = np.zeros(len(test))\ncombined = pd.concat([train,test_dummy])"]}, {"cell_type": "code", "execution_count": 1, "id": "70d8fb6d", "metadata": {}, "outputs": [], "source": ["combined.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "815b54b1", "metadata": {}, "outputs": [], "source": ["def missing_percent(data, n=35):\n    num_of_nulls = data.isnull().sum().sort_values(ascending=False)\n    percent = (data.isnull().sum()/data.isnull().count()).sort_values(ascending=False)\n    result = pd.concat([num_of_nulls, percent], axis=1, keys=['Number', 'Percent'])\n    return result.head(n)"]}, {"cell_type": "code", "execution_count": 1, "id": "13bb04c4", "metadata": {}, "outputs": [], "source": ["missing_percent(train)"]}, {"cell_type": "code", "execution_count": 1, "id": "32fff890", "metadata": {}, "outputs": [], "source": ["missing_percent(test)"]}, {"cell_type": "code", "execution_count": 1, "id": "f5a05b09", "metadata": {}, "outputs": [], "source": ["missing_percent(combined)"]}, {"cell_type": "markdown", "id": "d3cdefa4", "metadata": {}, "source": ["#### Conclusion\n* That's 19 columns with missing values in the training set and 33 in the test set."]}, {"cell_type": "markdown", "id": "2cdb3ba8", "metadata": {}, "source": ["## Fixing What's Missing"]}, {"cell_type": "markdown", "id": "33a30667", "metadata": {}, "source": ["There are features where the values are null for a reason. We'll fill these with their appropriate values according to the data description, and for the rest we will decide how to impute. "]}, {"cell_type": "markdown", "id": "a13767e4", "metadata": {}, "source": ["We want to take care of missing data both in the train and the test set. So let's write a function for it. "]}, {"cell_type": "code", "execution_count": 1, "id": "0bd211a6", "metadata": {}, "outputs": [], "source": ["import warnings\nwarnings.filterwarnings(\"ignore\")"]}, {"cell_type": "code", "execution_count": 1, "id": "870f0a04", "metadata": {}, "outputs": [], "source": ["def fill_missing(df):\n    \n    # Filling 'Functional' according to the data description\n    df['Functional'] = df['Functional'].fillna('Typ')\n    \n    # Filling some categorical values with mode\n    cats = ['Electrical','SaleType','Exterior2nd','KitchenQual','Exterior1st','MSZoning']\n    for cat in cats:\n        df[cat] = df[cat].fillna(df[cat].mode()[0])\n    \n    # Filling LotFrontage by grouping by neighborhood and taking the median\n    df['LotFrontage'] = df['LotFrontage'].fillna(\n        df.groupby('Neighborhood')['LotFrontage'].transform('median'))\n    \n    # Filling the rest of the categorical value with 'None'\n    # (For some features like those of basement and garage, NA means None.\n    # But for some features we don't know so let's just use None)\n    df_cat = df[list(df.select_dtypes(include='object').columns)]\n    df.update(df_cat.fillna('None'))\n    \n    # Filling the rest of the numerical values with 0\n    # (For some features like alley and LotFrontage, NA means 0.\n    # But for some features we don't know so let's just use 0)\n    df_num = df[list(df.select_dtypes(include='number').columns)]\n    df.update(df_num.fillna(0))\n    \n    return df"]}, {"cell_type": "code", "execution_count": 1, "id": "7e877c60", "metadata": {}, "outputs": [], "source": ["fill_missing(train).isnull().sum().sum()"]}, {"cell_type": "code", "execution_count": 1, "id": "fb45bc4e", "metadata": {}, "outputs": [], "source": ["fill_missing(test).isnull().sum().sum()"]}, {"cell_type": "code", "execution_count": 1, "id": "76a1ea62", "metadata": {}, "outputs": [], "source": ["test_dummy = test.copy()\ntest_dummy['SalePrice'] = np.zeros(len(test))\ncombined = pd.concat([train,test_dummy])"]}, {"cell_type": "code", "execution_count": 1, "id": "be462702", "metadata": {}, "outputs": [], "source": ["missing_percent(combined,1)"]}, {"cell_type": "markdown", "id": "56fb7be4", "metadata": {}, "source": ["#### Perfect!"]}, {"cell_type": "markdown", "id": "5e0bf89d", "metadata": {}, "source": ["## Fixing Skewed Features\nI came across this from the \"How I Made Top 0.3% on a Kaggle Competition\" by Lavanya Shukla's (@lavanyashukla01) notebook. This wasn't something I'd learned beforehand, so I had to do some research. From Erik Bruin's (@erikbruin) R notebook \"House prices: Lasso, XGBoost, and a detailed EDA\" I was able to understand this topic better. As he explains it, \"As a rule of thumb, skewness should be between -1 and 1. In this range, data are considered fairly symmetrical.\" <br><br> The former notebook fixes features with skew > 0.5, and the latter uses 0.8. I'll take inspiration from some of the Python code of the former and use skew > 0.6 for no reason."]}, {"cell_type": "markdown", "id": "8a6180be", "metadata": {}, "source": ["Also, let's use boxcox1p to transform the skewed features (again, 1p to avoid sadness because of zeros)"]}, {"cell_type": "code", "execution_count": 1, "id": "fef4c96d", "metadata": {}, "outputs": [], "source": ["from scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\ndef fix_skew(df):\n    \n    skew_index = []\n\n    # A function to show the top 10 skewed features\n    def show_skew(df):\n        df_num = df[list(df.select_dtypes(include='number').columns)]\n\n        skew_features = df_num.apply(lambda x: x.skew()).sort_values(ascending=False)\n\n        high_skew = skew_features[skew_features > 0.6]\n        nonlocal skew_index\n        skew_index = high_skew.index\n\n        print(\"{} features have skew > 0.6 :\".format(high_skew.shape[0]))\n        skewness = pd.DataFrame({'Skew' :high_skew})\n        print(skew_features.head(10))\n        \n    # Before transformation\n    show_skew(df)\n\n    # Transformation\n    for i in skew_index:\n        df[i] = boxcox1p(df[i], boxcox_normmax(df[i] + 1))\n        \n    # After transformation\n    show_skew(df)"]}, {"cell_type": "code", "execution_count": 1, "id": "e15774d6", "metadata": {}, "outputs": [], "source": ["fix_skew(combined)"]}, {"cell_type": "markdown", "id": "981ea78b", "metadata": {}, "source": ["## Making New Features!"]}, {"cell_type": "markdown", "id": "25b05e0b", "metadata": {}, "source": ["Let's list the features we can use"]}, {"cell_type": "code", "execution_count": 1, "id": "b819ca9f", "metadata": {}, "outputs": [], "source": ["combined.select_dtypes(include='number').columns"]}, {"cell_type": "code", "execution_count": 1, "id": "d9e3664c", "metadata": {}, "outputs": [], "source": ["combined.select_dtypes(include='object').columns"]}, {"cell_type": "markdown", "id": "edc7cb99", "metadata": {}, "source": ["### Does Size Matter?"]}, {"cell_type": "markdown", "id": "83a96133", "metadata": {}, "source": ["There might be some parts of the house for which just the existence of that part is important. For example, does only *having* a fireplace raise the house value over houses that have no fireplaces? "]}, {"cell_type": "markdown", "id": "ef9d0d83", "metadata": {}, "source": ["For features like basement, fireplace, garage, and pool we already have a 'None' category.\nLet's see some other features."]}, {"cell_type": "code", "execution_count": 1, "id": "e99c5ed9", "metadata": {}, "outputs": [], "source": ["combined['Has2ndFlr'] = combined['2ndFlrSF'].apply(lambda x : 1 if x > 0 else 0)\n\ncombined['HasWoodDeck'] = combined['WoodDeckSF'].apply(lambda x : 1 if x > 0 else 0)\ncombined['HasOpenPorch'] = combined['OpenPorchSF'].apply(lambda x : 1 if x > 0 else 0)\ncombined['HasEnclosedPorch'] = combined['EnclosedPorch'].apply(lambda x : 1 if x > 0 else 0)\ncombined['Has3SsnPorch'] = combined['3SsnPorch'].apply(lambda x : 1 if x > 0 else 0)\ncombined['HasScreenPorch'] = combined['ScreenPorch'].apply(lambda x : 1 if x > 0 else 0)"]}, {"cell_type": "markdown", "id": "4db8acf3", "metadata": {}, "source": ["#### PoolQC"]}, {"cell_type": "code", "execution_count": 1, "id": "31ba24b0", "metadata": {}, "outputs": [], "source": ["def binarize(column, data=combined):\n    print(combined[column].value_counts())\n    combined[column] = combined[column].apply(lambda x : 0 if x == 'None' else 1)\n    print(combined[column].value_counts())"]}, {"cell_type": "code", "execution_count": 1, "id": "d5aad318", "metadata": {}, "outputs": [], "source": ["binarize('PoolQC')"]}, {"cell_type": "markdown", "id": "b81d4bc3", "metadata": {}, "source": ["### Using Multiple Features"]}, {"cell_type": "markdown", "id": "3b4f89de", "metadata": {}, "source": ["Making features like total porch area, total house quality, etc."]}, {"cell_type": "code", "execution_count": 1, "id": "51aaecc8", "metadata": {}, "outputs": [], "source": ["combined['HouseQualAdd'] = combined['OverallQual'] + combined['OverallCond']\ncombined['HouseQualProd'] = combined['OverallQual'] * combined['OverallCond']"]}, {"cell_type": "code", "execution_count": 1, "id": "0cbf3db8", "metadata": {}, "outputs": [], "source": ["combined['TotalSqrFt'] = (combined['BsmtFinSF1'] + combined['BsmtFinSF2']\n                    + combined['1stFlrSF'] + combined['2ndFlrSF'])\ncombined['TotalSF'] = (combined['TotalBsmtSF'] + combined['1stFlrSF'] + combined['2ndFlrSF'])\ncombined['TotalPorchSF'] = (combined['WoodDeckSF'] + combined['OpenPorchSF']\n                         + combined['EnclosedPorch'] + combined['3SsnPorch'] \n                         + combined['ScreenPorch'])\ncombined['TotalBath'] = (combined['FullBath'] \n                      + (0.5 * combined['HalfBath']) \n                      + combined['BsmtFullBath'] \n                      + (0.5 * combined['BsmtHalfBath']))"]}, {"cell_type": "markdown", "id": "76fb6fb2", "metadata": {}, "source": ["### Dropping Street and Utilities "]}, {"cell_type": "code", "execution_count": 1, "id": "70d06710", "metadata": {}, "outputs": [], "source": ["combined.drop(['Street', 'Utilities'], axis=1, inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "275458eb", "metadata": {}, "outputs": [], "source": ["combined.shape"]}, {"cell_type": "markdown", "id": "bc1bc912", "metadata": {}, "source": ["## Encoding Categorical Features"]}, {"cell_type": "code", "execution_count": 1, "id": "7cb0fedb", "metadata": {}, "outputs": [], "source": ["combined_encoded = pd.get_dummies(combined).reset_index(drop=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "92258af7", "metadata": {}, "outputs": [], "source": ["combined_encoded.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "9bf92517", "metadata": {}, "outputs": [], "source": ["combined_encoded = combined_encoded.loc[:,~combined_encoded.columns.duplicated()]"]}, {"cell_type": "code", "execution_count": 1, "id": "2aa82ca4", "metadata": {}, "outputs": [], "source": ["combined_encoded.shape"]}, {"cell_type": "markdown", "id": "71582ecd", "metadata": {}, "source": ["# Splitting the Dataset"]}, {"cell_type": "code", "execution_count": 1, "id": "77ba9101", "metadata": {}, "outputs": [], "source": ["len(train)"]}, {"cell_type": "markdown", "id": "d0890ab4", "metadata": {}, "source": ["### Test"]}, {"cell_type": "code", "execution_count": 1, "id": "42173f21", "metadata": {}, "outputs": [], "source": ["y_test, X_test = (combined_encoded[len(train):][\"SalePrice\"], \n                 combined_encoded.drop([\"SalePrice\"], axis=1)[len(train):])"]}, {"cell_type": "code", "execution_count": 1, "id": "f83a60a5", "metadata": {}, "outputs": [], "source": ["y_test.shape, X_test.shape"]}, {"cell_type": "markdown", "id": "f008f6a3", "metadata": {}, "source": ["### Train and Valid"]}, {"cell_type": "code", "execution_count": 1, "id": "e65b2e82", "metadata": {}, "outputs": [], "source": ["y_train_full, X_train_full = (combined_encoded[:len(train)][\"SalePrice\"], \n                             combined_encoded.drop([\"SalePrice\"], axis=1)[:len(train)])"]}, {"cell_type": "code", "execution_count": 1, "id": "5096bbe2", "metadata": {}, "outputs": [], "source": ["y_train_full.shape, X_train_full.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "0e685e10", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X_train_full, y_train_full, test_size=0.20, random_state=42)"]}, {"cell_type": "code", "execution_count": 1, "id": "8e62fbfb", "metadata": {}, "outputs": [], "source": ["X_train.shape, X_valid.shape"]}, {"cell_type": "markdown", "id": "df9f9594", "metadata": {}, "source": ["# Model Training\nLet's do some cool stuff now!<br>\nModels used:\n* XGBoost\n* LGBM\n* Gradient Boosting Regressor\n* SVR\n* Ridge\n* Lasso\n* Elastic Net"]}, {"cell_type": "markdown", "id": "fcd4b86a", "metadata": {}, "source": ["Methods used:\n* Stacking\n* Blending\n* Cross-Validation\n* Robust Scaling\n* RMSLE (Which is the competition metric)\n* Hyperopt"]}, {"cell_type": "code", "execution_count": 1, "id": "a0263874", "metadata": {}, "outputs": [], "source": ["from sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\nimport hyperopt\nfrom hyperopt import hp\nfrom hyperopt import fmin, tpe, hp, anneal, Trials\nfrom sklearn.preprocessing import RobustScaler"]}, {"cell_type": "markdown", "id": "f2e9e999", "metadata": {}, "source": ["## Metrics\n### Main Metric"]}, {"cell_type": "code", "execution_count": 1, "id": "f9c6da07", "metadata": {}, "outputs": [], "source": ["def rmsle(y, y_pred):\n    # y and y_pred are already logarithmic as we've used the log1p transform\n    return np.sqrt(mean_squared_error(y, y_pred))"]}, {"cell_type": "markdown", "id": "499f0c71", "metadata": {}, "source": ["### Cross-Validation Metric"]}, {"cell_type": "code", "execution_count": 1, "id": "64ef1f19", "metadata": {}, "outputs": [], "source": ["kfolds = KFold(n_splits=10, random_state=42, shuffle=True)\n\ndef cv_rmse(model, X=X_train_full):\n    rmse = np.sqrt(-cross_val_score(model, X, y_train_full, \n                                    scoring='neg_mean_squared_error', cv=kfolds))\n    return (rmse)"]}, {"cell_type": "markdown", "id": "abb17ccd", "metadata": {}, "source": ["## Hyperopt\nIf you don't know much about this library, I encourage you to go through this kernel: https://www.kaggle.com/ilialar/hyperparameters-tunning-with-hyperopt\n<br><br>\nAt the time of making this notebook, Hyperopt was a new library for me too. If you find any way to optimize this code, please let me know. "]}, {"cell_type": "markdown", "id": "46fbb91b", "metadata": {}, "source": ["## XGBoost"]}, {"cell_type": "code", "execution_count": 1, "id": "34c968eb", "metadata": {}, "outputs": [], "source": ["def mse_cv(params, cv=kfolds, X=X_train_full, y=y_train_full):\n    # the function gets a set of variable parameters in \"params\"\n    params = {'max_depth': int(params['max_depth']),\n              'learning_rate': params['learning_rate'],\n              'gamma': params['gamma'],\n              'colsample_bytree': params['colsample_bytree'], \n              'subsample': params['subsample']\n             }\n    \n    # we use this params to create a new LGBM Regressor\n    model = XGBRegressor(objective='reg:linear', n_estimators=200,\n                          random_state=42, \n                          **params)\n    \n    # and then conduct the cross validation with the same folds as before\n    score = np.sqrt(-cross_val_score(model, X, y, cv=cv, scoring=\"neg_mean_squared_error\", n_jobs=-1).mean())\n\n    return score"]}, {"cell_type": "code", "execution_count": 1, "id": "e16c7ba2", "metadata": {}, "outputs": [], "source": ["# possible values of parameters\nspace={'max_depth' : hp.quniform('max_depth', 2, 10, 1),\n       'learning_rate': hp.loguniform('learning_rate', -5, 0), \n       'gamma': hp.loguniform('gamma', -1, 0), \n       'colsample_bytree': hp.loguniform('colsample_bytree', -1, 0), \n       'subsample': hp.loguniform('subsample', -1, 0)\n      }\n\n# trials will contain logging information\ntrials = Trials()\n\nbest=fmin(fn=mse_cv, # function to optimize\n          space=space, \n          algo=tpe.suggest, # optimization algorithm, hyperotp will select its parameters automatically\n          max_evals=200, # maximum number of iterations\n          trials=trials, # logging\n          rstate=np.random.RandomState(42) # fixing random state for the reproducibility\n         )"]}, {"cell_type": "code", "execution_count": 1, "id": "53110a67", "metadata": {}, "outputs": [], "source": ["# computing the score on the test set\nxgb_reg = XGBRegressor(random_state=42, n_estimators=5000,\n                     max_depth=int(best['max_depth']),learning_rate=best['learning_rate'], \n                     gamma=best['gamma'], colsample_bytree=best['colsample_bytree'], \n                     subsample=best['subsample'], objective='reg:linear', \n                     nthread=-1, scale_pos_weight=1)\n\nprint(np.mean(cv_rmse(xgb_reg))) \n\nprint(\"Best MSE {:.3f} params {}\".format( mse_cv(best), best)) "]}, {"cell_type": "code", "execution_count": 1, "id": "7fc74189", "metadata": {}, "outputs": [], "source": ["xgb_reg = XGBRegressor(random_state=42,\n                       n_estimators=7000, \n                       max_depth=2, \n                       learning_rate=0.138170657, \n                       gamma=0.38498075, \n                       colsample_bytree=0.599437614, \n                       objective='reg:linear', \n                       nthread=-1,\n                       scale_pos_weight=1,\n                       subsample=0.534382267)"]}, {"cell_type": "markdown", "id": "3886ab15", "metadata": {}, "source": ["## LGBM "]}, {"cell_type": "code", "execution_count": 1, "id": "c5337414", "metadata": {}, "outputs": [], "source": ["lgbm_reg = LGBMRegressor(random_state=42, \n                         n_estimators=7000, \n                         num_leaves=3, \n                         learning_rate=0.099384860, \n                         bagging_seed=5, \n                         feature_fraction_seed=5, \n                         bagging_fraction=0.407679661, \n                         feature_fraction=0.563479974, \n                         min_sum_hessian_in_leaf=20)"]}, {"cell_type": "markdown", "id": "cba940bc", "metadata": {}, "source": ["## SVR"]}, {"cell_type": "code", "execution_count": 1, "id": "083aecc2", "metadata": {}, "outputs": [], "source": ["svr_reg = make_pipeline(RobustScaler(), SVR(C=11,\n                                            gamma=0.006742321288,\n                                            epsilon=0.010861719298))"]}, {"cell_type": "markdown", "id": "a5abe443", "metadata": {}, "source": ["## GradientBoostingRegressor"]}, {"cell_type": "code", "execution_count": 1, "id": "15edc51d", "metadata": {}, "outputs": [], "source": ["gb_reg = GradientBoostingRegressor(max_depth=21,\n                                   min_samples_leaf=11,\n                                   min_samples_split=8,\n                                   learning_rate=0.038954801112873964, \n                                   loss='huber', max_features='sqrt',\n                                   n_estimators=7000, random_state=42)"]}, {"cell_type": "markdown", "id": "f8098dee", "metadata": {}, "source": ["## Ridge, Lasso and ElasticNet"]}, {"cell_type": "code", "execution_count": 1, "id": "b83c2c6b", "metadata": {}, "outputs": [], "source": ["ridge_alphas = [1e-10, 1e-8, 1e-5, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, \n                0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\n\nlasso_alphas = [5e-5, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n\nelastic_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008, 0.0009, 0.001]\nelastic_l1ratio = [0.5, 0.6, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.97, 0.99, 1]"]}, {"cell_type": "code", "execution_count": 1, "id": "2410ed5c", "metadata": {}, "outputs": [], "source": ["ridge_reg = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas, cv=kfolds))\nlasso_reg = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas=lasso_alphas, \n                                              random_state=42, cv=kfolds))\nelasticnet_reg = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, \n                                                            alphas=elastic_alphas, \n                                                            cv=kfolds, \n                                                            l1_ratio=elastic_l1ratio))                                "]}, {"cell_type": "markdown", "id": "a65d9903", "metadata": {}, "source": ["## Stacking"]}, {"cell_type": "code", "execution_count": 1, "id": "537fa3e5", "metadata": {}, "outputs": [], "source": ["stack_gen = StackingCVRegressor(regressors=(ridge_reg, lasso_reg, elasticnet_reg, xgb_reg, lgbm_reg, gb_reg, svr_reg),\n                                meta_regressor=xgb_reg,\n                                use_features_in_secondary=True)"]}, {"cell_type": "markdown", "id": "15b7ea95", "metadata": {}, "source": ["## Scores for Each Model"]}, {"cell_type": "code", "execution_count": 1, "id": "2924e182", "metadata": {}, "outputs": [], "source": ["scores = {}\n\nscore = cv_rmse(lgbm_reg)\nprint(\"lgbm_reg: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['lgbm'] = (score.mean(), score.std())\n\nscore = cv_rmse(xgb_reg)\nprint(\"xgb_reg: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['xgb'] = (score.mean(), score.std())\n\nscore = cv_rmse(svr_reg)\nprint(\"SVR: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['svr'] = (score.mean(), score.std())\n\nscore = cv_rmse(ridge_reg)\nprint(\"ridge_reg: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['ridge'] = (score.mean(), score.std())\n\nscore = cv_rmse(lasso_reg)\nprint(\"lasso_reg: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['lasso'] = (score.mean(), score.std())\n\nscore = cv_rmse(elasticnet_reg)\nprint(\"elasticnet_reg: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['elasticnet'] = (score.mean(), score.std())\n\nscore = cv_rmse(gb_reg)\nprint(\"gb_reg: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['gb'] = (score.mean(), score.std())"]}, {"cell_type": "markdown", "id": "4e263401", "metadata": {}, "source": ["## Fitting"]}, {"cell_type": "code", "execution_count": 1, "id": "c905bede", "metadata": {}, "outputs": [], "source": ["%%time\nprint('START Fit')\n\nprint('stack_gen')\nstack_gen_model = stack_gen.fit(np.array(X_train_full), np.array(y_train_full))\n\nprint('elasticnet')\nelastic_reg_model = elasticnet_reg.fit(X_train_full, y_train_full)\n\nprint('Lasso')\nlasso_reg_model = lasso_reg.fit(X_train_full, y_train_full)\n\nprint('Ridge') \nridge_reg_model = ridge_reg.fit(X_train_full, y_train_full)\n\nprint('Svr')\nsvr_reg_model = svr_reg.fit(X_train_full, y_train_full)\n\nprint('GradientBoosting')\ngb_reg_model = gb_reg.fit(X_train_full, y_train_full)\n\nprint('xgboost')\nxgb_reg_model = xgb_reg.fit(X_train_full, y_train_full)\n\nprint('lightgbm')\nlgbm_reg_model = lgbm_reg.fit(X_train_full, y_train_full)\n\nprint('END FIT')"]}, {"cell_type": "markdown", "id": "5ef31e6b", "metadata": {}, "source": ["# Blending"]}, {"cell_type": "code", "execution_count": 1, "id": "4b3157da", "metadata": {}, "outputs": [], "source": ["def blender(X):\n    return ((0.1 * elastic_reg_model.predict(X)) + \\\n            (0.1 * lasso_reg_model.predict(X)) + \\\n            (0.1 * ridge_reg_model.predict(X)) + \\\n            (0.1 * svr_reg_model.predict(X)) + \\\n            (0.1 * gb_reg_model.predict(X)) + \\\n            (0.1 * xgb_reg_model.predict(X)) + \\\n            (0.1 * lgbm_reg_model.predict(X)) + \\\n            (0.3 * stack_gen_model.predict(np.array(X))))"]}, {"cell_type": "code", "execution_count": 1, "id": "a21c2f86", "metadata": {}, "outputs": [], "source": ["print('RMSLE score on train data:')\nprint(rmsle(y_train_full, blender(X_train_full)))"]}, {"cell_type": "markdown", "id": "9192d68e", "metadata": {}, "source": ["# Submitting"]}, {"cell_type": "code", "execution_count": 1, "id": "9ea6fb2e", "metadata": {}, "outputs": [], "source": ["print('Predict submission')\nsubmission = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(blender(X_test)))"]}, {"cell_type": "code", "execution_count": 1, "id": "e44e35ee", "metadata": {}, "outputs": [], "source": ["submission.to_csv(\"submission.csv\", index=False)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}