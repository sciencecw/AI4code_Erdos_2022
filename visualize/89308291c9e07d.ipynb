{"cells": [{"cell_type": "markdown", "id": "ae8c1291", "metadata": {}, "source": ["# Intro\nWelcome to the [Tensorflow - Help Protect the Great Barrier Reef](https://www.kaggle.com/c/tensorflow-great-barrier-reef) compedition.\n\n![](https://storage.googleapis.com/kaggle-competitions/kaggle/31703/logos/header.png)\n\n<span style=\"color: royalblue;\">Please vote the notebook up if it helps you. Feel free to leave a comment above the notebook. Thank you. </span>"]}, {"cell_type": "markdown", "id": "3978b4bc", "metadata": {}, "source": ["# Libraries"]}, {"cell_type": "code", "execution_count": 1, "id": "8a117515", "metadata": {}, "outputs": [], "source": ["import os\nimport cv2\nimport ast\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt"]}, {"cell_type": "markdown", "id": "382aa5f7", "metadata": {}, "source": ["# Path"]}, {"cell_type": "code", "execution_count": 1, "id": "49a0fa9a", "metadata": {}, "outputs": [], "source": ["path = '/kaggle/input/tensorflow-great-barrier-reef/'\nos.listdir(path)"]}, {"cell_type": "markdown", "id": "fe86afce", "metadata": {}, "source": ["# Load Data"]}, {"cell_type": "code", "execution_count": 1, "id": "6e205568", "metadata": {}, "outputs": [], "source": ["train_data = pd.read_csv(path+'train.csv')\ntest_data = pd.read_csv(path+'test.csv')\nsamp_subm = pd.read_csv(path+'example_sample_submission.csv')"]}, {"cell_type": "markdown", "id": "669ee6e5", "metadata": {}, "source": ["# Overview"]}, {"cell_type": "code", "execution_count": 1, "id": "4ba73b62", "metadata": {}, "outputs": [], "source": ["print('Number train samples:', len(train_data))\nprint('Number test samples:', len(test_data))"]}, {"cell_type": "markdown", "id": "0e47fe84", "metadata": {}, "source": ["* video_id - ID number of the video the image was part of. The video ids are not meaningfully ordered.\n* video_frame - The frame number of the image within the video. Expect to see occasional gaps in the frame number from when the diver surfaced.\n* sequence - ID of a gap-free subset of a given video. The sequence ids are not meaningfully ordered.\n* sequence_frame - The frame number within a given sequence.\n* image_id - ID code for the image, in the format '{video_id}-{video_frame}'\n* annotations - The bounding boxes of any starfish detections in a string format that can be evaluated directly with Python. Does not use the same format as the predictions you will submit. Not available in test.csv. A bounding box is described by the pixel coordinate (x_min, y_min) of its lower left corner within the image together with its width and height in pixels."]}, {"cell_type": "code", "execution_count": 1, "id": "84d90e05", "metadata": {}, "outputs": [], "source": ["train_data.head()"]}, {"cell_type": "markdown", "id": "55fd2f3b", "metadata": {}, "source": ["# EDA\nThere are 3 values for the feature video_id which represent the number of the underlying folder."]}, {"cell_type": "code", "execution_count": 1, "id": "80d49452", "metadata": {}, "outputs": [], "source": ["train_data['video_id'].value_counts()"]}, {"cell_type": "markdown", "id": "6c393b0c", "metadata": {}, "source": ["There are 20 different sequences:"]}, {"cell_type": "code", "execution_count": 1, "id": "51799114", "metadata": {}, "outputs": [], "source": ["train_data['sequence'].value_counts()"]}, {"cell_type": "markdown", "id": "cd6c7fc2", "metadata": {}, "source": ["# Load Image Files\n**train_images/** - Folder containing training set photos of the form video_{video_id}/{video_frame_number}.jpg.\n\nWe consider the image with the video_frame id 7981:"]}, {"cell_type": "code", "execution_count": 1, "id": "7acf4bb2", "metadata": {}, "outputs": [], "source": ["video_frame = 7981\nfile_name = str(video_frame)+'.jpg'\ntrain_data[train_data['video_frame']==video_frame]"]}, {"cell_type": "markdown", "id": "a28f1d3f", "metadata": {}, "source": ["As we can see there are 2 images with this video_frame id. One in folder video_0 and another one in folder video_2:"]}, {"cell_type": "code", "execution_count": 1, "id": "40773961", "metadata": {}, "outputs": [], "source": ["print('file 7981.jpg in folder video_0:', file_name in os.listdir(path+'train_images/video_0'))\nprint('file 7981.jpg in folder video_2:', file_name in os.listdir(path+'train_images/video_2'))"]}, {"cell_type": "markdown", "id": "8d6b8b60", "metadata": {}, "source": ["We load the images:"]}, {"cell_type": "code", "execution_count": 1, "id": "1e44861e", "metadata": {}, "outputs": [], "source": ["image_folder_0 = cv2.imread(path+'train_images/video_0/'+file_name)\nimage_folder_2 = cv2.imread(path+'train_images/video_2/'+file_name)"]}, {"cell_type": "code", "execution_count": 1, "id": "294e3c62", "metadata": {}, "outputs": [], "source": ["print('shape of image in folder video_0:', image_folder_0.shape)\nprint('shape of image in folder video_2:', image_folder_2.shape)"]}, {"cell_type": "markdown", "id": "29b345a2", "metadata": {}, "source": ["**Plot images**\n\nThe image in folder video_0 has no annotations."]}, {"cell_type": "code", "execution_count": 1, "id": "60291d3f", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(1, 1, figsize=(20, 8))\nax.imshow(image_folder_0);\nax.set_xticklabels([])\nax.set_yticklabels([])\nplt.show()"]}, {"cell_type": "markdown", "id": "b0ae97a5", "metadata": {}, "source": ["The image in the folder video_2 has one annotation:"]}, {"cell_type": "code", "execution_count": 1, "id": "3802fb55", "metadata": {}, "outputs": [], "source": ["train_data[(train_data['video_frame']==video_frame)&(train_data['video_id']==2)]['annotations']"]}, {"cell_type": "code", "execution_count": 1, "id": "b5cb8c04", "metadata": {}, "outputs": [], "source": ["row = 20722"]}, {"cell_type": "markdown", "id": "ba87294b", "metadata": {}, "source": ["We extract the boxes:"]}, {"cell_type": "code", "execution_count": 1, "id": "1fc43b69", "metadata": {}, "outputs": [], "source": ["boxes = ast.literal_eval(train_data.loc[row, 'annotations'])"]}, {"cell_type": "code", "execution_count": 1, "id": "3bc6f3ea", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(1, 1, figsize=(20, 8))\nax.imshow(image_folder_2)\nfor box in boxes:\n    p = matplotlib.patches.Rectangle((box['x'], box['y']), box['width'], box['height'],\n                                     ec='r', fc='none', lw=2.)\n    ax.add_patch(p)\nax.set_xticklabels([])\nax.set_yticklabels([])\nplt.show()"]}, {"cell_type": "markdown", "id": "ca04c69b", "metadata": {}, "source": ["# Example With Multi Boxes"]}, {"cell_type": "code", "execution_count": 1, "id": "b15d34ee", "metadata": {}, "outputs": [], "source": ["row = 5454\nfile_name = str(train_data.loc[row, 'video_frame'])+'.jpg'\nvideo_folder = 'video_'+str(train_data.loc[row, 'video_id'])\nboxes = ast.literal_eval(train_data.loc[row, 'annotations'])"]}, {"cell_type": "code", "execution_count": 1, "id": "35448bb8", "metadata": {}, "outputs": [], "source": ["print('video folder:', video_folder)\nprint('file name:', file_name)"]}, {"cell_type": "code", "execution_count": 1, "id": "c20554d1", "metadata": {}, "outputs": [], "source": ["image = cv2.imread(path+'train_images/'+video_folder+'/'+file_name)\nimage.shape"]}, {"cell_type": "markdown", "id": "b7b50a1e", "metadata": {}, "source": ["Plot image and annotations:"]}, {"cell_type": "code", "execution_count": 1, "id": "9ebc8096", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(1, 1, figsize=(20, 8))\nax.imshow(image)\nfor box in boxes:\n    p = matplotlib.patches.Rectangle((box['x'], box['y']), box['width'], box['height'],\n                                     ec='r', fc='none', lw=2.)\n    ax.add_patch(p)\nax.set_xticklabels([])\nax.set_yticklabels([])\nplt.show()"]}, {"cell_type": "markdown", "id": "456c0517", "metadata": {}, "source": ["# Use API\nTo use the api we follow the instructions of [this notebook](https://www.kaggle.com/sohier/great-barrier-reef-api-tutorial/notebook). "]}, {"cell_type": "code", "execution_count": 1, "id": "1bab4d11", "metadata": {}, "outputs": [], "source": ["import PIL.Image\nimport greatbarrierreef\n\nenv = greatbarrierreef.make_env()\niter_test = env.iter_test()"]}, {"cell_type": "code", "execution_count": 1, "id": "13b7aab4", "metadata": {}, "outputs": [], "source": ["pixel_array, sample_prediction_df = next(iter_test)\npixel_array"]}, {"cell_type": "code", "execution_count": 1, "id": "b2ab7112", "metadata": {}, "outputs": [], "source": ["PIL.Image.fromarray(pixel_array)"]}, {"cell_type": "markdown", "id": "d7d5fb3f", "metadata": {}, "source": ["# Export Data"]}, {"cell_type": "code", "execution_count": 1, "id": "67eeda7a", "metadata": {}, "outputs": [], "source": ["samp_subm.to_csv('submission.csv', index=False)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}