{"cells": [{"cell_type": "markdown", "id": "30069862", "metadata": {}, "source": ["Obtener la ruta de los datasets"]}, {"cell_type": "code", "execution_count": 1, "id": "a9f1238b", "metadata": {}, "outputs": [], "source": ["# Add your PRIVATE credentials\n# Do not use \"!export KAGGLE_USERNAME= ...\" OR \"\" around your credential\n%env KAGGLE_USERNAME=''\n%env KAGGLE_KEY=''\n\n# Verify\n!export -p | grep KAGGLE_USERNAME\n!export -p | grep KAGGLE_KEY"]}, {"cell_type": "code", "execution_count": 1, "id": "d5729cb5", "metadata": {}, "outputs": [], "source": ["import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))"]}, {"cell_type": "code", "execution_count": 1, "id": "c4707dca", "metadata": {}, "outputs": [], "source": ["## Cargar archivos de la competencia"]}, {"cell_type": "code", "execution_count": 1, "id": "4cb5db51", "metadata": {}, "outputs": [], "source": ["import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('/kaggle/input/tp-n2-aprendizaje-profundo-2021-by-datitos-v2/fifa2021_training.csv')\ndf_infer = pd.read_csv('/kaggle/input/tp-n2-aprendizaje-profundo-2021-by-datitos-v2/fifa2021_test.csv')"]}, {"cell_type": "markdown", "id": "f7f175bb", "metadata": {}, "source": ["An\u00e1lisis exploratorio del dataset"]}, {"cell_type": "code", "execution_count": 1, "id": "510e87fa", "metadata": {}, "outputs": [], "source": ["df.iloc[0]"]}, {"cell_type": "markdown", "id": "74f7d108", "metadata": {}, "source": ["Particionar datasets"]}, {"cell_type": "code", "execution_count": 1, "id": "fc449751", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n\ndf_train, df_valid = train_test_split(df, stratify=df.Position, train_size=0.9, random_state=42)"]}, {"cell_type": "markdown", "id": "d334f239", "metadata": {}, "source": ["Definir transformaciones y columnas a utilizar"]}, {"cell_type": "code", "execution_count": 1, "id": "65c8161f", "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import make_column_transformer\n\nvariables_descartar = [\n   'Position', # variable objetivo\n    'ID',\n    'Name',\n    'Natinality',\n    'Potential',\n    'BirthDate',\n    'Value',\n    'Wage',\n    'Club',\n    'Club_KitNumber',\n    'Club_JoinedClub',\n    'Club_ContractLength',\n]\n\nvariables_categ\u00f3ricas = df.drop(columns=variables_descartar).select_dtypes(include=np.object).columns\nvariables_num\u00e9ricas   = df.drop(columns=variables_descartar).select_dtypes(include=np.number).columns\n\ntransformador = make_column_transformer(\n    (OneHotEncoder(handle_unknown='ignore'),  variables_categ\u00f3ricas), # PreferredFoot, PlayerWorkRate, Sex resolver nacionalidad con handle_unknown='ignore' Eso trae otro probl despues, asi q por ahora vamos con eliminar los dos paises q me joden\n    (StandardScaler(), variables_num\u00e9ricas),   # Overal, Potential, Height, etc.\n    remainder='drop' # descarta las columnas no mencionadas en las transformaciones\n)"]}, {"cell_type": "markdown", "id": "f123875e", "metadata": {}, "source": ["\nEntrenar transformador\n\nEsencialmente, calculamos los promedios y las desviaciones est\u00e1ndares que usaremos para la estandarizaci\u00f3n.\n\n SOLO PARA EL DATASET DEtransformador.fit(df_train) ENTRENAMIENTO."]}, {"cell_type": "code", "execution_count": 1, "id": "54ba96fa", "metadata": {}, "outputs": [], "source": ["transformador.fit(df_train)"]}, {"cell_type": "markdown", "id": "672e376a", "metadata": {}, "source": ["Transformar datasets"]}, {"cell_type": "code", "execution_count": 1, "id": "7218b44b", "metadata": {}, "outputs": [], "source": ["# Se agrega la columna a predecir en el dataset para que tenga las mismas columnas que df_train :/  \ndf_infer['Position'] = None\n\n\nX_train = transformador.transform(df_train)\nX_valid = transformador.transform(df_valid)\nX_infer = transformador.transform(df_infer)"]}, {"cell_type": "markdown", "id": "a0196104", "metadata": {}, "source": ["Transformar variable objetivo\n\nComo la variable objetivo es del tipo string, hay que llevarla a un tipo num\u00e9rico para que PyTorch pueda procesarla.\n\nEste transformador mapea posiciones DEF, FWD, GK, MID a enteros y viceversa \u2014 la transformaci\u00f3n inversa ser\u00e1 \u00fatil para convertir las predicciones (enteros) en posiciones otra vez."]}, {"cell_type": "code", "execution_count": 1, "id": "a63e8717", "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import LabelEncoder\n\ntransformador_etiquetas = LabelEncoder()\n\ntransformador_etiquetas.fit(df_train.Position)\n\ny_train = transformador_etiquetas.transform(df_train.Position)\ny_valid = transformador_etiquetas.transform(df_valid.Position)"]}, {"cell_type": "markdown", "id": "79ccc8be", "metadata": {}, "source": ["Instanciar Datasets de PyTorch\n\nEl aprendizaje profundo es especialmente efectivo para im\u00e1genes y texto; para datos tabulares (como un DataFrame) el aprendizaje de m\u00e1quinas cl\u00e1sico suele funcionar bastante bien, de ah\u00ed que PyTorch no cuente con facilidades para tratar este tipo de problemas."]}, {"cell_type": "code", "execution_count": 1, "id": "e49ecfcf", "metadata": {}, "outputs": [], "source": ["from torch.utils.data import Dataset\n\nclass Tabular(Dataset):\n    def __init__(self, X, y=None):\n        self.X = X.astype(np.float32) # soluciona \"Expected object of scalar type Float but got scalar type Double\"\n        self.y = y \n\n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, item):\n        if self.y is None:\n            return self.X[item]\n        else:\n            return self.X[item], self.y[item]\n\n        \nds_train = Tabular(X_train, y_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "6664c4c7", "metadata": {}, "outputs": [], "source": ["ds_train[10]"]}, {"cell_type": "markdown", "id": "980aa34a", "metadata": {}, "source": ["Instanciar DataLoaders de PyTorch"]}, {"cell_type": "code", "execution_count": 1, "id": "6d2e8479", "metadata": {}, "outputs": [], "source": ["from torch.utils.data import DataLoader\n\ndl_train = DataLoader(ds_train, batch_size=32, shuffle=False)"]}, {"cell_type": "code", "execution_count": 1, "id": "c6cd7188", "metadata": {}, "outputs": [], "source": ["import torch\nX_valid = torch.tensor(X_valid).float()\nX_infer = torch.tensor(X_infer).float()\ny_valid = torch.tensor(y_valid)"]}, {"cell_type": "markdown", "id": "129e3d45", "metadata": {}, "source": ["Instanciar modelo"]}, {"cell_type": "code", "execution_count": 1, "id": "3d09107a", "metadata": {}, "outputs": [], "source": ["import torch\nimport torch.nn as nn"]}, {"cell_type": "code", "execution_count": 1, "id": "27dbb5f3", "metadata": {}, "outputs": [], "source": ["IN  = X_train.shape[1]\nOUT = len(transformador_etiquetas.classes_)\n\nmodelo = nn.Sequential(\n    nn.Linear(IN,  8),\n    nn.Linear( 8, 64), nn.ReLU(),\n    nn.Linear(64, 32), nn.ReLU(), \n    nn.Linear(32, OUT)\n)"]}, {"cell_type": "code", "execution_count": 1, "id": "c34f9082", "metadata": {}, "outputs": [], "source": ["weights = torch.tensor([0.2,0.2,0.2,0.2], dtype=torch.float)\n\ncriterio = nn.CrossEntropyLoss(weight=weights) #weight=weights \noptimizador = torch.optim.Adam(modelo.parameters(), lr=0.01)"]}, {"cell_type": "markdown", "id": "bd1d6c98", "metadata": {}, "source": ["### Entrenar Modelo"]}, {"cell_type": "code", "execution_count": 1, "id": "fa4422cd", "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import balanced_accuracy_score\n\n\u00c9POCAS = 10\n\nfor \u00e9poca in range(\u00c9POCAS):\n    # activa capas Dropout, BatchNorm si las hubiese\n    modelo.train()\n\n    p\u00e9rdidas_train = []\n    \n    for X_lote, y_lote in dl_train:\n        optimizador.zero_grad()\n\n        predicciones = modelo(X_lote)\n        p\u00e9rdida = criterio(predicciones, y_lote)\n\n        p\u00e9rdida.backward()\n        optimizador.step()\n        \n        p\u00e9rdidas_train.append(p\u00e9rdida.item())\n    \n    # desactiva capas Dropout, BatchNorm si las hubiese\n    modelo.eval()\n    \n    with torch.no_grad():\n        predicciones = modelo(X_valid)\n        p\u00e9rdida = criterio(predicciones, y_valid)\n        \n        y_pred = predicciones.argmax(dim=1) # selecciona la clase con mayor probabilidad\n        \n        efectividad = balanced_accuracy_score(y_valid, y_pred)\n    \n    \n    print(f'{\u00e9poca:3d}  |  Train loss: {np.mean(p\u00e9rdidas_train):.3f}    Valid loss: {p\u00e9rdida:.3f}    Valid accuracy: {efectividad:.2f}')\ncriterio = nn.CrossEntropyLoss(weight=weights) \noptimizador = torch.optim.Adam(modelo.parameters(), lr=0.0001)"]}, {"cell_type": "markdown", "id": "31edd57a", "metadata": {}, "source": ["### Inferir datos de prueba"]}, {"cell_type": "code", "execution_count": 1, "id": "b95ae4dc", "metadata": {}, "outputs": [], "source": ["with torch.no_grad():\n    y_infer = modelo(X_infer).argmax(dim=1)\n\ndf_infer['Position'] = transformador_etiquetas.inverse_transform(y_infer)\n\n(\n    df_infer[['ID', 'Position']]\n    .rename(columns={'ID':'Id', 'Position':'Category'})\n    .to_csv('submit.csv', index=False)\n)"]}, {"cell_type": "code", "execution_count": 1, "id": "52133b29", "metadata": {}, "outputs": [], "source": ["#!pip install kaggle"]}, {"cell_type": "code", "execution_count": 1, "id": "2037fbfa", "metadata": {}, "outputs": [], "source": ["#!kaggle competitions submit -c tp-n2-aprendizaje-profundo-2021-by-datitos-v2 -f submit.csv -m \"Ultimo submit!\""]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}