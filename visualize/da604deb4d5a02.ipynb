{"cells": [{"cell_type": "code", "execution_count": 1, "id": "ebcf87ea", "metadata": {}, "outputs": [], "source": ["!pip install dtreeviz\n!pip install -Uqq fastbook\nimport fastbook\n\nfrom fastbook import *\nfrom fastai.tabular.all import *\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom dtreeviz.trees import *\nfrom IPython.display import Image, display_svg, SVG\n\npd.options.display.max_rows = 20\npd.options.display.max_columns = 8"]}, {"cell_type": "markdown", "id": "a4ed440a", "metadata": {}, "source": ["You are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period."]}, {"cell_type": "code", "execution_count": 1, "id": "fecc2b86", "metadata": {}, "outputs": [], "source": ["train = pd.read_csv('/kaggle/input/bike-sharing-demand/train.csv', low_memory=False)\ntest = pd.read_csv('/kaggle/input/bike-sharing-demand/test.csv', low_memory=False)\nsubmission = pd.read_csv('../input/bike-sharing-demand/sampleSubmission.csv', low_memory=False)\ntrain.head(1)"]}, {"cell_type": "code", "execution_count": 1, "id": "d3275dec", "metadata": {}, "outputs": [], "source": ["train.tail(1)"]}, {"cell_type": "markdown", "id": "7c37b2ea", "metadata": {}, "source": ["Training spans from the 1st day of each month to the 19th. The test set begins on the 20th to the end of the month."]}, {"cell_type": "code", "execution_count": 1, "id": "336dad81", "metadata": {}, "outputs": [], "source": ["len(train.columns), len(test.columns)"]}, {"cell_type": "code", "execution_count": 1, "id": "7e783e03", "metadata": {}, "outputs": [], "source": ["train.columns"]}, {"cell_type": "code", "execution_count": 1, "id": "f77c2e7a", "metadata": {}, "outputs": [], "source": ["test.columns"]}, {"cell_type": "markdown", "id": "0497c809", "metadata": {}, "source": ["### Column Metadata\n\n1. `datetime`: hourly date + timestamp  \n\n1. `season`:  \n    - `1` = spring \n    - `2` = summer\n    - `3` = fall\n    - `4` = winter \n\n1. `holiday`: whether the day is considered a holiday\n\n1. `workingday`: whether the day is neither a weekend nor holiday\n\n1. `weather`:\n    - `1`: Clear, Few clouds, Partly cloudy, Partly cloudy\n    - `2`: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n    - `3`: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n    - `4`: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n\n\n1. `temp`: temperature in Celsius\n\n1. `atemp`: \"feels like\" temperature in Celsius\n\n1. `humidity`: relative humidity\n\n1. `windspeed`: wind speed\n\n1. `casual`: number of non-registered user rentals initiated\n\n1. `registered`: number of registered user rentals initiated\n\n1. `count`: number of total rentals\n"]}, {"cell_type": "markdown", "id": "f2519bc6", "metadata": {}, "source": ["The columns below seem to be the only categorical columns, all having less than 4 discrete levels."]}, {"cell_type": "code", "execution_count": 1, "id": "70b339d7", "metadata": {}, "outputs": [], "source": ["train.season.unique(), train.holiday.unique(), train.weather.unique(), train.workingday.unique()"]}, {"cell_type": "markdown", "id": "6bf94c58", "metadata": {}, "source": ["All data types seem to fit well into their descriptions except for `datetime` which needs to be converted into a `pd.Datetime` object."]}, {"cell_type": "code", "execution_count": 1, "id": "4052942c", "metadata": {}, "outputs": [], "source": ["train.dtypes"]}, {"cell_type": "markdown", "id": "fc6cb741", "metadata": {}, "source": ["The number of bikes at the station is what needs to be predicted, this is represented as the `count` column and is the dependent variable. The dataset specifies to use the RMSLE (Root Mean Squared Log Error) as the loss function."]}, {"cell_type": "code", "execution_count": 1, "id": "203726fe", "metadata": {}, "outputs": [], "source": ["dep_var = 'count'\ntrain[dep_var] = np.log(train[dep_var])\n# test[dep_var] = np.log(test[dep_var])"]}, {"cell_type": "markdown", "id": "608ecdf4", "metadata": {}, "source": ["## Decision Trees\nDecision trees perfrom well with structured data like this dataset so it seems the obvious approach. It relies on a series of binary questions that tree asks to exclude certain possibilities, as we descend the tree the questions get more specific to narrow the prediction, similar to a game of 20 questions."]}, {"cell_type": "markdown", "id": "165e2557", "metadata": {}, "source": ["![image.png](attachment:image.png)"]}, {"cell_type": "markdown", "id": "9d86d118", "metadata": {}, "source": ["### Date Parsing\nDates are unlike most data types as they can seen as both continuous and categorical variables. Each date can be categorised into weekday/weekend or holiday/non-holiday and at the same time we can use the day of the month as a number. This gives dates the bonus of being able to create more data from data.\n- ordinality\n\n\nDates are usually a hassle to work with but, FastAI comes with a nifty function that does a lot of the date parsing for us."]}, {"cell_type": "code", "execution_count": 1, "id": "9b8a3649", "metadata": {}, "outputs": [], "source": ["train = add_datepart(train, 'datetime')\ntest = add_datepart(test, 'datetime')"]}, {"cell_type": "code", "execution_count": 1, "id": "8c77c513", "metadata": {}, "outputs": [], "source": ["train.columns"]}, {"cell_type": "markdown", "id": "c8782f8d", "metadata": {}, "source": ["The `add_datepart` function has created additional date columns. Prining out all unique values of the datepart shows a combination if continuous and categorical variables."]}, {"cell_type": "code", "execution_count": 1, "id": "05aa0df3", "metadata": {}, "outputs": [], "source": ["for i in train.columns[train.columns.str.contains('datetime')]:\n    print('\\n'+i+': '+str(train[i].unique()))"]}, {"cell_type": "code", "execution_count": 1, "id": "e2bd6831", "metadata": {}, "outputs": [], "source": ["len(train.columns), len(test.columns)"]}, {"cell_type": "markdown", "id": "1e9a158d", "metadata": {}, "source": ["### TabularPandas & TabularProcs\n`TabularPandas` provides a wrapper around a standard Pandas DataFrame to help with missing values. To use `TabularPandas`, `TabularProcs` must be use to tell FastAI how to handle the missing values, the ones used here are `Categorify` and `FillMissing`.\n\n`Categorify`: replace a column with a categorical one\n\n`FillMissing`: replaces missing values with the median of that column\n\n\n`TabularPandas` also handle the splitting of the training and validation sets, because this a time series prediction where the goal is to know how many bikes will be a at a station in the future, the validation set must be the latter portion of the set as we need this to verify the predictions."]}, {"cell_type": "code", "execution_count": 1, "id": "e14aa2b1", "metadata": {}, "outputs": [], "source": ["procs = [Categorify, FillMissing]"]}, {"cell_type": "code", "execution_count": 1, "id": "574d192c", "metadata": {}, "outputs": [], "source": ["train.columns[train.columns.str.contains('datetime', case=False)]"]}, {"cell_type": "code", "execution_count": 1, "id": "bcb5c2a5", "metadata": {}, "outputs": [], "source": ["# train.drop(columns=['casual', 'registered'], inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "75c0b2ce", "metadata": {}, "outputs": [], "source": ["len(train.columns), len(test.columns)"]}, {"cell_type": "markdown", "id": "6b1936e2", "metadata": {}, "source": ["The choice of split below was not terribly scientific; I roughly split the set into a 60:40 split in favour of training set."]}, {"cell_type": "code", "execution_count": 1, "id": "7a3076eb", "metadata": {}, "outputs": [], "source": ["cond = (train.datetimeYear<2011) | (train.datetimeMonth<8)\ntrain_idx = np.where( cond)[0]\nvalid_idx = np.where(~cond)[0]\n\nsplits = (list(train_idx),list(valid_idx))\n'training: ', len(train_idx), 'validation: ', len(valid_idx)"]}, {"cell_type": "markdown", "id": "123d80c3", "metadata": {}, "source": ["The function below splits the continuous and categorical columns for `TabularPandas` to understand."]}, {"cell_type": "code", "execution_count": 1, "id": "76b5e9d5", "metadata": {}, "outputs": [], "source": ["cont,cat = cont_cat_split(train, 1, dep_var=dep_var)"]}, {"cell_type": "code", "execution_count": 1, "id": "ed434ca1", "metadata": {}, "outputs": [], "source": ["to = TabularPandas(train, procs, cat, cont, y_names=dep_var, splits=splits)"]}, {"cell_type": "code", "execution_count": 1, "id": "2e636072", "metadata": {}, "outputs": [], "source": ["dls = to.dataloaders(bs=64)"]}, {"cell_type": "code", "execution_count": 1, "id": "4e148656", "metadata": {}, "outputs": [], "source": ["dls.show_batch()"]}, {"cell_type": "markdown", "id": "b0170155", "metadata": {}, "source": ["### Creating Decision Tree"]}, {"cell_type": "markdown", "id": "c396f4d9", "metadata": {}, "source": ["`xs` denotes the independent variable and `y` the dependent."]}, {"cell_type": "code", "execution_count": 1, "id": "cca8d064", "metadata": {}, "outputs": [], "source": ["xs, y = to.train.xs, to.train.y\nvalid_xs,valid_y = to.valid.xs,to.valid.y"]}, {"cell_type": "code", "execution_count": 1, "id": "5b8f67a6", "metadata": {}, "outputs": [], "source": ["model = DecisionTreeRegressor(max_leaf_nodes=4)\nmodel.fit(xs, y);"]}, {"cell_type": "markdown", "id": "2a71f438", "metadata": {}, "source": ["The visualisation below shows what factors that cause a split and gives a better insight into what is actually happening."]}, {"cell_type": "code", "execution_count": 1, "id": "a1446c96", "metadata": {}, "outputs": [], "source": ["draw_tree(model, xs, size=10, leaves_parallel=True, precision=2)"]}, {"cell_type": "markdown", "id": "0b763c96", "metadata": {}, "source": ["This plot shows the data distribution of each split. This can help in finding outliers in the data that may be skewing the predictions. The top node is the model's state before any data splits have been made. This node will predict the average of the whole set."]}, {"cell_type": "code", "execution_count": 1, "id": "ce4e4895", "metadata": {}, "outputs": [], "source": ["idx = np.random.permutation(len(y))[:500]\ndtreeviz(model, xs.iloc[idx], y.iloc[idx], xs.columns, dep_var,\n        fontname='DejaVu Sans', scale=1.6, label_fontsize=10,\n        orientation='LR')"]}, {"cell_type": "markdown", "id": "b55a20bc", "metadata": {}, "source": ["- write on this"]}, {"cell_type": "code", "execution_count": 1, "id": "8b6fd35b", "metadata": {}, "outputs": [], "source": ["def r_mse(pred,y): \n    return round(math.sqrt(((pred-y)**2).mean()), 6)\n\ndef m_rmse(m, xs, y): \n    return r_mse(m.predict(xs), y)"]}, {"cell_type": "markdown", "id": "97f11a6b", "metadata": {}, "source": ["## Random Forests"]}, {"cell_type": "code", "execution_count": 1, "id": "fb0ee0d7", "metadata": {}, "outputs": [], "source": ["def rf(xs, y, n_estimators=40, max_samples=4507,\n       max_features=0.5, min_samples_leaf=5, **kwargs):\n    return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators,\n        max_samples=max_samples, max_features=max_features,\n        min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)"]}, {"cell_type": "code", "execution_count": 1, "id": "6857ed7e", "metadata": {}, "outputs": [], "source": ["# model = rf(xs, y);\n# m_rmse(model, xs, y), m_rmse(model, valid_xs, valid_y)\nmodel = tabular_learner(dls, metrics=accuracy)"]}, {"cell_type": "code", "execution_count": 1, "id": "345ca9fc", "metadata": {}, "outputs": [], "source": ["model.fit_one_cycle(1)"]}, {"cell_type": "code", "execution_count": 1, "id": "00f0e261", "metadata": {}, "outputs": [], "source": ["model.show_results()"]}, {"cell_type": "markdown", "id": "d05deb06", "metadata": {}, "source": ["### Submission"]}, {"cell_type": "code", "execution_count": 1, "id": "314dc334", "metadata": {}, "outputs": [], "source": ["test.columns"]}, {"cell_type": "code", "execution_count": 1, "id": "25afb8a5", "metadata": {}, "outputs": [], "source": ["train.columns"]}, {"cell_type": "code", "execution_count": 1, "id": "33f13c07", "metadata": {}, "outputs": [], "source": ["test_df = train.copy()\ntest_df.drop(['count'], axis=1, inplace=True)\ndl = model.dls.test_dl(test_df)"]}, {"cell_type": "code", "execution_count": 1, "id": "a59bb349", "metadata": {}, "outputs": [], "source": ["preds = model.get_preds(dl=dl)"]}, {"cell_type": "code", "execution_count": 1, "id": "055f4692", "metadata": {}, "outputs": [], "source": ["# randomforest_preds = model.predict(test)"]}, {"cell_type": "code", "execution_count": 1, "id": "095be132", "metadata": {}, "outputs": [], "source": ["preds[0][:6492]"]}, {"cell_type": "code", "execution_count": 1, "id": "255efda8", "metadata": {}, "outputs": [], "source": ["preds_l = [float(i) for i in preds[0]]\npreds_l"]}, {"cell_type": "code", "execution_count": 1, "id": "1de1d109", "metadata": {}, "outputs": [], "source": ["submission['count'] = np.exp(preds_l[:6493])\nsubmission.to_csv('submission.csv', index=False)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}