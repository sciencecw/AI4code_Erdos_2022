{"cells": [{"cell_type": "markdown", "id": "7ecef05a", "metadata": {}, "source": ["## **Housing Prices Advanced Regression : EDA and Regression Models**\n\nIn this notebook I have performed thorough EDA on the housing dataset and tried to identify some keen underlying trends. Dtaa pre-processing is done after which different models have been applied. This kernel is an introduction to predictive modelling and demonstrates the various techniques. And it is must see if you are a beginner in regression and predictive modelling like me ;)\n\nLet's dive in !!!."]}, {"cell_type": "markdown", "id": "4df45d76", "metadata": {}, "source": ["## CONTENTS::"]}, {"cell_type": "markdown", "id": "52438bc5", "metadata": {}, "source": ["[ **1 ) Importing the Modules and Loading the Dataset**](#content1)"]}, {"cell_type": "markdown", "id": "edfecc23", "metadata": {}, "source": ["[ **2 ) Exploratory Data Analysis (EDA)**](#content2)"]}, {"cell_type": "markdown", "id": "4b63953a", "metadata": {}, "source": ["[ **3 ) Missing Values Treatment**](#content3)"]}, {"cell_type": "markdown", "id": "afbf7385", "metadata": {}, "source": ["[ **4 ) Handling Skewness of Features**](#content4)"]}, {"cell_type": "markdown", "id": "c004bbe4", "metadata": {}, "source": ["[ **5 ) Prepare the Data**](#content5)"]}, {"cell_type": "markdown", "id": "c9187f6f", "metadata": {}, "source": ["[ **6 ) Regression Models**](#content6)"]}, {"cell_type": "markdown", "id": "2b8bae02", "metadata": {}, "source": ["[ **7 ) Saving and Making Submission to Kaggle**](#content7)"]}, {"cell_type": "markdown", "id": "d435477e", "metadata": {}, "source": ["<a id=\"content1\"></a>\n## 1) Importing the Modules and Loading the Dataset"]}, {"cell_type": "code", "execution_count": 1, "id": "c1f42f9a", "metadata": {}, "outputs": [], "source": ["# Ignore  the warnings\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\n# data visualisation and manipulation\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nfrom matplotlib.legend_handler import HandlerBase\nimport seaborn as sns\nimport missingno as msno\n#configure\n# sets matplotlib to inline and displays graphs below the corressponding cell.\n%matplotlib inline  \nstyle.use('fivethirtyeight')\nsns.set(style='whitegrid',color_codes=True)\n\n#import the necessary modelling algos.\n\n#classifiaction.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC,SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n#regression\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso,RidgeCV,ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\n\n#model selection\nfrom sklearn.model_selection import train_test_split,cross_validate\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\n\n#evaluation metrics\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error # for regression\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score  # for classification\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew   # specifically for staistics"]}, {"cell_type": "code", "execution_count": 1, "id": "3d0807f7", "metadata": {}, "outputs": [], "source": ["train=pd.read_csv(r'../input/train.csv')\ntest=pd.read_csv(r'../input/test.csv')"]}, {"cell_type": "code", "execution_count": 1, "id": "e4318b63", "metadata": {}, "outputs": [], "source": ["train.head(10)\n#test.head(10)"]}, {"cell_type": "markdown", "id": "dc29bd09", "metadata": {}, "source": ["<a id=\"content2\"></a>\n## 2) Exploratory Data Analysis (EDA)"]}, {"cell_type": "markdown", "id": "21a7b324", "metadata": {}, "source": ["## 2.1 ) The Features and the 'Target' variable"]}, {"cell_type": "code", "execution_count": 1, "id": "a92b0176", "metadata": {}, "outputs": [], "source": ["df=train.copy()\n#df.head(10)\ndf.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "4b7f4d18", "metadata": {}, "outputs": [], "source": ["df.drop(['Id'],axis=1,inplace=True)\ntest.drop(['Id'],axis=1,inplace=True)"]}, {"cell_type": "markdown", "id": "07c33d89", "metadata": {}, "source": ["We can drop the 'Id' column as the frames are already indexed."]}, {"cell_type": "code", "execution_count": 1, "id": "f76a74be", "metadata": {}, "outputs": [], "source": ["df.index # the indices of the rows."]}, {"cell_type": "code", "execution_count": 1, "id": "4685258a", "metadata": {}, "outputs": [], "source": ["df.columns "]}, {"cell_type": "markdown", "id": "d4138cf7", "metadata": {}, "source": ["## 2.2 ) Check for Missing Values"]}, {"cell_type": "code", "execution_count": 1, "id": "f1df3722", "metadata": {}, "outputs": [], "source": ["df.isnull().any()"]}, {"cell_type": "code", "execution_count": 1, "id": "e08002af", "metadata": {}, "outputs": [], "source": ["msno.matrix(df) # just to visulaize. "]}, {"cell_type": "markdown", "id": "28b431cb", "metadata": {}, "source": ["* #### Many columns have missing values and that will be treated later in the notebook."]}, {"cell_type": "markdown", "id": "7deb0139", "metadata": {}, "source": ["## 2.3 ) Separate Dataframes (depending on data type)"]}, {"cell_type": "markdown", "id": "4c7960b3", "metadata": {}, "source": ["Might be useful when we consider features of different data types."]}, {"cell_type": "markdown", "id": "bdef0f29", "metadata": {}, "source": ["#### CATEGORICAL FEATURES"]}, {"cell_type": "code", "execution_count": 1, "id": "6a0d517d", "metadata": {}, "outputs": [], "source": ["cat_df=df.select_dtypes(include='object')"]}, {"cell_type": "code", "execution_count": 1, "id": "8e82f765", "metadata": {}, "outputs": [], "source": ["cat_df.head(10)\ncat_df.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "231bfb5d", "metadata": {}, "outputs": [], "source": ["cat_df.columns   # list of the categorical columns."]}, {"cell_type": "markdown", "id": "a24be4b5", "metadata": {}, "source": ["#### NUMERIC FEATURES"]}, {"cell_type": "code", "execution_count": 1, "id": "a717be53", "metadata": {}, "outputs": [], "source": ["num_df=df.select_dtypes(include='number')\nnum_df.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "fe7f921a", "metadata": {}, "outputs": [], "source": ["num_df.columns # list of numeric columns."]}, {"cell_type": "markdown", "id": "1853ba5b", "metadata": {}, "source": ["#### FEATURES WITH MISSING VALUES"]}, {"cell_type": "code", "execution_count": 1, "id": "8e4fca7a", "metadata": {}, "outputs": [], "source": ["nan_df=df.loc[:, df.isna().any()]\nnan_df.shape\nnan_df.columns   # list of columns with missing values."]}, {"cell_type": "markdown", "id": "b538e109", "metadata": {}, "source": ["#### MERGING THE TRAIN & TEST SETS"]}, {"cell_type": "code", "execution_count": 1, "id": "ed988dd8", "metadata": {}, "outputs": [], "source": ["all_data=pd.concat([train,test])"]}, {"cell_type": "code", "execution_count": 1, "id": "897b99e4", "metadata": {}, "outputs": [], "source": ["print(all_data.shape)\nall_data = all_data.reset_index(drop=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "3a567aaf", "metadata": {}, "outputs": [], "source": ["all_data.head(10)"]}, {"cell_type": "code", "execution_count": 1, "id": "4f2f2695", "metadata": {}, "outputs": [], "source": ["print(all_data.loc[1461:,'SalePrice'])  \n# note that it is Nan for the values in test set as expected. so we drop it here for now.\nall_data.drop(['SalePrice'],axis=1,inplace=True)\n"]}, {"cell_type": "markdown", "id": "3cbd766f", "metadata": {}, "source": ["## 2.4 ) Analyzing the Target i.e. 'SalePrice'"]}, {"cell_type": "code", "execution_count": 1, "id": "497c1739", "metadata": {}, "outputs": [], "source": ["# analyzing the target variable ie 'Saleprice'\nsns.distplot(a=df['SalePrice'],color='#ff4125',axlabel=False).set_title('Sale Price')"]}, {"cell_type": "markdown", "id": "d5ff8175", "metadata": {}, "source": ["#### **The distribution of target is a bit right skewed. Hence taking the 'log transform' is a reasonable option.**"]}, {"cell_type": "markdown", "id": "09f2d024", "metadata": {}, "source": ["#### ALSO LINEAR REGRESSION IS BASED ON THE ASSUMPTION OF THE 'HOMOSCADESITY' AND HENCE TAKING LOG WILL  BE A GOOD IDEA TO ENSURE 'HOMOSCADESITY' (that the varince of errors is constant.). A bit scary but simple ;) \n\n**You can read more about this on wikipedia.**"]}, {"cell_type": "code", "execution_count": 1, "id": "bc98d13b", "metadata": {}, "outputs": [], "source": ["#Get also the qq-plot (the quantile-quantile plot)\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()"]}, {"cell_type": "markdown", "id": "ab98c817", "metadata": {}, "source": ["####  TAKING 'Log Transform' OF THE TARGET"]}, {"cell_type": "code", "execution_count": 1, "id": "2d716c53", "metadata": {}, "outputs": [], "source": ["df['SalePrice']=np.log1p(df['SalePrice']) "]}, {"cell_type": "code", "execution_count": 1, "id": "b25da07d", "metadata": {}, "outputs": [], "source": ["# now again see the distribution.\nsns.distplot(a=df['SalePrice'],color='#ff4125',axlabel=False).set_title('log(1+SalePrice)')  # better.\n"]}, {"cell_type": "markdown", "id": "46e2ac76", "metadata": {}, "source": ["## 2.5 ) Most Related Features to the Target"]}, {"cell_type": "code", "execution_count": 1, "id": "eba81cb5", "metadata": {}, "outputs": [], "source": ["cor_mat= df[:].corr()\ncor_with_tar=cor_mat.sort_values(['SalePrice'],ascending=False)"]}, {"cell_type": "code", "execution_count": 1, "id": "121fbef0", "metadata": {}, "outputs": [], "source": ["print(\"The most relevant features (numeric) for the target are :\")\ncor_with_tar.SalePrice"]}, {"cell_type": "markdown", "id": "73ffa286", "metadata": {}, "source": ["#### INFERENCES--\n\n1. Note that some of the features have quite high corelation with the target. These features are really significant.\n\n2. Of these the features with corelation value >0.5 are really important. Some features like GrLivArea etc.. are even more important.\n\n3. We will consider these features (i.e. GrLivArea,OverallQual) etc.. in more detail in subsequent sections during univariate and bivariate analysis."]}, {"cell_type": "code", "execution_count": 1, "id": "44849ff7", "metadata": {}, "outputs": [], "source": ["# using a corelation map to visualize features with high corelation.\ncor_mat= df[['OverallQual','GrLivArea','GarageCars','GarageArea','TotalBsmtSF','1stFlrSF','FullBath',\n             'YearBuilt','YearRemodAdd','GarageYrBlt','TotRmsAbvGrd','SalePrice']].corr()\nmask = np.array(cor_mat)\nmask[np.tril_indices_from(mask)] = False\nfig=plt.gcf()\nfig.set_size_inches(30,12)\nsns.heatmap(data=cor_mat,mask=mask,square=True,annot=True,cbar=True)\n\n# some inference section."]}, {"cell_type": "markdown", "id": "129b8217", "metadata": {}, "source": ["## 2.6 ) Univariate Analysis"]}, {"cell_type": "markdown", "id": "83cea506", "metadata": {}, "source": ["In this section the univariate analysis is performed; More importantly I have considered the features that are more importanht with the 'Target' that  have high corelation with the Target.\n\nFor the numeric features I have used a 'distplot' and 'boxplot' to analyze their distribution.\n\nSimilarly for categorical features the most reasonable way to visualize the distribution is to use a 'countplot' which shows the relative counts for each category or class. Can use a pie-plot also to be a bit more fancy."]}, {"cell_type": "markdown", "id": "34f86df7", "metadata": {}, "source": ["#### NUMERIC FEATURES"]}, {"cell_type": "code", "execution_count": 1, "id": "2fa047b4", "metadata": {}, "outputs": [], "source": ["def plot_num(feature):\n    fig,axes=plt.subplots(1,2)\n    sns.boxplot(data=df,x=feature,ax=axes[0])\n    sns.distplot(a=df[feature],ax=axes[1],color='#ff4125')\n    fig.set_size_inches(15,5)"]}, {"cell_type": "code", "execution_count": 1, "id": "8114ea87", "metadata": {}, "outputs": [], "source": ["plot_num('GrLivArea')"]}, {"cell_type": "code", "execution_count": 1, "id": "83d4ca61", "metadata": {}, "outputs": [], "source": ["plot_num('GarageArea')"]}, {"cell_type": "code", "execution_count": 1, "id": "528a65d2", "metadata": {}, "outputs": [], "source": ["plot_num('TotalBsmtSF') "]}, {"cell_type": "markdown", "id": "aab41c72", "metadata": {}, "source": ["#### Note the features are a bit right skewed. We can therefore take 'log transform' of the features or a BoXCox transformation. Both shall work well. "]}, {"cell_type": "markdown", "id": "17556afc", "metadata": {}, "source": ["#### CATEGORICAL FEATURES"]}, {"cell_type": "code", "execution_count": 1, "id": "bbda2741", "metadata": {}, "outputs": [], "source": ["def plot_cat(feature):\n  sns.countplot(data=df,x=feature)\n  ax=sns.countplot(data=df,x=feature)\n   "]}, {"cell_type": "code", "execution_count": 1, "id": "e672c22c", "metadata": {}, "outputs": [], "source": ["plot_cat('OverallQual')"]}, {"cell_type": "markdown", "id": "3bf35235", "metadata": {}, "source": ["Most of them are in 'average','above average' or 'good' classes."]}, {"cell_type": "code", "execution_count": 1, "id": "54d5a6de", "metadata": {}, "outputs": [], "source": ["plot_cat('FullBath')"]}, {"cell_type": "code", "execution_count": 1, "id": "e7027f09", "metadata": {}, "outputs": [], "source": ["plot_cat('YearBuilt')"]}, {"cell_type": "code", "execution_count": 1, "id": "272c66b4", "metadata": {}, "outputs": [], "source": ["plot_cat('TotRmsAbvGrd') # most of the houses have 5-7 rooms above the grd floor."]}, {"cell_type": "markdown", "id": "686b8c04", "metadata": {}, "source": ["#### Lastly we plot the countplot for some important features that are numerical here but are actually categorica. It seems if they have been label encoded."]}, {"cell_type": "code", "execution_count": 1, "id": "1a57c555", "metadata": {}, "outputs": [], "source": ["plot_cat('GarageCars')"]}, {"cell_type": "code", "execution_count": 1, "id": "ad8aff3a", "metadata": {}, "outputs": [], "source": ["sns.factorplot(data=df,x='Neighborhood',kind='count',size=10,aspect=1.5)"]}, {"cell_type": "markdown", "id": "b4255ff7", "metadata": {}, "source": ["## 2.7 ) Bivariate Analysis"]}, {"cell_type": "markdown", "id": "ef0f26fd", "metadata": {}, "source": ["In this section the Bivariate Analysis have been done. I have plotted various numeric as well as categorical features against the target ie 'SalePrice'."]}, {"cell_type": "markdown", "id": "2b87f0b2", "metadata": {}, "source": ["#### NUMERIC FEATURES"]}, {"cell_type": "code", "execution_count": 1, "id": "6217907e", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots()\nax.scatter(x = df['GrLivArea'], y = df['SalePrice'])\nplt.ylabel('SalePrice')\nplt.xlabel('GrLivArea')\nplt.show()"]}, {"cell_type": "markdown", "id": "90e58d1a", "metadata": {}, "source": ["#### Note that there are two outliers on the lower right hand side and can remove them."]}, {"cell_type": "code", "execution_count": 1, "id": "9b595938", "metadata": {}, "outputs": [], "source": ["df = df.drop(df[(df['GrLivArea']>4000) & (df['SalePrice']<13)].index) # removing some outliers on lower right side."]}, {"cell_type": "code", "execution_count": 1, "id": "adc6d1d6", "metadata": {}, "outputs": [], "source": ["# again checking\nfig, ax = plt.subplots()\nax.scatter(x = df['GrLivArea'], y = df['SalePrice'])\nplt.ylabel('SalePrice')\nplt.xlabel('GrLivArea')\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "636580cc", "metadata": {}, "outputs": [], "source": ["# garage area\nfig, ax = plt.subplots()\nax.scatter(x =(df['GarageArea']), y = df['SalePrice'])\nplt.ylabel('SalePrice')\nplt.xlabel('GarageArea')\nplt.show()\n# can try to fremove the points with gargae rea > than 1200."]}, {"cell_type": "code", "execution_count": 1, "id": "b1ceec7e", "metadata": {}, "outputs": [], "source": ["# basment area\nfig, ax = plt.subplots()\nax.scatter(x =(df['TotalBsmtSF']), y = df['SalePrice'])\nplt.ylabel('SalePrice')\nplt.xlabel('TotalBsmtSF')\nplt.show()   # check >3000 can leave here."]}, {"cell_type": "markdown", "id": "99fba113", "metadata": {}, "source": ["#### CATEGORICAL FEATURES"]}, {"cell_type": "code", "execution_count": 1, "id": "f30ca102", "metadata": {}, "outputs": [], "source": ["#overall qual\nsns.factorplot(data=df,x='OverallQual',y='SalePrice',kind='box',size=5,aspect=1.5)"]}, {"cell_type": "markdown", "id": "22b08d9a", "metadata": {}, "source": ["The SalePrice increases with the overall quality as expected."]}, {"cell_type": "markdown", "id": "9f656cde", "metadata": {}, "source": ["**Similar inferences can be drawn from other plots and graphs.**"]}, {"cell_type": "code", "execution_count": 1, "id": "f6115de6", "metadata": {}, "outputs": [], "source": ["#garage cars\nsns.factorplot(data=df,x='GarageCars',y='SalePrice',kind='box',size=5,aspect=1.5)"]}, {"cell_type": "code", "execution_count": 1, "id": "1a52f161", "metadata": {}, "outputs": [], "source": ["#no of rooms\nsns.factorplot(data=df,x='TotRmsAbvGrd',y='SalePrice',kind='bar',size=5,aspect=1.5) # increasing rooms imply increasing SalePrice as expected."]}, {"cell_type": "code", "execution_count": 1, "id": "ab7f7535", "metadata": {}, "outputs": [], "source": ["#neighborhood\nsns.factorplot(data=df,x='Neighborhood',y='SalePrice',kind='box',size=10,aspect=1.5)"]}, {"cell_type": "markdown", "id": "305a4787", "metadata": {}, "source": ["Price varies with neighborhood.More posh areas of the city will have more price as expected."]}, {"cell_type": "code", "execution_count": 1, "id": "15b641c2", "metadata": {}, "outputs": [], "source": ["#sale conditioin\nsns.factorplot(data=df,x='SaleCondition',y='SalePrice',kind='box',size=10,aspect=1.5)"]}, {"cell_type": "markdown", "id": "bf775174", "metadata": {}, "source": ["<a id=\"content3\"></a>\n## 3 ) Missing Values Treatment"]}, {"cell_type": "markdown", "id": "003c276b", "metadata": {}, "source": ["In this section of the notebook I  have handled the missing values in the columns.\n\nFirstly I have droped a couple of columns that have a really high % of missing values.\n\nFor other features I have analyzed if it that feaure is important or not and accordingly either have drooped it or imputed the values in it.\n\nFor imputation I have considered the meaning of the corressponding feature from the description. Like for a categorical feature if values are missing I have imputed \"None\" just to mark a separate category meaning absence of that thing. Similarly for a numeric feature I have imputed with 0 in case the missing value implies the 'absence' of that feature.\n\nIn all other cases I have imputed the categorical features with 'mode' i.e the most frequent class and with 'mean' for the numeric features."]}, {"cell_type": "code", "execution_count": 1, "id": "9906b68e", "metadata": {}, "outputs": [], "source": ["nan_all_data = (all_data.isnull().sum())\nnan_all_data= nan_all_data.drop(nan_all_data[nan_all_data== 0].index).sort_values(ascending=False)\nnan_all_data\nmiss_df = pd.DataFrame({'Missing Ratio' :nan_all_data})\nmiss_df\n"]}, {"cell_type": "code", "execution_count": 1, "id": "d6a1c1c2", "metadata": {}, "outputs": [], "source": ["#delet some features withvery high number of missing values.  \nall_data.drop(['PoolQC','Alley','Fence','Id','MiscFeature'],axis=1,inplace=True)\n"]}, {"cell_type": "code", "execution_count": 1, "id": "44cc5550", "metadata": {}, "outputs": [], "source": ["test.drop(['PoolQC','Alley','Fence','MiscFeature'],axis=1,inplace=True)\ndf.drop(['PoolQC','Alley','Fence','MiscFeature'],axis=1,inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "a2b751dc", "metadata": {}, "outputs": [], "source": ["# FireplaceQu\n# it is useful but many of the values nearly half are missing makes no sense to fill half of the values. so deleting this\nall_data.drop(['FireplaceQu'],axis=1,inplace=True)\ntest.drop(['FireplaceQu'],axis=1,inplace=True)\ndf.drop(['FireplaceQu'],axis=1,inplace=True)\n"]}, {"cell_type": "code", "execution_count": 1, "id": "9cfb9f1a", "metadata": {}, "outputs": [], "source": ["#Lot Frontage\nprint(df['LotFrontage'].dtype)\nplt.scatter(x=np.log1p(df['LotFrontage']),y=df['SalePrice'])\ncr=df.corr()\nprint(df['LotFrontage'].describe())\nprint(\"The corelation of the LotFrontage with the Target : \" , cr.loc['LotFrontage','SalePrice'])\n"]}, {"cell_type": "markdown", "id": "65f7a6d7", "metadata": {}, "source": ["#### Above analysis shows that there is some relation of LotArea with the SalePrice both by scatter plot and also by the corelation value. Therefore instead of deleting I will impute the values with the mean for now."]}, {"cell_type": "code", "execution_count": 1, "id": "7d420c88", "metadata": {}, "outputs": [], "source": ["all_data['LotFrontage'].fillna(np.mean(all_data['LotFrontage']),inplace=True)\nall_data['LotFrontage'].isna().sum()"]}, {"cell_type": "code", "execution_count": 1, "id": "8e70c026", "metadata": {}, "outputs": [], "source": ["#Garage  related features.\n# these features eg like garage qual,cond,finish,type seems to be important and relevant for buying car. \n# hence I will not drop these features insted i will fill them with the 'none' for categorical and 0 for numeric as nan here implies that there is no garage.\n\nall_data['GarageYrBlt'].fillna(0,inplace=True)\nprint(all_data['GarageYrBlt'].isnull().sum())\n\nall_data['GarageArea'].fillna(0,inplace=True)\nprint(all_data['GarageArea'].isnull().sum())\n\nall_data['GarageCars'].fillna(0,inplace=True)\nprint(all_data['GarageCars'].isnull().sum())\n\nall_data['GarageQual'].fillna('None',inplace=True)   # creating a separate category 'none' which means no garage.\nprint(all_data['GarageQual'].isnull().sum())\n\nall_data['GarageFinish'].fillna('None',inplace=True)   # creating a separate category 'none' which means no garage.\nprint(all_data['GarageFinish'].isnull().sum())\n\nall_data['GarageCond'].fillna('None',inplace=True)   # creating a separate category 'none' which means no garage.\nprint(all_data['GarageCond'].isnull().sum())\n\nall_data['GarageType'].fillna('None',inplace=True)   # creating a separate category 'none' which means no garage.\nprint(all_data['GarageType'].isnull().sum())\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "e8cf109a", "metadata": {}, "outputs": [], "source": ["# basement related features.\n#missing values are likely zero for having no basement\n\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col].fillna(0,inplace=True)\n    \n# for categorical features we will create a separate class 'none' as before.\n\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col].fillna('None',inplace=True)\n    \nprint(all_data['TotalBsmtSF'].isnull().sum())\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "a99e632c", "metadata": {}, "outputs": [], "source": ["# MasVnrArea 0 and MasVnrType 'None'.\nall_data['MasVnrArea'].fillna(0,inplace=True)\nprint(all_data['MasVnrArea'].isnull().sum())\n\nall_data['MasVnrType'].fillna('None',inplace=True)\nprint(all_data['MasVnrType'].isnull().sum())"]}, {"cell_type": "code", "execution_count": 1, "id": "020b5e03", "metadata": {}, "outputs": [], "source": ["#MSZoning.\n# Here nan does not mean no so I will with the most common one ie the mode.\nall_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0],inplace=True)\nprint(all_data['MSZoning'].isnull().sum())"]}, {"cell_type": "code", "execution_count": 1, "id": "89b9a167", "metadata": {}, "outputs": [], "source": ["# utilities\nsns.factorplot(data=df,kind='box',x='Utilities',y='SalePrice',size=5,aspect=1.5)"]}, {"cell_type": "markdown", "id": "091387ca", "metadata": {}, "source": ["#### Note that training set has only 2 of the possible 4 categories (ALLPub and NoSeWa) while test set has other categories. Hence it is of no use to us."]}, {"cell_type": "code", "execution_count": 1, "id": "6720d89a", "metadata": {}, "outputs": [], "source": ["all_data.drop(['Utilities'],axis=1,inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "a5fb18ef", "metadata": {}, "outputs": [], "source": ["#functional\n# fill with mode\nall_data['Functional'].fillna(all_data['Functional'].mode()[0],inplace=True)\nprint(all_data['Functional'].isnull().sum())"]}, {"cell_type": "code", "execution_count": 1, "id": "7858c514", "metadata": {}, "outputs": [], "source": ["# other rem columns rae all cat like kitchen qual etc.. and so filled with mode.\nfor col in ['SaleType','KitchenQual','Exterior2nd','Exterior1st','Electrical']:\n  all_data[col].fillna(all_data[col].mode()[0],inplace=True)\n  print(all_data[col].isnull().sum())"]}, {"cell_type": "markdown", "id": "c3afcf1d", "metadata": {}, "source": ["#### Lastly checking if any null value still remains."]}, {"cell_type": "code", "execution_count": 1, "id": "b2bc22c2", "metadata": {}, "outputs": [], "source": ["nan_all_data = (all_data.isnull().sum())\nnan_all_data= nan_all_data.drop(nan_all_data[nan_all_data== 0].index).sort_values(ascending=False)\nnan_all_data\nmiss_df = pd.DataFrame({'Missing Ratio' :nan_all_data})\nmiss_df\n\n"]}, {"cell_type": "markdown", "id": "6725138c", "metadata": {}, "source": ["#### Finally no null value remain now;)"]}, {"cell_type": "code", "execution_count": 1, "id": "2abad634", "metadata": {}, "outputs": [], "source": ["all_data.shape"]}, {"cell_type": "markdown", "id": "ac1af7f5", "metadata": {}, "source": ["<a id=\"content4\"></a>\n## 4 ) Handling Skewness"]}, {"cell_type": "markdown", "id": "1f624b02", "metadata": {}, "source": ["For handling skewnesss I will take the log transform of the features with skewness > 0.5.\n\nYou can also try the BoxCox transformation as mentioned before."]}, {"cell_type": "code", "execution_count": 1, "id": "aa79a5ca", "metadata": {}, "outputs": [], "source": ["#log transform skewed numeric features:\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\nskewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.50]\nskewed_feats = skewed_feats.index\n\nall_data[skewed_feats] = np.log1p(all_data[skewed_feats])"]}, {"cell_type": "markdown", "id": "08c50463", "metadata": {}, "source": ["<a id=\"content5\"></a>\n## 5 ) Prepare the Data"]}, {"cell_type": "markdown", "id": "8bbb5e5c", "metadata": {}, "source": ["## 5.1 ) LabelEncode the Categorical Features"]}, {"cell_type": "code", "execution_count": 1, "id": "94c7c7c1", "metadata": {}, "outputs": [], "source": ["for col in all_data.columns:\n    if(all_data[col].dtype == 'object'):\n        le=LabelEncoder()\n        all_data[col]=le.fit_transform(all_data[col])"]}, {"cell_type": "markdown", "id": "b2e09889", "metadata": {}, "source": ["## 5.2 ) Splitting into Training and Validation Sets"]}, {"cell_type": "code", "execution_count": 1, "id": "e8e45f85", "metadata": {}, "outputs": [], "source": ["train=all_data.loc[:(df.shape)[0]+2,:]\ntest=all_data.loc[(df.shape)[0]+2:,:]"]}, {"cell_type": "code", "execution_count": 1, "id": "c6a43076", "metadata": {}, "outputs": [], "source": ["train['SalePrice']=df['SalePrice']\ntrain['SalePrice'].fillna(np.mean(train['SalePrice']),inplace=True)\ntrain.shape\nprint(train['SalePrice'].isnull().sum())"]}, {"cell_type": "code", "execution_count": 1, "id": "fa39aa15", "metadata": {}, "outputs": [], "source": ["print(train.shape)\nprint(test.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "47dc95cf", "metadata": {}, "outputs": [], "source": ["x_train,x_test,y_train,y_test=train_test_split(train.drop(['SalePrice'],axis=1),train['SalePrice'],test_size=0.20,random_state=42)"]}, {"cell_type": "markdown", "id": "1af16217", "metadata": {}, "source": ["<a id=\"content6\"></a>\n## 6 ) Regression Models"]}, {"cell_type": "markdown", "id": "58bae165", "metadata": {}, "source": ["Lastly it is the time to apply various regression models and check how are we doing. I have used various regression models from the scikit.\n\nParameter tuning using GridSearchCV is also done to improve performance of some algos."]}, {"cell_type": "markdown", "id": "92b0d006", "metadata": {}, "source": ["#### The evalauton metric that I have used is the Root Mean Squared Error between the 'Log of the actual price' and 'Log of the predicted value' which is also the evaluation metric used by the kaggle.\n\n#### To get abetter idea one may also use the K-fold cross validation insteadof the normal holdout set approach to cross validation."]}, {"cell_type": "markdown", "id": "e63777ea", "metadata": {}, "source": ["#### LINEAR REGRESSION"]}, {"cell_type": "code", "execution_count": 1, "id": "fe28f921", "metadata": {}, "outputs": [], "source": ["reg_lin=LinearRegression()\nreg_lin.fit(x_train,y_train)\npred=reg_lin.predict(x_test)\nprint(np.sqrt(mean_squared_error(y_test,pred)))"]}, {"cell_type": "markdown", "id": "53d57e9e", "metadata": {}, "source": ["#### LASSO (and tuning with GridSearchCV)"]}, {"cell_type": "code", "execution_count": 1, "id": "6e9c104c", "metadata": {}, "outputs": [], "source": ["reg_lasso=Lasso()\nreg_lasso.fit(x_train,y_train)\npred=reg_lasso.predict(x_test)\nprint(np.sqrt(mean_squared_error(y_test,pred)))"]}, {"cell_type": "code", "execution_count": 1, "id": "bb74c1a8", "metadata": {}, "outputs": [], "source": ["params_dict={'alpha':[0.001, 0.005, 0.01,0.05,0.1,0.5,1]}\nreg_lasso_CV=GridSearchCV(estimator=Lasso(),param_grid=params_dict,scoring='neg_mean_squared_error',cv=10)\nreg_lasso_CV.fit(x_train,y_train)\npred=reg_lasso_CV.predict(x_test)\nprint(np.sqrt(mean_squared_error(y_test,pred)))"]}, {"cell_type": "markdown", "id": "2551f8cf", "metadata": {}, "source": ["**Note the significant decrease in the RMSE on tuning the Lasso Regression.**"]}, {"cell_type": "code", "execution_count": 1, "id": "035bf79e", "metadata": {}, "outputs": [], "source": ["reg_lasso_CV.best_params_"]}, {"cell_type": "markdown", "id": "31f98a42", "metadata": {}, "source": ["#### RIDGE (and tuning with GridSearchCV)"]}, {"cell_type": "code", "execution_count": 1, "id": "ac2192de", "metadata": {}, "outputs": [], "source": ["reg_ridge=Ridge()\nreg_ridge.fit(x_train,y_train)\npred=reg_ridge.predict(x_test)\nprint(np.sqrt(mean_squared_error(y_test,pred)))"]}, {"cell_type": "code", "execution_count": 1, "id": "07a3a740", "metadata": {}, "outputs": [], "source": ["params_dict={'alpha':[0.1, 0.15, 0.20,0.25,0.30,0.35,0.4,0.45,0.50,0.55,0.60]}\nreg_ridge_CV=GridSearchCV(estimator=Ridge(),param_grid=params_dict,scoring='neg_mean_squared_error',cv=10)\nreg_ridge_CV.fit(x_train,y_train)\npred=reg_ridge_CV.predict(x_test)\nprint(np.sqrt(mean_squared_error(y_test,pred)))"]}, {"cell_type": "code", "execution_count": 1, "id": "085c82e5", "metadata": {}, "outputs": [], "source": ["reg_ridge_CV.best_params_"]}, {"cell_type": "markdown", "id": "78d8231c", "metadata": {}, "source": ["#### GRADIENT BOOSTING"]}, {"cell_type": "code", "execution_count": 1, "id": "27d6b18a", "metadata": {}, "outputs": [], "source": ["#the params are tuned with grid searchCV.\n\nreg_gb=GradientBoostingRegressor(n_estimators=2000,learning_rate=0.05,max_depth=3,min_samples_split=10,max_features='sqrt',subsample=0.75 ,loss='huber')\nreg_gb.fit(x_train,y_train)\npred=reg_gb.predict(x_test)\nprint(np.sqrt(mean_squared_error(y_test,pred)))"]}, {"cell_type": "markdown", "id": "68c7b2ac", "metadata": {}, "source": ["#### XGBoost"]}, {"cell_type": "code", "execution_count": 1, "id": "cfa1c61b", "metadata": {}, "outputs": [], "source": ["import xgboost as xgb\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nmodel_xgb.fit(x_train,y_train)\npred=model_xgb.predict(x_test)\nprint(np.sqrt(mean_squared_error(y_test,pred)))"]}, {"cell_type": "markdown", "id": "d2f6ded7", "metadata": {}, "source": ["Note that the parameters aren't optimized. This can get a lot better tahn this for sure."]}, {"cell_type": "markdown", "id": "623b56fc", "metadata": {}, "source": ["<a id=\"content7\"></a>\n## 7 ) Saving and Making Submission to Kaggle"]}, {"cell_type": "markdown", "id": "8344f143", "metadata": {}, "source": ["**The Gradient Boosting gives the best performance on the validation set and so I am using it to make predictions to Kaggle (on the test set).**"]}, {"cell_type": "code", "execution_count": 1, "id": "a752e6c2", "metadata": {}, "outputs": [], "source": ["# predictions on the test set.\n \npred=reg_gb.predict(test)\npred_act=np.exp(pred)\npred_act=pred_act-1\nlen(pred_act)"]}, {"cell_type": "code", "execution_count": 1, "id": "039f4cad", "metadata": {}, "outputs": [], "source": ["test_id=[]\nfor i in range (1461,2920):\n    test_id.append(i)\nd={'Id':test_id,'SalePrice':pred_act}\nans_df=pd.DataFrame(d)\nans_df.head(10)"]}, {"cell_type": "code", "execution_count": 1, "id": "1f5e5f6a", "metadata": {}, "outputs": [], "source": ["ans_df.to_csv('answer.csv',index=False)"]}, {"cell_type": "markdown", "id": "033cfe35", "metadata": {}, "source": ["## THE END!!!"]}, {"cell_type": "markdown", "id": "c9780535", "metadata": {}, "source": ["## [Please star/ upvote if u liked it.]"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}