{"cells": [{"cell_type": "markdown", "id": "76faadb0", "metadata": {}, "source": ["# **Import files**"]}, {"cell_type": "code", "execution_count": 1, "id": "95555d16", "metadata": {}, "outputs": [], "source": ["import numpy as np\nimport pandas as pd\nfrom pprint import pprint\nimport gensim\nimport gensim.downloader"]}, {"cell_type": "code", "execution_count": 1, "id": "2cc70464", "metadata": {}, "outputs": [], "source": ["import nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nstop_words = stopwords.words('english')"]}, {"cell_type": "code", "execution_count": 1, "id": "bdcc02e9", "metadata": {}, "outputs": [], "source": ["##Importing Libraries for Neural Nets\nimport tensorflow as tf\nimport keras\nfrom keras.models import Sequential, model_from_json\nfrom keras import layers\nfrom keras.layers import Input, Dense, Dropout, Activation, LSTM, GRU, GlobalAvgPool1D, GlobalMaxPool1D\nimport math\nfrom tqdm import tqdm\nimport pickle \nimport tensorflow as tf\nfrom keras.utils.np_utils import to_categorical\nfrom tqdm.keras import TqdmCallback"]}, {"cell_type": "code", "execution_count": 1, "id": "5158cf9b", "metadata": {}, "outputs": [], "source": ["!pip install keras-self-attention\nfrom keras_self_attention import SeqSelfAttention"]}, {"cell_type": "markdown", "id": "6c57f579", "metadata": {}, "source": ["# **Download embeddings**"]}, {"cell_type": "code", "execution_count": 1, "id": "258f100f", "metadata": {}, "outputs": [], "source": ["# download glove twitter embeddings\npprint(list(gensim.downloader.info()['models'].keys()))"]}, {"cell_type": "code", "execution_count": 1, "id": "8400ee26", "metadata": {}, "outputs": [], "source": ["# Takes about 5 minutes to execute, for 100-dim twitter vectors\n# Takes about 10+ minutes to execute, for 200-dim twitter vectors\n# glove_vectors_100 = gensim.downloader.load('glove-twitter-100')\nglove_vectors = gensim.downloader.load('glove-twitter-100')\nglove = glove_vectors\nembedding_length = 100"]}, {"cell_type": "markdown", "id": "c4e5b9ba", "metadata": {}, "source": ["# **Pre-process the text**"]}, {"cell_type": "code", "execution_count": 1, "id": "98ec4cab", "metadata": {}, "outputs": [], "source": ["path = '/kaggle/input/nlp-getting-started/'\ntrain = pd.read_csv(path + 'train.csv')\ntest = pd.read_csv(path + 'test.csv')\nprint(train.shape)\nprint(test.shape)\n\ntext_train = np.array(train['text'])\ntext_test = np.array(test['text'])\ntext = np.concatenate((text_train, text_test), axis = 0)\nprint(text.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "273838fb", "metadata": {}, "outputs": [], "source": ["# word tokenization \nfor i in range(len(text)):\n    text[i] = word_tokenize(text[i])\nprint('After tokenization:')\nprint(text[0])\n\n# filter out punctuation\nfor i in range(len(text)):\n    text[i] = [word for word in text[i] if word.isalpha()]\nprint('After filtering out punctuation:')\nprint(text[0])\n\n# make words lowercase \nfor i in range(len(text)):\n    text[i] = [word.lower() for word in text[i]]\nprint('After making lowercase:')\nprint(text[0])\n\n# remove stopwords\nfor i in range(len(text)):\n    text[i] = [word for word in text[i] if not word in stop_words]\nprint('After removing stopwords:')\nprint(text[0])\n\n# concatenate list of words\nfor i in range(len(text)):\n    text_concat = ''\n    for word in text[i]:\n        text_concat += word + ' '\n    text[i] = text_concat\nprint('After concatenating words:')\ntext = np.array(text)"]}, {"cell_type": "markdown", "id": "29a432b8", "metadata": {}, "source": ["# **Get embeddings for each word (averaging)**\n\n\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "3814da57", "metadata": {}, "outputs": [], "source": ["glove = glove_vectors"]}, {"cell_type": "code", "execution_count": 1, "id": "ba2b4c76", "metadata": {}, "outputs": [], "source": ["embeddings = []\nfor sentence in text:\n    embedding = np.zeros(100)\n    word_count = 0\n    for word in sentence:\n#         if word in glove.vocab:\n        if word in glove.key_to_index:\n            embedding += glove.get_vector(word)\n        word_count += 1\n    if word_count != 0:\n        embedding /= word_count\n    embeddings.append(embedding)\nembeddings = np.array(embeddings)"]}, {"cell_type": "code", "execution_count": 1, "id": "5ef69af7", "metadata": {}, "outputs": [], "source": ["embeddings.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "0e4f9a3a", "metadata": {}, "outputs": [], "source": ["X_train = embeddings[:7613]\nX_test = embeddings[7613:] \n\ny_train = train['target'].values"]}, {"cell_type": "code", "execution_count": 1, "id": "0a2e30ca", "metadata": {}, "outputs": [], "source": ["print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)"]}, {"cell_type": "markdown", "id": "93b51adb", "metadata": {}, "source": ["# **Create Model**\n#### (Average the embeddings for each word of the tweet and learn to classify using a feedforward NN model)"]}, {"cell_type": "code", "execution_count": 1, "id": "8c636334", "metadata": {}, "outputs": [], "source": ["act = 'tanh'\nbatch_len = 32\nopt = 'adam'\nepoch = 10\nval_split = 0.2"]}, {"cell_type": "code", "execution_count": 1, "id": "d1bae994", "metadata": {}, "outputs": [], "source": ["keras.backend.clear_session()\ninputs = keras.Input(shape = (X_train.shape[1]))\nx = layers.Dense(32, activation = act)(inputs)\nx = layers.Dense(32, activation = act)(x)\nx = layers.Dense(16, activation = act)(x)\noutputs = layers.Dense(1)(x)\nmodel = keras.Model(inputs = inputs, outputs = outputs, name = 'Deep-Averaging-Network')"]}, {"cell_type": "code", "execution_count": 1, "id": "d8d75872", "metadata": {}, "outputs": [], "source": ["model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])\nmodel.fit(X_train, y_train, epochs = epoch, batch_size = batch_len, verbose = 1, validation_split=val_split)"]}, {"cell_type": "markdown", "id": "b7bba22c", "metadata": {}, "source": ["# **Get embeddings for each word (concatenate)**"]}, {"cell_type": "code", "execution_count": 1, "id": "0ea3e192", "metadata": {}, "outputs": [], "source": ["embeddings = []\nfor sentence in text:\n    embedding = []\n    for word in sentence.split():\n        if word in glove.key_to_index:\n            embedding.extend(glove.get_vector(word))\n        else:\n            embedding.extend(np.zeros(100).tolist())\n    # pad extra zeros to make length of each embedding = 2200\n    if len(embedding) < 2200:\n        padding_len = 2200 - len(embedding)\n        embedding.extend(np.zeros(padding_len).tolist())\n    embedding = np.array(embedding)\n    embeddings.append(embedding)\nembeddings = np.array(embeddings)"]}, {"cell_type": "code", "execution_count": 1, "id": "0f023866", "metadata": {}, "outputs": [], "source": ["X_train = embeddings[:7613]\nX_test = embeddings[7613:] \ny_train = train['target'].values\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)"]}, {"cell_type": "markdown", "id": "03dd846e", "metadata": {}, "source": ["# **Create Model (Concatenated Embeddings Model)**"]}, {"cell_type": "code", "execution_count": 1, "id": "42f72eba", "metadata": {}, "outputs": [], "source": ["act = 'tanh'\nbatch_len = 32\nopt = 'adam'\nepoch = 10\nval_split = 0.2"]}, {"cell_type": "code", "execution_count": 1, "id": "c670db51", "metadata": {}, "outputs": [], "source": ["keras.backend.clear_session()\ninputs = keras.Input(shape = (X_train.shape[1]))\nx = layers.Dense(32, activation = act, input_dim = X_train.shape[1])(inputs)\nx = layers.Dropout(0.3)(x)\nx = layers.Dense(32, activation = act)(x)\nx = layers.Dropout(0.3)(x)\nx = layers.Dense(16, activation = act)(x)\nx = layers.Dropout(0.2)(x)\noutputs = layers.Dense(1)(x)\nmodel = keras.Model(inputs = inputs, outputs = outputs, name = 'Glove-FFN-Concatenated')"]}, {"cell_type": "code", "execution_count": 1, "id": "9e686b6a", "metadata": {}, "outputs": [], "source": ["model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])\nmodel.fit(X_train, y_train, epochs = epoch, batch_size = batch_len, verbose = 1, validation_split=val_split)"]}, {"cell_type": "markdown", "id": "38301364", "metadata": {}, "source": ["# **Create Embeddings (for LSTM)**"]}, {"cell_type": "markdown", "id": "3aebd99e", "metadata": {}, "source": ["Understanding input sizes for using LSTM: https://stackoverflow.com/questions/50418973/how-lstm-work-with-word-embeddings-for-text-classification-example-in-keras"]}, {"cell_type": "code", "execution_count": 1, "id": "83837435", "metadata": {}, "outputs": [], "source": ["embeddings = []\nfor sentence in text:\n    embedding = []\n    for word in sentence.split():\n        if word in glove.key_to_index:\n            embedding.append(glove.get_vector(word).tolist())\n        else:\n            embedding.append(np.zeros(embedding_length).tolist())\n    # pad extra zeros to make length of each sentence = 22 \n    while len(embedding) < 22:\n        embedding.append(np.zeros(embedding_length).tolist())\n    embeddings.append(embedding)\nembeddings = np.array(embeddings)\nprint(embeddings.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "b0114575", "metadata": {}, "outputs": [], "source": ["X_train = embeddings[:7613]\nX_test = embeddings[7613:]\ny_train = train['target'].values\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)"]}, {"cell_type": "markdown", "id": "55f84f34", "metadata": {}, "source": ["# **Create and train LSTM model**\n#### (Using only the text feature)"]}, {"cell_type": "code", "execution_count": 1, "id": "ec1373fc", "metadata": {}, "outputs": [], "source": ["act = 'tanh'\nbatch_len = 32\nopt = 'adam'\nepoch = 10\nval_split = 0.2"]}, {"cell_type": "code", "execution_count": 1, "id": "49f9362a", "metadata": {}, "outputs": [], "source": ["keras.backend.clear_session()\ninputs = keras.Input(shape = (22, embedding_length))\nx = layers.LSTM(64)(inputs)\nx = layers.Dense(32, activation = act)(x)\nx = layers.Dense(16, activation = act)(x)\noutputs = layers.Dense(1)(x)\nmodel = keras.Model(inputs = inputs, outputs = outputs, name = 'Glove-LSTM')"]}, {"cell_type": "code", "execution_count": 1, "id": "1af0be42", "metadata": {}, "outputs": [], "source": ["model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])\nmodel.fit(X_train, y_train, epochs = epoch, batch_size = batch_len, verbose = 1, validation_split=val_split)"]}, {"cell_type": "markdown", "id": "2a4e27e7", "metadata": {}, "source": ["# **Create and train GRU model**"]}, {"cell_type": "code", "execution_count": 1, "id": "d16e83c0", "metadata": {}, "outputs": [], "source": ["act = 'tanh'\nbatch_len = 16\nopt = 'adam'\nepoch = 10\nval_split = 0.2"]}, {"cell_type": "code", "execution_count": 1, "id": "664da39a", "metadata": {}, "outputs": [], "source": ["keras.backend.clear_session()\ninputs = keras.Input(shape = (22, embedding_length))\nx = layers.GRU(64)(inputs)\nx = layers.Dense(32, activation = act)(x)\nx = layers.Dense(16, activation = act)(x)\noutputs = layers.Dense(1)(x)\nmodel = keras.Model(inputs = inputs, outputs = outputs, name = 'Glove-GRU')"]}, {"cell_type": "code", "execution_count": 1, "id": "b12d8948", "metadata": {}, "outputs": [], "source": ["model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])\nmodel.fit(X_train, y_train, epochs = epoch, batch_size = batch_len, verbose = 1, validation_split=val_split)"]}, {"cell_type": "markdown", "id": "c4e1b42a", "metadata": {}, "source": ["# **LSTM model with self-attention**"]}, {"cell_type": "code", "execution_count": 1, "id": "eeb9bbaa", "metadata": {}, "outputs": [], "source": ["act = 'tanh'\nbatch_len = 16\nopt = 'adam'\nepoch = 10\nval_split = 0.2"]}, {"cell_type": "code", "execution_count": 1, "id": "d86626f9", "metadata": {}, "outputs": [], "source": ["keras.backend.clear_session()\ninputs = keras.Input(shape = (22, embedding_length))\nx = layers.GRU(64, return_sequences = True)(inputs)\nx = SeqSelfAttention(attention_activation = 'tanh')(x)\nx = layers.GlobalMaxPool1D()(x)\nx = layers.Dense(32, activation = act)(x)\nx = layers.Dense(16, activation = act)(x)\noutputs = layers.Dense(1)(x)\nmodel = keras.Model(inputs = inputs, outputs = outputs, name = 'GRU-self-attention')"]}, {"cell_type": "code", "execution_count": 1, "id": "c7fabd7c", "metadata": {}, "outputs": [], "source": ["model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])\nmodel.fit(X_train, y_train, epochs = epoch, batch_size = batch_len, verbose = 1, validation_split=val_split)"]}, {"cell_type": "markdown", "id": "19b20722", "metadata": {}, "source": ["# **Preparing \"keyword\" feature**"]}, {"cell_type": "code", "execution_count": 1, "id": "c409b2ec", "metadata": {}, "outputs": [], "source": ["train['keyword'].unique()"]}, {"cell_type": "code", "execution_count": 1, "id": "7a70eee7", "metadata": {}, "outputs": [], "source": ["## TRAIN DATA \"KEYWORDS\" PROCESSING\n# replace \"%20\" with \" \"\nkeywords = train['keyword'].fillna('none').replace('%20', ' ', regex = True).tolist()\n\n# replace keywords with embeddings\nkeyword_embeddings = []\nfor keyword in keywords:\n    embedding = np.zeros(embedding_length)\n    word_count = 0\n    for word in keyword.split():\n        if word in glove.key_to_index: \n            embedding += glove.get_vector(word)\n            word_count += 1\n    if word_count != 0:\n        embedding /= word_count\n    keyword_embeddings.append(embedding)\n\nkeyword_embeddings_train = np.array(keyword_embeddings)\n\n## TEST DATA \"KEYWORDS\" PROCESSING\n# replace \"%20\" with \" \"\nkeywords = test['keyword'].fillna('none').replace('%20', ' ', regex = True).tolist()\n\n# replace keywords with embeddings\nkeyword_embeddings = []\nfor keyword in keywords:\n    embedding = np.zeros(embedding_length)\n    word_count = 0\n    for word in keyword.split():\n        if word in glove.key_to_index: \n            embedding += glove.get_vector(word)\n            word_count += 1\n    if word_count != 0:\n        embedding /= word_count\n    keyword_embeddings.append(embedding)\n\nkeyword_embeddings_test = np.array(keyword_embeddings)"]}, {"cell_type": "code", "execution_count": 1, "id": "45c097c3", "metadata": {}, "outputs": [], "source": ["print(keyword_embeddings_train.shape)\nprint(keyword_embeddings_test.shape)"]}, {"cell_type": "markdown", "id": "1afc7e24", "metadata": {}, "source": ["# **Preparing \"location\" feature**"]}, {"cell_type": "code", "execution_count": 1, "id": "97a832b0", "metadata": {}, "outputs": [], "source": ["print(train['location'].nunique())\ntrain['location'].unique()"]}, {"cell_type": "code", "execution_count": 1, "id": "ad586268", "metadata": {}, "outputs": [], "source": ["## TRAIN DATA \"KEYWORDS\" PROCESSING\nlocations = train['location'].fillna('none').replace('[^a-zA-Z ]', ' ', regex = True).tolist()\n\n# replace locations with embeddings\nlocation_embeddings = []\nfor location in locations:\n    embedding = np.zeros(embedding_length)\n    word_count = 0\n    for word in location.split():\n        if word in glove.key_to_index: \n            embedding += glove.get_vector(word)\n            word_count += 1\n    if word_count != 0:\n        embedding /= word_count\n    location_embeddings.append(embedding)\n\nlocation_embeddings_train = np.array(location_embeddings)\n\n## TEST DATA \"KEYWORDS\" PROCESSING\nlocations = test['location'].fillna('none').replace('[^a-zA-Z ]', ' ', regex = True).tolist()\n\n# replace locations with embeddings\nlocation_embeddings = []\nfor location in locations:\n    embedding = np.zeros(embedding_length)\n    word_count = 0\n    for word in location.split():\n        if word in glove.key_to_index: \n            embedding += glove.get_vector(word)\n            word_count += 1\n    if word_count != 0:\n        embedding /= word_count\n    location_embeddings.append(embedding)\n\nlocation_embeddings_test = np.array(location_embeddings)"]}, {"cell_type": "code", "execution_count": 1, "id": "e99cab52", "metadata": {}, "outputs": [], "source": ["print(location_embeddings_train.shape)\nprint(location_embeddings_test.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "371f149a", "metadata": {}, "outputs": [], "source": ["# concatenate \"keyword\" and location features\nnon_sequential_train = np.concatenate((keyword_embeddings_train, location_embeddings_train), axis = 1)\nnon_sequential_test = np.concatenate((keyword_embeddings_test, location_embeddings_test), axis = 1)\nprint(non_sequential_train.shape)\nprint(non_sequential_test.shape)"]}, {"cell_type": "markdown", "id": "6c891a02", "metadata": {}, "source": ["# **Non-linear LSTM model (with both seq and non-seq inputs)**"]}, {"cell_type": "code", "execution_count": 1, "id": "2aeeff5b", "metadata": {}, "outputs": [], "source": ["act = 'tanh'\nbatch_len = 16\nopt = 'adam'\nepoch = 20\nval_split = 0.2"]}, {"cell_type": "code", "execution_count": 1, "id": "fa3ad12f", "metadata": {}, "outputs": [], "source": ["keras.backend.clear_session()\nseq_input = keras.Input(shape = (22, embedding_length))\nnon_seq_input = keras.Input(shape = (2 * embedding_length))\nx = layers.GRU(64, return_sequences = True)(seq_input)\nx = SeqSelfAttention(attention_activation = 'tanh')(x)\nx = layers.GlobalMaxPool1D()(x)\nx = layers.concatenate([x, non_seq_input])\nx = layers.Dense(64, activation = act)(x)\nx = layers.Dense(16, activation = act)(x)\noutput = layers.Dense(1)(x)\nmodel = keras.Model(inputs = [seq_input, non_seq_input], outputs = output, name = 'complete_model')"]}, {"cell_type": "code", "execution_count": 1, "id": "c45c1065", "metadata": {}, "outputs": [], "source": ["model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])"]}, {"cell_type": "code", "execution_count": 1, "id": "e4f3017b", "metadata": {}, "outputs": [], "source": ["model.fit([X_train, non_sequential_train], y_train, epochs = epoch, batch_size = batch_len, verbose = 1, validation_split=val_split)"]}, {"cell_type": "code", "execution_count": 1, "id": "e2b58635", "metadata": {}, "outputs": [], "source": ["tf.keras.utils.plot_model(model, show_shapes = True)"]}, {"cell_type": "markdown", "id": "3e0e39bb", "metadata": {}, "source": ["# **Get results for test set and generate CSV**"]}, {"cell_type": "code", "execution_count": 1, "id": "1d4debf7", "metadata": {}, "outputs": [], "source": ["# y_test = model.predict(X_test)\ny_test = model.predict([X_test, non_sequential_test])\ny_pred = []\nfor i in range(len(y_test)):\n    if y_test[i][0] > 0.5:\n        y_pred.append(1)\n    else:\n        y_pred.append(0)\ntest['target'] = y_pred\nfinal = test[['id', 'target']]\nfinal.to_csv('pred.csv', index = False)\nfinal"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}