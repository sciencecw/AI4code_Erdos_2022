{"cells": [{"cell_type": "code", "execution_count": 1, "id": "3aa7d8ad", "metadata": {}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\n\n# data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output."]}, {"cell_type": "markdown", "id": "b88b4eac", "metadata": {}, "source": ["Here, we will be working with the famous [Titanic Dataset from Kaggle](https://www.kaggle.com/c/titanic). We\u2019ll be trying to predict a classification: survival or deceased. We\u2019ll use a \u201csemi-cleaned\u201d version of the titanic dataset. Also, we have made two separate files for training and testing. The training data can be found by clicking [here](https://github.com/meetnandu05/LogisticRegression/blob/master/titanic_train.csv) and the test data can be found by clicking [here](https://github.com/meetnandu05/LogisticRegression/blob/master/titanic_test.csv)."]}, {"cell_type": "markdown", "id": "7debb00e", "metadata": {}, "source": ["## Get the Data\n\nFirst, let us now import the data and let\u2019s look at how this data looks."]}, {"cell_type": "code", "execution_count": 1, "id": "fb3f5301", "metadata": {}, "outputs": [], "source": ["train = pd.read_csv('/kaggle/input/titanic_train.csv')\ntest = pd.read_csv('/kaggle/input/titanic_test.csv')\n\ntrain.head()"]}, {"cell_type": "markdown", "id": "d8828d3c", "metadata": {}, "source": ["## Exploratory Data Analysis\n\nLet\u2019s begin some exploratory data analysis. \n\nWe\u2019ll start by checking out missing data. We can use seaborn to create a simple heatmap to see where we are missing data."]}, {"cell_type": "code", "execution_count": 1, "id": "07738da4", "metadata": {}, "outputs": [], "source": ["train.isnull().sum()"]}, {"cell_type": "code", "execution_count": 1, "id": "36ef0940", "metadata": {}, "outputs": [], "source": ["ax = sns.heatmap(train.isnull())"]}, {"cell_type": "code", "execution_count": 1, "id": "d2f86bc3", "metadata": {}, "outputs": [], "source": ["ax = sns.heatmap(train.isnull(), yticklabels=False, cbar=False, cmap='viridis')"]}, {"cell_type": "markdown", "id": "67d2c31d", "metadata": {}, "source": ["Roughly 20 percent of the Age data is missing. The proportion of Age missing is likely small enough for reasonable replacement with some form of imputation. \n\nLooking at the Cabin column, it looks like we are just missing too much of that data to do something useful with at a basic level. We\u2019ll probably drop this or change it to another feature like \u201cCabin Known: 1 or 0\u201d."]}, {"cell_type": "markdown", "id": "d6119470", "metadata": {}, "source": ["Let\u2019s continue on by visualizing some more of the data. \n\nLet\u2019s show the count of people who survived."]}, {"cell_type": "code", "execution_count": 1, "id": "9d1963de", "metadata": {}, "outputs": [], "source": ["sns.countplot(data=train, x='Survived')"]}, {"cell_type": "markdown", "id": "be3b4cfb", "metadata": {}, "source": ["Let\u2019s show the count of males and females survived."]}, {"cell_type": "code", "execution_count": 1, "id": "2aa1c8f0", "metadata": {}, "outputs": [], "source": ["sns.countplot(data=train, x= 'Survived', hue='Sex')"]}, {"cell_type": "markdown", "id": "04887ac6", "metadata": {}, "source": ["## Data Cleaning\n\nWe want to fill in missing age data instead of just dropping the missing age data rows. \n\nOne way to do this is by filling in the mean age of all the passengers (imputation). However we can be smarter about this and check the average age by passenger class. \n\nLet's show the average age of people belonging to different class in the ship."]}, {"cell_type": "code", "execution_count": 1, "id": "af89b3fb", "metadata": {}, "outputs": [], "source": ["sns.boxplot(data=train, x='Pclass', y='Age')"]}, {"cell_type": "markdown", "id": "41cb8f5b", "metadata": {}, "source": ["We can see the wealthier passengers in the higher classes tend to be older, which makes sense. We\u2019ll use these average age values to impute based on Pclass for Age."]}, {"cell_type": "code", "execution_count": 1, "id": "68f46693", "metadata": {}, "outputs": [], "source": ["train_age_nill = train[train.Age.isnull()].PassengerId.values"]}, {"cell_type": "code", "execution_count": 1, "id": "307e8444", "metadata": {}, "outputs": [], "source": ["train_age_nill"]}, {"cell_type": "code", "execution_count": 1, "id": "5d8358a3", "metadata": {}, "outputs": [], "source": ["train.Age.fillna(train.groupby('Pclass').Age.transform(\"median\"), inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "4599d7c5", "metadata": {}, "outputs": [], "source": ["train[train.PassengerId.isin(train_age_nill)].head()"]}, {"cell_type": "code", "execution_count": 1, "id": "3be6b0ba", "metadata": {}, "outputs": [], "source": ["test_age_nill = test[test.Age.isnull()].PassengerId.values\ntest_age_nill"]}, {"cell_type": "code", "execution_count": 1, "id": "cb5b0c4c", "metadata": {}, "outputs": [], "source": ["pclass_mean = train.groupby('Pclass').Age.median().to_dict()\npclass_mean\n"]}, {"cell_type": "code", "execution_count": 1, "id": "72659b99", "metadata": {}, "outputs": [], "source": ["\ndef age_map(x):\n    Age = x[0]\n    Pclass = x[1]\n    if pd.isnull(Age):\n        return pclass_mean[Pclass]\n    else:\n        return Age\n#Alternativas\n#test['Age'] = test[['Age', 'Pclass']].apply(lambda x: age_map(x), axis = 1)\n\n#test['Age'] = test[['Age','Pclass']].apply(age_map,axis=1)\n\n#test.loc[test.Age.isnull(),'Age'] = test.Pclass.map(pclass_mean)\n\ntest.Age.fillna(test.Pclass.map(pclass_mean), axis =0, inplace=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "36d9faf7", "metadata": {}, "outputs": [], "source": ["test[test.PassengerId.isin(test_age_nill)].head(10)"]}, {"cell_type": "markdown", "id": "cd8ac5e1", "metadata": {}, "source": ["Now let\u2019s check that heat map again."]}, {"cell_type": "code", "execution_count": 1, "id": "6bb9f5ad", "metadata": {}, "outputs": [], "source": ["ax = sns.heatmap(train.isnull(), yticklabels=False, cbar=False, cmap='viridis')"]}, {"cell_type": "markdown", "id": "786af5ab", "metadata": {}, "source": ["We can now see there are no null values in the Age column."]}, {"cell_type": "markdown", "id": "4a5ce5d0", "metadata": {}, "source": ["Let\u2019s go ahead and drop the Cabin column and the row in Embarked that is NaN."]}, {"cell_type": "code", "execution_count": 1, "id": "db1568a7", "metadata": {}, "outputs": [], "source": ["train.drop('Cabin', axis=1, inplace=True)\ntest.drop('Cabin', axis=1, inplace=True)"]}, {"cell_type": "markdown", "id": "0a1e37c2", "metadata": {}, "source": ["## Converting Categorical Features\n\nWe\u2019ll need to convert categorical features to dummy variables. Otherwise the machine learning algorithm won\u2019t be able to directly take in those features as inputs."]}, {"cell_type": "code", "execution_count": 1, "id": "c6625489", "metadata": {}, "outputs": [], "source": ["train.info()"]}, {"cell_type": "markdown", "id": "0ec27f9c", "metadata": {}, "source": ["As we can see, there are 4 categorical columns namely Name, Sex, Ticket and Embarked. Out of these 4, the Name and Ticket column have no relationship with whether the person survived or not. \n\nSo we drop these 2 columns and we convert the other two columns into numerical values. Then the data will be ready for the model."]}, {"cell_type": "code", "execution_count": 1, "id": "dd00618e", "metadata": {}, "outputs": [], "source": ["train.drop('Name', axis=1, inplace= True)\ntrain.drop('Ticket', axis=1, inplace = True)\ntest.drop('Name', axis=1, inplace= True)\ntest.drop('Ticket', axis=1, inplace = True)\n\ntrain_objs_num = len(train)\ndataset = pd.concat(objs=[train, test], axis=0, sort = False)\n\ndataset_preprocessed = pd.get_dummies(dataset, drop_first = True)\n\ntrain = dataset_preprocessed[:train_objs_num]\ntest = dataset_preprocessed[train_objs_num:]\ntrain.head()\n"]}, {"cell_type": "code", "execution_count": 1, "id": "d74a11b6", "metadata": {}, "outputs": [], "source": ["train.info()"]}, {"cell_type": "code", "execution_count": 1, "id": "6ade32e9", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(10,10))  \nsns.heatmap(train.corr(), annot=True, cmap='viridis', ax=ax)"]}, {"cell_type": "code", "execution_count": 1, "id": "b489300d", "metadata": {}, "outputs": [], "source": ["from sklearn.decomposition import PCA\npca =  PCA(n_components=1)\n\ndf_1 = train[['Fare','Pclass']]\ncol_1 = pca.fit_transform(df_1)\n\n#calcula la mediana de Fare para cada Pclass\npclass_fare = train.groupby('Pclass').Fare.median().to_dict()\npclass_fare\n\n#mapea los Fare null, con la mediana de Fare seg\u00fan la Pclass\ntest.Fare.fillna(test.Pclass.map(pclass_fare), axis =0, inplace=True)\n\ntest = test.drop(['Survived'], axis=1)\n\ndf_2 = test[['Fare', 'Pclass']]\ncol_2 = pca.transform(df_2)\n\ntrain.insert(2,'Fare_Pclass', col_1[:,0], True)\ntest.insert(2,'Fare_Pclass', col_2[:,0], True)\n\ntrain=train.drop(['Fare','Pclass'], axis=1)\ntest=test.drop(['Fare','Pclass'], axis=1)\n"]}, {"cell_type": "markdown", "id": "a37c0587", "metadata": {}, "source": ["## Logistic Regression\n\nLogistic regression is a statistical method for predicting binary classes. The outcome or target variable is dichotomous in nature. Dichotomous means there are only two possible classes. \n\nThe real life example of classification example would be, to categorize the mail as spam or not spam, to categorize the tumor as malignant or benign and to categorize the transaction as fraudulent or genuine. All these problem\u2019s answers are in categorical form i.e. Yes or No. and that is why they are two class classification problems."]}, {"cell_type": "markdown", "id": "7d20922d", "metadata": {}, "source": ["## Building a Logistic Regression model\n\nLet\u2019s start by splitting the data into a training  and test. \n\nNOTE: There is another test file that we can play around with in case we want to use all this data for training."]}, {"cell_type": "code", "execution_count": 1, "id": "e4b504d8", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test =  train_test_split(train.drop('Survived', axis = 1), \n                                                     train.Survived, \n                                                     test_size = 0.20, random_state = 0)"]}, {"cell_type": "markdown", "id": "0975c439", "metadata": {}, "source": ["Now, let\u2019s move on to train the model  and predict using it."]}, {"cell_type": "code", "execution_count": 1, "id": "a472dc0d", "metadata": {}, "outputs": [], "source": ["from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\nstandard_scaler = StandardScaler()\n\nX_train_std = standard_scaler.fit_transform(X_train)\nX_test_std = standard_scaler.transform(X_test)\n\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "89fb5322", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test =  train_test_split(train.drop('Survived', axis = 1), \n                                                     train.Survived, \n                                                     test_size = 0.20, random_state = 0)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\nstandard_scaler = StandardScaler()\n\nX_train_std = standard_scaler.fit_transform(X_train)\nX_test_std = standard_scaler.transform(X_test)\n\n\nlr = LogisticRegression(solver = 'lbfgs', penalty='l2', C=0.1)\nfrom sklearn.metrics import classification_report\nfrom sklearn import metrics\n\nlr.fit(X_train_std, y_train)\n\ny_predict = lr.predict(X_test_std)\n\n\nprint(classification_report(y_test,y_predict))\nprint(\"Accuracy:\", metrics.accuracy_score(y_test, y_predict))"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}