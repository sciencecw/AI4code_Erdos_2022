{"cells": [{"cell_type": "markdown", "id": "45b7af99", "metadata": {}, "source": ["**Iniciaci\u00f3n del cuaderno interactivo**\n\nSe realiza la importaci\u00f3n de algunas librerias que vamos a utilizar, y se muestran los ficheros de entrada al cuaderno. En nuestro caso nuestro fichero en formato CSV."]}, {"cell_type": "code", "execution_count": 1, "id": "437bf5cb", "metadata": {}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pandas_profiling as pp\nfrom datetime import datetime\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,scale \nfrom sklearn.cluster import MeanShift, estimate_bandwidth\nimport pickle\nfrom keras.models import Sequential, Model, load_model\nfrom keras.layers import LSTM,Input,Dense,Flatten,SpatialDropout1D,Dropout,CuDNNLSTM,Reshape,Concatenate,BatchNormalization\nfrom keras.layers import CuDNNLSTM, RepeatVector, TimeDistributed\nfrom keras.callbacks import ModelCheckpoint\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output."]}, {"cell_type": "markdown", "id": "4a809380", "metadata": {}, "source": ["**Lectura de fichero CSV**\n\nSe cambia el formato con la que mostramos el tiempo (Estampa de tiempo en ms a Fecha)"]}, {"cell_type": "code", "execution_count": 1, "id": "cbce6985", "metadata": {}, "outputs": [], "source": ["data = pd.read_csv(\"../input/iotusd/IOTUSD.csv\",parse_dates=[1], index_col=1, encoding='UTF-8', date_parser=lambda x: datetime.fromtimestamp(int(x) / 1e3)).drop(['Unnamed: 0'], axis = 1)[500:]\n"]}, {"cell_type": "markdown", "id": "4e79e18b", "metadata": {}, "source": ["**Exploraci\u00f3n de los datos**\nMostramos las caracteristicas de los datos en bruto."]}, {"cell_type": "code", "execution_count": 1, "id": "915a2bff", "metadata": {}, "outputs": [], "source": ["data.describe()"]}, {"cell_type": "code", "execution_count": 1, "id": "ebaf7973", "metadata": {}, "outputs": [], "source": ["data.describe()\ndata[['OPEN','CLOSE','HIGH','LOW']].plot()\ndata[['OPEN','CLOSE','HIGH','LOW']].hist()\ndata[['VOLUME']].plot()\ndata[['VOLUME']].hist()"]}, {"cell_type": "code", "execution_count": 1, "id": "089f49db", "metadata": {}, "outputs": [], "source": ["prob = stats.probplot(data.OPEN,plot=plt)\nplt.show()\nprob = stats.probplot(data.HIGH,plot=plt)\nplt.show()\nprob = stats.probplot(data.VOLUME,plot=plt)\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "1d224867", "metadata": {}, "outputs": [], "source": ["data.kurtosis()"]}, {"cell_type": "code", "execution_count": 1, "id": "d93c5ad1", "metadata": {}, "outputs": [], "source": ["from scipy.cluster import hierarchy\nfrom scipy.spatial import distance\nimport seaborn as sns\n\ncorr_matrix = data.corr()\ncorrelations_array = np.asarray(corr_matrix)\n\nlinkage = hierarchy.linkage(distance.pdist(correlations_array), \\\n                            method='average')\n\ng = sns.clustermap(corr_matrix,row_linkage=linkage,col_linkage=linkage,\\\n                   row_cluster=True,col_cluster=True,figsize=(10,10),cmap='Greens')\nplt.setp(g.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\nplt.show()\n\nlabel_order = corr_matrix.iloc[:,g.dendrogram_row.reordered_ind].columns"]}, {"cell_type": "markdown", "id": "231e9dd7", "metadata": {}, "source": ["****Preprocesamos los datos****\n"]}, {"cell_type": "code", "execution_count": 1, "id": "5e4d5b35", "metadata": {}, "outputs": [], "source": ["pdata = pd.DataFrame()"]}, {"cell_type": "code", "execution_count": 1, "id": "052dd63f", "metadata": {}, "outputs": [], "source": ["split_rate = 0.8\ncolumns = data.columns"]}, {"cell_type": "code", "execution_count": 1, "id": "45e29163", "metadata": {}, "outputs": [], "source": ["pdata['OPEN'] = data['OPEN'].pct_change().clip(0.2,-0.2)\npdata['HIGH'] = data['HIGH'].pct_change().clip(0.2,-0.2)\npdata['LOW'] = data['LOW'].pct_change().clip(0.2,-0.2)\npdata['CLOSE'] = data['CLOSE'].pct_change().clip(0.2,-0.2)\npdata['VOLUME'] = data['VOLUME'].apply(np.log1p).clip(14,0)\npdata.dropna(inplace=True)\n\nX_train = pdata[:int(pdata.shape[0]*split_rate)]\nX_test = pdata[int(pdata.shape[0]*split_rate):]\n\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "07519be1", "metadata": {}, "outputs": [], "source": ["scaler = MinMaxScaler().fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\nX_train = pd.DataFrame(X_train,columns = columns)\nX_test = pd.DataFrame(X_test,columns = columns)\n"]}, {"cell_type": "markdown", "id": "b401f40e", "metadata": {}, "source": ["Generator"]}, {"cell_type": "code", "execution_count": 1, "id": "b6b748ed", "metadata": {}, "outputs": [], "source": ["import numpy as np\nimport keras\n\nclass DataGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, X,input_columns, output_columns, batch_size, sequence_lenght,steps_ahead,normalise = False):\n        'Initialization'\n        self.batch_size = batch_size\n        self.X = X\n        self.input_columns = input_columns\n        self.output_columns = output_columns\n        self.sequence_lenght = sequence_lenght\n        self.steps_ahead = steps_ahead\n        self.on_epoch_end()\n        self.normalise = normalise\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor((len(self.X)-self.sequence_lenght)/self.batch_size))\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.X))\n        \n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate samples of the batch\n        data_windows = np.zeros([self.batch_size,self.sequence_lenght,len(self.input_columns)])\n        for i in range(self.batch_size):\n            data_windows[i] = self.X[index*self.batch_size+i:index*self.batch_size+i+self.sequence_lenght]\n        # Generate data\n        #X,X = self.__data_generation(data_windows)\n\n        return data_windows,data_windows\n\n\n    def __data_generation(self, data_windows,):\n        'Generates data containing batch_size samples' # X : (n_samples, lenght, input dim) Y :(n_samples,output_dim)\n        # Initialization\n\n        X = np.empty((self.batch_size, self.sequence_lenght, len(self.input_columns)))\n        # Generate data\n        for i,window in enumerate(data_windows):\n            X[i,:,:] = window.values\n\n                    \n\n        return X,X"]}, {"cell_type": "code", "execution_count": 1, "id": "e44752e7", "metadata": {}, "outputs": [], "source": ["timesteps = 1024\nbatch_size = 128\nn_features = 5\ntrain_generator = DataGenerator(X_train,columns,[],batch_size,timesteps,1,False)\ntest_generator = DataGenerator(X_test,columns,[],batch_size,timesteps,1,False)"]}, {"cell_type": "code", "execution_count": 1, "id": "2741ec32", "metadata": {}, "outputs": [], "source": ["from keras.callbacks import Callback\nfrom keras import backend as K\nimport numpy as np\n\n\nclass CyclicLR(Callback):\n    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n    The method cycles the learning rate between two boundaries with\n    some constant frequency.\n    # Arguments\n        base_lr: initial learning rate which is the\n            lower boundary in the cycle.\n        max_lr: upper boundary in the cycle. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore\n            max_lr may not actually be reached depending on\n            scaling function.\n        step_size: number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch.\n        mode: one of {triangular, triangular2, exp_range}.\n            Default 'triangular'.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n        gamma: constant in 'exp_range' scaling function:\n            gamma**(cycle iterations)\n        scale_fn: Custom scaling policy defined by a single\n            argument lambda function, where\n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored\n        scale_mode: {'cycle', 'iterations'}.\n            Defines whether scale_fn is evaluated on\n            cycle number or cycle iterations (training\n            iterations since start of cycle). Default is 'cycle'.\n    The amplitude of the cycle can be scaled on a per-iteration or\n    per-cycle basis.\n    This class has three built-in policies, as put forth in the paper.\n    \"triangular\":\n        A basic triangular cycle w/ no amplitude scaling.\n    \"triangular2\":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    \"exp_range\":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each\n        cycle iteration.\n    For more detail, please see paper.\n    # Example for CIFAR-10 w/ batch size 100:\n        ```python\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., mode='triangular')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n    Class also supports custom scaling functions:\n        ```python\n            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., scale_fn=clr_fn,\n                                scale_mode='cycle')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n    # References\n      - [Cyclical Learning Rates for Training Neural Networks](\n      https://arxiv.org/abs/1506.01186)\n    \"\"\"\n\n    def __init__(\n            self,\n            base_lr=0.001,\n            max_lr=0.006,\n            step_size=2000.,\n            mode='triangular',\n            gamma=1.,\n            scale_fn=None,\n            scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        if mode not in ['triangular', 'triangular2',\n                        'exp_range']:\n            raise KeyError(\"mode must be one of 'triangular', \"\n                           \"'triangular2', or 'exp_range'\")\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn is None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1 / (2.**(x - 1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma ** x\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        \"\"\"Resets cycle iterations.\n        Optional boundary/step size adjustment.\n        \"\"\"\n        if new_base_lr is not None:\n            self.base_lr = new_base_lr\n        if new_max_lr is not None:\n            self.max_lr = new_max_lr\n        if new_step_size is not None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n\n    def clr(self):\n        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr - self.base_lr) * \\\n                np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr - self.base_lr) * \\\n                np.maximum(0, (1 - x)) * self.scale_fn(self.clr_iterations)\n\n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())\n\n    def on_batch_end(self, epoch, logs=None):\n\n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n        K.set_value(self.model.optimizer.lr, self.clr())\n\n        self.history.setdefault(\n            'lr', []).append(\n            K.get_value(\n                self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        logs['lr'] = K.get_value(self.model.optimizer.lr)"]}, {"cell_type": "code", "execution_count": 1, "id": "0bd17d24", "metadata": {}, "outputs": [], "source": ["model = Sequential()\nmodel.add(CuDNNLSTM(100,input_shape=(timesteps,n_features),return_sequences=True))\nmodel.add(CuDNNLSTM(50,input_shape=(timesteps,n_features),return_sequences=False))\nmodel.add(RepeatVector(timesteps))\nmodel.add(CuDNNLSTM(50,return_sequences=True))\nmodel.add(CuDNNLSTM(100,return_sequences=True))\nmodel.add(TimeDistributed(Dense(n_features)))\nmodel.compile(optimizer='adam', loss='mse')\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "def9bc1a", "metadata": {}, "outputs": [], "source": ["filepath=\"1024-weights.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\ncyclic = CyclicLR(mode='triangular')\nmodel.fit_generator(train_generator,epochs = 5, validation_data = test_generator,callbacks = [checkpoint,cyclic])"]}, {"cell_type": "code", "execution_count": 1, "id": "701788dd", "metadata": {}, "outputs": [], "source": ["prediction = model.predict(train_generator[0][0])"]}, {"cell_type": "code", "execution_count": 1, "id": "024960cb", "metadata": {}, "outputs": [], "source": ["plt.plot(train_generator[0][0][6])"]}, {"cell_type": "code", "execution_count": 1, "id": "e2513db4", "metadata": {}, "outputs": [], "source": ["plt.plot(prediction[60])"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}