{"cells": [{"cell_type": "code", "execution_count": 1, "id": "53b5ada9", "metadata": {}, "outputs": [], "source": ["import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPool2D, Dense, Dropout, Flatten\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.preprocessing.image import ImageDataGenerator\n\nimport warnings\nwarnings.filterwarnings('ignore')"]}, {"cell_type": "code", "execution_count": 1, "id": "c9911069", "metadata": {}, "outputs": [], "source": ["train_data = pd.read_csv('../input/digit-recognizer/train.csv')\ntest_data = pd.read_csv('../input/digit-recognizer/test.csv')\ntrain_data.shape, test_data.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "985d6b2d", "metadata": {}, "outputs": [], "source": ["train_labels = train_data['label']\ndel train_data['label']\ntrain_labels.shape, train_data.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "0253000f", "metadata": {}, "outputs": [], "source": ["# sns.countplot(train_labels)\nlabel, count = np.unique(train_labels, return_counts = True)\nprint(train_labels.value_counts())\nprint('-'*50)\nprint('There are', train_labels.isnull().sum() ,'null values in labels')\nprint('-'*50)\nsns.barplot(label, count)"]}, {"cell_type": "code", "execution_count": 1, "id": "0a26a085", "metadata": {}, "outputs": [], "source": ["# converting the data to a grayscale and 28*28 image\ntrain_data = train_data / 255.0\ntest_data = test_data / 255.0\nx_train = train_data.values.reshape(-1, 28, 28, 1)\nx_test = test_data.values.reshape(-1, 28, 28,1)"]}, {"cell_type": "code", "execution_count": 1, "id": "9089ee5a", "metadata": {}, "outputs": [], "source": ["# one hot encoding labels\ny_train = to_categorical(train_labels, 10)"]}, {"cell_type": "code", "execution_count": 1, "id": "d7840068", "metadata": {}, "outputs": [], "source": ["# split data to training and validation sets (validation = 10% of the data)\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.1, random_state = 1)\nx_train.shape, y_train.shape, x_val.shape, y_val.shape"]}, {"cell_type": "code", "execution_count": 1, "id": "a2dc4542", "metadata": {}, "outputs": [], "source": ["fig = plt.figure(figsize = (8, 8))\nrow = 3\ncol = 4\nfor i in range(row*col):\n    fig.add_subplot(row, col, i+1)\n    plt.imshow(x_train[i][:, :, 0])\nplt.show()"]}, {"cell_type": "markdown", "id": "99457145", "metadata": {}, "source": ["### Defining CNN Model\n\n* Define a sequential model\n* \n* -----------------------------------------SET 1-------------------------------------------------\n* Layer1 => conv2D -> filters = 32, filter_size = 5*5, padding = Same, activation function = relu \n* Layer2 => conv2D -> filters = 32, filter_size = 5*5, padding = Same, activation function = relu \n* Layer3 => Maxpool Layer -> 2*2\n* Layer4 => Dropout -> prob = 0.25\n\n* -----------------------------------------SET 2-------------------------------------------------\n* Layer5 => conv2D -> filters = 64, filter_size = 3*3, padding = Same, activation function = relu \n* Layer6 => conv2D -> filters = 64, filter_size = 3*3, padding = Same, activation function = relu \n* Layer7 => Maxpool Layer -> 2 \\* 2, stride = 2 \\* 2\n* Layer8 => Dropout -> prob = 0.25\n* ----------------------------------------------------------------------------------------------- \n* Layer9 => Faltten\n* Layer10 => Dense -> filters = 256, activation = relu (hidden layer)\n* Layer11 => Dropout -> prob = 0.50\n* Layer12 => Dense -> filters = 10, activation = softmax (output layer)"]}, {"cell_type": "code", "execution_count": 1, "id": "b1f23ad7", "metadata": {}, "outputs": [], "source": ["model = Sequential()\n# set 1\nmodel.add(Conv2D(filters = 32, kernel_size = (5, 5), padding = 'SAME', activation = 'relu', input_shape = (28, 28, 1)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5, 5), padding = 'SAME', activation = 'relu'))\nmodel.add(MaxPool2D(2, 2))\nmodel.add(Dropout(0.25))\n\n# set 2\nmodel.add(Conv2D(filters = 64, kernel_size = (3, 3), padding = 'SAME', activation = 'relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = (3, 3), padding = 'SAME', activation = 'relu'))\nmodel.add(MaxPool2D(pool_size = (2, 2), strides = (2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = 'relu')) # hidden later\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = 'softmax')) # output layer"]}, {"cell_type": "markdown", "id": "e409dbec", "metadata": {}, "source": ["* We use optimizer to optimize the cost and to find the minima in most efficient way\n* ADAM vs RMSProp -> both can be compared"]}, {"cell_type": "code", "execution_count": 1, "id": "a66071fe", "metadata": {}, "outputs": [], "source": ["# optimizer = Adam(learning_rate=0.001)\noptimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)"]}, {"cell_type": "code", "execution_count": 1, "id": "e8c2e3e2", "metadata": {}, "outputs": [], "source": ["# compile the model\nmodel.compile(optimizer=optimizer, loss = 'categorical_crossentropy', metrics=['accuracy'])"]}, {"cell_type": "code", "execution_count": 1, "id": "452502e3", "metadata": {}, "outputs": [], "source": ["# setting up an annealer\nLR_reducer = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=3, verbose=1, min_lr=0.00001)"]}, {"cell_type": "code", "execution_count": 1, "id": "eb768b10", "metadata": {}, "outputs": [], "source": ["# define epochs and batch size\nepochs = 30\nbatch_size = 86"]}, {"cell_type": "markdown", "id": "ddfac433", "metadata": {}, "source": ["### Data Augmentation\n* Approaches that alter the training data in ways that change the array representation while keeping the label same are known as data augmentation techniques - grayscale, flip horizontal/vertical, zoom, random crops, rotations, translations, etc.\n* We can increase the ammount of training data by x2 or x3 by using data augmentation techniques"]}, {"cell_type": "code", "execution_count": 1, "id": "ab739a12", "metadata": {}, "outputs": [], "source": ["image_data = ImageDataGenerator(\n                featurewise_center = False,\n                samplewise_center = False,\n                featurewise_std_normalization = False,\n                samplewise_std_normalization = False,\n                zca_whitening = False,\n                rotation_range = 10,\n                width_shift_range = 0.1,\n                height_shift_range = 0.1,\n                zoom_range = 0.1,\n                horizontal_flip = False,\n                vertical_flip= False\n            )"]}, {"cell_type": "code", "execution_count": 1, "id": "b121757e", "metadata": {}, "outputs": [], "source": ["# fit the training data on ImageData Generator\nimage_data.fit(x_train)"]}, {"cell_type": "code", "execution_count": 1, "id": "2387da8a", "metadata": {}, "outputs": [], "source": ["# fit the model\ntrain_generator = image_data.flow(x_train, y_train, batch_size = batch_size)\ntrained_model = model.fit_generator(train_generator, epochs=epochs, validation_data=(x_val, y_val), verbose=1, steps_per_epoch=len(x_train)//batch_size,\n                   callbacks=[LR_reducer])"]}, {"cell_type": "code", "execution_count": 1, "id": "76c2b6dc", "metadata": {}, "outputs": [], "source": ["# printing training and validation loss and accuracy\nfig, ax = plt.subplots(2, 1)\nax[0].plot(trained_model.history['loss'], color = 'b', label = 'Training Loss')\nax[0].plot(trained_model.history['val_loss'], color = 'r', label = 'Validation Loss')\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(trained_model.history['accuracy'], color = 'b', label = 'Training Accuracy')\nax[1].plot(trained_model.history['val_accuracy'], color = 'r', label = 'Validation Accuracy')\nlegend = ax[1].legend(loc='best', shadow=True)"]}, {"cell_type": "code", "execution_count": 1, "id": "3bf59a37", "metadata": {}, "outputs": [], "source": ["# printing confusion matrix\nval_pred = model.predict(x_val)\nval_pred_classes = np.argmax(val_pred, axis = 1)\nval_true_classes = np.argmax(y_val, axis = 1)\nresults = confusion_matrix(val_true_classes, val_pred_classes)\nprint(results)"]}, {"cell_type": "code", "execution_count": 1, "id": "d7afefed", "metadata": {}, "outputs": [], "source": ["# printing some errorneous data\nerror_data = (val_pred_classes - val_true_classes != 0)\nerror_pred = val_pred[error_data]\nerror_pred_classes = val_pred_classes[error_data]\nerror_true = y_val[error_data]\nerror_true_classes = val_true_classes[error_data]\nerror_x_val = x_val[error_data]\nprint('Number of wrong predcitions in validation data = ', error_data.sum())\nfig = plt.figure(figsize = (8, 8))\nrow = 3\ncol = 4\nfor i in range(row*col):\n    fig.add_subplot(row, col, i+1)\n    plt.imshow(error_x_val[i][:, :, 0])\n    plt.title('Predicted Label: ' + str(error_pred_classes[i]) + '\\nTrue Label: ' + str(error_true_classes[i]))\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "1d762982", "metadata": {}, "outputs": [], "source": ["results = model.predict(x_test)\nresults_classes = np.argmax(results, axis = 1)"]}, {"cell_type": "code", "execution_count": 1, "id": "dbc159fa", "metadata": {}, "outputs": [], "source": ["result_data = {'ImageID': [i for i in range(1, len(results_classes)+1)],\n              'Label': results_classes\n              }\nresults_df = pd.DataFrame(result_data)\nresults_df.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "978b7c6f", "metadata": {}, "outputs": [], "source": ["results_df.to_csv('MNIST_data_output_using_CNN_30_epochs.csv', index = False)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}