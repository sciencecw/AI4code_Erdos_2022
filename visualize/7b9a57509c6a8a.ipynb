{"cells": [{"cell_type": "code", "execution_count": 1, "id": "bb008d81", "metadata": {}, "outputs": [], "source": ["# Import the required packages\nimport os\nimport numpy as np \nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport pathlib\nimport librosa.display\nfrom tqdm import tqdm\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\nimport librosa"]}, {"cell_type": "code", "execution_count": 1, "id": "b65416d2", "metadata": {}, "outputs": [], "source": ["# Get the data directories\ndata_dir = \"../input/speaker-recognition-dataset/16000_pcm_speeches/\"\nos.listdir(data_dir)"]}, {"cell_type": "markdown", "id": "261bef08", "metadata": {}, "source": ["# Process training dataset"]}, {"cell_type": "code", "execution_count": 1, "id": "1b66d0a7", "metadata": {}, "outputs": [], "source": ["# get wav paths\ndef get_wav_paths(speaker):\n    speaker_path = data_dir + speaker\n    all_paths = [item for item in os.listdir(speaker_path)]\n    return all_paths"]}, {"cell_type": "code", "execution_count": 1, "id": "11ea4302", "metadata": {}, "outputs": [], "source": ["nelson_mandela_paths = get_wav_paths(\"Nelson_Mandela\")\nmargaret_thatcher_paths = get_wav_paths(\"Magaret_Tarcher\")\nbenjamin_netanyau_paths = get_wav_paths(\"Benjamin_Netanyau\")\njens_stoltenberg_paths = get_wav_paths( 'Jens_Stoltenberg')\njulia_gillard_paths = get_wav_paths(\"Julia_Gillard\")\n\nnoise1_paths = get_wav_paths(\"_background_noise_\")\nnoise2_paths = get_wav_paths(\"other\")"]}, {"cell_type": "code", "execution_count": 1, "id": "ba38feab", "metadata": {}, "outputs": [], "source": ["# load the data\ndef load_wav(wav_path, speaker):\n    with tf.compat.v1.Session(graph=tf.compat.v1.Graph()) as sess:\n        wav_path = data_dir + speaker + \"/\" + wav_path\n        wav_filename_placeholder = tf.compat.v1.placeholder(tf.compat.v1.string, [])\n        wav_loader = tf.io.read_file(wav_filename_placeholder)\n        wav_decoder = tf.audio.decode_wav(wav_loader, desired_channels=1)\n        wav_data = sess.run(\n            wav_decoder, feed_dict={\n                wav_filename_placeholder: wav_path\n            }).audio.flatten().reshape((1, 16000))\n        sess.close()\n    return wav_data"]}, {"cell_type": "code", "execution_count": 1, "id": "6d97f5cd", "metadata": {}, "outputs": [], "source": ["# create training data\ndef generate_training_data(speaker_paths, speaker, label):\n    wavs, labels = [], []\n    for i in tqdm(speaker_paths):\n        wav = load_wav(i, speaker)\n        wavs.append(wav)\n        labels.append(label)\n    return wavs, labels\n"]}, {"cell_type": "code", "execution_count": 1, "id": "4262fe12", "metadata": {}, "outputs": [], "source": ["nelson_mandela_wavs, nelson_mandela_labels = generate_training_data(nelson_mandela_paths, \"Nelson_Mandela\", 0) \nmargaret_thatcher_wavs, margaret_thatcher_labels = generate_training_data(margaret_thatcher_paths, \"Magaret_Tarcher\", 1) \nbenjamin_netanyau_wavs, benjamin_netanyau_labels = generate_training_data(benjamin_netanyau_paths, \"Benjamin_Netanyau\", 2) \njens_stoltenberg_wavs, jens_stoltenberg_labels = generate_training_data(jens_stoltenberg_paths, \"Jens_Stoltenberg\", 3) \njulia_gillard_wavs, julia_gillard_labels = generate_training_data(julia_gillard_paths, \"Julia_Gillard\", 4) "]}, {"cell_type": "code", "execution_count": 1, "id": "7573c049", "metadata": {}, "outputs": [], "source": ["# remove the extra wav for Julia Gillard\njulia_gillard_labels = julia_gillard_labels[1:]\njulia_gillard_wavs = julia_gillard_wavs[1:]"]}, {"cell_type": "code", "execution_count": 1, "id": "e342fa1f", "metadata": {}, "outputs": [], "source": ["all_wavs = nelson_mandela_wavs + margaret_thatcher_wavs + benjamin_netanyau_wavs + jens_stoltenberg_wavs + julia_gillard_wavs\nall_labels = nelson_mandela_labels + margaret_thatcher_labels + benjamin_netanyau_labels + jens_stoltenberg_labels + julia_gillard_labels"]}, {"cell_type": "code", "execution_count": 1, "id": "e102756a", "metadata": {}, "outputs": [], "source": ["from scipy.io.wavfile import read\nfrom scipy.io.wavfile import write\nfrom random import randint\n\ndef cut_random_section(noise2, size2):\n    size21 = noise2.size\n    starting_point2 = randint(0,(noise2.size - size2))\n    end_point2 = starting_point2 + size2\n    noise_cut_part2 = noise2[starting_point2:end_point2]\n    return noise_cut_part2\n\ndef mix(audio1, noise1, snr1):\n    audio_max = max(audio1)\n    if audio_max==0:\n        audio_max = int(np.random.uniform(0.7,1)*32767)\n    audio1 = audio1*1.\n    audio1 = audio1/audio_max\n    noise1 = cut_random_section(noise1, audio1.size)\n    noise1 = noise1*1.\n    noise1 = noise1/max(noise1)\n    gain = pow(10,(snr1/10.))\n    numerator = np.mean(abs(audio1)**2)\n    denominator = numerator/gain\n    noise_power = np.mean(abs(noise1)**2)\n    mult_value = (denominator/noise_power)**0.5\n    noisy1 = audio1 + noise1*mult_value\n    if max(audio1)==0:\n        noisy1 = noise1\n    else:    \n        noisy1 = noisy1/max(noisy1)\n    noisy1 = np.array(noisy1*audio_max, dtype='int16')\n    return noise1*mult_value, mult_value, noisy1\n\nnoise_wavs = []\nnoise_labels = []\nsnr_dB = 10\nfor i in range(len(all_wavs)):\n    for noise in os.listdir(data_dir + 'other'):\n        fs, noise_file = read(data_dir + 'other/' + noise)\n        x = all_wavs[i][0]\n        noise_temp, mult_value, noisy = mix(x, noise_file, snr_dB)\n        if noisy.any() != 0:\n            noise_wavs.append(noisy)\n            noise_labels.append(all_labels[i])\n    for noise in os.listdir(data_dir + '_background_noise_'):\n        fs, noise_file = read(data_dir + '_background_noise_/' + noise)\n        x = all_wavs[i][0]\n        if len(noise_file.shape) > 1:\n            noise_file = np.reshape(noise_file, (noise_file.shape[0]*noise_file.shape[1]))\n        noise_temp, mult_value, noisy = mix(x, noise_file, snr_dB)\n        if noisy.any() != 0:\n            noise_wavs.append(noisy)\n            noise_labels.append(all_labels[i]) \n    if i%200 == 0:\n        print(i)"]}, {"cell_type": "code", "execution_count": 1, "id": "503c372c", "metadata": {}, "outputs": [], "source": ["for i in range(len(all_wavs)):\n    noise_labels.append(all_labels[i])\n    noise_wavs.append(all_wavs[i][0])\nfinal_wavs = np.array(noise_wavs)\nfinal_labels = np.array(noise_labels)\n\nprint(final_wavs.shape, final_labels.shape)"]}, {"cell_type": "code", "execution_count": 1, "id": "915f5f7b", "metadata": {}, "outputs": [], "source": ["# split the dataset into trainin and testing set\\\ntrain_wavs, test_wavs, train_labels, test_labels = train_test_split(final_wavs, final_labels, test_size=0.1)"]}, {"cell_type": "code", "execution_count": 1, "id": "22f4431d", "metadata": {}, "outputs": [], "source": ["train_x, train_y = np.array(train_wavs), np.array(train_labels)\ntest_x, test_y = np.array(test_wavs), np.array(test_labels)"]}, {"cell_type": "code", "execution_count": 1, "id": "90460090", "metadata": {}, "outputs": [], "source": ["train_y = tf.keras.utils.to_categorical(train_y)\ntest_y = tf.keras.utils.to_categorical(test_y)"]}, {"cell_type": "code", "execution_count": 1, "id": "47dc7279", "metadata": {}, "outputs": [], "source": ["# import csv\n# with open('train_x.csv', 'wt') as f:\n#     csv_writer = csv.writer(f, quoting=csv.QUOTE_NONE)\n#     csv_writer.writerows(train_x)\n# with open('test_x.csv', 'wt') as f:\n#     csv_writer = csv.writer(f, quoting=csv.QUOTE_NONE)\n#     csv_writer.writerows(test_x)\n# with open('train_y.csv', 'wt') as f:\n#     csv_writer = csv.writer(f, quoting=csv.QUOTE_NONE)\n#     csv_writer.writerows(train_y)\n# with open('test_y.csv', 'wt') as f:\n#     csv_writer = csv.writer(f, quoting=csv.QUOTE_NONE)\n#     csv_writer.writerows(test_y)"]}, {"cell_type": "code", "execution_count": 1, "id": "b28c4005", "metadata": {}, "outputs": [], "source": ["# MFCC Feature Extraction\n\ntrain_x_new = []\ntest_x_new = []\nINPUT_SHAPE = (126,40)\n\ntrain_x_new = np.zeros((train_x.shape[0], INPUT_SHAPE[0], INPUT_SHAPE[1]), dtype=np.float64)\n\ncount = 0\nfor sample in train_x:\n    mfcc = librosa.feature.mfcc(y=sample, sr=16000, hop_length=128, n_fft=256, n_mfcc=20)\n    mfcc_delta = librosa.feature.delta(mfcc)[:10, :]\n    mfcc_double_delta = librosa.feature.delta(mfcc, order=2)[:10, :]\n    train_x_new[count, :, :20] = mfcc.T\n    train_x_new[count, :, 20:30] = mfcc_delta.T\n    train_x_new[count, :, 30:] = mfcc_double_delta.T\n    count += 1\n    if count%500 == 0:\n        print('Train', count)\n        \ntest_x_new = np.zeros((test_x.shape[0], INPUT_SHAPE[0], INPUT_SHAPE[1]), dtype=np.float64)\n\ncount = 0\nfor sample in test_x:\n    mfcc = librosa.feature.mfcc(y=sample, sr=16000, hop_length=128, n_fft=256, n_mfcc=20)\n    mfcc_delta = librosa.feature.delta(mfcc)[:10, :]\n    mfcc_double_delta = librosa.feature.delta(mfcc, order=2)[:10, :]\n    test_x_new[count, :, :20] = mfcc.T\n    test_x_new[count, :, 20:30] = mfcc_delta.T\n    test_x_new[count, :, 30:] = mfcc_double_delta.T\n    count += 1\n    if count%500 == 0:\n        print('Test', count)"]}, {"cell_type": "code", "execution_count": 1, "id": "d247a0d1", "metadata": {}, "outputs": [], "source": ["train_x_new = np.expand_dims(train_x_new, axis=3)\ntest_x_new = np.expand_dims(test_x_new, axis=3)\nprint(train_x_new.shape, test_x_new.shape)"]}, {"cell_type": "markdown", "id": "757cd843", "metadata": {}, "source": ["# Create a simple model"]}, {"cell_type": "code", "execution_count": 1, "id": "d6266410", "metadata": {}, "outputs": [], "source": ["# create a model\ndef create_model(speech_feature):\n    model = tf.keras.Sequential()\n    if speech_feature == \"spectrogram\":\n        model.add(Spectrogram(n_dft=512, n_hop=256, input_shape=(1, 16000),\n                            return_decibel_spectrogram=True, power_spectrogram=2.0,\n                            trainable_kernel=False, name='static_stft'))\n    elif speech_feature == \"melspectrogram\":\n        model.add(Melspectrogram(sr=16000, n_mels=128,n_dft=512, n_hop=256,\n                            input_shape=(1 , 16000),return_decibel_melgram=True,\n                            trainable_kernel=False, name='melgram'))\n        \n    elif speech_feature == \"mfcc\":\n        model.add(tf.keras.layers.Conv2D(128, (3, 3), activation=\"relu\", input_shape=(126,40,1)))\n        model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n        model.add(tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"))\n#         model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n\n        model.add(tf.keras.layers.Flatten())        \n        model.add(tf.keras.layers.Dense(5, activation=\"softmax\"))\n        model.compile(optimizer=tf.keras.optimizers.Adam(lr=3e-4)\n                , loss = \"categorical_crossentropy\"\n                , metrics = [\"accuracy\"])\n        return model\n\n    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"))\n    model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(5, activation=\"softmax\"))\n    model.compile(optimizer=tf.keras.optimizers.Adam(lr=3e-4)\n            , loss = \"categorical_crossentropy\"\n            , metrics = [\"accuracy\"])\n    return model"]}, {"cell_type": "code", "execution_count": 1, "id": "b7c067fa", "metadata": {}, "outputs": [], "source": ["# mfcc model\nmodel3 = create_model(\"mfcc\")\nmodel3.summary()"]}, {"cell_type": "code", "execution_count": 1, "id": "20f01937", "metadata": {}, "outputs": [], "source": ["model3.fit(x=train_x_new, y=train_y, epochs=5, validation_data=(test_x_new, test_y))"]}, {"cell_type": "code", "execution_count": 1, "id": "8eeba37c", "metadata": {}, "outputs": [], "source": ["model3.save('speaker_ver1a_model.h5')"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}