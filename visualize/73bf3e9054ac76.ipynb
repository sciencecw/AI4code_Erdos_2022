{"cells": [{"cell_type": "markdown", "id": "13412506", "metadata": {}, "source": ["# Things you will find in this notebook:\n* A step-by-step guide on how to approach any competition on Kaggle.\n* Exploratory Data Analysis on the famous Titanic Dataset.\n* Feature Selection and Feature Engineering for better results.\n* What are Ensembling Machine Learning Techniques and how we can use them optimally on any dataset. \n* Comparing different ensembling techniques on the basis of score and optimizing them."]}, {"cell_type": "markdown", "id": "40e9d06d", "metadata": {}, "source": ["#### The objective of this notebook is to introduce the concept of ensemble learning and understand the algorithms which use this technique. To cement your understanding of this diverse topic, I will explain the advanced algorithms in Python using a hands-on case study on the titanic dataset."]}, {"cell_type": "markdown", "id": "b5ba7af1", "metadata": {}, "source": ["> Prerequisites: \nThe notebook assumes that you have a basic understanding of the following\nconcepts. If you don\u2019t, I encourage you to go through the links I have provided:<br> <br>\nEDA, Missing Values, Outlier Treatment, etc.\nTo understand the topics, you can go through\nthis article: [Ultimate guide for Data Exploration in Python using NumPy, Matplotlib and\nPandas.](https://www.analyticsvidhya.com/blog/2015/04/comprehensive-guide-data-exploration-sas-using-python-numpy-scipy-matplotlib-pandas/)<br><br>\nA basic understanding of Machine Learning algorithms. I would recommend going through\nthis article to familiarize yourself with these concepts: [Commonly used Machine Learning\nAlgorithms (with Python and R Codes)](https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/)<br><br>\n> For tuning the hyper parameters of decision tree and other based models used, refer [here](https://www.kaggle.com/tug004/surviving-titanic-the-right-way).<br><br>\n    \n1) Simple:\nIn this section, we will look at a few simple but powerful techniques, namely:\n1. Max Voting\n2. Averaging\n3. Weighted Averaging<br><br>\n\n2) Advanced<br>\nIn this section, we will look at advanced techniques, namely:\n1.  Stacking\n1.  Blending\n1.  The other advanced EL techniques are Bagging and Boosting.<br><br>\n\n3) EL models based on Bagging and Boosting:\n1. Bagging<br>\na) Bagging Meta Estimator\nb) Random Forest\n2. Boosting algorithms:<br>\na) GBM\nb) XGBoost\nc) AdaBoost\nd) Light GBM\ne) CatBoost"]}, {"cell_type": "markdown", "id": "cae02e81", "metadata": {}, "source": ["> Do upvote if you find this notebook useful!\n"]}, {"cell_type": "markdown", "id": "526df307", "metadata": {}, "source": ["## Imports "]}, {"cell_type": "code", "execution_count": 1, "id": "465abc65", "metadata": {}, "outputs": [], "source": ["import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler,StandardScaler\nfrom sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV,KFold,StratifiedKFold,cross_val_score,cross_val_predict\nfrom sklearn.metrics import accuracy_score,precision_score,confusion_matrix,recall_score,f1_score,roc_auc_score,auc,roc_curve\nimport re as re\nimport statistics\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import VotingClassifier,StackingClassifier, BaggingClassifier, AdaBoostClassifier,RandomForestClassifier,GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nimport warnings \nwarnings.filterwarnings('ignore')\n"]}, {"cell_type": "code", "execution_count": 1, "id": "5d4917ab", "metadata": {}, "outputs": [], "source": ["train_data = pd.read_csv('../input/titanic/train.csv')\ntest_data = pd.read_csv('../input/titanic/test.csv',)\ncombined = [train_data,test_data]\ntrain_data.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "77dc4f92", "metadata": {}, "outputs": [], "source": ["train_data.info()\nprint('-' * 50)\ntest_data.info()"]}, {"cell_type": "markdown", "id": "b779a581", "metadata": {}, "source": ["### Comparing survival rate with gender."]}, {"cell_type": "code", "execution_count": 1, "id": "ad7f7721", "metadata": {}, "outputs": [], "source": ["sns.set(style=\"whitegrid\")\nsns.countplot(train_data['Survived'],data = train_data)\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "607e7be4", "metadata": {}, "outputs": [], "source": ["print('Percentage of null values in each column of train data:\\n')\n(train_data.isnull().sum() / train_data.shape[0]) * 100"]}, {"cell_type": "code", "execution_count": 1, "id": "b87cf966", "metadata": {}, "outputs": [], "source": ["print('Percentage of null values in each column of test data:\\n')\n(test_data.isnull().sum() / test_data.shape[0]) * 100"]}, {"cell_type": "code", "execution_count": 1, "id": "daabc01b", "metadata": {}, "outputs": [], "source": ["train_data.describe(include = 'all')"]}, {"cell_type": "code", "execution_count": 1, "id": "2f29ad6d", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize = (10,8))\nsns.heatmap(train_data.corr(),annot=True,cbar = True)\nplt.show()"]}, {"cell_type": "markdown", "id": "7b49624d", "metadata": {}, "source": ["> Dropping passengerid as it does not affect survived class. <br>\n> Dropping cabin as it has lots of null values."]}, {"cell_type": "code", "execution_count": 1, "id": "ab5ac399", "metadata": {}, "outputs": [], "source": ["train_data.drop(['PassengerId','Cabin'],inplace=True,axis = 1)\ntest_data.drop(['PassengerId','Cabin'],inplace=True,axis = 1)"]}, {"cell_type": "code", "execution_count": 1, "id": "5e1fe431", "metadata": {}, "outputs": [], "source": ["train_data.columns"]}, {"cell_type": "code", "execution_count": 1, "id": "f83c7013", "metadata": {}, "outputs": [], "source": ["train_data.loc[0,train_data.dtypes == object]"]}, {"cell_type": "markdown", "id": "982988ad", "metadata": {}, "source": ["> Finding co-relations between target variable and name, sex, ticket and embarked columns."]}, {"cell_type": "code", "execution_count": 1, "id": "624cc750", "metadata": {}, "outputs": [], "source": ["print(train_data.groupby(['Sex','Survived'])['Survived'].count())\n\nsns.set(style=\"whitegrid\")\nsns.countplot(train_data['Survived'],hue = 'Sex',data = train_data)\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "abdbae13", "metadata": {}, "outputs": [], "source": ["train_data.drop(['Ticket'],axis = 1,inplace = True)\ntest_data.drop(['Ticket'],axis = 1,inplace = True)"]}, {"cell_type": "code", "execution_count": 1, "id": "cf6a2d65", "metadata": {}, "outputs": [], "source": ["print(train_data.groupby(['Embarked','Survived'])['Survived'].count())\nsns.set(style=\"whitegrid\")\nsns.countplot(train_data['Survived'],hue = 'Embarked',data = train_data)\nplt.show()"]}, {"cell_type": "markdown", "id": "68eb1c37", "metadata": {}, "source": ["> Feature engineering on Name,Age,SibSp + Parch and Fare features. "]}, {"cell_type": "code", "execution_count": 1, "id": "0712db68", "metadata": {}, "outputs": [], "source": ["for dataset in combined:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\nprint(pd.crosstab(train_data['Title'], train_data['Sex']))\nprint('-' * 50)\nprint(pd.crosstab(test_data['Title'], test_data['Sex']))"]}, {"cell_type": "code", "execution_count": 1, "id": "b0257ea6", "metadata": {}, "outputs": [], "source": ["for dataset in combined:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\nprint(train_data[['Title', 'Survived']].groupby(['Title'], as_index = False).mean())"]}, {"cell_type": "code", "execution_count": 1, "id": "0b0cecab", "metadata": {}, "outputs": [], "source": ["for dataset in combined:\n    age_avg \t   = dataset['Age'].mean()\n    age_std \t   = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    \n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\n    \ntrain_data['CategoricalAge'] = pd.cut(train_data['Age'], 5)\n\nprint (train_data[['CategoricalAge', 'Survived']].groupby(['CategoricalAge'], as_index=False).mean())"]}, {"cell_type": "code", "execution_count": 1, "id": "16d2c379", "metadata": {}, "outputs": [], "source": ["for dataset in combined:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\ntrain_data[['FamilySize','Survived']].groupby('FamilySize',as_index = False).mean()"]}, {"cell_type": "code", "execution_count": 1, "id": "99fe510a", "metadata": {}, "outputs": [], "source": ["for dataset in combined:\n    dataset['FamilySizeCategory'] = 0\n    for i in range(len(dataset)) : \n        if(dataset.loc[i,'FamilySize'] <= 4):\n            dataset.loc[i,'FamilySizeCategory'] = 2\n        elif((dataset.loc[i,'FamilySize'] > 4) & (dataset.loc[i,'FamilySize'] < 8)):\n            dataset.loc[i,'FamilySizeCategory'] = 1\n        else:\n            dataset.loc[i,'FamilySizeCategory'] = 0           \n    \nprint (train_data[['FamilySizeCategory', 'Survived']].groupby(['FamilySizeCategory'], as_index=False).mean())\ntrain_data['FamilySizeCategory'].nunique()"]}, {"cell_type": "code", "execution_count": 1, "id": "f5ad06f3", "metadata": {}, "outputs": [], "source": ["for dataset in combined:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\nprint (train_data[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean())"]}, {"cell_type": "code", "execution_count": 1, "id": "bddcc0bc", "metadata": {}, "outputs": [], "source": ["for dataset in combined:\n    dataset['Fare'] = dataset['Fare'].fillna(train_data['Fare'].median())\ntrain_data['CategoricalFare'] = pd.qcut(train_data['Fare'], 4)\nprint (train_data[['CategoricalFare', 'Survived']].groupby(['CategoricalFare'], as_index=False).mean())"]}, {"cell_type": "code", "execution_count": 1, "id": "9620e644", "metadata": {}, "outputs": [], "source": ["for dataset in combined:\n    dataset['Embarked'] = dataset['Embarked'].fillna(train_data['Embarked'].mode()[0])\ntrain_data[['Embarked','Survived']].groupby(['Embarked'],as_index = False).mean()"]}, {"cell_type": "code", "execution_count": 1, "id": "4d3c2c1c", "metadata": {}, "outputs": [], "source": ["train_data.info()"]}, {"cell_type": "code", "execution_count": 1, "id": "167bceb1", "metadata": {}, "outputs": [], "source": ["for dataset in combined:\n    dataset['Sex'] = dataset['Sex'].map( {'female' : 0,'male' : 1} )\n    \n    dataset.loc[dataset['Age'] <= 16,'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32),'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48),'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64),'Age'] = 3\n    dataset.loc[(dataset['Age'] > 64) & (dataset['Age'] <= 80),'Age'] = 4\n\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} )\n\n    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    drop_columns = ['Name','SibSp','Parch','FamilySize']\n    dataset.drop(drop_columns,axis = 1,inplace = True)\n\ndrop_col = ['CategoricalAge','CategoricalFare']\ntrain_data.drop(drop_col,axis = 1,inplace = True)"]}, {"cell_type": "code", "execution_count": 1, "id": "6cba7702", "metadata": {}, "outputs": [], "source": ["train_data.FamilySizeCategory.nunique()"]}, {"cell_type": "code", "execution_count": 1, "id": "186b3ccc", "metadata": {}, "outputs": [], "source": ["train_data = pd.get_dummies(train_data, prefix = ['Title'], columns = ['Title'])\ntest_data = pd.get_dummies(test_data, prefix = ['Title'], columns = ['Title'])"]}, {"cell_type": "code", "execution_count": 1, "id": "0dab5379", "metadata": {}, "outputs": [], "source": ["train_data.FamilySizeCategory.nunique()"]}, {"cell_type": "code", "execution_count": 1, "id": "7a1a6d96", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize = (12,10))\nsns.heatmap(train_data.corr(),annot=True,cbar = True)\nplt.show()"]}, {"cell_type": "code", "execution_count": 1, "id": "4a1a517d", "metadata": {}, "outputs": [], "source": ["x_train = train_data.iloc[:,1:]\ny_train = train_data.iloc[:,0]\nx_test = test_data\ny_train"]}, {"cell_type": "markdown", "id": "1579ec9f", "metadata": {}, "source": ["## Prediction Using Ensembling Methods"]}, {"cell_type": "markdown", "id": "a32d2038", "metadata": {}, "source": ["> Ensemble models in machine learning combine the decisions from multiple models to improve the overall performance. This can be achieved in various ways, which you will discover in this notebook."]}, {"cell_type": "markdown", "id": "78417bca", "metadata": {}, "source": ["1. ## Simple Techniques"]}, {"cell_type": "markdown", "id": "1fead8d4", "metadata": {}, "source": ["### 1. Max Voting\n\n> The Max Voting method is generally used for classication problems. In this technique, multiple models are used to make predictions for each data point. The predictions by each model are considered as a \u2018vote\u2019. The predictions which we get from the majority of the models are used as the final prediction.<br><br>\nFor example, when you asked 5 of your colleagues to rate your movie (out of 5); we\u2019ll assume three of them rated it as 4 while two of them gave it a 5. Since the majority gave a rating of 4, the final rating will be taken as 4. You can consider this as taking the mode of all the predictions.\n"]}, {"cell_type": "code", "execution_count": 1, "id": "25dfdaf0", "metadata": {}, "outputs": [], "source": ["model1 = DecisionTreeClassifier()\nmodel2 = KNeighborsClassifier()\nmodel3 = LogisticRegression()\nmodel4 = GaussianNB()\nmodel5 = LinearSVC()\n\nmodel1.fit(x_train,y_train)\nmodel2.fit(x_train,y_train)\nmodel3.fit(x_train,y_train)\nmodel4.fit(x_train,y_train)\nmodel5.fit(x_train,y_train)\n\n\ny1 = model1.predict(x_test)\ny2 = model2.predict(x_test)\ny3 = model3.predict(x_test)\ny4 = model4.predict(x_test)\ny5 = model5.predict(x_test)\n\n\ny = np.zeros(len(x_test))\nfor i in range(0,len(x_test)):\n    count = y1[i] + y2[i] + y3[i] + y4[i] + y5[i]\n    y[i] = (count // 3)\ny = y.astype(int)"]}, {"cell_type": "code", "execution_count": 1, "id": "01ee0dea", "metadata": {}, "outputs": [], "source": ["submit = pd.DataFrame(y,columns = ['Survived'],index = [i + 892 for i in range(0,418)])\nsubmit.index.name = 'PassengerId'\nsubmit.to_csv('maxvoting1.csv')"]}, {"cell_type": "markdown", "id": "79792b1a", "metadata": {}, "source": ["#### Another way to implement Max Voting:"]}, {"cell_type": "code", "execution_count": 1, "id": "ae294515", "metadata": {}, "outputs": [], "source": ["model = VotingClassifier(estimators=[('dtree', model1), ('knn', model2), ('lr', model3),('gnb',model4),('svc',model5)], voting='hard')\nmodel.fit(x_train,y_train)\n"]}, {"cell_type": "code", "execution_count": 1, "id": "9000db2e", "metadata": {}, "outputs": [], "source": ["y2 = model.predict(x_test)"]}, {"cell_type": "code", "execution_count": 1, "id": "7fe0ce17", "metadata": {}, "outputs": [], "source": ["submit = pd.DataFrame(y2,columns = ['Survived'],index = [i + 892 for i in range(0,418)])\nsubmit.index.name = 'PassengerId'\nsubmit.to_csv('maxvoting2.csv')"]}, {"cell_type": "markdown", "id": "1e05abe1", "metadata": {}, "source": ["> Both the implementation methods gave 0.7751 as public score."]}, {"cell_type": "markdown", "id": "717efd56", "metadata": {}, "source": ["### 2. Averaging "]}, {"cell_type": "markdown", "id": "e498165a", "metadata": {}, "source": ["> Similar to the max voting technique, multiple predictions are made for each data point in averaging. In this method, we take an average of predictions from all the models and use it to make the final prediction. Averaging can be used for making predictions in regression problems or while calculating probabilities for classication problems."]}, {"cell_type": "markdown", "id": "ad569e66", "metadata": {}, "source": ["### 3.Weighted Averaging"]}, {"cell_type": "markdown", "id": "7aa0ec09", "metadata": {}, "source": ["> This is an extension of the averaging method. All models are assigned different weights defining the importance of each model for prediction. For instance, if two of your colleagues are critics, while others have no prior experience in this field, then the answers by these two friends are given more importance as compared to the other people.\n"]}, {"cell_type": "markdown", "id": "d875d05a", "metadata": {}, "source": ["> We can assign weights to the models by looking at the data, and then calculate the mean of the predicted values."]}, {"cell_type": "code", "execution_count": 1, "id": "00094006", "metadata": {}, "outputs": [], "source": ["y3 = np.zeros(len(x_test))\nfor i in range(0,len(x_test)):\n    count = 0.5 * y1[i] + 0.5 + y2[i] + 2 * y3[i] + y4[i] + 2 * y5[i]\n    y3[i] = (count // 3)\ny3 = y3.astype(int)"]}, {"cell_type": "code", "execution_count": 1, "id": "ec286328", "metadata": {}, "outputs": [], "source": ["submit = pd.DataFrame(y3,columns = ['Survived'],index = [i + 892 for i in range(0,418)])\nsubmit.index.name = 'PassengerId'\nsubmit.to_csv('weighted_averaging.csv')"]}, {"cell_type": "markdown", "id": "c2598ab4", "metadata": {}, "source": ["## 2. Advanced techniques"]}, {"cell_type": "markdown", "id": "b2ea90db", "metadata": {}, "source": ["### 1.Stacking"]}, {"cell_type": "markdown", "id": "2dac7a9f", "metadata": {}, "source": ["\n> Stacking is an ensemble learning technique that uses predictions from multiple models (for example decision tree, knn or svm) to build a new model. This model is used for making predictions on the test set."]}, {"cell_type": "markdown", "id": "21756252", "metadata": {}, "source": ["#### a). Implementing stacking from scratch"]}, {"cell_type": "code", "execution_count": 1, "id": "55089abf", "metadata": {}, "outputs": [], "source": ["def predictor(model,x,y,x_test):\n    pred_train = cross_val_predict(model,x,y,cv = 4)   \n    \n    model.fit(x,y)\n    pred_test = model.predict(x_test)\n    \n    return pred_train,pred_test\n\nM1_pred,M1_pred_test = predictor(model1,x_train,y_train,x_test)\nM2_pred,M2_pred_test = predictor(model2,x_train,y_train,x_test)\nM3_pred,M3_pred_test = predictor(model3,x_train,y_train,x_test)\n\nstacked_x_train = {\n    'DT': M1_pred,\n    'KNN': M2_pred,\n    'LR': M3_pred\n}\nstacked_x_train = pd.DataFrame(stacked_x_train) \nstacked_x_train"]}, {"cell_type": "code", "execution_count": 1, "id": "5519914e", "metadata": {}, "outputs": [], "source": ["stacked_x_test = {\n    'DT': M1_pred_test,\n    'KNN': M2_pred_test,\n    'LR': M3_pred_test\n}\nstacked_x_test = pd.DataFrame(stacked_x_test) \nstacked_x_test"]}, {"cell_type": "markdown", "id": "35b4e028", "metadata": {}, "source": ["> Using decision tree,knn and logistic regression as our base models and SVC as our meta-model."]}, {"cell_type": "code", "execution_count": 1, "id": "047b9ad5", "metadata": {}, "outputs": [], "source": ["model5.fit(stacked_x_train,y_train)\ny4 = model5.predict(stacked_x_test)\n\nsubmit = pd.DataFrame(y4,columns = ['Survived'],index = [i + 892 for i in range(0,418)])\nsubmit.index.name = 'PassengerId'\nsubmit.to_csv('stacking_from_scratch.csv')"]}, {"cell_type": "markdown", "id": "c0bcdd22", "metadata": {}, "source": ["> We achieved score of 0.76794 by using the above technique.[](http://) "]}, {"cell_type": "markdown", "id": "209bd14c", "metadata": {}, "source": ["#### b.) Implementing stacking using SkLearn's StackingClassifier"]}, {"cell_type": "code", "execution_count": 1, "id": "cbf24b1f", "metadata": {}, "outputs": [], "source": ["estimators = [\n    ('dt',model1),\n    ('knn',model2),\n    ('lr',model3)\n]\n\nclf = StackingClassifier(estimators = estimators, final_estimator = model5,cv = 10)\nclf.fit(x_train, y_train)\n\ny5 = clf.predict(x_test)\nsubmit = pd.DataFrame(y5,columns = ['Survived'],index = [i + 892 for i in range(0,418)])\nsubmit.index.name = 'PassengerId'\nsubmit.to_csv('stacking.csv')"]}, {"cell_type": "markdown", "id": "78e6b692", "metadata": {}, "source": [" > We achieved score of 0.78229 by using the above technique, which is the best score yet!!!!!"]}, {"cell_type": "markdown", "id": "be7f225e", "metadata": {}, "source": ["### 2. Blending"]}, {"cell_type": "markdown", "id": "6c35d701", "metadata": {}, "source": ["> Blending follows the same approach as stacking but uses only a holdout (validation) set from the train set to make predictions. In other words, unlike stacking, the predictions are made on the holdout set only. The holdout set and the predictions are used to build a model which is run on the test set."]}, {"cell_type": "code", "execution_count": 1, "id": "06f4c0c9", "metadata": {}, "outputs": [], "source": ["base_x_train,x_val,base_y_train,y_val = train_test_split(x_train,y_train,random_state = 42,train_size = 0.8)\n\ndef blending_predictor(model,x,y,x_val):\n    model.fit(x,y)\n    val_pred = model.predict(x_val)\n    test_pred = model.predict(x_test)\n    \n    return val_pred,test_pred\n\nM1_val_pred,M1_test_pred = blending_predictor(model1,base_x_train,base_y_train,x_val)\nM2_val_pred,M2_test_pred = blending_predictor(model2,base_x_train,base_y_train,x_val)\nM3_val_pred,M3_test_pred = blending_predictor(model3,base_x_train,base_y_train,x_val)\n"]}, {"cell_type": "code", "execution_count": 1, "id": "7a051244", "metadata": {}, "outputs": [], "source": ["M1_val_pred = pd.DataFrame(M1_val_pred)\nM2_val_pred = pd.DataFrame(M2_val_pred)\nM3_val_pred = pd.DataFrame(M3_val_pred)\n\n\nM1_test_pred = pd.DataFrame(M1_test_pred)\nM2_test_pred = pd.DataFrame(M2_test_pred)\nM3_test_pred = pd.DataFrame(M3_test_pred)\n"]}, {"cell_type": "code", "execution_count": 1, "id": "2f85b052", "metadata": {}, "outputs": [], "source": ["x_val.index = range(179)\ndf_val = pd.concat([x_val, M1_val_pred,M2_val_pred,M3_val_pred],axis=1,ignore_index = True)\ndf_test = pd.concat([x_test, M1_test_pred,M2_test_pred,M3_test_pred],axis=1)\n\nmodel5.fit(df_val,y_val)\ny6 = model5.predict(df_test)\n\nsubmit = pd.DataFrame(y6,columns = ['Survived'],index = [i + 892 for i in range(0,418)])\nsubmit.index.name = 'PassengerId'\nsubmit.to_csv('blending.csv')"]}, {"cell_type": "markdown", "id": "551f5a7f", "metadata": {}, "source": ["> Score received: 0.76076"]}, {"cell_type": "markdown", "id": "2bdc3098", "metadata": {}, "source": ["### 3.EL models based on Bagging and Boosting:"]}, {"cell_type": "markdown", "id": "ac6f7e4f", "metadata": {}, "source": ["### 1. Bagging:"]}, {"cell_type": "markdown", "id": "c7a0b081", "metadata": {}, "source": ["#### What is Bagging?\n\n> The idea behind bagging is combining the results of multiple models (for instance, all decision trees) to get a generalized result. <br>\nHere\u2019s a question: If you create all the models on the same set of data and combine it, will it be useful? There is a high chance that these models will give the\nsame result since they are getting the same input. <br><br>\nSo how can we solve this problem? One of the techniques is bootstrapping.\n<br>\nBootstrapping is a sampling technique in which we create subsets of observations from the\noriginal dataset, with replacement. The size of the subsets is the same as the size of the original set.\nBagging (or Bootstrap Aggregating) technique uses these subsets (bags) to get a fair idea of the distribution (complete set). The size of subsets created for bagging may be less than the original set."]}, {"cell_type": "markdown", "id": "435a17b7", "metadata": {}, "source": ["#### Procedure to follow:\n1. Multiple subsets are created from the original dataset, selecting observations with\nreplacement.\n2. A base model (weak model) is created on each of these subsets.\n3. The models run in parallel and are independent of each other.\n4. The final predictions are determined by combining the predictions from all the models."]}, {"cell_type": "markdown", "id": "87c200ac", "metadata": {}, "source": ["#### a) Bagging Meta-Estimator\n\n> Bagging meta-estimator is an ensembling algorithm that can be used for both classication (BaggingClassier) and regression (BaggingRegressor) problems. It follows the typical bagging technique to make predictions.<br><br>\nFollowing are the steps for the bagging meta-estimator algorithm:\n1. Random subsets are created from the original dataset (Bootstrapping).\n2. The subset of the dataset includes all features.\n3. A user-specied base estimator is fitted on each of these smaller sets.\n4. Predictions from each model are combined to get the final result."]}, {"cell_type": "markdown", "id": "deb75343", "metadata": {}, "source": ["> For tuning the hyper parameters of decision tree and other based models used, refer [here](https://www.kaggle.com/tug004/surviving-titanic-the-right-way)."]}, {"cell_type": "code", "execution_count": 1, "id": "4af5f810", "metadata": {}, "outputs": [], "source": ["param_grid = {\n    'n_estimators': list(range(20))\n}\ngscv = GridSearchCV(BaggingClassifier(random_state = 0),param_grid = param_grid, cv = 10)\ngscv.fit(x_train,y_train)\ngscv.best_params_"]}, {"cell_type": "code", "execution_count": 1, "id": "5b604d87", "metadata": {}, "outputs": [], "source": ["gscv.best_estimator_"]}, {"cell_type": "code", "execution_count": 1, "id": "7c52c670", "metadata": {}, "outputs": [], "source": ["clf = BaggingClassifier(model1,n_estimators = 19,random_state = 0)\nclf.fit(x_train,y_train)\n\ny7 = clf.predict(x_test)\n\nsubmit = pd.DataFrame(y7,columns = ['Survived'],index = [i + 892 for i in range(0,418)])\nsubmit.index.name = 'PassengerId'\nsubmit.to_csv('bagging.csv')"]}, {"cell_type": "markdown", "id": "79963290", "metadata": {}, "source": ["> We achieved score of 0.77511 by using the above technique."]}, {"cell_type": "markdown", "id": "006df772", "metadata": {}, "source": ["#### b) Random Forests<br>\n> Random Forest is considered to be a panacea of all data science problems. On a funny note, when you can\u2019t think of any algorithm (irrespective of situation), use random forest!<br><br>\n> Random forest is like bootstrapping algorithm with Decision tree (CART) model. Say, we have 1000 observation in the complete population with 10 variables. Random forest tries to build multiple CART models with different samples and different initial variables. For instance, it will take a random sample of 100 observation and 5 randomly chosen initial variables to build a CART model. It will repeat the process (say) 10 times and then make a final prediction on each observation. Final prediction is a function of each prediction. This final prediction can simply be the mean of each prediction."]}, {"cell_type": "code", "execution_count": 1, "id": "d385a2e2", "metadata": {}, "outputs": [], "source": ["param_grid = {\n    'n_estimators': [5, 10, 15, 20, 25], \n    'max_depth': [3, 5, 7, 9, 11, 13],    \n    'criterion': ['gini', 'entropy']\n}\ngscv = GridSearchCV(RandomForestClassifier(random_state = 0),param_grid = param_grid, cv = 10,scoring = 'accuracy')\ngscv.fit(x_train,y_train)\ngscv.best_params_"]}, {"cell_type": "code", "execution_count": 1, "id": "a1fa6db0", "metadata": {}, "outputs": [], "source": ["gscv.best_estimator_"]}, {"cell_type": "code", "execution_count": 1, "id": "e0d32867", "metadata": {}, "outputs": [], "source": ["clf = RandomForestClassifier(n_estimators = 20,max_depth = 5,criterion = 'gini',random_state = 0)\nclf.fit(x_train,y_train)\n\ny8 = clf.predict(x_test)\n\nsubmit = pd.DataFrame(y8,columns = ['Survived'],index = [i + 892 for i in range(0,418)])\nsubmit.index.name = 'PassengerId'\nsubmit.to_csv('random_forest.csv')"]}, {"cell_type": "markdown", "id": "d572b55e", "metadata": {}, "source": ["> We achieved score of 0.77990 by using the above technique."]}, {"cell_type": "code", "execution_count": 1, "id": "5200cf5c", "metadata": {}, "outputs": [], "source": ["clf.feature_importances_"]}, {"cell_type": "markdown", "id": "62786cc9", "metadata": {}, "source": ["### 2. Boosting"]}, {"cell_type": "markdown", "id": "bbfbd0d3", "metadata": {}, "source": ["> Boosting is a sequential process, where each subsequent model attempts to correct the errors of the previous model. The succeeding models are dependent on the previous model.<br> <br>\nLet\u2019s understand the way boosting works in detail:\n1. A subset is created from the original dataset.\n2. Initially, all data points are given equal weights.\n3. A base model is created on this subset.\n4. This model is used to make predictions on the whole dataset.\n5. Errors are calculated using the actual values and predicted values.\n6. The observations which are incorrectly predicted, are given higher weights.\n7. Another model is created and predictions are made on the dataset.\n8. Similarly, multiple models are created, each correcting the errors of the previous model.\n9. The final model (strong learner) is the weighted mean of all the models (weak learners).\n10. Thus, the boosting algorithm combines a number of weak learners to form a strong learner. The individual models would not perform well on the entire dataset, but they work well for some part of the dataset. Thus, each model actually boosts the performance of the ensemble."]}, {"cell_type": "markdown", "id": "e6c5e8d8", "metadata": {}, "source": ["#### a). Gradient Boosting Machine (GBM)"]}, {"cell_type": "code", "execution_count": 1, "id": "4ca65b01", "metadata": {}, "outputs": [], "source": ["param_grid = {\n    'loss': ['deviance','exponential'],\n    'max_depth': [3, 5, 7, 9], \n    'n_estimators': [5, 10, 15, 20, 25, 50, 100],\n    'learning_rate': [0.01, 0.05, 0.1]\n    \n}\ngscv = GridSearchCV(GradientBoostingClassifier(random_state = 0),param_grid = param_grid, cv = 10)\ngscv.fit(x_train,y_train)\ngscv.best_params_"]}, {"cell_type": "code", "execution_count": 1, "id": "1d7541d4", "metadata": {}, "outputs": [], "source": ["clf = GradientBoostingClassifier(\n    loss = 'exponential',\n    learning_rate = 0.1,\n    n_estimators = 50 ,\n    criterion = 'mae',\n    min_samples_split = 10,\n    min_samples_leaf = 3,\n    max_depth = 6,\n    max_features = 'sqrt',\n    random_state = 0)\n\nclf.fit(x_train,y_train)\n\ny9 = clf.predict(x_test)\n\nsubmit = pd.DataFrame(y9,columns = ['Survived'],index = [i + 892 for i in range(0,418)])\nsubmit.index.name = 'PassengerId'\nsubmit.to_csv('gradientboosting.csv')"]}, {"cell_type": "markdown", "id": "71d56eff", "metadata": {}, "source": ["#### b). Xtreme Gradient Boosting Machine (XGBM)"]}, {"cell_type": "code", "execution_count": 1, "id": "babce8d8", "metadata": {}, "outputs": [], "source": ["param_grid = {\n    'max_depth': [3, 5, 7, 9], \n    'n_estimators': [5, 10, 15, 20, 25, 50, 100],\n    'learning_rate': [0.01, 0.05, 0.1]\n}\ngscv = GridSearchCV(XGBClassifier(random_state = 0),param_grid = param_grid, cv = 10,scoring = 'accuracy')\ngscv.fit(x_train,y_train)\ngscv.best_params_"]}, {"cell_type": "code", "execution_count": 1, "id": "00ba9a63", "metadata": {}, "outputs": [], "source": ["gscv.best_score_"]}, {"cell_type": "code", "execution_count": 1, "id": "287fa26f", "metadata": {}, "outputs": [], "source": ["clf = XGBClassifier(learning_rate = 0.05, max_depth = 7, n_estimators = 100,random_state = 42)\n\nclf.fit(x_train,y_train)\n\ny10 = clf.predict(x_test)\n\nsubmit = pd.DataFrame(y10,columns = ['Survived'],index = [i + 892 for i in range(0,418)])\nsubmit.index.name = 'PassengerId'\nsubmit.to_csv('xgboosting.csv')"]}, {"cell_type": "markdown", "id": "75bb5a09", "metadata": {}, "source": ["#### c). Adaptive Boosting (AdaBoost)"]}, {"cell_type": "code", "execution_count": 1, "id": "72a6bd74", "metadata": {}, "outputs": [], "source": ["param_grid = {\n    'n_estimators': [5, 10, 15, 20, 25, 50, 75, 100], \n    'learning_rate': [0.001, 0.01, 0.1, 1.],\n}\ngscv = GridSearchCV(AdaBoostClassifier(random_state = 0),param_grid = param_grid, cv = 10,scoring = 'accuracy')\ngscv.fit(x_train,y_train)\nprint(gscv.best_score_)\nprint(gscv.best_params_)"]}, {"cell_type": "code", "execution_count": 1, "id": "9ed5e53b", "metadata": {}, "outputs": [], "source": ["clf = AdaBoostClassifier(learning_rate = 0.1, n_estimators = 75,random_state = 42)\n\nclf.fit(x_train,y_train)\n\ny11 = clf.predict(x_test)\n\nsubmit = pd.DataFrame(y11,columns = ['Survived'],index = [i + 892 for i in range(0,418)])\nsubmit.index.name = 'PassengerId'\nsubmit.to_csv('adaboost.csv')"]}, {"cell_type": "markdown", "id": "c088477b", "metadata": {}, "source": ["> We achieved score of 0.77990 by using the above technique."]}, {"cell_type": "markdown", "id": "8d13e5d4", "metadata": {}, "source": ["#### d.) LightGBM"]}, {"cell_type": "markdown", "id": "ef81098f", "metadata": {}, "source": ["> Before discussing how Light GBM works, let\u2019s first understand why we need this algorithm when we have so many others (like the ones we have seen above). Light GBM beats all the other algorithms when the dataset is extremely large. Compared to the other algorithms, Light GBM takes lesser time to run on a huge dataset.\n<br><br>\nLightGBM is a gradient boosting framework that uses tree-based algorithms and follows leafwise approach while other algorithms work in a level-wise approach pattern. <br>\n\n"]}, {"cell_type": "code", "execution_count": 1, "id": "1f765a1a", "metadata": {}, "outputs": [], "source": ["param_grid = {\n    'n_estimators': [5, 10, 15, 20, 25, 50, 100],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'num_leaves': [7, 15, 31],\n    'max_depth': [3,5,7,9,13]\n}\ngscv = GridSearchCV(LGBMClassifier(random_state = 0),param_grid = param_grid, cv = 10,scoring = 'accuracy')\ngscv.fit(x_train,y_train)\nprint(gscv.best_score_)\nprint(gscv.best_params_)"]}, {"cell_type": "code", "execution_count": 1, "id": "42a2182e", "metadata": {}, "outputs": [], "source": ["clf = LGBMClassifier(learning_rate = 0.05, n_estimators = 50,max_depth = 5,num_leaves = 15,random_state = 42)\n\nclf.fit(x_train,y_train)\ny12 = clf.predict(x_test)\n\nsubmit = pd.DataFrame(y12,columns = ['Survived'],index = [i + 892 for i in range(0,418)])\nsubmit.index.name = 'PassengerId'\nsubmit.to_csv('lgbm.csv')"]}, {"cell_type": "markdown", "id": "850e12c8", "metadata": {}, "source": ["#### e.) CatBoost"]}, {"cell_type": "markdown", "id": "4e6c36e0", "metadata": {}, "source": ["> Handling categorical variables is a tedious process, especially when you have a large number of such variables. When your categorical variables have too many labels (i.e. they are highly cardinal), performing one-hot-encoding on them exponentially increases the dimensionality and it becomes really difcult to work with the dataset.<br><br>\nCatBoost can automatically deal with categorical variables and does not require extensive data preprocessing like other machine learning algorithms."]}, {"cell_type": "code", "execution_count": 1, "id": "cdf973e1", "metadata": {}, "outputs": [], "source": ["param_grid = {\n    'iterations': [5, 10, 15, 20, 25, 50, 100],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'depth': [3, 5, 7, 9, 11, 13],\n}\ngscv = GridSearchCV(CatBoostClassifier(random_state = 0,verbose = False),param_grid = param_grid, cv = 10,scoring = 'accuracy')\ngscv.fit(x_train,y_train)\nprint(gscv.best_score_)\nprint(gscv.best_params_)"]}, {"cell_type": "code", "execution_count": 1, "id": "442d619a", "metadata": {}, "outputs": [], "source": ["clf = CatBoostClassifier(learning_rate = 0.1,depth = 5,iterations = 10,random_state = 42)\n\nclf.fit(x_train,y_train)\n\ny13 = clf.predict(x_test)\n\nsubmit = pd.DataFrame(y13,columns = ['Survived'],index = [i + 892 for i in range(0,418)])\nsubmit.index.name = 'PassengerId'\nsubmit.to_csv('catboost.csv')"]}, {"cell_type": "markdown", "id": "dffeca36", "metadata": {}, "source": [" > Thanks for going through the notebook. Do comment in case you have any doubts/suggestions.<br>\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}