{"cells": [{"cell_type": "markdown", "id": "e36ab364", "metadata": {}, "source": ["# Generating various bimodal predictions"]}, {"cell_type": "markdown", "id": "2b9dd811", "metadata": {}, "source": ["Sharing some of my predictions that displayed bimodal distribution. Public scores are around 0.70. Tried some ensembles but public score isn't necessarily any better than \"unimodal\" predictions.."]}, {"cell_type": "code", "execution_count": 1, "id": "48a81037", "metadata": {}, "outputs": [], "source": ["import pandas as pd\nimport numpy as np\nimport glob\nimport os\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nimport plotly_express as px\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error"]}, {"cell_type": "markdown", "id": "b78416b3", "metadata": {}, "source": ["# Distribution of target in train set"]}, {"cell_type": "code", "execution_count": 1, "id": "99823161", "metadata": {}, "outputs": [], "source": ["train = pd.read_csv('../input/tabular-playground-series-jan-2021/train.csv')"]}, {"cell_type": "code", "execution_count": 1, "id": "969ed60a", "metadata": {}, "outputs": [], "source": ["fig = ff.create_distplot(\n    [train.target[:10000]], \n    group_labels = ['kde']\n)\n\nfig.update_xaxes(range=[5, 10])\n\nfig"]}, {"cell_type": "markdown", "id": "7d6a894d", "metadata": {}, "source": ["# Distribution of predictions for test set (ensemble, public score = 0.69685 )"]}, {"cell_type": "code", "execution_count": 1, "id": "c119c6b3", "metadata": {}, "outputs": [], "source": ["df_69685 = pd.read_csv('../input/dataset/0.69685-others-submission6.csv')\nfig = ff.create_distplot(\n    [df_69685.target[:10000]], \n    group_labels = ['kde']\n)\n\nfig.update_xaxes(range=[5, 10])"]}, {"cell_type": "markdown", "id": "20646002", "metadata": {}, "source": ["# Simulating the shape of target in the train set"]}, {"cell_type": "markdown", "id": "12e66c43", "metadata": {}, "source": ["I trained 5 additional models: \n\n- XGBRegressor,  using train set where target is <9. \n- Same XGBRegressor model, except using train set where target >7\n- LightGBM, using train set where target is <9. \n- Same LightGBM model, except using train set where target >7\n- CatBoostRegressor, using train set where target is <9. \n- Same CatBoostRegressor model, except using train set where target >7\n\nAll 6 models were tuned using Optuna. Each model has an RMSE of bewteen 0.3-0.5 for prediction within its specified region. Then I used each of the models to predict the **full** test set"]}, {"cell_type": "code", "execution_count": 1, "id": "9d3500f1", "metadata": {}, "outputs": [], "source": ["# light GBM optuna tuned params\nlight_opt_best_random = {'n_estimators': 8540,\n 'min_data_per_group': 45,\n 'num_leaves': 126,\n 'max_depth': 20,\n 'learning_rate': 0.0032598944879946414,\n 'subsample_for_bin': 32553,\n 'lambda_l1': 0.11917413918151999,\n 'lambda_l2': 6.857359561808505e-05,\n 'bagging_fraction': 0.8910429482743759,\n 'min_data_in_leaf': 94,\n 'min_sum_hessian_in_leaf': 0.01,\n 'bagging_freq': 2,\n 'feature_fraction': 0.4699812049606955,\n 'min_child_samples': 61}\n\n# xgboost upotuna tuned params\nxgb_opt_best_random = {'learning_rate': 0.004138539806617361, \n 'gamma': 0.020496820582462844, \n 'max_depth': 19, \n 'min_child_weight': 308, \n 'max_delta_step': 9, \n 'subsample': 0.6437442427644592, \n 'colsample_bytree': 0.41845630929589844, \n 'lambda': 0.0038484657676066394, \n 'alpha': 0.09281553090596092, \n 'n_estimators': 4767\n}\n\n# catboost optuna tuned params\ncatboost_opt_best_random= {\n    'n_estimators': 9639, \n    'learning_rate': 0.025621857270512527, \n    'reg_lambda': 0.03261099593456338, \n    'subsample': 0.6319711159148579, \n    'depth': 7, \n    'min_child_samples': 48, \n    'colsample_bylevel': 0.14898612913306458, \n    'langevin': False, \n    'model_shrink_rate': 0.28621265987632455, \n    'model_shrink_mode': 'Decreasing', \n    'model_size_reg': 2.373053070327802\n}"]}, {"cell_type": "code", "execution_count": 1, "id": "132197d9", "metadata": {}, "outputs": [], "source": ["# loading predictions for the 6 models above\nsub26a = pd.read_csv('../input/dataset/submission26a-optuna-light-sm.csv')\nsub26b = pd.read_csv('../input/dataset/submission26b-optuna-light-lg.csv')\nsub26c = pd.read_csv('../input/dataset/submission26c-optuna-xgb-sm.csv')\nsub26d = pd.read_csv('../input/dataset/submission26d-optuna-xgb-lg.csv')\nsub26e = pd.read_csv('../input/dataset/submission26e-optuna-catboost-sm.csv')\nsub26f = pd.read_csv('../input/dataset/submission26f-optuna-catboost-lg.csv')"]}, {"cell_type": "markdown", "id": "29d286a3", "metadata": {}, "source": ["# Generate some bi-modal predictions"]}, {"cell_type": "markdown", "id": "7b0b906f", "metadata": {}, "source": ["## Mix 1 - public score = 0.7034"]}, {"cell_type": "code", "execution_count": 1, "id": "091cc583", "metadata": {}, "outputs": [], "source": ["ls = []\nfor idx, num in enumerate(df_69685.target):\n    \n    if np.mean([sub26b.target[idx], sub26d.target[idx], sub26f.target[idx]]) > 8.1:# change this threshold or swap the if/elif clause, you will get different shapes\n        ls.append(np.mean([sub26b.target[idx], sub26d.target[idx], sub26f.target[idx]]))\n    elif np.mean([sub26a.target[idx], sub26c.target[idx], sub26e.target[idx]]) < 8.1:\n        ls.append( np.mean([sub26a.target[idx], sub26c.target[idx], sub26e.target[idx]]))\n\n    else:\n        ls.append(num)\n"]}, {"cell_type": "code", "execution_count": 1, "id": "95ad5b41", "metadata": {}, "outputs": [], "source": ["\nfig = ff.create_distplot(\n    [ls[:1000]], \n    group_labels = ['kde']\n)\n\nfig.update_xaxes(range=[5, 10])"]}, {"cell_type": "markdown", "id": "9f8a9996", "metadata": {}, "source": ["## Mix 2 - public score = 0.70714"]}, {"cell_type": "code", "execution_count": 1, "id": "3a5c1644", "metadata": {}, "outputs": [], "source": ["ls = []\nfor idx, num in enumerate(df_69685.target):\n    \n    if np.mean([sub26a.target[idx], sub26c.target[idx], sub26f.target[idx]]) < 7.8: # change this threshold or swap the if/elif clause, you will get different shapes\n        ls.append( np.mean([sub26a.target[idx], sub26c.target[idx], sub26f.target[idx]]))\n    \n    elif np.mean([sub26b.target[idx], sub26d.target[idx], sub26e.target[idx]]) > 7.8 :\n        ls.append(np.mean([sub26b.target[idx], sub26d.target[idx], sub26e.target[idx]]))\n    else:\n        ls.append(num)"]}, {"cell_type": "code", "execution_count": 1, "id": "68640306", "metadata": {}, "outputs": [], "source": ["fig = ff.create_distplot(\n    [ls[:10000]], \n    group_labels = ['kde']\n)\n\nfig.update_xaxes(range=[5, 10])"]}, {"cell_type": "markdown", "id": "afa0e4e3", "metadata": {}, "source": ["So in theory you can generate predictions with bimodal distributions, but their does not seem to be as good as the best model I have, which only has 1 peak. Any thoughts?"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}