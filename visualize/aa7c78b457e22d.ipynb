{"cells": [{"cell_type": "markdown", "id": "e70e1d73", "metadata": {}, "source": ["# Segmentation Model for Brain Tumors using Fastai"]}, {"cell_type": "markdown", "id": "029b4f5e", "metadata": {}, "source": ["## Introduction"]}, {"cell_type": "markdown", "id": "8c97622e", "metadata": {}, "source": ["## Imports and Exploring the Data"]}, {"cell_type": "code", "execution_count": 1, "id": "817fc219", "metadata": {}, "outputs": [], "source": ["%reload_ext autoreload\n%autoreload 2\n%matplotlib inline"]}, {"cell_type": "code", "execution_count": 1, "id": "43643a3b", "metadata": {}, "outputs": [], "source": ["from fastai import *\nfrom fastai.vision import *\nfrom fastai.callbacks.hooks import *\nfrom fastai.utils.mem import *"]}, {"cell_type": "markdown", "id": "e785bed7", "metadata": {}, "source": ["The following is from Peter Butterfill's notebook \"brain-segmentation-fastai.\" This is done to make the pre-trained model available to the kernel without internet."]}, {"cell_type": "code", "execution_count": 1, "id": "e306cfd2", "metadata": {}, "outputs": [], "source": ["# 1) manually download the model from:\n#   * https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /tmp/.cache/torch/checkpoints/resnet34-333f7ec4.pth\n# 2) use \"+ Add Data\" on the kaggle's data panel on the right to make the model available\n# 3) make the temp folder (that pytorch wants to use) if it doesn't exist already\nPath('/tmp/.cache/torch/checkpoints').mkdir(parents=True, exist_ok=True)\n# 4) copy the model from input to the temp folder\n!cp /kaggle/input/pytorch-models/resnet34-333f7ec4.pth /tmp/.cache/torch/checkpoints/resnet34-333f7ec4.pth\n# 5) then make sure we set path='/kaggle/working' when creating the learner - otherwise it'll use the dir of the model (which is read-only)\noutput_path = '/kaggle/working'"]}, {"cell_type": "markdown", "id": "697df0c2", "metadata": {}, "source": ["We want a function which would give us the path and file name of an image's mask given the file name of that image. We can see that the mask's file name is just the raw image's file name with '_mask' added before the suffix. Thus, the function would look something like:"]}, {"cell_type": "code", "execution_count": 1, "id": "651c28a2", "metadata": {}, "outputs": [], "source": ["def get_mask_fn(x): return x.parents[0] / (x.stem + '_mask' + x.suffix)"]}, {"cell_type": "code", "execution_count": 1, "id": "dfbba795", "metadata": {}, "outputs": [], "source": ["# Designating the path to the data\npath = Path('/kaggle/input/lgg-mri-segmentation/kaggle_3m')"]}, {"cell_type": "code", "execution_count": 1, "id": "66e2b550", "metadata": {}, "outputs": [], "source": ["# Designating the path to two images we are going to inspect.\nimage_temp_file = path/'TCGA_CS_6666_20011109/TCGA_CS_6666_20011109_19.tif'\nmask_temp_file = get_mask_fn(image_temp_file)"]}, {"cell_type": "code", "execution_count": 1, "id": "a34c0473", "metadata": {}, "outputs": [], "source": ["# Show an example of an image with a tumor\nimage = open_image(image_temp_file)\nimage.show(figsize = (5,5))"]}, {"cell_type": "markdown", "id": "7393a0ce", "metadata": {}, "source": ["Now, we want the masks to be normalized so that the pixels of each mask are only 0 or 1, as to indicate whether the tumor is there or not, rather than a range of values between 0-255, where 0 is black (no tumor) and 255 is white (tumor). To open the files like so, we use div = True."]}, {"cell_type": "code", "execution_count": 1, "id": "435d71ff", "metadata": {}, "outputs": [], "source": ["#Show the mask for that image\nmask = open_image(mask_temp_file, div = True)\nmask.show(figsize = (5,5))\nmask_size = np.array(mask.shape)\nimage_size = np.array(mask.shape[1:])\nmask_size"]}, {"cell_type": "markdown", "id": "53adfd12", "metadata": {}, "source": ["## Dataset"]}, {"cell_type": "markdown", "id": "3ca2b201", "metadata": {}, "source": ["It would be poor practice to include images from a patient measured in the training set for the validation set because of leakage. Thus, I am specifying certain patients whose images I will use only in the validation set."]}, {"cell_type": "code", "execution_count": 1, "id": "47d36c08", "metadata": {}, "outputs": [], "source": ["# Validation set must come from patients that the model has not seen before. \n# Thus, I am specifying certain patients for the validation set.\nvalidataion_folders = [\n        'TCGA_HT_7694_19950404', 'TCGA_DU_5874_19950510', 'TCGA_DU_7013_19860523',\n        'TCGA_HT_8113_19930809', 'TCGA_DU_6399_19830416', 'TCGA_HT_7684_19950816',\n        'TCGA_CS_5395_19981004', 'TCGA_FG_6688_20020215', 'TCGA_DU_8165_19970205',\n        'TCGA_DU_7019_19940908', 'TCGA_HT_7855_19951020', 'TCGA_DU_A5TT_19980318',\n        'TCGA_DU_7300_19910814', 'TCGA_DU_5871_19941206', 'TCGA_DU_5855_19951217']\n"]}, {"cell_type": "markdown", "id": "2b68f4c0", "metadata": {}, "source": ["We are classifying the tumor in two categories:\n* t = True, there is a tumor\n* f = False, there is no tumor"]}, {"cell_type": "code", "execution_count": 1, "id": "008442cd", "metadata": {}, "outputs": [], "source": ["codes = ['t','f']"]}, {"cell_type": "markdown", "id": "6c2e1120", "metadata": {}, "source": ["The following is a used to make a customized SegmentationItemList that normalizes the pixel values from 0-255 to 0 or 1, the operation performed by div=True when opening the image."]}, {"cell_type": "code", "execution_count": 1, "id": "c5ed4ac8", "metadata": {}, "outputs": [], "source": ["# This comes from ilikemath's \"Ultrasound Nerve Segmentation with fastai\" notebook cited below.\nclass SegmentationLabelListWithDiv(SegmentationLabelList):\n    def open(self, fn): return open_mask(fn, div=True)\nclass SegmentationItemListWithDiv(SegmentationItemList):\n    _label_cls = SegmentationLabelListWithDiv"]}, {"cell_type": "markdown", "id": "5dbf4201", "metadata": {}, "source": ["* The following is from fastai's data block API to handle the data.\n* The features will be the raw images and the labels will be the masks. Thus, we are fitting a model to convert raw images into segmented, masked images, where each pixel in the masked image is either 't' or 'f' (designated by codes) to indicate whether the pixel is displaying a tumor."]}, {"cell_type": "code", "execution_count": 1, "id": "53b9bdc4", "metadata": {}, "outputs": [], "source": ["src = (SegmentationItemListWithDiv\n       # get each the data from the folders in this path\n       .from_folder(path)\n        # the inputs of the data are all files whose name does not end with '_mask.tif' (we are extracting all the raw images)\n       .filter_by_func(lambda x: not x.name.endswith('_mask.tif'))\n       # designate the validation set as the folders with names listed in validation_folders\n       .split_by_valid_func(lambda x: x.parts[-2] in validataion_folders)\n       # designate the labels of each raw image as their corresponding masks\n       .label_from_func(get_mask_fn, classes = codes))"]}, {"cell_type": "code", "execution_count": 1, "id": "16179e7e", "metadata": {}, "outputs": [], "source": ["# We are working with MRI images which are usually similar in orientation, so the only transform I am applying is a flip\n# horizontally across the y-axis with a probability of .5 since I figure the brain is pretty symmetrical.\ntfms = get_transforms(max_rotate = 0, p_affine = 0, p_lighting = 0)\n\ndata = (src.transform(get_transforms(), size = image_size)\n# the batch size is generally dependent of GPU capacity, but 16 is usually a decent number for full-sized images on Kaggle's GPU\n        .databunch(bs=16)\n        .normalize(imagenet_stats))"]}, {"cell_type": "markdown", "id": "72006741", "metadata": {}, "source": ["A suggestion for an alternative process in training the model is to use progressive resizing:\n* integer divide the image_size variable in data above by 2\n* fit on the frozen and unfrozen models with half-sized images\n* re-instantiate the data variable with a size of image_size\n* fit the frozen and unfrozen models with full-sized images\n\nProgressive resizing is a form of transfer learning that assumes that the images with dimensions of half the size are practically an entirely new dataset as the full-sized images. This gives suprisingly good results, and if you want to see the implementation using progressive resizing, I suggest to look at Peter Butterfield's notebook (cited at the end of this notebook)."]}, {"cell_type": "code", "execution_count": 1, "id": "f46a07c7", "metadata": {}, "outputs": [], "source": ["data.show_batch(figsize = (7,8))"]}, {"cell_type": "markdown", "id": "21b171c7", "metadata": {}, "source": ["## Model"]}, {"cell_type": "markdown", "id": "f90e878e", "metadata": {}, "source": ["We now instantiate the model. We are using a unet architecture, which is state-of-the-art for segmentation problems."]}, {"cell_type": "code", "execution_count": 1, "id": "c6de82aa", "metadata": {}, "outputs": [], "source": ["# wd stands for weight decay- a measure to reduce overfitting\nwd = 1e-2"]}, {"cell_type": "markdown", "id": "2c26ae91", "metadata": {}, "source": ["* The dice similarity coefficient metric is a measure of how similar two sample are, in this case, the similarity between two\nsegmented images. The a high dice value means our prediction is similar to the label (an image manually segmented by a medical school graduate and reviewed by an expert radiologist), so a higher dice value is better."]}, {"cell_type": "code", "execution_count": 1, "id": "c7fbc995", "metadata": {}, "outputs": [], "source": ["learn = unet_learner(data, models.resnet34, metrics = dice, wd = wd, path = output_path)"]}, {"cell_type": "code", "execution_count": 1, "id": "bbdc392e", "metadata": {}, "outputs": [], "source": ["learn.lr_find()"]}, {"cell_type": "code", "execution_count": 1, "id": "7c3c013d", "metadata": {}, "outputs": [], "source": ["learn.recorder.plot()"]}, {"cell_type": "code", "execution_count": 1, "id": "f7fff937", "metadata": {}, "outputs": [], "source": ["learn.save('pre-trained')"]}, {"cell_type": "markdown", "id": "0bda7e27", "metadata": {}, "source": ["The best learning rate can be guessed by the graph as an order of magnitude less than the minimum, but is usually determined experimentally."]}, {"cell_type": "code", "execution_count": 1, "id": "f4825a28", "metadata": {}, "outputs": [], "source": ["lr = 1e-3\nlearn.fit_one_cycle(10, lr)"]}, {"cell_type": "code", "execution_count": 1, "id": "0c8c61a0", "metadata": {}, "outputs": [], "source": ["learn.save('stage-1')"]}, {"cell_type": "markdown", "id": "3e4776a1", "metadata": {}, "source": ["## Second Round"]}, {"cell_type": "markdown", "id": "62cae7b1", "metadata": {}, "source": ["We are doing the same process on the entire, unfrozen, model."]}, {"cell_type": "code", "execution_count": 1, "id": "9f18762d", "metadata": {}, "outputs": [], "source": ["learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot()"]}, {"cell_type": "code", "execution_count": 1, "id": "37c73cd1", "metadata": {}, "outputs": [], "source": ["learn.load('stage-1')"]}, {"cell_type": "code", "execution_count": 1, "id": "ccef8df0", "metadata": {}, "outputs": [], "source": ["# pct_start means we are applying the cyclic learning rate on only 80% of the epochs. In other words, the feature of \n# fit_one_cycle to increase the learning rate then decrease the learning rate is applied on the epoch with a probability\n# of .8.\nlearn.fit_one_cycle(7, slice(lr/800, lr/8), pct_start = .8)"]}, {"cell_type": "code", "execution_count": 1, "id": "0a4b4bc4", "metadata": {}, "outputs": [], "source": ["learn.save('stage-2')"]}, {"cell_type": "code", "execution_count": 1, "id": "ed8fb471", "metadata": {}, "outputs": [], "source": ["# let's inspect the results now\nlearn.show_results(rows = 3, figsize = (8,8))"]}, {"cell_type": "markdown", "id": "28a94229", "metadata": {}, "source": ["# Discussion and Conclusion: why did we do this?\n According to **[1]** (paper cited below), the mean dice value between segmentations of two expert radiologists is 82%."]}, {"cell_type": "markdown", "id": "c6ea70a6", "metadata": {}, "source": ["> Thank you to these two notebooks for more or less serving as references for my code: (give these guys an upvote)\n*     https://www.kaggle.com/tanlikesmath/ultrasound-nerve-segmentation-with-fastai\n*     https://www.kaggle.com/peter88b/brain-segmentation-fastai\n\n> And thank you to the following papers for providing information about the segmentation process:\n* Mateusz Buda, AshirbaniSaha, Maciej A. Mazurowski \"Association of genomic subtypes of lower-grade gliomas with shape features automatically extracted by a deep learning algorithm.\" Computers in Biology and Medicine, 2019. **[1]**"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}