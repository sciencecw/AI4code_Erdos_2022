{"cells": [{"cell_type": "markdown", "id": "b80aa999", "metadata": {}, "source": ["# TELCO CUSTOMER CHURN PREDICTION MODEL\n\nIn this study, I worked on customer churn prediction using logistic regression.\n\nChurn data is important for the companies. The decision makers could take precautions and maybe offer deals to the customers who would likely to end relationship with the company in the future by understanding the past customers' churn data.\n\nDesigning a good classifier model is important to predict the future customers who would end the relationship with the company. While we wouldn't like to miss the customers who would churn, also we wouldn't like to spend exstra resources (contacting the customers, offering discounts or deals) for the customers who wouldn't churn. Deciding the right evaluation metric (accuracy, precision, recall, f1 score) becomes important for these kind of business concerns.  \n\nThe important parts of the study are;\n\na. The target variable labels (churn or not churn) are unbalanced,\n\nb. The difference between the model evaluation metrics (accuracy, precision, recall, f1 score),\n\nc. Deciding which metric to use for this specific application and setting a new threshold for the logistic regression prediction."]}, {"cell_type": "code", "execution_count": 1, "id": "336f6107", "metadata": {}, "outputs": [], "source": ["import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.set_option(\"display.max_columns\", 50)"]}, {"cell_type": "code", "execution_count": 1, "id": "58f897df", "metadata": {}, "outputs": [], "source": ["dataset = pd.read_csv('../input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndf = dataset.copy()"]}, {"cell_type": "code", "execution_count": 1, "id": "33e278a0", "metadata": {}, "outputs": [], "source": ["df.head()"]}, {"cell_type": "markdown", "id": "a9423854", "metadata": {}, "source": ["### EDA:\n\n**We should understand the dataset before implement a prediction model.**\n\n**Analyzing the customers' basic demography and churn distribution.**"]}, {"cell_type": "code", "execution_count": 1, "id": "398fa85f", "metadata": {}, "outputs": [], "source": ["gender_label = df.gender.value_counts().index\ngender_size = df.gender.value_counts().values\n\npartner_label = df['Partner'].value_counts().index\npartner_size = df['Partner'].value_counts().values\n\nplt.figure(figsize=(10,10))\nplt.subplot(1,2,1)\nplt.pie(gender_size, labels=gender_label, colors=['cyan', 'pink'], autopct='%1.1f%%', shadow=True, startangle=45, textprops={'fontsize':25})\nplt.title('Gender Distribution of the Customers', color='navy', fontsize=15)\n\nplt.subplot(1,2,2)\nplt.pie(partner_size, labels=partner_label, colors=['cyan', 'pink'], autopct='%1.1f%%', shadow=True, startangle=45, textprops={'fontsize':25})\nplt.title('Maritial Status of the Customers', color='navy', fontsize=15);"]}, {"cell_type": "code", "execution_count": 1, "id": "a66d53a8", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(8,5))\nsns.countplot(df['Partner'], hue=df['Dependents'], palette='hls')\nplt.title('Maritial and Dependent Status of the Customers', color='navy', fontsize=15)\nplt.xlabel('Having Partner')\nplt.ylabel('No of Customers with Dependents');"]}, {"cell_type": "code", "execution_count": 1, "id": "2089d2f8", "metadata": {}, "outputs": [], "source": ["churn_label = df['Churn'].value_counts().index\nchurn_color = ['cyan', 'red']\nchurn_size = df['Churn'].value_counts().values\n\nplt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nplt.pie(churn_size, labels=churn_label, colors=churn_color, autopct='%1.1f%%', shadow=True, startangle=90, textprops={'fontsize':25})\nplt.title('Percentage of Churn', color='navy', fontsize=15)\n\nplt.subplot(1,2,2)\nsns.countplot(df['Churn'], palette={'No':'cyan', 'Yes':'red'})\nplt.title('Counts of Churn and No Churn', color='navy', fontsize=15)\nplt.ylabel('Counts of Churn');"]}, {"cell_type": "markdown", "id": "46ce7315", "metadata": {}, "source": ["**We can see that gender and marital status of the customers are balanced.**\n\n**But dependents information and churn distributions are inbalanced in our dataset.**\n\n**Now let's see the monthly and total charges distributions and the customers' loyalty to the company (Tenure: Length of time as months the customers have been a member of the company)**"]}, {"cell_type": "code", "execution_count": 1, "id": "1a6e6c10", "metadata": {}, "outputs": [], "source": ["#'Total Charges' feature should be float but it has an object type. So I will convert it into float type:\ndf['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n\ndf[['MonthlyCharges', 'TotalCharges', 'tenure']].describe()"]}, {"cell_type": "code", "execution_count": 1, "id": "7fed08e3", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(7,17))\nplt.subplot(3,1,1)\nsns.distplot(df['MonthlyCharges'])\nplt.title('Distribution of Monthly Charges', color='navy', fontsize=15)\n\nplt.subplot(3,1,2)\nsns.distplot(df['TotalCharges'])\nplt.title('Distribution of Total Charges', color='navy', fontsize=15)\n\nplt.subplot(3,1,3)\nsns.distplot(df['tenure'])\nplt.title('Tenure (Months) of the Customers', color='navy', fontsize=15);"]}, {"cell_type": "markdown", "id": "166afe89", "metadata": {}, "source": ["**Now we can analyze the contract and internet usage information.**"]}, {"cell_type": "code", "execution_count": 1, "id": "35f91e05", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(15,15))\nplt.subplot(3,2,1)\nsns.countplot(df['Contract'], hue=df['Churn'], palette=['cyan', 'red'])\n\nplt.subplot(3,2,2)\nsns.countplot(df['PhoneService'], hue=df['Churn'], palette=['cyan', 'red'])\n\nplt.subplot(3,2,3)\nsns.countplot(df['InternetService'], hue=df['Churn'], palette=['cyan', 'red'])\n\nplt.subplot(3,2,4)\nsns.countplot(df['StreamingTV'], hue=df['Churn'], palette=['cyan', 'red'])\n\nplt.subplot(3,2,5)\nsns.countplot(df['StreamingMovies'], hue=df['Churn'], palette=['cyan', 'red'])\n\nplt.subplot(3,2,6)\nsns.countplot(df['PaymentMethod'], hue=df['Churn'], palette=['cyan', 'red'])\nplt.xticks(rotation=45);"]}, {"cell_type": "markdown", "id": "000fba29", "metadata": {}, "source": ["## Data Preparation and Feature Engineering:\n \n**For the binary features I will use Label Encoding and for the others I will use One Hot Encoding method to prepare my features. I could have only use one hot encoding instead, but I wanted to demonstrate how to use label encoder method as well.**\n\n### Label Encoding:"]}, {"cell_type": "code", "execution_count": 1, "id": "14966e1d", "metadata": {}, "outputs": [], "source": ["# Label encoding for 'Yes' and 'No' features:\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\n\nbinary_cols = ['gender', 'Partner', 'Dependents', 'PhoneService', 'PaperlessBilling', 'Churn']\ndf[binary_cols] = df[binary_cols].astype('category')\n\nfor each in binary_cols:\n    df[each] = encoder.fit_transform(df[each])"]}, {"cell_type": "markdown", "id": "e4889410", "metadata": {}, "source": ["### One Hot Encoding:"]}, {"cell_type": "code", "execution_count": 1, "id": "15147f2d", "metadata": {}, "outputs": [], "source": ["# Features with multiple values:\nprint('MultipleLines Values   : ', df['MultipleLines'].unique())\nprint('Contract Values        : ', df['Contract'].unique())\nprint('InternetService Values : ', df['InternetService'].unique())\nprint('PaymentMethod Values   : ', df['PaymentMethod'].unique())\nprint('OnlineSecurity Values  : ', df['OnlineSecurity'].unique())\nprint('OnlineBackup Values    : ', df['OnlineBackup'].unique())\nprint('DeviceProtection Values: ', df['DeviceProtection'].unique())\nprint('TechSupport Values     : ', df['TechSupport'].unique())\nprint('StreamingTV Values     : ', df['TechSupport'].unique())\nprint('StreamingMovies Values : ', df['TechSupport'].unique())"]}, {"cell_type": "code", "execution_count": 1, "id": "0e9dc226", "metadata": {}, "outputs": [], "source": ["cols_for_dummies = ['MultipleLines', 'Contract', 'InternetService', \n                    'PaymentMethod', 'OnlineSecurity', 'OnlineBackup', \n                    'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']\ndf_ = df.drop(cols_for_dummies, axis=1)\ndms = pd.get_dummies(df[cols_for_dummies])\ndf = pd.concat([df_, dms], axis=1)"]}, {"cell_type": "markdown", "id": "3a8bb771", "metadata": {}, "source": ["**Converting all the column names into lowercase.**"]}, {"cell_type": "code", "execution_count": 1, "id": "bc527390", "metadata": {}, "outputs": [], "source": ["df.columns = df.columns.str.lower()"]}, {"cell_type": "markdown", "id": "e12452c3", "metadata": {}, "source": ["**Are there any null values?**"]}, {"cell_type": "code", "execution_count": 1, "id": "fadafaf1", "metadata": {}, "outputs": [], "source": ["df.isnull().sum()"]}, {"cell_type": "markdown", "id": "4d2d2e9c", "metadata": {}, "source": ["**There aren't a lot of null values. So I decided to delete 11 customer datapoints with the null values. And I want to delete Customer IDs as well because it is not related with my predictions.**"]}, {"cell_type": "code", "execution_count": 1, "id": "4d273e2f", "metadata": {}, "outputs": [], "source": ["df.dropna(inplace=True)\ndf = df.drop('customerid', axis=1)"]}, {"cell_type": "markdown", "id": "89f7b256", "metadata": {}, "source": ["**Now it is time for scaling the 'tenure', 'monthly charges', and 'total charges' features.**"]}, {"cell_type": "code", "execution_count": 1, "id": "88463cd0", "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\ndf[['tenure', 'monthlycharges', 'totalcharges']] = scaler.fit_transform(df[['tenure', 'monthlycharges', 'totalcharges']])"]}, {"cell_type": "markdown", "id": "b4e8e3f9", "metadata": {}, "source": ["**The final dataset:**"]}, {"cell_type": "code", "execution_count": 1, "id": "2565e24e", "metadata": {}, "outputs": [], "source": ["df.head(10)"]}, {"cell_type": "markdown", "id": "3d8dbe0d", "metadata": {}, "source": ["**Let's see our features' correlation with the churn data. But I want to see only the features with 0.20 (and -0.20) or higher correlation because otherwise the graph becomes really crowded.**"]}, {"cell_type": "code", "execution_count": 1, "id": "63644349", "metadata": {}, "outputs": [], "source": ["corrmatrix = pd.DataFrame(df.corr()['churn'].sort_values(ascending=True)).rename(columns={'churn':'correlation'})\n\nstronger_corr = corrmatrix[(corrmatrix.correlation > 0.20) | (corrmatrix.correlation < -0.20)].drop('churn')\n\nplt.figure(figsize=(10,5))\nsns.barplot(x=stronger_corr.correlation, y=stronger_corr.index)\nplt.title('Features and Churn Correlation', color='navy', fontsize=15);"]}, {"cell_type": "markdown", "id": "4f0cf8ee", "metadata": {}, "source": ["**From the correlation matrix according to the churn data, we can have some insights for the business actions.**\n\n**We can see from above,** \n\n* The customers who have a longer membership (tenure and two year contract) and who doesn't use internet services are less likely to churn.\n\n* The customers who have month to month contracts and who don't have online security, tech support, online backup services are more likely to churn.\n\n* So the client should convince the customers to increase their contract time and support customers to purchase technical support and backup services."]}, {"cell_type": "markdown", "id": "a28ba3d2", "metadata": {}, "source": ["## Train and Test Split\n\n**Before separeting the dataset into train and test, I need to separete target column from the other features.**"]}, {"cell_type": "code", "execution_count": 1, "id": "e0adfcde", "metadata": {}, "outputs": [], "source": ["y = df['churn']\nX = df.drop('churn', axis=1)"]}, {"cell_type": "markdown", "id": "dc6de0a2", "metadata": {}, "source": ["**Splitting the dataset into train and test.**"]}, {"cell_type": "code", "execution_count": 1, "id": "5066a58d", "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n\nprint('X_train shape : ', X_train.shape)\nprint('y_train shape : ', y_train.shape)\nprint('X_test shape  : ', X_test.shape)\nprint('y_test shape  : ', y_test.shape)"]}, {"cell_type": "markdown", "id": "10c57495", "metadata": {}, "source": ["## Logistic Regression Model"]}, {"cell_type": "code", "execution_count": 1, "id": "21f7d361", "metadata": {}, "outputs": [], "source": ["from sklearn.linear_model import LogisticRegression\n\nloj = LogisticRegression(solver='liblinear')\n\nloj_model = loj.fit(X_train, y_train)\n\ny_pred = loj_model.predict(X_test)\ny_pred_prob = loj_model.predict_proba(X_test)"]}, {"cell_type": "markdown", "id": "345ead64", "metadata": {}, "source": ["**Prediction of our default model:**"]}, {"cell_type": "code", "execution_count": 1, "id": "6aebf03d", "metadata": {}, "outputs": [], "source": ["y_pred"]}, {"cell_type": "markdown", "id": "849bd49d", "metadata": {}, "source": ["**Predictions with the probabilities:**\n\n**The first column is the probability of predicting 0 (no churn) and the second column is the probability of predicting 1 (churn).**"]}, {"cell_type": "code", "execution_count": 1, "id": "293d95fd", "metadata": {}, "outputs": [], "source": ["y_pred_prob"]}, {"cell_type": "markdown", "id": "caed569e", "metadata": {}, "source": ["**Let's evaluate our default logistic regression model:**\n\n**Accuracy is the percent of predictions that are correct. Accuracy is not a good choice for unbalanced datasets like this one.**\n\n**Precision = TP / TP + FP**\n\n**Recall = TP / TP + FN**\n\n**Precision is interested in the predicted positives.**\n\n**Recall is interested in the actual positives.**\n\n**Precision is sureness while recall is sensitivity.**\n\n**With a high value for a precision treshold, the model will need higher proof to accept for a prediction. So there would be some false negatives (Predicted as no churn, but actually churn).**\n\n**With a high value for a recall the model will be more willing to accept a prediction with less proof. So there would be some false positives (Predicted as churn, but actually no churn).**\n\n**F1 score = 2 * (precision * recall) / (precision + recall)**\n\n**F1 score is the harmonic mean of the precision and the recall. It is a balanced value between precision and recall.**\n\n**Let's see our default model's performance which has 0.5 as the treshold.**"]}, {"cell_type": "code", "execution_count": 1, "id": "c4d1171b", "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n\nprint('Accuracy of the default model  : %.2f' % accuracy_score(y_test, y_pred))\nprint('Precision of the default model : %.2f' % precision_score(y_test, y_pred))\nprint('Recall of the default model    : %.2f' % recall_score(y_test, y_pred))\nprint('F1 Score of the default model  : %.2f' % f1_score(y_test, y_pred))\n\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, annot_kws={\"fontsize\":20}, fmt='d', cbar=False, cmap='PuBu')\nplt.title('Confusion Matrix of the Default Model', color='navy', fontsize=15)\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values');"]}, {"cell_type": "code", "execution_count": 1, "id": "f5423fd6", "metadata": {}, "outputs": [], "source": ["logit_roc_auc = roc_auc_score(y_test, y_pred)\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob[:,1])\n\nplt.figure(figsize=(7,7))\nplt.plot(fpr, tpr, label='AUC (Area = %0.2f)' %logit_roc_auc)\nplt.plot([0,1], [0,1], 'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('Sensitivity : False Positive Ratio')\nplt.ylabel('1 - Specifity : True Positive Ratio')\nplt.title('ROC and AUC of the Default Model')\nplt.legend()\nplt.show()"]}, {"cell_type": "markdown", "id": "4555c24d", "metadata": {}, "source": ["**I am not so happy with my default model. Because 169 customers have churned, but we didn't predict that they are going to churn. We would loose those 169 customers if our company don't act because of following our model's prediction.**\n\n**And there are 116 customers who wouldn't churn, but the company might have acted and spent it's resources.**\n\n**For this study, I would act to decrease those 169 customers. That means I want my model to be more sensitive for predictions. So I am going to decrease my threshold from it's default number 0.5 to something like 0.4 or 0.3.**"]}, {"cell_type": "code", "execution_count": 1, "id": "9ac33200", "metadata": {}, "outputs": [], "source": ["# New predictions with 0.4 threshold:\ny_pred_4 = loj_model.predict_proba(X_test)[:,1] >= 0.4\n\n# New predictions with 0.3 threshold:\ny_pred_3 = loj_model.predict_proba(X_test)[:,1] >= 0.3"]}, {"cell_type": "code", "execution_count": 1, "id": "daae96d0", "metadata": {}, "outputs": [], "source": ["print('Accuracy of the default model             : %.2f' % accuracy_score(y_test, y_pred))\nprint('Precision of the default model            : %.2f' % precision_score(y_test, y_pred))\nprint('Recall of the default model               : %.2f' % recall_score(y_test, y_pred))\nprint('F1 Score of the default model             : %.2f' % f1_score(y_test, y_pred))\nprint('--'*24)\nprint('Accuracy of the model with 0.4 threshold  : %.2f' % accuracy_score(y_test, y_pred_4))\nprint('Precision of the model with 0.4 threshold : %.2f' % precision_score(y_test, y_pred_4))\nprint('Recall of the model with 0.4 threshold    : %.2f' % recall_score(y_test, y_pred_4))\nprint('F1 Score of the model with 0.4 threshold  : %.2f' % f1_score(y_test, y_pred_4))\nprint('--'*24)\nprint('Accuracy of the model with 0.3 threshold  : %.2f' % accuracy_score(y_test, y_pred_3))\nprint('Precision of the model with 0.3 threshold : %.2f' % precision_score(y_test, y_pred_3))\nprint('Recall of the model with 0.3 threshold    : %.2f' % recall_score(y_test, y_pred_3))\nprint('F1 Score of the model with 0.3 threshold  : %.2f' % f1_score(y_test, y_pred_3))"]}, {"cell_type": "code", "execution_count": 1, "id": "b0463cbc", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(15,4))\nplt.subplot(1,3,1)\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, annot_kws={\"fontsize\":20}, fmt='d', cbar=False, cmap='PuBu')\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Default Model', color='navy', fontsize=15)\n\nplt.subplot(1,3,2)\nsns.heatmap(confusion_matrix(y_test, y_pred_4), annot=True, annot_kws={\"fontsize\":20}, fmt='d', cbar=False, cmap='PuBu')\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Model with 0.4 Threshold', color='navy', fontsize=15)\n\nplt.subplot(1,3,3)\nsns.heatmap(confusion_matrix(y_test, y_pred_3), annot=True, annot_kws={\"fontsize\":20}, fmt='d', cbar=False, cmap='PuBu')\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Model with 0.3 Threshold', color='navy', fontsize=15);"]}, {"cell_type": "markdown", "id": "cc980183", "metadata": {}, "source": ["**If we decide to choose 0.3 as the model's threshold, the company would only loose 79 customers instead of 169, and it would offer 259 customers who wouldn't choose to churn at all.**\n\n**For the scenario of this study, I am going to choose the 0.3 threshold. Because my imaginary client thinks the company would earn more profit by decreasing the number of the customers who are going to churn than offering discounts or offers to more customers.**"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}