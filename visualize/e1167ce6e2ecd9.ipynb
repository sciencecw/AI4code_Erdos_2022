{"cells": [{"cell_type": "code", "execution_count": 1, "id": "be8009d4", "metadata": {}, "outputs": [], "source": ["!pip install flair\n!pip install transformers\n!pip install node2vec\n!pip install wordcloud #https://github.com/amueller/word_cloud"]}, {"cell_type": "code", "execution_count": 1, "id": "fae015dd", "metadata": {}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom flair.models import SequenceTagger\nfrom flair.data import Sentence\nfrom transformers import pipeline\npd.set_option('display.max_columns', 30)\nfrom collections import Counter \nimport nltk\nimport matplotlib.pyplot as plt\nimport re\nfrom tqdm.autonotebook import tqdm\nimport networkx as nx\nfrom wordcloud import WordCloud\nfrom sklearn.cluster import DBSCAN\n\nimport plotly.graph_objects as go\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output."]}, {"cell_type": "code", "execution_count": 1, "id": "f834a276", "metadata": {}, "outputs": [], "source": ["def tokenizer(text):\n    text = text.split(',')\n    res = []\n    for t in text:\n        res.extend(nltk.word_tokenize(t))\n        \n    return res"]}, {"cell_type": "code", "execution_count": 1, "id": "bd5245f4", "metadata": {}, "outputs": [], "source": ["data = pd.read_csv(\"/kaggle/input/novel-corona-virus-2019-dataset/COVID19_line_list_data.csv\")\ndata.head()"]}, {"cell_type": "code", "execution_count": 1, "id": "559058aa", "metadata": {}, "outputs": [], "source": ["text = data.summary.dropna().values\nprint(f\"Text lenght: {len(text)}\")"]}, {"cell_type": "code", "execution_count": 1, "id": "744151dc", "metadata": {}, "outputs": [], "source": ["symptoms = data.symptom.dropna().tolist()\nprint(len(symptoms))"]}, {"cell_type": "code", "execution_count": 1, "id": "dc4914f8", "metadata": {}, "outputs": [], "source": ["print(f\"Total dataset len: {len(data)}\")\nprint(f\"Death: {len(data.loc[data.death == '1'])}\")\nprint(f\"Recovered: {len(data.loc[data.recovered == '1'])}\")"]}, {"cell_type": "markdown", "id": "4bb85fa8", "metadata": {}, "source": ["# NLP"]}, {"cell_type": "markdown", "id": "e7f9c8eb", "metadata": {}, "source": ["## Question answering"]}, {"cell_type": "markdown", "id": "a87e32f5", "metadata": {}, "source": ["We can collect some data using NLP question answering. Adding more questions, and filtering by confidesce score we can fill some missing walues"]}, {"cell_type": "code", "execution_count": 1, "id": "fab80767", "metadata": {}, "outputs": [], "source": ["ner_tagger = SequenceTagger.load('ner')\n\nnlp_qa = pipeline('question-answering')"]}, {"cell_type": "markdown", "id": "af901940", "metadata": {}, "source": ["Look like NER is not useful for us, if we want parse dates its better to use regular expressions."]}, {"cell_type": "code", "execution_count": 1, "id": "aaaf012e", "metadata": {}, "outputs": [], "source": ["match = re.findall(r'(\\d+/\\d+/\\d+)',text[1])\nprint(match)"]}, {"cell_type": "code", "execution_count": 1, "id": "75dbfcdf", "metadata": {}, "outputs": [], "source": ["sentence = Sentence(text[1])\nner_tagger.predict(sentence)\nprint(sentence.to_tagged_string())"]}, {"cell_type": "markdown", "id": "1c5357a9", "metadata": {}, "source": ["Try to use question answering. \nSome sample questions"]}, {"cell_type": "code", "execution_count": 1, "id": "fac80047", "metadata": {}, "outputs": [], "source": ["questions = [\n    'How old?',\n    'Gender?',\n    'Where from?',\n    'When symptoms onset?',\n    'When Hospitalized?',\n    'When quarantined?',\n]"]}, {"cell_type": "code", "execution_count": 1, "id": "654239bb", "metadata": {}, "outputs": [], "source": ["print(text[0])\nfor q in questions:\n    print(f\"Question: {q}, answer: {nlp_qa(context = text[0], question=q)}\")"]}, {"cell_type": "markdown", "id": "15f60014", "metadata": {}, "source": ["Wordcloud basic"]}, {"cell_type": "code", "execution_count": 1, "id": "6aef77c1", "metadata": {}, "outputs": [], "source": ["wordcloud = WordCloud().generate(' '.join(symptoms))\n\nplt.figure(figsize=[10, 6])\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")"]}, {"cell_type": "code", "execution_count": 1, "id": "469997fe", "metadata": {}, "outputs": [], "source": ["from flair.embeddings import WordEmbeddings, DocumentRNNEmbeddings\n\nglove_embedding = WordEmbeddings('glove')\n\ndocument_embeddings = DocumentRNNEmbeddings([glove_embedding])"]}, {"cell_type": "code", "execution_count": 1, "id": "96e698b3", "metadata": {}, "outputs": [], "source": ["embeddings = np.empty((len(text), 128))\nfor idx, t in tqdm(enumerate(text)):\n    sentence = Sentence(t)\n    document_embeddings.embed(sentence)\n    embeddings[idx, :] = (sentence.get_embedding().detach().numpy())"]}, {"cell_type": "markdown", "id": "c2bd8bee", "metadata": {}, "source": ["## Clustering"]}, {"cell_type": "code", "execution_count": 1, "id": "b79ec7ab", "metadata": {}, "outputs": [], "source": ["dbscan=DBSCAN(eps=0.08, min_samples=2,metric='cosine' ).fit(embeddings)\n\ndf_cluster = pd.DataFrame({\"text\":text, \"cluster\":dbscan.labels_})"]}, {"cell_type": "code", "execution_count": 1, "id": "7d0d13f9", "metadata": {}, "outputs": [], "source": ["df_cluster.cluster.value_counts()"]}, {"cell_type": "code", "execution_count": 1, "id": "a702b644", "metadata": {}, "outputs": [], "source": ["df_cluster.loc[df_cluster['cluster'] == 31].head()"]}, {"cell_type": "code", "execution_count": 1, "id": "bfc17c31", "metadata": {}, "outputs": [], "source": ["from sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\ndef plot_pca(train,y, text=\"Plot\", algo = 'TSNE', size = 2):\n    \"\"\"Function visualizating PCA/TSNE\"\"\"\n\n    plt.figure(figsize=(20,8))\n    if algo == 'PCA':\n        pca = PCA(n_components = 2,copy=False)\n    elif algo == 'TSNE':\n        pca = TSNE(n_components = 2)\n    else:\n        print('Unknown algo, using PCA...')\n        pca = PCA(n_components = 2, copy=False)\n        \n    train_pca = pca.fit_transform(train)\n\n    plt.scatter(train_pca[:,0], train_pca[:,1],c=y, edgecolor='none', alpha=0.9,\n            cmap=plt.cm.get_cmap('seismic', size))\n    plt.title(text)\n    plt.xlabel('component 1')\n    plt.ylabel('component 2')\n    plt.colorbar()"]}, {"cell_type": "code", "execution_count": 1, "id": "f5eac344", "metadata": {}, "outputs": [], "source": ["np.unique(dbscan.labels_)"]}, {"cell_type": "code", "execution_count": 1, "id": "2fb44873", "metadata": {}, "outputs": [], "source": ["plot_pca(embeddings, dbscan.labels_, size = len(np.unique(dbscan.labels_)))"]}, {"cell_type": "code", "execution_count": 1, "id": "744cea15", "metadata": {}, "outputs": [], "source": ["'''plt.figure(figsize=[10, 8])\nplt.title(\"Text embedding\")\nplt.scatter(transformed[:,0], transformed[:,1], edgecolor='none', alpha=0.9,)\nplt.xlabel('X-comp')\nplt.ylabel('Y-comp')\nplt.show()'''"]}, {"cell_type": "markdown", "id": "4af45e06", "metadata": {}, "source": ["Look like we have some clusters at document embedding space"]}, {"cell_type": "markdown", "id": "83557db5", "metadata": {}, "source": ["# Symptoms Node2Vec"]}, {"cell_type": "markdown", "id": "16cb24cc", "metadata": {}, "source": ["Create symptoms graph and Node2Vec model for finding similar symptom and get a probability of next potential symptoms"]}, {"cell_type": "code", "execution_count": 1, "id": "1aad21d0", "metadata": {}, "outputs": [], "source": ["symptoms[:10]"]}, {"cell_type": "code", "execution_count": 1, "id": "69f6ccdf", "metadata": {}, "outputs": [], "source": ["counter = Counter()\nlinks = dict(dict())\n\nfor row in symptoms:\n    \n    row = tokenizer(row)    \n    \n    for symptome in row:\n        counter[symptome] += 1\n        \n    for subsymptome in row:\n        if not(symptome in links.keys()):\n            links[symptome] = dict()\n            \n        if not(subsymptome in links[symptome].keys()):\n            links[symptome][subsymptome] = 0\n            \n        if symptome != subsymptome:    \n            links[symptome][subsymptome] += 1\n            \nfor key1 in links.keys():\n    for key2 in links[key1].keys():\n        links[key1][key2] /= counter[key1]\n        \nlinks_dict = dict()\nfor key1, val1 in links.items():\n    for key2, val2 in links[key1].items():\n        #if val2 >= 0.5:\n        if key1 != key2:\n            links_dict[(key1, key2)] = val2\n        "]}, {"cell_type": "code", "execution_count": 1, "id": "61c2cd86", "metadata": {}, "outputs": [], "source": ["counter.most_common(10)"]}, {"cell_type": "code", "execution_count": 1, "id": "9f0e3df1", "metadata": {}, "outputs": [], "source": ["size_df = pd.DataFrame.from_dict(counter, orient='index').reset_index()\nsize_df.columns = ['label', 'size']\n\nlinks_df = pd.DataFrame([[list(key)[0], val, list(key)[1]] for key, val in links_dict.items() if list(key)[0] != list(key)[1]], columns = ['source', 'weight', 'target'])"]}, {"cell_type": "code", "execution_count": 1, "id": "0f80417b", "metadata": {}, "outputs": [], "source": ["G = nx.Graph()\nfor idx, row in size_df.iterrows():\n    G.add_node(row['label'], size = float(row['size']))\n\nfor idx, row in links_df.iterrows():\n    G.add_edge(row['source'], row['target'], weight=float(row['weight']))"]}, {"cell_type": "code", "execution_count": 1, "id": "efdcfe62", "metadata": {}, "outputs": [], "source": ["print(f\"Nodes len: {len(list(G.nodes()))}\")\nprint(f\"Edges len: {len(list(G.edges()))}\")\n# %%\nplt.figure(figsize=[20, 8])\n\nparams = {\n    'edge_color'    : '#FFDEA2',\n    'width'         : 1,\n    'with_label'    : True,\n    'font_weight'   : 'regular'\n}\n\nnode_list = [i*10 for i in size_df['label'].values]\nnode_size = [i*10 for i in size_df['size'].values]\nnx.draw_networkx(G,node_list = node_list,node_size=node_size, **params)"]}, {"cell_type": "markdown", "id": "cdf0f69f", "metadata": {}, "source": ["Create node2vec for symptomes prediction."]}, {"cell_type": "code", "execution_count": 1, "id": "44725bd0", "metadata": {}, "outputs": [], "source": ["from node2vec import Node2Vec\n\nn2v = Node2Vec(G, dimensions=15, num_walks=100, workers=4)\nnode_model = n2v.fit(size=3, window=2, seed=42, iter=1, sg=1)"]}, {"cell_type": "code", "execution_count": 1, "id": "3297b994", "metadata": {}, "outputs": [], "source": ["node_model.most_similar(['fever'], topn=15)"]}, {"cell_type": "code", "execution_count": 1, "id": "f10772db", "metadata": {}, "outputs": [], "source": ["node_model.most_similar(['fever', 'cough'], topn=15)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}